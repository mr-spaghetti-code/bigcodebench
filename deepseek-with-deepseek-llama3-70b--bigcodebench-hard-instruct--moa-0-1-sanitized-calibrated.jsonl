{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # List all files in the directory\n    files = ftp.nlst()\n    \n    # Close the FTP connection\n    ftp.quit()\n    \n    # Use wget to download all files\n    for file in files:\n        try:\n            subprocess.run(['wget', '-r', '-nH', '--cut-dirs=1', '--user', ftp_user, '--password', ftp_password, '--no-parent', f'ftp://{ftp_server}{ftp_dir}/{file}'], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download file {file}: {str(e)}\")\n    \n    return files"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Ensure the output directory exists\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    output_files = []\n    \n    # Read commands from CSV file\n    with open(commands_file_path, mode='r') as file:\n        reader = csv.reader(file)\n        for idx, command in enumerate(reader):\n            if command:  # Check if the command is not empty\n                output_file_path = os.path.join(output_dir_path, f'command_{idx}_output.txt')\n                output_files.append(output_file_path)\n                \n                try:\n                    # Execute the command\n                    result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(result.stdout)\n                except subprocess.CalledProcessError as e:\n                    # Handle command execution failure\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(f\"Command failed with error: {e.stderr}\\nExit code: {e.returncode}\")\n    \n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, terminate it\n            proc.terminate()\n            time.sleep(1)  # wait for the process to terminate\n            # Restart the process\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    # Process is not running, start it\n    subprocess.Popen([process_name])\n    return f\"Process not found. Starting {process_name}.\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The specified directory does not exist: {directory}\")\n    \n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n    \n    # If there are no files, return None\n    if not files:\n        return None\n    \n    # Create the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n    \n    # Create the zip file and add all files to it\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text_without_urls = re.sub(r'http\\S+', '', text)\n    \n    # Split the text into words\n    words = text_without_urls.split()\n    \n    # Check if there are any words available to generate a word cloud\n    if len(words) == 0:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Separate the features and the target variable\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the Random Forest classifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Calculate feature importances\n    feature_importances = pd.DataFrame({\n        'Feature': X.columns,\n        'Importance': rf_classifier.feature_importances_\n    })\n    \n    # Sort the feature importances by descending order\n    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n    \n    # Plotting\n    ax = sns.barplot(x='Importance', y='Feature', data=feature_importances)\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    \n    # Show the plot\n    plt.show()\n    \n    return rf_classifier, ax"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    users = [User(1, 'user1', 'password1'), User(2, 'user2', 'password2')]\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users:\n            if user.id == int(user_id):\n                return user\n        return None\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.username == form.username.data and user.check_password(form.password.data):\n                    login_user(user)\n                    return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', username=current_user.username)\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Standardize the column\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = stats.zscore(standardized_data)\n    \n    # Identify and remove outliers\n    outlier_indices = np.where(np.abs(z_scores) > outlier_z_score)[0]\n    data_without_outliers = data.drop(outlier_indices)\n    \n    # Visualize the data\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], label='Data with Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_without_outliers, outlier_indices"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on a dataset and generate a scatter plot visualizing the clusters and their centroids.\n\n    Args:\n        data (pd.DataFrame): Input dataset.\n        n_clusters (int, optional): Number of clusters. Defaults to 3.\n\n    Returns:\n        tuple: A tuple containing the cluster labels and the Axes object with the scatter plot.\n\n    Raises:\n        ValueError: If 'data' is not a pd.DataFrame.\n        ValueError: If 'n_clusters' is not an integer greater than 1.\n    \"\"\"\n\n    # Check if 'data' is a pd.DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pd.DataFrame\")\n\n    # Check if 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(data)\n\n    # Generate a scatter plot visualizing the clusters and their centroids\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis')\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='*', s=200)\n\n    # Adding legend for clusters\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n\n    return cluster_labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert transformed data to DataFrame for output\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    transformed_df = pd.DataFrame(data=transformed_data, columns=columns)\n    \n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(transformed_df['PC1'], transformed_df['PC2'])\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n    else:\n        ax.plot(transformed_df.iloc[:, 0], marker='o')\n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel(f'Principal Component {1}')\n    \n    ax.set_title('PCA Transformed Data')\n    plt.show()\n    \n    return transformed_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Series(iris.target).map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    g = sns.pairplot(iris_df, hue='species', markers=[\"o\", \"s\", \"D\"])\n    g.fig.suptitle('Iris Dataset Pair Plot', y=1.02)  # Set the title\n\n    # Label the axes\n    for i, j in zip(*plt.np.triu_indices_from(g.axes, 1)):\n        g.axes[i, j].set_visible(False)\n    for i in range(len(iris.feature_names)):\n        for j in range(len(iris.feature_names)):\n            if i != j:\n                g.axes[i, j].set_xlabel(iris.feature_names[j])\n                g.axes[i, j].set_ylabel(iris.feature_names[i])\n\n    # Return the figure object\n    return g.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.today()\n        start_date = end_date - timedelta(days=30)\n        dates = pd.date_range(start=start_date, end=end_date, freq='D')\n\n        # Generate random values for each date\n        values = [random.random() for _ in range(len(dates))]\n\n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Plotting\n        plt.rcParams['font.family'] = 'Arial'\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], marker='o')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load the Boston Housing dataset\n        data = pd.read_csv(data_url, sep=\"\\s+\", header=None)\n        data.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n\n        # Calculate the correlation matrix\n        corr_matrix = data.corr()\n\n        # Generate the heatmap\n        plt.figure(figsize=(12, 10))\n        ax = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n\n        # Save the plot if a save path is provided\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    df (DataFrame): Input DataFrame with a 'value' column.\n    freq (str): Frequency string (e.g., 'D' for daily, 'M' for monthly). Defaults to 'D'.\n    decomposition_model (str): Decomposition model ('additive' or 'multiplicative'). Defaults to 'multiplicative'.\n\n    Returns:\n    tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n\n    Raises:\n    ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n    ValueError: If 'freq' is not a valid frequency string.\n    ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a DataFrame\")\n\n    # Check if df has required columns\n    required_columns = ['value']\n    if not all(col in df.columns for col in required_columns):\n        raise ValueError(\"Input 'df' must have a 'value' column\")\n\n    # Check if df contains invalid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"Input 'df' must have numeric data types in 'value' column\")\n\n    # Check if freq is a valid frequency string\n    valid_frequencies = ['D', 'M', 'Q', 'A']\n    if freq not in valid_frequencies:\n        raise ValueError(\"Invalid frequency string 'freq'. Valid options are 'D', 'M', 'Q', 'A'.\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid decomposition model 'decomposition_model'. Valid options are 'additive' or 'multiplicative'.\")\n\n    # Perform seasonal decomposition\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=int(freq[:-1]))\n\n    # Plot the decomposition result\n    fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n    decomposition_result.observed.plot(ax=ax[0], legend=False)\n    ax[0].set_ylabel('Observed')\n    decomposition_result.trend.plot(ax=ax[1], legend=False)\n    ax[1].set_ylabel('Trend')\n    decomposition_result.seasonal.plot(ax=ax[2], legend=False)\n    ax[2].set_ylabel('Seasonal')\n    decomposition_result.resid.plot(ax=ax[3], legend=False)\n    ax[3].set_ylabel('Residual')\n    plt.tight_layout()\n\n    return decomposition_result, ax"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range, including both start_date and end_date, with an optional seed for reproducibility.\n\n    Args:\n        start_date (datetime.datetime): The start date of the range (inclusive). Defaults to datetime(2020, 1, 1).\n        end_date (datetime.datetime): The end date of the range (inclusive). Defaults to datetime(2020, 12, 31).\n        seed (int): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.Series: A Series object containing random dates within the specified range, with each date being a datetime.datetime object. The series length matches the number of days in the specified range.\n\n    Raises:\n        ValueError: If 'start_date' or 'end_date' is not a datetime.datetime instance, or if 'start_date' is later than 'end_date'.\n    \"\"\"\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both start_date and end_date must be datetime.datetime instances\")\n\n    # Check if start_date is not later than end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n\n    # Set the seed for the random number generator\n    random_seed(seed)\n\n    # Calculate the number of days in the range\n    num_days = (end_date - start_date).days + 1\n\n    # Generate a list of random dates within the range\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    # Create a pandas Series from the list of random dates\n    series = pd.Series(random_dates)\n\n    return series"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The 'my_list' parameter must be a list.\")\n    \n    # Add '12' to the list\n    my_list.append(12)\n    \n    # Calculate the number of files to concatenate\n    num_files_to_concat = sum(my_list)\n    \n    # Get the list of CSV files in the directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(file_pattern)\n    \n    # Check if there are any files in the directory\n    if not files:\n        raise FileNotFoundError(f\"No files found in the directory '{file_dir}' with extension '{file_ext}'.\")\n    \n    # Ensure we do not try to concatenate more files than available\n    num_files_to_concat = min(num_files_to_concat, len(files))\n    \n    # Read and concatenate the selected CSV files into a DataFrame\n    dfs = [pd.read_csv(file) for file in files[:num_files_to_concat]]\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Check if my_list contains only numeric elements\n    if not all(isinstance(item, (int, float)) for item in my_list):\n        raise ValueError(\"my_list must contain only numeric elements (int or float)\")\n    \n    # Append 12 to my_list\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    sum_of_elements = sum(my_list)\n    \n    # Determine the size of the random numbers list\n    random_list_size = min(sum_of_elements, size)\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Generate a list of random integers\n    random_numbers = [randint(1, 100) for _ in range(random_list_size)]\n    \n    # End timing\n    end_time = time.time()\n    \n    # Calculate the time taken\n    time_taken = end_time - start_time\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Random Numbers')\n    \n    # Return the time taken and the Axes object\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Failed to connect to {url}: {e}\")\n\n    try:\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Find the first table in the parsed content\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page.\")\n        \n        # Extract headers if they exist\n        headers = []\n        header_row = table.find('tr')\n        if header_row:\n            headers = [header.text.strip() for header in header_row.find_all('th')]\n        \n        # Extract rows from the table\n        rows = []\n        for row in table.find_all('tr')[1:]:  # Skip the header row if it exists\n            cells = row.find_all('td')\n            if cells:\n                rows.append([cell.text.strip() for cell in cells])\n        \n        # Create a DataFrame\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n        \n        return df\n    except Exception as e:\n        raise ValueError(f\"Failed to parse the page content: {e}\")"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n    \n    Parameters:\n    df (pd.DataFrame): Input DataFrame.\n    \n    Returns:\n    list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \n    Raises:\n    ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    \n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n    \n    # Select numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    # Check if there are numeric columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns in the DataFrame\")\n    \n    # Create a figure with subplots for each numeric column\n    fig, axs = plt.subplots(len(numeric_cols), figsize=(8, 6*len(numeric_cols)))\n    \n    # If there's only one numeric column, axs is a single Axes object, not a list\n    if len(numeric_cols) == 1:\n        axs = [axs]\n    \n    # Create histograms for each numeric column\n    for i, col in enumerate(numeric_cols):\n        axs[i].hist(df[col], bins=10)\n        axs[i].set_title(col)\n        axs[i].set_xlabel('Value')\n        axs[i].set_ylabel('Frequency')\n    \n    # Layout so plots do not overlap\n    fig.tight_layout()\n    \n    # Return the list of Axes objects\n    return axs"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    # Dictionary to store the results\n    results = {}\n\n    # Function to check if a port is open on a single IP address\n    def check_port(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(1)\n            try:\n                sock.connect((ip, port))\n                results[ip] = True\n            except (socket.timeout, ConnectionRefusedError, socket.error):\n                results[ip] = False\n\n    # List of IP addresses to check\n    ip_addresses = [str(ip) for ip in IPv4Network(ip_range, strict=False)]\n\n    # Create and start threads for each IP address\n    threads = []\n    for ip in ip_addresses:\n        thread = Thread(target=check_port, args=(ip,))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Return the results dictionary\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information from a log file and stores the data in a CSV format.\n    \n    Args:\n        log_file (str): The path to the log file.\n    \n    Returns:\n        str: The file path to the newly created CSV file which contains the structured log data.\n    \n    Raises:\n        ValueError: If the timestamp in any log entry is invalid or if no valid log entries are found.\n    \"\"\"\n\n    # Regular expression pattern to match log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.*)'\n\n    # Initialize an empty list to store the extracted log data\n    log_data = []\n\n    # Open the log file and read it line by line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use the regular expression pattern to match the log entry\n            match = re.match(pattern, line)\n            \n            # If the log entry matches the pattern, extract the data\n            if match:\n                # Extract the message type, timestamp, and message\n                msg_type, timestamp, message = match.groups()\n                \n                # Try to parse the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    # If the timestamp is invalid, raise a ValueError\n                    raise ValueError(f\"Invalid timestamp: {timestamp}\")\n                \n                # Append the extracted data to the log_data list\n                log_data.append({\n                    'Message Type': msg_type,\n                    'Timestamp': timestamp,\n                    'Message': message.strip()\n                })\n    \n    # If no valid log entries are found, raise a ValueError\n    if not log_data:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create a Pandas DataFrame from the log_data list\n    df = pd.DataFrame(log_data)\n    \n    # Create a CSV file path by replacing the log file extension with '.csv'\n    csv_file = log_file.replace('.log', '.csv')\n    \n    # Write the DataFrame to the CSV file\n    df.to_csv(csv_file, index=False)\n    \n    # Return the CSV file path\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n\n    Parameters:\n    text (str): The input text to analyze.\n    rwidth (float, optional): The relative width of the histogram bars. Defaults to 0.8.\n\n    Returns:\n    matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \"\"\"\n    # Use regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Filter out words with length 0 (if any)\n    words = [word for word in words if len(word) > 0]\n    \n    # Calculate the lengths of all words\n    word_lengths = [len(word) for word in words]\n    \n    # If there are no words, return an empty plot\n    if not word_lengths:\n        fig, ax = plt.subplots()\n        ax.set_title('Word Length Distribution')\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        return ax\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    bins = np.arange(1, max(word_lengths) + 2) - 0.5\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, edgecolor='black')\n    \n    # Set plot labels and title\n    ax.set_title('Word Length Distribution')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    \n    # Return the Axes object containing the histogram\n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\ndef task_func(df):\n    # Check if DataFrame is empty or missing necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Filter articles with titles containing \"like\" or \"what\" (case-insensitive)\n    filtered_df = df[df['Title'].str.contains('like|what', case=False, na=False)]\n\n    # Initialize a list to store words\n    words = []\n\n    # Process each article's content\n    for content in filtered_df['Content']:\n        # Remove punctuation from the content\n        content_no_punct = re.sub('['+punctuation+']', '', content)\n        # Tokenize the content into words\n        content_words = nltk.word_tokenize(content_no_punct)\n        # Convert to lowercase and add to the list\n        words.extend([word.lower() for word in content_words])\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return dict(word_freq)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Preprocessing function\n    def preprocess_text(text):\n        # Remove numbers and punctuation\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Convert to lowercase and split into words\n        words = text.lower().split()\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n\n    # Apply preprocessing to the specified column\n    dataframe['cleaned_text'] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe['cleaned_text'])\n\n    # Convert the vectorized data to a DataFrame\n    vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return vectorized_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges.\n\n    Parameters:\n    dic (dict): A dictionary containing the ranges for longitude and latitude. Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    cities (list): A list of city names. Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n    GeoDataFrame: A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects)\n    \"\"\"\n    # Check if 'Lon' and 'Lat' keys are present in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys\")\n\n    # Check if values for 'Lon' and 'Lat' are tuples\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples\")\n\n    # Generate random coordinates for each city\n    coordinates = [Point(np.random.uniform(dic['Lon'][0], dic['Lon'][1]), np.random.uniform(dic['Lat'][0], dic['Lat'][1])) for _ in cities]\n\n    # Create a GeoPandas DataFrame\n    data = {'City': cities, 'Coordinates': coordinates}\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n\n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n\n    Args:\n        utc_datetime (datetime): The UTC datetime for which the weather report is generated.\n        cities (list, optional): A list of city names. Defaults to ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'].\n        weather_conditions (list, optional): A list of possible weather conditions. Defaults to ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'].\n        timezones (dict, optional): A dictionary mapping city names to their respective timezones. Defaults to {'New York': 'America/New_York', 'London': 'Europe/London', 'Beijing': 'Asia/Shanghai', 'Tokyo': 'Asia/Tokyo', 'Sydney': 'Australia/Sydney'}.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the weather report.\n\n    Raises:\n        ValueError: If utc_datetime is not a datetime object or if any of the other parameters are not in the expected format.\n    \"\"\"\n\n    # Validate input types and formats\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All elements in cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All elements in weather_conditions must be strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n\n    # Set the random seed for reproducibility\n    set_seed(seed)\n\n    # Initialize an empty list to store the weather report data\n    weather_data = []\n\n    # Loop through each city to generate the weather report\n    for city in cities:\n        # Get the timezone for the city\n        timezone = pytz.timezone(timezones[city])\n        # Convert UTC datetime to local datetime\n        local_datetime = utc_datetime.replace(tzinfo=pytz.utc).astimezone(timezone)\n        # Format the local datetime\n        local_time_str = local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')\n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        # Append the data to the list\n        weather_data.append([city, local_time_str, weather_condition])\n\n    # Create a DataFrame from the weather data\n    df = pd.DataFrame(weather_data, columns=['City', 'Local Time', 'Weather Condition'])\n\n    return df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random walk\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = pd.Series(walk).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    stats_dict = {\n        'count': stats['count'],\n        'mean': stats['mean'],\n        'std': stats['std'],\n        'min': stats['min'],\n        '5th percentile': stats['5%'],\n        '25th percentile': stats['25%'],\n        'median': stats['50%'],\n        '75th percentile': stats['75%'],\n        '95th percentile': stats['95%'],\n        'max': stats['max']\n    }\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    \n    # Return descriptive statistics and plot\n    return stats_dict, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Download a zip file from a URL, extract its contents to the specified directory, and return the list of extracted files.\n    The downloaded zip file is kept in the destination directory.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory where the zip file's contents will be extracted.\n        headers (dict, optional): HTTP headers to include in the request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers, stream=True)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Save the zip file to the destination directory\n    zip_file_path = os.path.join(destination_directory, 'downloaded.zip')\n    with open(zip_file_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n\n    # Extract the zip file\n    extracted_files = []\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Args:\n        seed (int, optional): Seed for the random number generator. Defaults to 42.\n        image_size (tuple, optional): Size of the image. Defaults to (100, 100, 3).\n        range_low (int, optional): Lower bound of the random values. Defaults to 0.\n        range_high (int, optional): Upper bound of the random values. Defaults to 255.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axes object of the plot.\n        image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis labels\n\n    # Return the axis and the image\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file '{audio_file}' does not exist.\")\n\n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the Sound Pressure Level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / np.max(np.abs(matrix)) * spl\n\n    # Generate the spectrogram\n    D = librosa.stft(data)\n    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n\n    # Plot the spectrogram with logarithmic frequency scale and linear time scale\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(S_db, sr=samplerate, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram with Logarithmic Frequency Scale')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = [value for tup in original for value in tup if isinstance(value, (int, float))]\n    numeric_array = np.array(numeric_values)\n\n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_array),\n        'standard_deviation': np.std(numeric_array),\n        'minimum': np.min(numeric_array),\n        'maximum': np.max(numeric_array)\n    }\n\n    # Generate a histogram with an overlaid probability density function (PDF)\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, density=True, alpha=0.6, bins='auto')\n    ax.set_title('Histogram with Overlaid PDF')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Probability Density')\n\n    # Fit a normal distribution to the data and plot the PDF\n    mean, std_dev = stats.norm.fit(numeric_array)\n    x = np.linspace(numeric_array.min(), numeric_array.max(), 100)\n    pdf = stats.norm.pdf(x, mean, std_dev)\n    ax.plot(x, pdf, 'r--', label='Normal Distribution PDF')\n    ax.legend()\n\n    return numeric_array, statistics, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Create a numeric array from the \"original\" list\n    original_array = np.array(original)\n    \n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])[0]\n    \n    # Draw the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array, label='Normalized')\n    ax.legend()\n    ax.set_title('Original and Normalized Arrays')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Show the plot\n    plt.show()\n    \n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\"\n    signal = np.array(list(data.values()))\n    \n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal)) * sample_rate\n    \n    # Step 4: Plot and return the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(freqs, np.abs(fft_signal))\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('FFT of the Signal')\n    \n    # Return the FFT of the signal and the plot axes\n    return (fft_signal, ax)"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\ndef task_func():\n    class DataHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check the Content-Type header\n            if 'content-type' not in self.headers or self.headers['content-type'] != 'application/json':\n                self.send_error(400, \"Content-Type header is not application/json\")\n                return\n\n            # Read the length of the request and parse the JSON data\n            length = int(self.headers['content-length'])\n            try:\n                data = self.rfile.read(length).decode('utf-8')\n                json_data = json.loads(data)\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON\")\n                return\n\n            # Check if the 'data' key is present in the JSON object\n            if 'data' not in json_data:\n                self.send_error(400, \"No data key in request\")\n                return\n\n            # If everything is valid, send a success response\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            response = json.dumps(SUCCESS_RESPONSE)\n            self.wfile.write(response.encode('utf-8'))\n\n        def send_error(self, code, message):\n            self.send_response(code)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            error_response = json.dumps({**ERROR_RESPONSE, 'message': message})\n            self.wfile.write(error_response.encode('utf-8'))\n\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, DataHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Read the content-length header\n            content_length = int(self.headers['Content-Length'])\n            # Read the request body\n            post_data = self.rfile.read(content_length)\n            \n            try:\n                # Parse the JSON data\n                email_data = json.loads(post_data)\n                \n                # Check for required keys\n                if not all(key in email_data for key in ('subject', 'message', 'to')):\n                    raise ValueError(\"Missing required keys in JSON data\")\n                \n                # Prepare the email\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n                \n                # Send the email\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n                \n                # Send a success response\n                self.send_response(200)\n                self.send_header('Content-type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'Email sent successfully')\n            \n            except json.JSONDecodeError:\n                self.send_error(400, 'Invalid JSON')\n            except ValueError as e:\n                self.send_error(400, str(e))\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, 'Authentication Failed')\n            except Exception as e:\n                self.send_error(500, str(e))\n\n    return EmailRequestHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    # Initialize a counter for words\n    word_counter = Counter()\n    \n    # Walk through the directory to find .txt files\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    # Read the file content and split into words\n                    content = f.read()\n                    words = content.split()\n                    word_counter[file] = len(words)\n    \n    # Calculate the total number of words\n    total_words = sum(word_counter.values())\n    \n    # Export the counts to a JSON file\n    with open(filename, 'w', encoding='utf-8') as json_file:\n        json.dump(word_counter, json_file, ensure_ascii=False, indent=4)\n    \n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, plot=False):\n    \"\"\"\n    Process a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Parameters:\n        df (pandas DataFrame): Input DataFrame with 'Date' and 'Value' columns.\n        plot (bool, optional): Whether to visualize the correlation matrix using a heatmap. Defaults to False.\n\n    Returns:\n        DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n        Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n\n    Raises:\n        ValueError: If the DataFrame input is empty or has invalid 'Value'.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if the 'Value' column exists and contains lists\n    if 'Value' not in df.columns or not df['Value'].apply(lambda x: isinstance(x, list)).all():\n        raise ValueError(\"Invalid 'Value' column\")\n\n    # Split the lists in the 'Value' column into separate columns\n    df_split = df['Value'].apply(pd.Series)\n\n    # Calculate the Pearson correlation coefficient between the columns\n    corr_df = df_split.corr()\n\n    # Optionally visualize the correlation matrix using a heatmap\n    if plot:\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(corr_df, annot=True, cmap='coolwarm', square=True)\n        plt.title('Correlation Heatmap')\n        plt.show()\n\n    return corr_df, plt.gca() if plot else None"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {student: [random.randint(0, 100) for _ in all_fields] for student in STUDENTS}\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, index=all_fields)\n    \n    # Calculate average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate average grade for each subject\n    df.loc['Average'] = df.mean()\n    \n    # Ensure 'Average Grade' column is at the end\n    cols = df.columns.tolist()\n    cols.remove('Average Grade')\n    cols.append('Average Grade')\n    df = df[cols]\n    \n    return df\nadditional_fields = ['Art', 'Music']"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate random data for 100 people\n    people_data = []\n    for i in range(PEOPLE_COUNT):\n        name = f\"Person {i + 1}\"\n        age = random.randint(18, 65)\n        height = round(random.uniform(150, 200), 2)\n        weight = round(random.uniform(50, 120), 2)\n        people_data.append([name, age, height, weight])\n    \n    # Calculate averages\n    ages = [person[1] for person in people_data]\n    heights = [person[2] for person in people_data]\n    weights = [person[3] for person in people_data]\n    average_age = round(mean(ages), 2)\n    average_height = round(mean(heights), 2)\n    average_weight = round(mean(weights), 2)\n    \n    # Append averages to the data\n    people_data.append(['Average', average_age, average_height, average_weight])\n    \n    # Write data to CSV file\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(people_data)\n    \n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Regular expression pattern to match the first text not enclosed in square brackets\n    pattern = r'^\\[[^]]*\\]\\s*([^\\[]+)'\n\n    # Dictionary to store the created subdirectories and their corresponding files\n    subdirectories = {}\n\n    # Iterate over the files in the directory\n    for filename in os.listdir(directory):\n        # Construct the full file path\n        filepath = os.path.join(directory, filename)\n\n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(filepath):\n            # Use the regular expression pattern to extract the first text not enclosed in square brackets\n            match = re.search(pattern, filename)\n\n            # If a match is found, move the file to the corresponding subdirectory\n            if match:\n                # Get the matched text (the first text not enclosed in square brackets)\n                subdirectory_name = match.group(1).strip()\n\n                # Create the subdirectory if it doesn't exist\n                subdirectory_path = os.path.join(directory, subdirectory_name)\n                if not os.path.exists(subdirectory_path):\n                    os.makedirs(subdirectory_path)\n\n                # Move the file to the subdirectory\n                shutil.move(filepath, subdirectory_path)\n\n                # Add the file to the list of files in the subdirectory\n                if subdirectory_name not in subdirectories:\n                    subdirectories[subdirectory_name] = []\n                subdirectories[subdirectory_name].append(filename)\n\n    # Return the directory path with organized files and the dictionary of subdirectories and files\n    return directory, subdirectories"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport threading\ndef task_func(file_list):\n    # Initialize list to store exit codes\n    exit_codes = [None] * len(file_list)\n\n    # Function to run a subprocess\n    def run_subprocess(index, file_path):\n        try:\n            # Run the subprocess and capture the exit code\n            exit_codes[index] = subprocess.call(['python', file_path])\n        except Exception as e:\n            # Handle any exceptions and set exit code to 1\n            print(f\"Error running {file_path}: {e}\")\n            exit_codes[index] = 1\n\n    # Create and start a thread for each file\n    threads = []\n    for i, file_path in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(i, file_path))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Find and run all .bat files in a given directory, returning their file names and exit codes.\n\n    Args:\n        directory_path (str): The path to the directory containing the .bat files.\n\n    Returns:\n        list of tuples: A list where each tuple contains the file name and its exit code.\n        The exit code is None if the file could not be executed.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Use glob to find all .bat files in the directory\n    for file in glob.glob(os.path.join(directory_path, \"*.bat\")):\n        try:\n            # Run the .bat file using subprocess and capture the exit code\n            exit_code = subprocess.call([file])\n            results.append((os.path.basename(file), exit_code))\n        except Exception as e:\n            # If an error occurs, append the file name with an exit code of None\n            results.append((os.path.basename(file), None))\n            print(f\"Error running {file}: {str(e)}\", file=sys.stderr)\n\n    return results\ndirectory_path = r\"C:\\path\\to\\directory\""}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame.\")\n    \n    # Check if df is not empty\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(\"The specified column must exist in the DataFrame.\")\n    \n    # Create a figure with two subplots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    # Plot histogram with kernel density estimate on the first subplot\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    axes[0].set_title(f'Histogram and KDE of {col}')\n    \n    # Plot box plot on the second subplot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box Plot of {col}')\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Return the figure\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n\n    Args:\n        script_path (str): The path to the Python script.\n        wait (bool, optional): Whether to wait for the process to complete. Defaults to True.\n        *args: Variable number of arguments to pass to the script.\n\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n\n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(\"The script does not exist.\")\n    \n    # Prepare the command to run the script with arguments\n    command = [sys.executable, script_path] + list(args)\n    \n    # Run the script as a subprocess\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # If wait is True, wait for the process to complete and check for errors\n    if wait:\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, command, output=stdout, stderr=stderr)\n        return process.returncode\n    else:\n        return None"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    # Load the data from the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"The sheet named '{sheet_name}' does not exist in the workbook.\") from e\n    \n    # Calculate mean and standard deviation for each column\n    stats = df.describe().loc[['mean', 'std']].T.to_dict()\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    index = np.arange(len(df.columns))\n    bar_width = 0.35\n    \n    rects1 = ax.bar(index, [stats[col]['mean'] for col in df.columns], bar_width, label='Mean')\n    rects2 = ax.bar(index + bar_width, [stats[col]['std'] for col in df.columns], bar_width, label='Standard Deviation')\n    \n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(df.columns)\n    ax.legend()\n    \n    fig.tight_layout()\n    \n    return stats, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Initialize a dictionary to count activities per day of the week\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime('%A')  # Get the full name of the day of the week\n        day_counts[day_of_week] += 1\n\n    # Define the order of days for the x-axis\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    # Prepare data for plotting\n    x_labels = days_of_week\n    y_values = [day_counts[day] for day in days_of_week]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(x_labels, y_values)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n\n    Args:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    seed (int): The seed for the random number generator. Defaults to 100.\n\n    Returns:\n    str: The name of the file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n    \n    # Check if the source directory exists\n    if not os.path.isdir(src_dir):\n        raise ValueError(\"Source directory does not exist\")\n    \n    # Check if the destination directory exists, if not, create it\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files in the source directory\n    if not files:\n        raise ValueError(\"No files found in the source directory\")\n    \n    # Select a random file from the list of files\n    file_to_move = random.choice(files)\n    \n    # Construct full file paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n    \n    # Move the file\n    shutil.move(src_path, dest_path)\n    \n    # Return the name of the file moved\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n    \n    Args:\n        directory_path (str): The path to the directory containing the Excel files. Defaults to './xlsx_files/'.\n    \n    Returns:\n        int: The number of Excel files processed.\n    \"\"\"\n    # Ensure the directory path ends with a slash\n    if not directory_path.endswith('/'):\n        directory_path += '/'\n    \n    # Initialize a counter for the number of Excel files processed\n    num_files_processed = 0\n    \n    # Find all .xlsx files in the directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Iterate over each Excel file\n    for file_path in excel_files:\n        # Load the workbook\n        wb = load_workbook(file_path)\n        \n        # Iterate over all sheets in the workbook\n        for sheet_name in wb.sheetnames:\n            sheet = wb[sheet_name]\n            \n            # Iterate over all cells in the sheet\n            for row in sheet.iter_rows():\n                for cell in row:\n                    if cell.value:\n                        # Protect double quotes by prefixing them with a double backslash\n                        cell.value = re.sub(r'\"', r'\\\\\"', str(cell.value))\n        \n        # Save the modified workbook\n        wb.save(file_path)\n        \n        # Increment the files processed counter\n        num_files_processed += 1\n    \n    # Return the number of files processed\n    return num_files_processed"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    # Check for negative frequency\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative\")\n    \n    # Check for non-positive sample size\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n    \n    # Generate the time array\n    t = np.linspace(0, 2 * np.pi, sample_size)\n    \n    # Generate the sine and cosine waves\n    sine_wave = np.sin(frequency * t)\n    cosine_wave = np.cos(frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine wave')\n    ax.plot(t, cosine_wave, label='Cosine wave')\n    \n    # Add labels and title\n    ax.set_xlabel('Time [s]')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves with Frequency {} Hz'.format(frequency))\n    ax.legend()\n    \n    # Return the figure and axes objects\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    # Create a Flask application instance\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, using defaults if not set\n    mail_settings = {\n        'MAIL_SERVER': os.getenv('MAIL_SERVER', 'localhost'),\n        'MAIL_PORT': int(os.getenv('MAIL_PORT', 25)),\n        'MAIL_USE_TLS': os.getenv('MAIL_USE_TLS', 'False').lower() in ('true', '1', 't'),\n        'MAIL_USERNAME': os.getenv('MAIL_USERNAME', None),\n        'MAIL_PASSWORD': os.getenv('MAIL_PASSWORD', None)\n    }\n\n    # Configure the Flask-Mail settings\n    app.config.update(mail_settings)\n\n    # Initialize the Flask-Mail instance\n    mail = Mail(app)\n\n    # Return a tuple containing the Flask-Mail instance and the app's mail configurations\n    return mail, mail_settings"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n    \n    # Load the Excel file into a DataFrame\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise ValueError(f\"Error reading Excel file: {e}\")\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Extract the data from the specified column\n    data = df[column_name]\n    \n    # Calculate the mean, median, and standard deviation\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    std_val = np.std(data)\n    \n    # Return the results in a dictionary\n    result = {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_val\n    }\n    \n    return result"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model with one dense hidden layer and sigmoid activation\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    # Fit the model to the training data, evaluating on the test set\n    history = model.fit(X_train, Y_train, epochs=50, verbose=0, validation_data=(X_test, Y_test))\n\n    # Plot the model's training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    # Return the trained model and the Axes object\n    return model, ax"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    # Fit the model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n\n    # Compute ROC curve and AUC\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    ax = plt.gca()\n\n    return model, ax"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n    \n    # Read the image\n    original_image = cv2.imread(image_path)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n    \n    # Reshape the image to a 2D array with each row representing a pixel's RGB value\n    pixel_values = original_image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n    \n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_image = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_image = segmented_image.reshape(original_image.shape)\n    segmented_image = np.uint8(segmented_image)\n    \n    # Save each region as a separate image\n    for i in range(n_clusters):\n        region_mask = kmeans.labels_ == i\n        region_image = np.zeros_like(original_image)\n        region_image[region_mask.reshape(original_image.shape[:2])] = original_image[region_mask.reshape(original_image.shape[:2])]\n        cv2.imwrite(f\"region_{i}.jpg\", cv2.cvtColor(region_image, cv2.COLOR_RGB2BGR))  # Convert back to BGR for saving\n    \n    # Return the original and segmented images as numpy arrays\n    return original_image, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', \n    flatten the result, apply KMeans clustering to the flattened data, \n    and visualize it.\n\n    Parameters:\n    P (np.ndarray): The input matrix.\n    T (np.ndarray): The input 3D tensor.\n    n_clusters (int): The number of clusters for KMeans clustering (default: 3).\n    random_state (int): The random seed for KMeans clustering (default: 0).\n    n_init (int): The number of initializations for KMeans clustering (default: 10).\n\n    Returns:\n    cluster_result (np.ndarray): The result of KMeans clustering.\n    ax (matplotlib.axes.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Ensure the dimensions are compatible for matrix multiplication\n    assert P.shape[1] == T.shape[0], \"Incompatible shapes for matrix multiplication\"\n\n    # Calculate the product of matrix 'P' and 3D tensor 'T'\n    product = np.dot(P, T)\n\n    # Flatten the product\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(cluster_result)), flattened, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Flattened Data Values')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Check if points is a numpy array and has the correct shape\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input must be a 2D numpy array with 2 columns\")\n    \n    # Apply jittering to avoid duplicate points\n    np.random.seed(seed)\n    jitter = np.random.normal(0, 1e-9, points.shape)\n    points_jittered = points + jitter\n    \n    # Calculate the Voronoi diagram\n    vor = Voronoi(points_jittered)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    plt.axis('equal')\n    plt.show()\n    \n    return (vor, ax)"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"The source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"The destination directory '{dest_dir}' does not exist.\")\n\n    # Find files with the specified extension in the source directory\n    files_to_move = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize a list to store the paths of moved files\n    moved_files = []\n\n    # Iterate over the files to move\n    for file in files_to_move:\n        # Get the file name\n        file_name = os.path.basename(file)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(os.path.join(dest_dir, file_name)):\n            # Move the file\n            shutil.move(file, dest_dir)\n            # Add the file path to the list of moved files\n            moved_files.append(os.path.join(dest_dir, file_name))\n\n    # Return the list of moved files\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n    \n    # Load JSON string into a dictionary\n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    \n    # Function to double numerical values\n    def double_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, list):\n            return [double_value(v) for v in value]\n        elif isinstance(value, str):\n            # Use regex to check if the string can be interpreted as a number\n            number_match = re.match(r'^([\\+\\-]?\\d+(\\.\\d*)?|\\.\\d+)([eE][\\+\\-]?\\d+)?$', value)\n            if number_match:\n                number = float(value)\n                return number * 2\n        return value\n    \n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {key: double_value(value) for key, value in data_dict.items()}\n    \n    # Create a Pandas DataFrame from the dictionary\n    df = pd.DataFrame.from_dict(normalized_dict, orient='index').transpose()\n    \n    return df\njson_example = '{\"a\": 1, \"b\": \"2\", \"c\": [3, 4.5], \"d\": \"not a number\"}'"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    # Start the subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Initialize metrics\n    cpu_usage = 0\n    memory_usage = 0\n    start_time = time.time()\n\n    try:\n        # Monitor the process\n        while process.poll() is None:\n            # Check if the timeout has been reached\n            if time.time() - start_time > timeout:\n                process.terminate()\n                raise TimeoutError(\"The script execution exceeded the specified timeout.\")\n\n            # Get the process object for psutil monitoring\n            proc = psutil.Process(process.pid)\n\n            # Accumulate CPU and memory usage\n            cpu_usage += proc.cpu_percent(interval=0.1)\n            memory_usage += proc.memory_info().rss\n\n    except psutil.NoSuchProcess:\n        pass  # The process has already terminated\n\n    finally:\n        # Ensure the process is terminated\n        process.terminate()\n        process.wait()\n\n    # Return the metrics\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)  # Set seed for reproducibility\n    \n    # Generate random values for x and y\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Determine the number of categories to use\n    num_categories = len(CATEGORIES)\n    if N >= num_categories:\n        # Ensure each category appears at least once\n        category = np.random.choice(CATEGORIES, N)\n    else:\n        # Randomly sample categories without replacement\n        category = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': category\n    })\n    \n    # Plot scatter plot\n    fig, ax = plt.subplots()\n    for cat in CATEGORIES:\n        subset = df[df['category'] == cat]\n        ax.scatter(subset['x'], subset['y'], label=cat)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    plt.show()\n    \n    return (df, ax)"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Convert start_time and end_time to datetime objects if they are not already\n    if not isinstance(start_time, datetime):\n        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    if not isinstance(end_time, datetime):\n        end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Generate the time series index\n    time_index = pd.date_range(start=start_time, end=end_time, freq=step)\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate values from a normal distribution\n    values = np.random.normal(size=len(time_index))\n    \n    # Add a linear trend\n    trend_values = np.arange(len(time_index)) * trend\n    values += trend_values\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data=values, index=time_index, columns=['Value'])\n    \n    # Plot the time series\n    fig, ax = plt.subplots()\n    df.plot(ax=ax)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    \n    return ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Validate input\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"random_seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"products must be a list with exactly 5 elements\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"all elements in products must be strings\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Get current date\n    end_date = datetime.now()\n\n    # Initialize data\n    data = []\n\n    # Generate sales data for each day between start_date and end_date\n    for date in pd.date_range(start=start_date, end=end_date):\n        for product in products:\n            # Generate random sales data\n            sales = random.randint(10, 50)\n            # Append data to list\n            data.append([product, date, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['Product', 'Date', 'Sales'])\n\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert JSON strings to an Excel file, including handling empty JSON arrays.\n\n    Parameters:\n    json_str (str): The JSON string to be converted.\n    filename (str): The name of the Excel file to be created.\n    sheet_name (str, optional): The name of the sheet in the Excel file. Defaults to \"sheet1\".\n\n    Returns:\n    str: The absolute path of the created Excel file.\n\n    Raises:\n    ValueError: If `json_str` is not valid JSON.\n    TypeError: If `json_str` is not a string, bytes, or bytearray.\n    Exception: For other general errors related to file writing.\n    \"\"\"\n    # Validate input type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Try to parse JSON\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string\")\n    \n    # Convert JSON to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create Excel file\n    try:\n        # Ensure the filename has the correct extension\n        if not filename.endswith('.xls'):\n            filename += '.xls'\n        \n        # Save DataFrame to Excel\n        df.to_excel(filename, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(f\"Error writing to file: {e}\")\n    \n    # Return the absolute path of the created file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates\n    end_date = datetime.today()\n    start_date = end_date - timedelta(days=days_in_past)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Generate data\n    data = []\n    for date in date_range:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity')\n    ax.set_title('Daily Activity Durations')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Creates a DataFrame of stock prices for a specified number of days in the past using random data.\n\n    Args:\n        days_in_past (int): The number of days in the past to generate stock prices for. Defaults to 7.\n        stock_names (list): A list of stock names. Defaults to [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"].\n        random_seed (int): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n        DataFrame: A pandas DataFrame containing random stock prices for the specified number of days.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate dates for the specified number of days in the past\n    end_date = datetime.now()\n    dates = [(end_date - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    # Generate random stock prices\n    stock_prices = np.random.rand(days_in_past, len(stock_names))\n\n    # Create DataFrame\n    df = pd.DataFrame(stock_prices, columns=stock_names, index=dates)\n\n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the contents of the files\n        with open(file_path1, newline='', encoding='utf-8') as f1, open(file_path2, newline='', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            \n            # Convert readers to lists of lines\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n            \n            # Check if files are empty\n            if not lines1:\n                raise ValueError(f\"{file_path1} is empty\")\n            if not lines2:\n                raise ValueError(f\"{file_path2} is empty\")\n            \n            # Generate the diff\n            diff = list(ndiff(lines1, lines2))\n            \n            # Prepare the DataFrame\n            data = []\n            line_number = 1\n            for line in diff:\n                status = line[0]\n                content = line[2:].strip()\n                if status != ' ':\n                    data.append([line_number, status, content])\n                line_number += 1\n            \n            df = pd.DataFrame(data, columns=['Line Number', 'Status', 'Content'])\n            return df\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Value error: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        stats = {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n        # Create an empty plot\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n    else:\n        # Calculate statistics\n        stats = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max()\n        }\n        # Create a pie chart using the 'Age' column as labels\n        fig, ax = plt.subplots()\n        ax.pie(df[column], labels=df['Age'], autopct='%1.1f%%')\n    \n    plt.show()\n    return (stats, ax)\ndata = [\n    {'Age': '25', 'Salary': 50000},\n    {'Age': '30', 'Salary': 60000},\n    {'Age': '35', 'Salary': 70000}\n]\ncolumn = 'Salary'"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Validate that the data list is not empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n    \n    # Convert the data list to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Validate that the specified column exists\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the data\")\n    \n    # Validate that steps, calories burned, and distance walked are non-negative\n    for col in ['steps', 'calories burned', 'distance walked']:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in column '{col}'\")\n    \n    # Calculate the sum, mean, min, max of the specified column\n    col_sum = df[column].sum()\n    col_mean = df[column].mean()\n    col_min = df[column].min()\n    col_max = df[column].max()\n    \n    # Prepare the output dictionary\n    result_dict = {\n        'sum': col_sum,\n        'mean': col_mean,\n        'min': col_min,\n        'max': col_max\n    }\n    \n    # Plot the line chart\n    ax = df.plot(x='Date', y=column, title=f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    \n    # Return the result tuple\n    return (result_dict, ax)"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize defaultdict to store sums and counts for each key\n    sums = defaultdict(float)\n    counts = defaultdict(int)\n    medians = defaultdict(list)\n    \n    # Iterate over the list of dictionaries\n    for dictionary in data:\n        for key, value in dictionary.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):\n                # Update sum and count for the key\n                sums[key] += value\n                counts[key] += 1\n                # Store the value for median calculation\n                medians[key].append(value)\n    \n    # Calculate means and medians\n    means = {key: sums[key] / counts[key] for key in sums}\n    medians = {key: np.median(medians[key]) for key in medians}\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'mean': means, 'median': medians})\n    \n    # Sort the DataFrame by index (variable names)\n    df = df.sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file is a CSV file\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file\")\n\n    # Read the CSV file and find duplicates\n    with open(file_path, mode='r', newline='') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n        duplicate_counter = Counter(tuple(row) for row in rows)\n\n    # Filter out unique rows and prepare data for DataFrame\n    duplicates = {row: count for row, count in duplicate_counter.items() if count > 1}\n    df_data = {'Row': list(duplicates.keys()), 'Count': list(duplicates.values())}\n    df = pd.DataFrame(df_data)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='Row', y='Count', ax=ax, legend=False)\n    ax.set_title('Duplicate Rows Frequency')\n    ax.set_xlabel('Rows')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    # Output the dictionary and the Axes object\n    return duplicates, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check for negative ages and round down to nearest integer\n    if 'age' in df.columns:\n        df['age'] = df['age'].apply(lambda x: np.floor(x) if x >= 0 else raise_negative_age_error())\n    \n    # Identify duplicate names\n    duplicate_names = df[df['name'].duplicated(keep=False)]\n    \n    # If no duplicates, return empty Counter and None for plot\n    if duplicate_names.empty:\n        return Counter(), None\n    \n    # Record age distribution among duplicate names\n    age_distribution = Counter(duplicate_names['age'].astype(int))\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    sns.histplot(duplicate_names['age'], bins=range(int(duplicate_names['age'].min() - 0.5), int(duplicate_names['age'].max() + 1.5)), ax=ax)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution of Duplicate Names')\n    \n    return age_distribution, ax\ndef raise_negative_age_error():\n    raise ValueError(\"Age must not be negative\")"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    \n    # Prepare data for plotting\n    values = df['value']\n    mean = np.mean(values)\n    std = np.std(values)\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(values, bins=bins, color='green', alpha=0.6, density=True, label='Data')\n    \n    # Overlay normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std)\n    ax.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n    \n    # Set plot details\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    # Return the results\n    return (value_counts, ax)"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    # Ensure the length of 'b' does not exceed the number of predefined columns\n    if len(b) > len(COLUMNS):\n        raise ValueError(\"Length of 'b' exceeds the number of predefined columns.\")\n    \n    # Create a DataFrame with random values\n    data = np.random.rand(len(a), len(b))\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:len(b)])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 7))\n    plt.title('Random Data Bar Chart')\n    plt.ylabel('Values')\n    plt.xlabel('Indices')\n    plt.xticks(rotation=45)\n    plt.legend(title='Columns')\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Ensure data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame\")\n\n    # Check if data has the required columns\n    required_columns = ['month', 'value']\n    if not all(col in data.columns for col in required_columns):\n        raise ValueError(\"Input data must have 'month' and 'value' columns\")\n\n    # Ensure 'month' is in datetime format\n    data['month'] = pd.to_datetime(data['month'])\n    \n    # Extract year for the title\n    year = data['month'].dt.year.unique()[0]\n    \n    # Sort data by month\n    data = data.sort_values('month')\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.bar(data['month'].dt.strftime('%b'), data['value'])\n    \n    # Set plot labels and title\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Show plot\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the string data to a numeric pandas Series\n    numeric_data = pd.to_numeric(data)\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(numeric_data.min(), numeric_data.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(numeric_data, bins=bins, align='mid', edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate a sine wave\n    y_true = np.sin(x)\n    \n    # Add noise to the sine wave\n    y_noisy = y_true + noise_level * np.random.randn(array_length)\n    \n    # Define the function to fit\n    def sine_func(x, a, b, c, d):\n        return a * np.sin(b * x + c) + d\n    \n    # Initial guess for the parameters\n    initial_guess = [1, 1, 0, 0]\n    \n    # Fit the curve\n    params, _ = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n    \n    # Generate the fitted curve\n    y_fit = sine_func(x, *params)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(x, y_noisy, 'bo', label='Noisy data')\n    ax.plot(x, y_fit, 'r-', label='Fitted curve')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    \n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        # Read the CSV file and normalize text to ASCII\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = []\n            for row in reader:\n                for cell in row:\n                    normalized_text = unicodedata.normalize('NFKD', cell).encode('ascii', 'ignore').decode('ascii')\n                    text_data.append(normalized_text)\n        \n        # Join all text data into a single string and split into words\n        all_text = ' '.join(text_data)\n        words = all_text.split()\n        \n        # Convert words to lowercase and count the frequency of each word\n        words = [word.lower() for word in words]\n        word_counts = Counter(words)\n        \n        # Get the 10 most common words\n        most_common_words = word_counts.most_common(10)\n        \n        # Create a bar plot for the most common words\n        words, frequencies = zip(*most_common_words)\n        fig, ax = plt.subplots()\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        \n        # Return the plot and the list of tuples\n        return (ax, most_common_words)\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file {csv_file}.\")"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and an axis\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    \n    # Calculate the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, 0, 1)\n    \n    # Plot the PDF\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    # Set labels and title\n    ax.set_title('Histogram and PDF of Normally Distributed Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    # Return the figure object\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate random password and nonce\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(12)\n\n    # Encrypt private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    encrypted_private_key, tag = cipher.encrypt_and_digest(private_key.save_pkcs1())\n\n    # Save encrypted private key to file\n    with open(filename, \"wb\") as f:\n        f.write(encrypted_private_key + tag + nonce)\n\n    # Return public key, filename, password, and nonce\n    return public_key, filename, password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Read the file to be encrypted\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n    \n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256-bit key\n    \n    # Pad the data to be encrypted with AES\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n    \n    # Encrypt the data with AES\n    iv = os.urandom(16)  # Initialization vector\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n    \n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as file:\n        file.write(iv + encrypted_data)\n    \n    # Save the encrypted AES key\n    encrypted_aes_key_path = file_path + '.key.enc'\n    with open(encrypted_aes_key_path, 'wb') as file:\n        file.write(encrypted_aes_key)\n    \n    # Return the RSA public key, and the filenames of the encrypted file and the encrypted AES key\n    return public_key, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    # Validate the URL\n    if not url:\n        raise ValueError(\"The provided URL is empty.\")\n    \n    try:\n        # Fetch the HTML content from the URL\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except ValueError:\n        raise ValueError(\"The provided URL is invalid.\")\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Network connectivity or server issue: {e.reason}\")\n    \n    # Parse the HTML content using PyQuery\n    d = pq(html_content)\n    \n    # Extract the text and href attributes from all anchor tags\n    anchors = d('a')\n    data = [{'text': anchor.text, 'href': anchor.attrib.get('href'), 'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')} for anchor in anchors]\n    \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import uniform\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the CSV file path\n    csv_file_path = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Start time for the data\n    start_time = datetime.now()\n\n    # Open the CSV file for writing\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Time'] + SENSORS)\n\n        # Generate data for each hour\n        for hour in range(hours):\n            current_time = start_time + timedelta(hours=hour)\n            temperature = uniform(20.0, 30.0)  # Random temperature between 20.0 and 30.0\n            humidity = uniform(30.0, 70.0)      # Random humidity between 30.0 and 70.0\n            pressure = uniform(980.0, 1030.0)   # Random pressure between 980.0 and 1030.0\n            writer.writerow([current_time.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n\n    print(f\"Sensor data for {hours} hours has been saved to {csv_file_path}\")"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n        for vehicle in VEHICLE_TYPES:\n            row.append(randint(0, 100))  # Random count between 0 and 100\n        data.append(row)\n        current_time += timedelta(hours=1)\n    \n    # Define CSV file path\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    \n    # Write data to CSV\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n    \n    # Read data for plotting\n    df = pd.read_csv(csv_file_path)\n    \n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n    \n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Save plot to file (optional)\n    plot_file_path = os.path.join(output_dir, 'traffic_plot.png')\n    plt.savefig(plot_file_path)\n    \n    return csv_file_path, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Ensure backup directory exists\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Generate weather data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        data.append([current_time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(data)\n\n    # Backup the CSV file\n    backup_file_path = os.path.join(backup_dir, 'weather_data.csv')\n    shutil.copy(csv_file_path, backup_file_path)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Generate a DataFrame with random goals and penalties\n    df = pd.DataFrame({\n        'Team': [team for team in TEAMS for _ in range(goals)],\n        'Goals': [randint(0, 5) for _ in range(goals * len(TEAMS))],\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in range(goals * len(TEAMS))]\n    })\n\n    # Calculate the total goals and penalty costs for each team\n    df_total = df.groupby('Team').sum().reset_index()\n\n    # Create a seaborn plot for goals\n    plt.figure(figsize=(10, 6))\n    ax1 = sns.barplot(x='Team', y='Goals', data=df_total)\n    ax1.set_title('Total Goals per Team')\n    ax1.set_xlabel('Team')\n    ax1.set_ylabel('Goals')\n\n    # Create a seaborn plot for penalty costs\n    plt.figure(figsize=(10, 6))\n    ax2 = sns.barplot(x='Team', y='Penalty Cost', data=df_total)\n    ax2.set_title('Total Penalty Costs per Team')\n    ax2.set_xlabel('Team')\n    ax2.set_ylabel('Penalty Cost (USD)')\n\n    # Return the DataFrame and the plot objects\n    return df, [ax1, ax2]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.apply(lambda col: (col != 0).sum())\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Values in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    \n    # Show plot\n    plt.show()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define courses\n    courses = ['Math', 'Science', 'History', 'English']\n    \n    # Generate random student IDs\n    student_ids = sample(range(1000, 9999), num_students)\n    \n    # Create DataFrame with random grades\n    data = {course: np.random.randint(0, 101, size=num_students) for course in courses}\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate average grade in each course\n    average_grades = df.mean()\n    \n    # Calculate number of students with passing grade (>= 60)\n    passing_counts = (df >= 60).sum()\n    \n    # Prepare data for plotting\n    plot_data = pd.DataFrame({'Average Grade': average_grades, 'Passing Count': passing_counts})\n    \n    # Create bar plot\n    ax = plot_data.plot(kind='bar', figsize=(10, 6), rot=0)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Counts / Grades')\n    ax.legend(loc='upper left')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Convert the array to a numpy array for easier manipulation\n    array = np.array(array)\n    \n    # Filter the array to get the rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n    \n    # Extract the indices and the corresponding values to fit\n    indices = np.arange(len(filtered_array))\n    values = filtered_array[:, 1]\n    \n    # Define the exponential decay function to fit\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Initial guess for the parameters\n    p0 = [1, 1, 1]\n    \n    # Fit the exponential decay function to the data\n    popt, _ = optimize.curve_fit(exp_decay, indices, values, p0=p0)\n    \n    # Create a plot to visualize the fit\n    fig, ax = plt.subplots()\n    ax.scatter(indices, values, label='Data')\n    ax.plot(indices, exp_decay(indices, *popt), label='Fitted function', color='red')\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Exponential Decay Fit')\n    \n    # Return the optimized parameters and the Axes object\n    return (popt, ax)"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    # Preprocess the input texts\n    preprocessed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert all characters to lowercase\n        text = text.lower()\n        # Remove stopwords\n        text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n        preprocessed_texts.append(text)\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_vectors = vectorizer.fit_transform(preprocessed_texts)\n\n    # Apply NMF to extract the specified number of topics\n    nmf = NMF(n_components=num_topics, random_state=0)\n    nmf.fit(tfidf_vectors)\n\n    # Extract the most significant words for each topic\n    topics = []\n    feature_names = vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(nmf.components_):\n        topic_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(topic_words)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Download stopwords if not provided\n    if stopwords is None:\n        nltk.download('stopwords')\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n    \n    # Function to clean and tokenize text\n    def clean_and_tokenize(text):\n        # Remove non-alphanumeric characters except space, and lowercase\n        cleaned_text = ALPHANUMERIC.sub(' ', text).lower()\n        # Tokenize and remove stopwords\n        tokens = nltk.word_tokenize(cleaned_text)\n        return [word for word in tokens if word not in stopwords]\n    \n    # Process all texts\n    processed_texts = [clean_and_tokenize(text) for text in texts]\n    \n    # Train Word2Vec model\n    word2vec_model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return word2vec_model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Ensure the path exists\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The directory {path} does not exist.\")\n\n    # Create the processed directory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    # Initialize an empty DataFrame to store all data\n    df = pd.DataFrame()\n\n    # List all JSON files in the directory and sort them alphabetically\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()\n\n    for file_name in json_files:\n        file_path = os.path.join(path, file_name)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        \n        # Convert the JSON data to a DataFrame\n        temp_df = pd.DataFrame(data)\n        \n        # Add the \"Source\" column\n        temp_df['Source'] = file_name\n        \n        # Append the DataFrame to the main DataFrame\n        df = pd.concat([df, temp_df], ignore_index=True)\n        \n        # Move the processed file to the processed directory\n        shutil.move(file_path, os.path.join(processed_path, file_name))\n\n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the directory and file path\n    dir_path = \"task_func_data\"\n    file_path = os.path.join(dir_path, \"Output.txt\")\n    \n    # Create the directory if it doesn't exist\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    \n    # Generate random sensor data\n    temperature = round(random.uniform(20.0, 30.0), 2)  # Random temperature between 20.0 and 30.0\n    humidity = round(random.uniform(30.0, 70.0), 2)     # Random humidity between 30.0 and 70.0\n    \n    # Prepare the data to write\n    data = {\n        'datetime': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n        'temperature': temperature,                                # Formatted temperature\n        'humidity': humidity                                       # Formatted humidity\n    }\n    \n    # Write data to the CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['datetime', 'temperature', 'humidity'])\n        if file.tell() == 0:  # Check if the file is empty\n            writer.writeheader()\n        writer.writerow(data)\n    \n    # Return the file path before deletion\n    return_path = file_path\n    \n    # Delete the file after use\n    if os.path.exists(file_path):\n        os.remove(file_path)\n    \n    return return_path"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a GET request to the URL\n    response = urllib.request.urlopen(url)\n    # If the GET request is successful, the status code will be 200\n    if response.getcode() == 200:\n        # Get the content of the response\n        page_content = response.read()\n        # Create a BeautifulSoup object and specify the parser\n        soup = BeautifulSoup(page_content, 'html.parser')\n        # Find all the paragraph tags on the webpage\n        paragraphs = soup.find_all('p')\n        # Create a list to store the scraped data\n        scraped_data = []\n        # Iterate over each paragraph tag\n        for paragraph in paragraphs:\n            # Get the text of the paragraph\n            text = paragraph.get_text()\n            # Add the text to the scraped data list\n            scraped_data.append([text])\n        # Check if the CSV file already exists\n        if os.path.exists(CSV_FILE_PATH):\n            # If the file exists, append the scraped data to it\n            with open(CSV_FILE_PATH, 'a', newline='') as csvfile:\n                writer = csv.writer(csvfile)\n                writer.writerows(scraped_data)\n        else:\n            # If the file does not exist, create it and write the scraped data to it\n            with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n                writer = csv.writer(csvfile)\n                writer.writerows(scraped_data)\n        # Return the path of the CSV file\n        return CSV_FILE_PATH\n    else:\n        # If the GET request is not successful, return None\n        return None"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    \"\"\"\n    This function trains a linear regression model and returns the model score of the test set.\n    \n    Parameters:\n    data (pd.DataFrame): The DataFrame used as training data.\n    target_column (str): The column used as target in training the model.\n    test_size (float, optional): The proportion of the data to include in the test set. Defaults to 0.2.\n    random_state (int, optional): The seed used to shuffle the data before splitting. Defaults to 0.\n    \n    Returns:\n    float: The model's score.\n    \"\"\"\n    \n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    \n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(\"Target column is not a column of data.\")\n    \n    # Check if data contains non-numeric values\n    if not all(data.dtypes == np.number):\n        raise ValueError(\"Data contains non-numeric values.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state is not an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"Test size is not between 0 and 1.\")\n    \n    # Split the data into a training set and a test set\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Train a linear regression model on the training set\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    # Set the random seed for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Combine the lists of names\n    names = latin_names + other_names\n    \n    # Initialize lists to store the generated data\n    ids = []\n    names_list = []\n    dates_of_birth = []\n    emails = []\n    \n    # Generate 100 records\n    for i in range(100):\n        # Randomly select a name\n        name = np.random.choice(names)\n        \n        # Correct any improper encoding of Latin characters\n        name = codecs.decode(name, 'utf-8')\n        \n        # Generate a random date of birth\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 29)  # Simplified to avoid dealing with month lengths\n        date_of_birth = datetime.datetime(year, month, day)\n        \n        # Construct the email\n        email = f\"{name.lower()}{year}@{email_domain}\"\n        \n        # Add the generated data to the lists\n        ids.append(i + 1)\n        names_list.append(name)\n        dates_of_birth.append(date_of_birth)\n        emails.append(email)\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names_list,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a dictionary to store the results\n    results = defaultdict(dict)\n\n    # Calculate the mean and median for each key\n    for key in data[0].keys():\n        values = [d[key] for d in data if isinstance(d[key], (int, float))]\n        results[key]['mean'] = np.mean(values)\n        results[key]['median'] = np.median(values)\n\n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['key', 'mean', 'median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for key, value in results.items():\n            writer.writerow({'key': key, 'mean': value['mean'], 'median': value['median']})\n\n    return results"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, \n    and if it has content, then copies it to a target directory.\n\n    Args:\n    kwargs (dict): A dictionary containing file paths as keys and their contents as values.\n    target_dir (str): The target directory where files will be copied. Defaults to \"non_none_files\".\n\n    Returns:\n    dict: A dictionary with a key \"copied_files\" containing a list of full file paths that were copied.\n    \"\"\"\n    # Initialize an empty list to store the paths of copied files\n    copied_files = []\n\n    # Create the target directory if it doesn't exist\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True, exist_ok=True)\n\n    # Iterate over the dictionary items\n    for file_path, file_content in kwargs.items():\n        # Check if the file exists and has content\n        if os.path.exists(file_path) and file_content:\n            # Copy the file to the target directory\n            target_file_path = target_dir_path / Path(file_path).name\n            shutil.copy2(file_path, target_file_path)\n            # Add the full path of the copied file to the list\n            copied_files.append(str(target_file_path))\n\n    return {\"copied_files\": copied_files}"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    This function processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n\n    Args:\n        directory (str): The directory where the CSV files are located.\n        pattern (str): A regular expression pattern to match the CSV file names. Defaults to r'^(.*?)-\\d+\\.csv$'.\n\n    Returns:\n        list: A list of strings, where each string is the filename of a new CSV file created by the function.\n    \"\"\"\n    new_files = []\n    directory_path = Path(directory)\n    \n    # Ensure the directory exists\n    if not directory_path.is_dir():\n        raise ValueError(f\"The directory {directory} does not exist.\")\n    \n    # Compile the regex pattern\n    regex = re.compile(pattern)\n    \n    # Iterate over all files in the directory\n    for file_path in directory_path.iterdir():\n        if file_path.is_file() and file_path.suffix == '.csv':\n            match = regex.match(file_path.name)\n            if match:\n                # Extract the base name from the pattern\n                base_name = match.group(1)\n                # Create a new file name\n                new_file_name = f\"{base_name}.csv\"\n                new_file_path = directory_path / new_file_name\n                \n                # Copy the content to the new file\n                with open(file_path, 'r', newline='') as old_file, open(new_file_path, 'w', newline='') as new_file:\n                    reader = csv.reader(old_file)\n                    writer = csv.writer(new_file)\n                    for row in reader:\n                        writer.writerow(row)\n                \n                new_files.append(str(new_file_path))\n    \n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzip all zip files in a directory whose name matches a certain pattern.\n\n    Parameters:\n    directory (str): The directory where the zip files are located.\n    pattern (str): A regular expression pattern to match the zip file names.\n\n    Returns:\n    list: A list of directories where the files were extracted.\n    \"\"\"\n    # Compile the pattern\n    regex = re.compile(pattern)\n\n    # Initialize an empty list to store the directories where files were extracted\n    extracted_dirs = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file name matches the pattern\n        if regex.match(filename):\n            # Extract the prefix part of the filename\n            prefix = filename.rsplit('-', 1)[0]\n\n            # Create the directory to extract the files if it does not exist\n            extract_dir = os.path.join(directory, prefix)\n            if not os.path.exists(extract_dir):\n                os.makedirs(extract_dir)\n\n            # Extract the zip file\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n\n            # Add the directory to the list of extracted directories\n            extracted_dirs.append(extract_dir)\n\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport zipfile\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    if not files_to_archive:\n        raise FileNotFoundError(f\"No files found matching the pattern: {pattern}\")\n\n    # Create a unique archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, f\"archive_{hash(pattern)}.zip\")\n\n    # Archive the files\n    with zipfile.ZipFile(archive_file, 'w') as zf:\n        for file in files_to_archive:\n            zf.write(file, os.path.basename(file))\n            os.remove(file)  # Delete the original file after adding to the archive\n\n    return archive_file"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter object to store the total counts\n    count = Counter({'goals': 0, 'penalties': 0})\n\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            # Increment the counts for each row\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n\n    # Update the counts with the given goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Compile the regular expression pattern\n    pattern = re.compile(file_pattern)\n    \n    # Initialize a counter for moved files\n    moved_files_count = 0\n    \n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the file name matches the pattern\n        if pattern.match(filename):\n            # Construct full file path\n            source_file_path = os.path.join(source_dir, filename)\n            target_file_path = os.path.join(target_dir, filename)\n            \n            # Move the file to the target directory\n            shutil.move(source_file_path, target_file_path)\n            \n            # Increment the counter\n            moved_files_count += 1\n    \n    # Return the number of files that were moved\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean texts\n    clean_text1 = re.sub(ALPHANUMERIC, ' ', text1).lower()\n    clean_text2 = re.sub(ALPHANUMERIC, ' ', text2).lower()\n    \n    # Tokenize texts\n    words1 = clean_text1.split()\n    words2 = clean_text2.split()\n    \n    # Compute term frequencies\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n    \n    # Get unique words from both texts\n    unique_words = set(words1).union(set(words2))\n    \n    # Create vectors\n    vector1 = np.array([tf1.get(word, 0) for word in unique_words])\n    vector2 = np.array([tf2.get(word, 0) for word in unique_words])\n    \n    # Compute cosine similarity\n    cosine_sim = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n    \n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(clean_text1, clean_text2)\n    \n    return (cosine_sim, levenshtein_ratio)\ntext1 = \"Hello, world!\"\ntext2 = \"Hello, everyone!\""}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Check if numbers is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all numbers are non-negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative\")\n    \n    # Handle the case of an empty list\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations of the list\n    all_permutations = list(permutations(numbers))\n    \n    # Calculate the sum of the factorials of each number in each permutation\n    factorial_sums = [reduce(lambda x, y: x + math.factorial(y), perm, 0) for perm in all_permutations]\n    \n    return factorial_sums, all_permutations"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    \n    # Ensure the destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    # Convert extensions to a list if it's not already\n    if isinstance(EXTENSIONS, str):\n        EXTENSIONS = [EXTENSIONS]\n    \n    # Iterate over each extension\n    for ext in EXTENSIONS:\n        # Find all files with the current extension in the source directory\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*.{ext}'))\n        \n        # Transfer each file\n        for file_path in files:\n            try:\n                # Construct the destination path\n                dest_path = os.path.join(DEST_DIR, os.path.basename(file_path))\n                \n                # Copy the file to the destination directory\n                shutil.copy2(file_path, dest_path)\n                \n                # Record the successfully transferred file\n                transferred_files.append(os.path.basename(file_path))\n            except Exception as e:\n                # Issue a warning if the file could not be transferred\n                warnings.warn(f\"Could not transfer {file_path}: {e}\")\n    \n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item[0] for item in data]\n    counts = np.array([item[1] for item in data]).reshape(-1, 1)\n    weights = np.array([item[2] for item in data]).reshape(-1, 1)\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts, axis=0)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights)\n    \n    # Create a DataFrame with the items, normalized counts, and normalized weights\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts.flatten(),\n        'Normalized Weight': normalized_weights.flatten()\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Transpose the list of tuples\n    transposed_data = list(zip(*data_list))\n\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over each position in the transposed data\n    for position in transposed_data:\n        # Filter out non-numeric values\n        numeric_values = [value for value in position if isinstance(value, (int, float))]\n\n        # Calculate the mean of the numeric values\n        mean_value = np.mean(numeric_values) if numeric_values else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame({'Mean Value': mean_values}, index=[f'Position {i}' for i in range(len(mean_values))])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    \"\"\"\n    Perform a chi-square test of independence of variables in a contingency table.\n\n    Parameters:\n    data (DataFrame): A DataFrame containing categorical data.\n    col1 (str): The name of the first column.\n    col2 (str): The name of the second column.\n\n    Returns:\n    float: The p-value of the chi-square test of independence.\n\n    Raises:\n    ValueError: If 'data' is empty, if 'col1' or 'col2' are not in 'data', \n                 if one or both of the columns do not have multiple categories, \n                 or if some categories have less than 5 observations.\n    TypeError: If one or both of the columns contain non-categorical data.\n    \"\"\"\n    \n    # Check if 'data' is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame 'data' is empty.\")\n    \n    # Check if 'col1' and 'col2' are in 'data'\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns 'col1' and 'col2' are not in 'data'.\")\n    \n    # Check if 'col1' and 'col2' are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns 'col1' and 'col2' contain non-categorical data.\")\n    \n    # Check if 'col1' and 'col2' have multiple categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns 'col1' and 'col2' do not have multiple categories.\")\n    \n    # Check if some categories have less than 5 observations\n    if (data[col1].value_counts() < 5).any() or (data[col2].value_counts() < 5).any():\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Construct the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Return the p-value\n    return p\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    # Set seed for reproducibility\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(6, dtype=int)\n    for result in results:\n        frequency[result - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(1, 8) - 0.5, rwidth=0.8, color='blue', edgecolor='black')\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    \n    # Return the frequency array and the Axes object\n    return (frequency, ax)"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    \n    The function identifies processed files by the '_processed' suffix in the filename.\n    \n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        archive_name (str, optional): The name of the archive file. Defaults to 'archive.zip'.\n    \n    Returns:\n        str: The path to the created archive.\n    \"\"\"\n    \n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Define the pattern to match processed files\n    processed_pattern = re.compile(r'.*_processed\\..+')\n\n    # Create a list to hold paths of processed files\n    processed_files = []\n\n    # Walk through the source directory to find processed files\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if processed_pattern.match(file):\n                file_path = os.path.join(root, file)\n                processed_files.append(file_path)\n\n    # Define the full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create the zip archive\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in processed_files:\n            zipf.write(file, os.path.relpath(file, source_dir))\n\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the DataFrame has a 'Close' column and a datetime index\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must have a 'Close' column\")\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame must have a datetime index\")\n\n    # Prepare the data for linear regression\n    X = np.array(df.index.astype(np.int64) // 10**9).reshape(-1, 1)  # Convert datetime to Unix timestamp\n    y = df['Close'].values\n\n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps for the next 7 days\n    last_timestamp = X[-1][0]\n    future_timestamps = np.array([last_timestamp + i * 24 * 60 * 60 for i in range(1, 8)]).reshape(-1, 1)\n\n    # Predict the closing prices for the next 7 days\n    future_prices = model.predict(future_timestamps)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df.index, y, label='Historical Prices')\n    future_dates = pd.to_datetime([pd.Timestamp(ts * 10**9) for ts in future_timestamps[:, 0]])\n    ax.plot(future_dates, future_prices, label='Predicted Prices', linestyle='--')\n    ax.legend()\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Prices Prediction')\n\n    return (future_prices.tolist(), ax)"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[np.abs(z_scores) > z_threshold]\n    \n    # Plotting the data\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df.index, df['closing_price'], label='Closing Price', color='blue')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outlier')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    ax.grid(True)\n    \n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Parameters:\n    df (pd.DataFrame): A pandas DataFrame containing the stock closing prices.\n    \n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    \n    # Ensure the DataFrame has a column named 'Close'\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a column named 'Close'\")\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Plot the box plot\n    sns.boxplot(x=df['Close'], ax=axs[0])\n    axs[0].set_title('Box Plot of Closing Prices')\n\n    # Plot the histogram\n    sns.histplot(df['Close'], kde=True, ax=axs[1])\n    axs[1].set_title('Histogram of Closing Prices')\n\n    # Adjust layout for better visualization\n    plt.tight_layout()\n\n    # Return the axes objects\n    return axs"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has a datetime index\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    \n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5,1,0))  # Example order, you might need to tune this\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecast_values = forecast.tolist()\n    \n    # Plot the forecast\n    fig, ax = plt.subplots()\n    df['Close'].plot(ax=ax, label='Historical Prices')\n    forecast.plot(ax=ax, label='Forecasted Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n    \n    return forecast_values, ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    This function creates a dictionary containing all possible two-letter combinations \n    of the lowercase English alphabets. The dictionary values represent the frequency \n    of these two-letter combinations in the given word. If a combination does not \n    appear in the word, its value will be 0.\n\n    Args:\n        word (str): The input word.\n\n    Returns:\n        dict: A dictionary with keys as two-letter alphabet combinations and values \n        as their counts in the word.\n    \"\"\"\n    \n    # Generate all possible two-letter combinations of the lowercase English alphabets\n    all_combinations = [''.join(p) for p in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Convert the word to lowercase\n    word = word.lower()\n    \n    # Initialize a dictionary with all combinations as keys and 0 as values\n    combination_counts = {combination: 0 for combination in all_combinations}\n    \n    # Count the frequency of each combination in the word\n    for i in range(len(word) - 1):\n        combination = word[i:i+2]\n        if combination in combination_counts:\n            combination_counts[combination] += 1\n    \n    return combination_counts"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Initialize DataFrame\n    data = {\n        'Date': np.repeat(dates, len(categories)),\n        'Category': categories * periods\n    }\n    \n    # Generate random sales data\n    np.random.seed(0)  # For reproducibility\n    data['Sales'] = np.random.randint(100, 1000, size=len(data['Date']))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n    \n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate sales data if not provided\n    if sales_data is None:\n        np.random.seed(0)\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the sales data\n    df = pd.DataFrame({'date': date_range, 'sales': sales_data})\n    \n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Set the date column as the index\n    df.set_index('date', inplace=True)\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Prepare the data for the model\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['sales'].values\n    \n    # Fit the model\n    model.fit(X, y)\n    \n    # Forecast future sales\n    future_X = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_X)\n    \n    # Return the forecasted future sales\n    return future_sales"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_task_list = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Randomly select n_tasks from the sanitized task list\n    selected_tasks = random.sample(sanitized_task_list, n_tasks)\n\n    # Assign each task to a random employee\n    assignments = [\n        {\n            \"Task Name\": task,\n            \"Assigned To\": random.choice(employees),\n            \"Due Date\": datetime.today().strftime('%Y-%m-%d')\n        }\n        for task in selected_tasks\n    ]\n\n    # Create a DataFrame from the assignments\n    df = pd.DataFrame(assignments)\n\n    return df"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Replace spaces with underscores and convert to lowercase\n    modified_words = [word.replace(' ', '_').lower() for word in text.split()]\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(modified_words)\n    \n    # Prepare data for plotting\n    words, frequencies = zip(*word_counts.items())\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to make room for rotated labels\n    \n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n\n    Args:\n        source_directory (str): The path to the source directory.\n        target_directory (str): The path to the target directory.\n        zip_name (str): The name of the zip file.\n\n    Returns:\n        str: The full path to the created zip file.\n\n    Raises:\n        OSError: If the source_directory does not exist.\n    \"\"\"\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n\n    # Define the valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create the target directory if it does not exist\n    target_dir_path = Path(target_directory)\n    target_dir_path.mkdir(parents=True, exist_ok=True)\n\n    # Create the zip file\n    zip_file_path = target_dir_path / f\"{zip_name}.zip\"\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        # Iterate over the files in the source directory\n        for file_path in glob.glob(os.path.join(source_directory, '*')):\n            # Check if the file has a valid extension\n            file_ext = os.path.splitext(file_path)[1]\n            if file_ext in valid_extensions:\n                # Add the file to the zip file\n                zip_file.write(file_path, os.path.basename(file_path))\n\n    # Return the full path to the created zip file\n    return str(zip_file_path)"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    \n    Args:\n        source_directory (str): The path to the source directory.\n        target_directory (str): The path to the target directory.\n    \n    Returns:\n        int: The number of files successfully converted to CSV.\n    \n    Raises:\n        FileNotFoundError: If the source directory does not exist.\n    \"\"\"\n    \n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory {source_directory} does not exist.\")\n    \n    # Create the target directory if it does not exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # List of supported file extensions\n    supported_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n    \n    # Counter for successfully converted files\n    converted_count = 0\n    \n    # Iterate over all files in the source directory\n    for file_path in Path(source_directory).iterdir():\n        if file_path.suffix in supported_extensions:\n            # Read the content based on the file type\n            if file_path.suffix == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            elif file_path.suffix == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([para.text for para in doc.paragraphs])\n            elif file_path.suffix == '.xlsx':\n                df = pd.read_excel(file_path)\n                content = df.to_csv(index=False)\n            elif file_path.suffix == '.csv':\n                df = pd.read_csv(file_path)\n                content = df.to_csv(index=False)\n            \n            # Create a DataFrame with the content\n            df = pd.DataFrame({'Text': [content]})\n            \n            # Save the DataFrame to a CSV file in the target directory\n            target_file_path = Path(target_directory) / (file_path.stem + '.csv')\n            df.to_csv(target_file_path, index=False, quoting=csv.QUOTE_ALL)\n            \n            # Increment the counter\n            converted_count += 1\n    \n    return converted_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n\n    Returns:\n        pd.DataFrame: A DataFrame where each column contains the normalized cumulative sum of the\n            respective column in the input DataFrame, retaining the original column names.\n\n    Raises:\n        TypeError: If the DataFrame contains non-numeric data types.\n        ValueError: If the DataFrame is empty or contains NaN values.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if the DataFrame contains non-numeric data types\n    if not df.applymap(np.isreal).all().all():\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n\n    # Check if the DataFrame contains NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the MinMaxScaler to each column\n    normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Compute the cumulative sum for each column\n    cumulative_sum_df = normalized_df.cumsum()\n\n    return cumulative_sum_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size, \n    creation time, and last modification time without recursing into subdirectories.\n\n    Args:\n        directory_path (str): The path to the directory to be analyzed.\n\n    Returns:\n        list: A list of tuples, each containing the file name, file size in bytes, \n              creation time in ISO format, and modification time in ISO format.\n\n    Raises:\n        ValueError: If the provided directory does not exist.\n    \"\"\"\n    # Check if the directory exists\n    if not Path(directory_path).exists():\n        raise ValueError(\"The provided directory does not exist.\")\n\n    # Initialize an empty list to store the file information\n    file_info = []\n\n    # Iterate over each file in the directory\n    for file in os.listdir(directory_path):\n        # Construct the full path to the file\n        file_path = os.path.join(directory_path, file)\n\n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(file_path):\n            # Get the file size in bytes\n            file_size = os.path.getsize(file_path)\n\n            # Get the creation time and modification time in seconds since the epoch\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n\n            # Convert the times to datetime objects in UTC\n            creation_time = datetime.fromtimestamp(creation_time, timezone.utc)\n            modification_time = datetime.fromtimestamp(modification_time, timezone.utc)\n\n            # Convert the datetime objects to ISO format strings\n            creation_time = creation_time.isoformat()\n            modification_time = modification_time.isoformat()\n\n            # Append the file information to the list\n            file_info.append((file, file_size, creation_time, modification_time))\n\n    # Return the list of file information\n    return file_info"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Validate and parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    \n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n    \n    # Prepare DataFrame\n    df_data = []\n    for item in data:\n        country = item.get(\"Country\")\n        population = item.get(\"Population\")\n        \n        if not isinstance(country, str):\n            raise ValueError(\"Country names must be strings.\")\n        \n        if not isinstance(population, (int, float)):\n            raise ValueError(\"Population must be numeric.\")\n        \n        if population < 0:\n            raise ValueError(\"Population cannot be negative.\")\n        \n        df_data.append({\"Country\": country, \"Population\": math.floor(population)})\n    \n    # Create DataFrame\n    df = pd.DataFrame(df_data, columns=[\"Country\", \"Population\"])\n    \n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate file path\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n    \n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n\n    Args:\n        dir_path (str): The path to the directory to evaluate.\n        predicates (list): A list of functions that take a file/directory name as input and return a boolean.\n\n    Returns:\n        dict: A dictionary with directory items as keys and the results of condition checks as values.\n\n    Raises:\n        ValueError: If no valid predicates are provided.\n        FileNotFoundError: If the specified directory does not exist or is not a directory.\n    \"\"\"\n    # Check if the directory exists and is a directory\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The directory '{dir_path}' does not exist or is not a directory.\")\n\n    # Check if predicates are provided\n    if not predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Deduplicate predicates\n    predicates = list(set(predicates))\n\n    # Initialize the result dictionary\n    result = {}\n\n    # Iterate over the directory items\n    for item in os.listdir(dir_path):\n        # Evaluate the item against each predicate\n        results = [predicate(item) for predicate in predicates]\n        # Store the results in the dictionary\n        result[item] = all(results)\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Decode the hexadecimal string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n    \n    # Convert bytes to a UTF-8 string\n    utf8_string = byte_data.decode('utf-8')\n    \n    # Encode the UTF-8 string into different formats\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(byte_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_string if all(ord(c) < 128 for c in utf8_string) else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n    \n    return result"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_md5 = md5_hash.hexdigest()\n        \n        # Validate the MD5 checksum\n        if calculated_md5 == EXPECTED_MD5_CHECKSUM:\n            # Extract the contents of the tar.gz file\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall()\n            os.remove(TARGET_TAR_FILE)  # Remove the tar.gz file after extraction\n            return True\n        else:\n            # Delete the downloaded file if the checksum does not match\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        # Handle any exceptions that occur during the process\n        print(f\"An error occurred: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise Exception(f\"Failed to download CSV file from {url}: {e}\")\n\n    try:\n        # Read the CSV file and count occurrences of each value in the specified column\n        value_counts = collections.defaultdict(int)\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            for row in reader:\n                value = row[column_name]\n                value_counts[value] += 1\n    except Exception as e:\n        raise e\n    finally:\n        # Delete the downloaded CSV file\n        os.remove(csv_file_path)\n\n    return dict(value_counts)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        response = urllib.request.urlopen(url)\n        xml_data = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(\"Invalid URL or unable to fetch XML file\") from e\n\n    try:\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(\"Invalid XML syntax\") from e\n\n    # Check if the XML structure matches the expected format\n    if root.tag != 'root' or len(root) == 0:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Extract the 'item' elements and their child elements\n    items = []\n    for item in root:\n        item_dict = {}\n        for child in item:\n            item_dict[child.tag] = child.text\n        items.append(item_dict)\n\n    # Convert the list of dictionaries to a Pandas DataFrame\n    df = pd.DataFrame(items)\n\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n    \n    # Use a regular expression to find all words (case-sensitive)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    most_common_words = word_counts.most_common(10)\n    \n    # Extract words and their counts for plotting\n    words, counts = zip(*most_common_words)\n    \n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Frequent Words')\n    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to make room for rotated labels\n    \n    # Return the Counter object and the Axes object\n    return (word_counts, ax)"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download path exists\n        os.makedirs(download_path, exist_ok=True)\n        \n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        \n        # Check if the content type is 'application/zip'\n        content_type = response.headers.get('content-type')\n        if content_type != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Save the file to the download path\n        file_path = os.path.join(download_path, 'downloaded_file.zip')\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Extract the ZIP file\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Return the path to the directory containing the extracted contents\n        return download_path\n    \n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    \n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for unsuccessful HTTP requests\n    except requests.exceptions.HTTPError as http_err:\n        raise http_err\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    # Convert the table to a DataFrame\n    df = pd.read_html(StringIO(table.prettify()))[0]\n\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Ensure directories exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n\n    # Path to the zip file\n    zip_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the zip file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        with open(zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n    except requests.RequestException as e:\n        return (\"Error: Network-related exception - \" + str(e), [])\n    except IOError as e:\n        return (\"Error: File-related exception during download - \" + str(e), [])\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            filenames = [Path(name).name for name in zip_ref.namelist()]\n        return (\"Success\", filenames)\n    except zipfile.BadZipFile as e:\n        return (\"Error: Bad zip file - \" + str(e), [])\n    except IOError as e:\n        return (\"Error: File-related exception during extraction - \" + str(e), [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Fetch the webpage\n    response = requests.get(url)\n    response.raise_for_status()\n\n    # Parse the webpage\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all hyperlinks\n    links = set()\n    for a_tag in soup.find_all('a', href=True):\n        absolute_url = urljoin(base_url, a_tag['href'])\n        links.add(absolute_url)\n\n    # Save the links to a CSV file\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['URL'])\n        for link in links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the webpage content\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Failed to fetch the webpage: {e}\")\n\n    try:\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n        tables = tree.xpath('//table')\n        \n        if not tables or not tables[0].findall('tr'):\n            return 0  # No table found or table is empty in the HTML content\n        \n        # Convert the first table to a DataFrame\n        df = pd.read_html(response.content)[0]\n        \n        if df.empty:\n            return 0  # The table is empty\n        \n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n        \n        # Drop the existing table if it exists\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        \n        # Create a new table with the DataFrame's columns\n        df.to_sql('my_table', conn, index=False)\n        \n        # Commit the transaction and close the connection\n        conn.commit()\n        conn.close()\n        \n        return len(df)  # Return the number of rows in the table\n    \n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database error: {e}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {e}\")"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Extract text using OCR\n            ocr_text = pytesseract.image_to_string(img)\n            # Convert the extracted text to the target encoding\n            converted_text = ocr_text.encode(from_encoding).decode(to_encoding)\n            return converted_text\n    except (UnicodeDecodeError, LookupError) as e:\n        # Fallback to image comment processing\n        try:\n            comment = img.info.get(\"comment\", \"\")\n            if comment:\n                converted_comment = comment.decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                return converted_comment\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\")\n    except Exception as e:\n        # If both OCR extraction and comment processing fail, return an empty string\n        return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Initiate HTTP GET request with a timeout of 5 seconds\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Retrieve the response content in raw bytes\n        raw_content = response.content\n\n        # Handle empty content\n        if not raw_content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding is None:\n            detected_encoding = chardet.detect(raw_content)\n            from_encoding = detected_encoding.get('encoding')\n\n            # Raise an exception if encoding cannot be detected\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the raw content using the detected or provided encoding\n        decoded_content = raw_content.decode(from_encoding)\n\n        # Re-encode the content to the desired encoding\n        re_encoded_content = decoded_content.encode(to_encoding)\n\n        # Parse the JSON data\n        json_data = json.loads(re_encoded_content.decode(to_encoding))\n\n        return json_data\n\n    except requests.RequestException as e:\n        # Handle any request exceptions (e.g., connection error, timeout)\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        print(f\"JSON decoding failed: {e}\")\n        return {}\n    except ValueError as e:\n        # Handle value errors (e.g., encoding detection failure)\n        print(f\"Value error: {e}\")\n        raise"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n    column_name (str): The name of the column containing date values.\n    date_format (str): The format of the date values in the CSV file. Defaults to \"%Y-%m-%d\".\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing the filtered and sorted data.\n\n    Raises:\n    FileNotFoundError: If the specified CSV file is not found at the given path.\n    ValueError: If the specified column is not present in the CSV file.\n    \"\"\"\n\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Try to read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # If the file is empty, return an empty DataFrame\n        return pd.DataFrame()\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n\n    # Convert the date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date <= current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Secure the client socket using SSL/TLS\n        ssl_socket = ssl.wrap_socket(client_socket, server_side=True, certfile=cert_file, keyfile=key_file)\n        \n        # Receive the file path request from the client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the file exists\n        if not os.path.exists(file_path):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n        \n        # Open the file and compute the SHA256 hash\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        \n        # Convert the hash to a hexadecimal string\n        hash_hex = sha256_hash.hexdigest()\n        \n        # Send the hash back to the client\n        ssl_socket.sendall(hash_hex.encode('utf-8'))\n        \n        return hash_hex\n    \n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Run a non-blocking echo server that appends the server's current time to received data and sends it back to the client.\n    \n    Parameters:\n    server_address (str): The address of the server. Defaults to \"localhost\".\n    server_port (int): The port of the server. Defaults to 12345.\n    buffer_size (int): The size of the buffer for receiving and sending data. Defaults to 1024.\n    run_duration (int): The duration of the server's operation in seconds. Defaults to 5.\n    \n    Returns:\n    str: A status message indicating the server's operation and run duration.\n    \"\"\"\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n    server.bind((server_address, server_port))\n    server.listen(5)\n\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n\n    timeout = run_duration * 60  # Convert run_duration to seconds\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=timeout)\n\n    while inputs and datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n        for s in readable:\n            if s is server:\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    response = f\"{data.decode()} (Server Time: {current_time})\"\n                    message_queues[s].put(response)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                s.send(next_msg.encode())\n\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    for s in inputs:\n        s.close()\n\n    run_time = datetime.now() - start_time\n    status_message = f\"Server operated for {run_time.total_seconds()} seconds.\"\n    return status_message"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive data from the client socket\n    data = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Prompt for email details\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your email password: \")\n    \n    # Create the email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client socket'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    \n    # Send the email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, password)\n        server.send_message(msg)\n        server.quit()\n        print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Failed to send email: {e}\")"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Check if the file exists\n        try:\n            with open(file_path):\n                pass\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The file at path {file_path} was not found.\")\n\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n\n        # Ensure the DataFrame has at least one column\n        if df.shape[1] == 0:\n            raise Exception(\"The CSV file does not contain any columns.\")\n\n        # Use the first column as text data if no header is present\n        text_column = df.columns[0] if 'Text' not in df.columns else 'Text'\n        text_data = df[text_column]\n\n        # Initialize CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        X = vectorizer.fit_transform(text_data)\n\n        # Get word frequencies\n        word_frequencies = X.toarray().sum(axis=0)\n        word_freq_dict = dict(zip(vectorizer.get_feature_names_out(), word_frequencies))\n\n        # Sort words by frequency and get top 10\n        top_words = sorted(word_freq_dict.items(), key=lambda item: item[1], reverse=True)[:10]\n\n        # Plotting\n        words, frequencies = zip(*top_words)\n        plt.figure(figsize=(10, 5))\n        plt.bar(words, frequencies, color='blue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n\n        # Save or display the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.show()\n            return plt.gca()\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Define default lists for animals and foods\n    default_animals = ['Lion', 'Tiger', 'Bear']\n    default_foods = ['Meat', 'Fish', 'Vegetables']\n    \n    # Handle special cases\n    if animals is None or not animals:\n        animals = default_animals\n    if foods is None or not foods:\n        foods = default_foods\n    \n    # If both lists are empty, return an empty DataFrame\n    if not animals or not foods:\n        return pd.DataFrame()\n    \n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations randomly\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame from the shuffled combinations\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    \n    # Format the cells as 'animal:food'\n    df['Animal:Food'] = df['Animal'] + ':' + df['Food']\n    \n    # Pivot the DataFrame to have animals as rows and foods as columns\n    pivot_df = df.pivot(index='Animal', columns='Food', values='Animal:Food')\n    \n    return pivot_df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Convert each timestamp to the specified timezone\n    tz = pytz.timezone(timezone)\n    time_stamps_tz = [datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n    \n    # Calculate the absolute time difference in seconds between each consecutive pair of timestamps\n    time_diffs = [(b - a).total_seconds() for a, b in zip(time_stamps_tz[:-1], time_stamps_tz[1:])]\n    \n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n    \n    # Calculate the mean (average) time difference in seconds\n    mean_time_diff = np.mean(time_diffs)\n    \n    return mean_time_diff"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation and split into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Extract words and counts for plotting\n    words, counts = zip(*top_10_words)\n    \n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to match URLs\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    \n    # Find the first URL in the string\n    url_match = re.search(url_pattern, myString)\n    \n    if url_match is None:\n        return \"No valid URL found in the provided string.\"\n    \n    url = url_match.group()\n    \n    # Check if the URL is valid\n    try:\n        result = urlparse(url)\n        if not all([result.scheme, result.netloc]):\n            return \"No valid URL found in the provided string.\"\n    except ValueError:\n        return \"No valid URL found in the provided string.\"\n    \n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the HTML content\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find the title tag\n    title = soup.find('title')\n    \n    if title is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title.text.strip()"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Find the URL associated with the specified key\n    url = data.get(unknown_key)\n    if not url:\n        raise KeyError(f\"Key '{unknown_key}' not found in JSON data\")\n    \n    # Get the current timestamp in the specified format\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create the filename with the timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory, default to current working directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    \n    # Ensure the save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Construct the full path for the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an error for bad status codes\n    \n    # Save the file to the specified path\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the downloaded file\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n\n    Args:\n        url (str): The URL of the webpage or local file to extract phone numbers from.\n        output_path (str): The path to the file where the extracted phone numbers will be saved in JSON format.\n\n    Returns:\n        list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Check if the URL is a local file\n    if url.startswith('file://'):\n        # If it's a local file, open it and read the contents\n        with open(url.replace('file://', ''), 'r') as file:\n            html = file.read()\n    else:\n        # If it's a URL, send a GET request and get the HTML response\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure we got the content successfully\n        html = response.text\n\n    # Parse the HTML using BeautifulSoup\n    soup = bs4.BeautifulSoup(html, 'html.parser')\n\n    # Use regular expressions to find phone numbers in the HTML\n    phone_numbers = re.findall(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', soup.get_text())\n\n    # Save the phone numbers to a JSON file\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers"}
