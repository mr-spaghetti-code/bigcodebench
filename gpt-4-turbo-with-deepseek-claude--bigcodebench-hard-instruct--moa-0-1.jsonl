{"task_id": "BigCodeBench/13", "solution": "Here is a Python function that meets your specifications for downloading all files from a specific directory on an FTP server using `wget` via a subprocess. It handles exceptions during connection, login, and directory change, and it returns a list of filenames attempted to be downloaded:\n\n```python\nimport subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        ftp.close()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.close()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Get list of files in the directory\n    try:\n        file_list = ftp.nlst()\n    except Exception as e:\n        ftp.close()\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.close()\n\n    # Construct the FTP URL\n    ftp_url = f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/'\n\n    # Download files using wget\n    downloaded_files = []\n    for file in file_list:\n        file_url = ftp_url + file\n        try:\n            subprocess.run(['wget', '-q', file_url], check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError:\n            print(f\"Failed to download {file}\")\n\n    return downloaded_files\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        downloaded = task_func()\n        print(\"Downloaded files:\", downloaded)\n    except Exception as e:\n        print(str(e))\n```\n\nThis function attempts to connect to the provided FTP server, logs in with the given credentials, and changes to the specified directory. It retrieves a list of files in that directory, constructs an FTP URL for each file, and uses `wget` in a subprocess to download each file. It handles exceptions at each step by closing the FTP connection where necessary and raising an appropriate error message. The function returns a list of filenames that were attempted to be downloaded."}
{"task_id": "BigCodeBench/15", "solution": "Here's a Python function that reads shell commands from a CSV file, executes each command, and saves the output or error to a unique file in the specified output directory. The function raises a `FileNotFoundError` if the commands file does not exist and returns a list of paths to the output files created.\n\n```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the CSV file containing the commands exists\n    if not os.path.isfile(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    # Read commands from the CSV file\n    with open(commands_file_path, 'r') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        for index, row in enumerate(csv_reader, start=1):\n            if row:  # Skip empty rows\n                command = row[0]\n                output_file = os.path.join(output_dir_path, f\"command_{index}_output.txt\")\n                output_files.append(output_file)\n\n                try:\n                    # Execute the command and capture output\n                    result = subprocess.run(command, shell=True, capture_output=True, text=True, check=True)\n                    output = result.stdout\n                except subprocess.CalledProcessError as e:\n                    # If the command fails, capture the error message and exit code\n                    output = f\"Command failed with exit code {e.returncode}\\n\"\n                    output += f\"Error message: {e.stderr}\"\n\n                # Write output to file\n                with open(output_file, 'w') as f:\n                    f.write(output)\n\n    return output_files\n\n# Example usage:\n# output_files = task_func(\"commands.csv\", \"output_directory\")\n# print(output_files)\n```\n\n### Key Functionality:\n1. **File Existence Check**: The function verifies if the commands file exists, raising a `FileNotFoundError` if it does not.\n2. **Directory Creation**: It ensures the specified output directory exists, creating it if necessary.\n3. **Command Execution**: Commands are read from the CSV and executed. Each command's output or error is written to a uniquely named file.\n4. **Error Handling**: If a command execution fails, the error message and exit code are captured and written to the output file.\n5. **Output List**: It returns a list of paths to the files containing the command outputs, facilitating easy access to the results.\n\nThis solution is designed to be robust, handling both successful command executions and errors gracefully, ensuring that all output is captured and stored as specified."}
{"task_id": "BigCodeBench/17", "solution": "Here's a Python function that checks if a specified process is running, starts it if it's not, or restarts it if it is already running. This function utilizes the `psutil` library to manage processes and the `subprocess` module to start processes.\n\n```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    running = False\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            running = True\n            pid = proc.pid\n            break\n    \n    if not running:\n        # Process is not running, start it\n        try:\n            subprocess.Popen([process_name])\n            return f\"Process not found. Starting {process_name}.\"\n        except FileNotFoundError:\n            return f\"Error: {process_name} not found. Unable to start the process.\"\n    else:\n        # Process is running, terminate and restart it\n        try:\n            process = psutil.Process(pid)\n            process.terminate()\n            \n            # Wait for the process to terminate\n            while psutil.pid_exists(pid):\n                time.sleep(0.1)\n            \n            # Start the process again\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n        except psutil.NoSuchProcess:\n            # In case the process disappeared between finding it and trying to terminate it\n            subprocess.Popen([process_name])\n            return f\"Process disappeared. Starting {process_name}.\"\n        except FileNotFoundError:\n            return f\"Error: {process_name} not found. Unable to restart the process.\"\n\n# Example usage:\n# print(task_func(\"notepad.exe\"))  # Replace \"notepad.exe\" with the actual process name you want to manage\n```\n\nThis function does the following:\n1. It checks if the process is running by iterating through all active processes.\n2. If the process is not running:\n   - It attempts to start it and returns a message indicating that the process was started.\n   - If the executable is not found, it returns an error message.\n3. If the process is running:\n   - It terminates the process and waits for it to fully terminate.\n   - It then restarts the process and returns a message indicating that the process was restarted.\n4. The function handles cases where the process might disappear in the middle of operation, ensuring robustness against such edge cases.\n\nMake sure to handle permissions and potential impacts of terminating and restarting processes, as this can affect system stability and data integrity."}
{"task_id": "BigCodeBench/19", "solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n\n    # Get all files in the directory (not including subdirectories)\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]\n\n    # If there are no files, return None\n    if not files:\n        return None\n\n    # Create the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n\n    # Create a zip file and add all files to it\n    with zipfile.ZipFile(zip_path, 'w') as zip_file:\n        for file in files:\n            zip_file.write(file, os.path.basename(file))\n\n    return zip_path\n```\n\nThis Python function:\n- Checks if the specified directory exists, and raises a `FileNotFoundError` if it does not.\n- Collects all files (excluding subdirectories) in the directory.\n- If no files are found, it returns `None`.\n- It then creates a zip file named 'files.zip' in the directory and adds all the files to it.\n- Finally, it returns the path to the created zip file, ensuring the function meets all specified requirements."}
{"task_id": "BigCodeBench/34", "solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text using a regular expression\n    text_without_urls = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove extra whitespace and check if there are any words left\n    cleaned_text = ' '.join(text_without_urls.split())\n    if not cleaned_text.strip():\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud from the cleaned text\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n    # Return the WordCloud object\n    return wordcloud\n\n# Example usage:\n# text = \"This is some sample text with a URL http://example.com and more text.\"\n# word_cloud = task_func(text)\n```\n\nThis Python function `task_func` takes a string `text` as its parameter, removes any URLs, checks if there are any words left to generate a word cloud, and if so, generates and plots the word cloud, then returns the WordCloud object. Ensure you have the `wordcloud` package installed (`pip install wordcloud`) to use this function."}
{"task_id": "BigCodeBench/37", "solution": "```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Initialize and train the RandomForestClassifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n\n    # Retrieve and sort feature importances\n    feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns).sort_values(ascending=False)\n\n    # Plotting the feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=feature_importances, y=feature_importances.index)\n\n    # Setting the labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    plt.tight_layout()  # Adjusts the plot to ensure everything fits without overlap\n\n    # Return the classifier and the Axes object\n    return rf_classifier, ax\n\n# Example usage:\n# Assuming 'df' is your DataFrame and 'target_column' is the name of your target column\n# classifier, ax = task_func(df, 'target_column')\n# plt.show()  # To display the plot\n```\n\nThis code defines a function `task_func` that trains a RandomForestClassifier on a provided DataFrame `df` and the specified target column `target_column`. It then plots the feature importances in a bar plot, with the feature names on the y-axis and their corresponding importance scores on the x-axis. The plot is tailored with appropriate labels and a title, and the function returns both the trained RandomForestClassifier and the Axes object of the plot for further manipulation or display."}
{"task_id": "BigCodeBench/82", "solution": "```python\nfrom flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password_hash = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password_hash, password)\n\n    users = {\n        '1': User('1', 'user1', 'password1'),\n        '2': User('2', 'user2', 'password2'),\n        '3': User('3', 'user3', 'password3')\n    }\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return users.get(user_id)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        if current_user.is_authenticated:\n            return redirect(url_for('protected'))\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = users.get('1')  # Example: Only user1 is able to login\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! This is a protected page.'\n\n    return app\n\n# Example usage:\n# app = task_func('your_secret_key', 'templates')\n# app.run(debug=True)\n```\n\nThis Flask application configures user authentication using Flask-Login. It defines routes for login, logout, and accessing a protected page. The User class handles user data and password checking. The application can be dynamically configured with a secret key and template folder path. The example usage at the end shows how to instantiate and run the application with the required configurations."}
{"task_id": "BigCodeBench/89", "solution": "Certainly! Below is a self-contained Python function that identifies and removes outliers from a specified column of a dataset based on the Z-score. It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a specified threshold. The function also provides visualizations before and after outlier removal to illustrate the impact on the dataset. The output is a tuple containing the original data, the data without outliers, and the indices of the outliers.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Ensure the data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    # Check if column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"The specified column '{column}' does not exist in the DataFrame.\")\n    \n    # Extract the specified column data\n    column_data = data[column].values.reshape(-1, 1)\n    \n    # Standardize the column data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data)\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n    \n    # Identify outlier indices\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers from the data\n    data_without_outliers = data.drop(index=data.index[outlier_indices])\n    \n    # Visualization\n    plt.figure(figsize=(12, 6))\n    \n    # Plot original data with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='blue', alpha=0.6, label='Original Data')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    \n    # Plot data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', alpha=0.6, label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the original data, data without outliers, and indices of the outliers\n    return (data, data_without_outliers, outlier_indices)\n\n# Example usage:\n# Assuming 'df' is your DataFrame and 'column_name' is the column you want to analyze for outliers\n# result = task_func(df, 'column_name', 3)\n# print(\"Original Data:\", result[0])\n# print(\"Data without Outliers:\", result[1])\n# print(\"Outlier Indices:\", result[2])\n```\n\nIn this function, after checking for data integrity and column presence, the specified column's data is standardized and its Z-scores are calculated. Points with a Z-score greater than the threshold are flagged as outliers and removed from the dataset. The function then visualizes the original data and the cleaned data side by side for a clear visual comparison and returns the datasets and outlier indices as a tuple."}
{"task_id": "BigCodeBench/92", "solution": "Here's a complete Python function that performs K-means clustering on a dataset and generates a scatter plot visualizing the clusters and their centroids, with the appropriate exception handling for the input parameters:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"The 'data' parameter must be a pd.DataFrame.\")\n    \n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"The 'n_clusters' parameter must be an integer greater than 1.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(data)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot data points colored by cluster\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis', alpha=0.6)\n    \n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=200, linewidths=3, color='red', label='Centroids')\n    \n    # Set plot labels and title\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    ax.set_title(f'K-means Clustering (k={n_clusters})')\n    \n    # Add legend\n    ax.legend()\n    \n    # Add colorbar\n    plt.colorbar(scatter, label='Cluster')\n    \n    return labels, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate sample data\n    data = pd.DataFrame({\n        'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        'y': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n    })\n    \n    # Call the function\n    try:\n        labels, ax = task_func(data, n_clusters=3)\n        plt.show()\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```\n\nThis function starts by ensuring that the input `data` is a pandas DataFrame and that `n_clusters` is an integer greater than 1. It then uses the scikit-learn `KMeans` algorithm to cluster the data and matplotlib to visualize the results. The function returns a tuple containing the cluster labels and the matplotlib Axes object for further customization or display."}
{"task_id": "BigCodeBench/93", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    # Validate n_components argument\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA on the data\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert the transformed data into a DataFrame\n    column_names = [f'PC{i+1}' for i in range(n_components)]\n    df_transformed = pd.DataFrame(transformed_data, columns=column_names)\n    \n    # Generate a scatter plot of the transformed data\n    fig, ax = plt.subplots(figsize=(8, 6))\n    if n_components >= 2:\n        ax.scatter(df_transformed['PC1'], df_transformed['PC2'], alpha=0.7)\n        ax.set_xlabel('First Principal Component')\n        ax.set_ylabel('Second Principal Component')\n        ax.set_title('Scatter Plot of PCA Results')\n    elif n_components == 1:\n        ax.scatter(range(len(df_transformed)), df_transformed['PC1'], alpha=0.7)\n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel('First Principal Component')\n        ax.set_title('Scatter Plot of PCA Result (1 Component)')\n\n    plt.tight_layout()\n    plt.show()\n    \n    return df_transformed, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some example data\n    np.random.seed(42)\n    example_data = np.random.rand(100, 5)  # 100 samples, 5 features\n\n    # Call the function\n    try:\n        transformed_df, plot_ax = task_func(example_data, n_components=2)\n        print(\"Transformed data:\")\n        print(transformed_df.head())\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```\n\nThis code snippet defines a function `task_func` which takes a dataset and an optional parameter `n_components` (default is 2) and performs PCA on the data. It raises a `ValueError` if `n_components` is not a positive integer. The function generates a DataFrame containing the transformed data and a scatter plot of the first two principal components (or the first component against sample indices if only one component is used). The example usage at the end demonstrates how to use this function with random data, handling exceptions appropriately."}
{"task_id": "BigCodeBench/99", "solution": "Here's a self-contained function that meets your specifications for generating a seaborn pair plot of the iris dataset with the Arial font:\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Set the global font to Arial for better readability and visual appeal\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Create the pair plot\n    g = sns.pairplot(iris_df, hue='species', height=2.5)\n\n    # Set the plot title and format the labels for better readability\n    g.fig.suptitle('Iris Dataset Pair Plot', fontsize=16, y=1.02)\n\n    # Format the labels on each axis\n    for ax in g.axes.flatten():\n        # Format x-axis labels\n        if ax.get_xlabel():\n            ax.set_xlabel(ax.get_xlabel().replace(' (cm)', '').title() + ' (cm)')\n        # Format y-axis labels\n        if ax.get_ylabel():\n            ax.set_ylabel(ax.get_ylabel().replace(' (cm)', '').title() + ' (cm)')\n\n    # Adjust layout to prevent overlap and ensure the title is well positioned\n    plt.tight_layout()\n    g.fig.subplots_adjust(top=0.95)  # Adjust the top margin to accommodate the title\n\n    return g.fig\n\n# Example of how to use the function:\n# fig = task_func()\n# plt.show()\n```\n\nThis function sets the global font to Arial, loads the iris dataset, and creates a seaborn pair plot with each species colored differently. It also sets the title 'Iris Dataset Pair Plot' and adjusts the labels on the axes to include units and improve readability. The function returns a matplotlib `Figure` object containing the seaborn pair plot. To display the plot, you can uncomment the last two lines of the code snippet."}
{"task_id": "BigCodeBench/100", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.now().date()\n        start_date = end_date - timedelta(days=29)\n        dates = [start_date + timedelta(days=i) for i in range(30)]\n\n        # Generate random values\n        values = [random.uniform(0, 100) for _ in range(30)]\n\n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Create the plot\n        plt.figure(figsize=(12, 6))\n        ax = plt.gca()\n\n        # Plot the data\n        ax.plot(df['Date'], df['Value'], marker='o')\n\n        # Customize the plot\n        ax.set_xlabel('Date', fontname='Arial', fontsize=12)\n        ax.set_ylabel('Value', fontname='Arial', fontsize=12)\n        ax.set_title('Random Time Series Data', fontname='Arial', fontsize=14, fontweight='bold')\n\n        # Rotate x-axis labels for better readability\n        plt.xticks(rotation=45, ha='right', fontname='Arial')\n        plt.yticks(fontname='Arial')\n\n        # Adjust layout to prevent cutting off labels\n        plt.tight_layout()\n\n        # Set font for legend and tick labels\n        for label in ax.get_xticklabels() + ax.get_yticklabels():\n            label.set_fontname('Arial')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"Error generating plot: {str(e)}\")\n\n# Example usage:\n# ax = task_func(seed=123)\n# plt.show()\n```\n\nThis function `task_func` follows your specifications by creating a plot of random time series data for the past 30 days. It uses the `seed` parameter to control reproducibility and styles the plot with Arial font for better readability. It raises a `ValueError` if any issues occur during data generation or plotting, ensuring robust error handling. The resulting plot labels dates and values appropriately, with a clear title, and returns the `Axes` object for further manipulation or display."}
{"task_id": "BigCodeBench/101", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    \"\"\"\n    Draw the correlation heatmap of the Boston Housing dataset using Seaborn.\n\n    Args:\n    data_url (str): URL of the Boston Housing dataset.\n    seed (int): Random seed for reproducibility.\n    save_path (str, optional): Path to save the plot. If None, the plot is not saved.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n\n    Raises:\n    ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    try:\n        # Set random seed for reproducibility\n        np.random.seed(seed)\n\n        # Load the Boston Housing dataset\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n\n        # Create a DataFrame with feature names\n        feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n                         'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        df = pd.DataFrame(data, columns=feature_names)\n        df['MEDV'] = target\n\n        # Calculate the correlation matrix\n        corr_matrix = df.corr()\n\n        # Create a new figure and axis\n        fig, ax = plt.subplots(figsize=(12, 10))\n\n        # Draw the heatmap\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n\n        # Set the title\n        ax.set_title('Correlation Heatmap of Boston Housing Dataset')\n\n        # Save the plot if a save path is provided\n        if save_path:\n            plt.savefig(save_path)\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {str(e)}\")\n\n# Example usage:\n# ax = task_func(save_path=\"boston_housing_heatmap.png\")\n# plt.show()\n```\n\nThis function includes all the necessary imports and defines `task_func` to load the Boston Housing dataset, create a correlation heatmap, and optionally save it to a file. It handles exceptions by raising a `ValueError` with an appropriate error message if anything goes wrong during the plot generation or saving. The function returns the `Axes` object containing the heatmap."}
{"task_id": "BigCodeBench/108", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Ensure the DataFrame contains the required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Check if 'value' column is numeric\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must be numeric.\")\n    \n    # Validate freq parameter\n    try:\n        pd.date_range(end='2020-01-01', periods=1, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' must be a valid frequency string.\")\n    \n    # Validate decomposition_model parameter\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n    \n    # Perform the seasonal decomposition\n    result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.infer_freq(df.index) or 1)\n    \n    # Create plot\n    fig, axes = plt.subplots(4, 1, figsize=(10, 8))\n    result.observed.plot(ax=axes[0], title='Observed')\n    result.trend.plot(ax=axes[1], title='Trend')\n    result.seasonal.plot(ax=axes[2], title='Seasonal')\n    result.resid.plot(ax=axes[3], title='Residual')\n    plt.tight_layout()\n\n    return result, axes\n```\n\nThis function begins by confirming that the input `df` is a pandas DataFrame and includes a numeric 'value' column. It checks the validity of the 'freq' and 'decomposition_model' parameters to ensure they adhere to expected values. The function then proceeds to perform seasonal decomposition on the 'value' column and generates a series of plots to visually represent the observed series, trend, seasonality, and residuals. The result of the decomposition and the plot axes are returned as a tuple, allowing for further analysis or customization of the plots if desired."}
{"task_id": "BigCodeBench/120", "solution": "Here is the self-contained Python function `task_func` which generates a pandas Series of random dates within a specified date range. This function includes input validation to ensure the types and values of dates are correct and allows for reproducibility through an optional seed parameter:\n\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate the input dates to ensure they are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n    # Ensure start_date is not after end_date\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must not be later than 'end_date'.\")\n\n    # Set the random seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the inclusive number of days between start_date and end_date\n    date_range = (end_date - start_date).days + 1\n\n    # Generate random dates within the specified inclusive range\n    random_dates = [start_date + timedelta(days=randint(0, date_range - 1)) for _ in range(date_range)]\n\n    # Return a pandas Series containing the random dates\n    return pd.Series(random_dates)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Default usage\n    result = task_func()\n    print(\"Default result:\")\n    print(result.head())\n    print(f\"Series length: {len(result)}\")\n\n    # Custom date range\n    custom_start = datetime(2023, 1, 1)\n    custom_end = datetime(2023, 12, 31)\n    custom_result = task_func(custom_start, custom_end, seed=123)\n    print(\"\\nCustom date range result:\")\n    print(custom_result.head())\n    print(f\"Series length: {len(custom_result)}\")\n\n    # Error handling example\n    try:\n        task_func(start_date=\"2023-01-01\", end_date=datetime(2023, 12, 31))\n    except ValueError as e:\n        print(f\"\\nError caught: {e}\")\n```\n\nThis code not only validates the inputs and generates the series of dates but also provides example usage to demonstrate how to use the function with default parameters, custom ranges, and how it handles errors."}
{"task_id": "BigCodeBench/123", "solution": "Here's a self-contained Python function that meets your specifications. This function will modify the input list by adding the element '12', then concatenate the specified number of CSV files into a single DataFrame, with the number determined by the sum of the elements in the list. It includes the necessary error checks for the type of `my_list` and whether any files are found in the specified directory.\n\n```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The provided 'my_list' is not a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append(12)\n    \n    # Calculate the sum of the elements in the list to determine the number of files to concatenate\n    num_files_to_concat = sum(my_list)\n    \n    # Construct the file path pattern for glob to find matching files\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(file_pattern)\n    \n    # Check if there are any files in the directory\n    if not files:\n        raise FileNotFoundError(f\"No files found in the specified directory: {file_dir}\")\n    \n    # Sort files to ensure the order is consistent\n    files.sort()\n    \n    # Ensure we do not try to concatenate more files than available\n    files_to_read = files[:num_files_to_concat]\n    \n    # Read and concatenate the selected CSV files into a single DataFrame\n    concatenated_df = pd.concat([pd.read_csv(file) for file in files_to_read], ignore_index=True)\n    \n    return concatenated_df\n\n# Example usage:\n# my_list = [1, 2, 3]\n# try:\n#     result_df = task_func(my_list)\n#     print(result_df)\n# except Exception as e:\n#     print(e)\n```\n\nThis code snippet initializes by importing necessary modules and defines `task_func` with appropriate parameters and default values. It checks if the input is a valid list, modifies the list, identifies the requisite number of files, and ensures error handling for both non-existent files and incorrect input types. The function reads the specified number of files, concatenates them into a DataFrame, and returns this DataFrame. You can test the functionality using the commented example usage section, which handles exceptions gracefully."}
{"task_id": "BigCodeBench/124", "solution": "Here's a self-contained Python function that meets your specifications:\n\n```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Validate that my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input 'my_list' must be a list\")\n\n    # Validate that my_list has only numeric elements\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in 'my_list' must be numeric (int or float)\")\n\n    # Append 12 to my_list\n    my_list.append(12)\n\n    # Set the random seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the sum of my_list\n    list_sum = sum(my_list)\n\n    # Determine the size of the random numbers list based on the sum of my_list and the specified size limit\n    random_list_size = min(list_sum, size)\n\n    # Start timing the random number generation\n    start_time = time.time()\n\n    # Generate the list of random integers\n    random_list = [randint(1, 100) for _ in range(random_list_size)]\n\n    # Stop timing the generation process\n    end_time = time.time()\n\n    # Calculate the time taken to generate the random numbers\n    time_taken = end_time - start_time\n\n    # Create a histogram to visualize the distribution of the random numbers\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 102), align='left', rwidth=0.8)\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlim(0, 101)\n\n    # Return the time taken and the matplotlib Axes object for further manipulation or display\n    return (time_taken, ax)\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        my_list = [10, 20, 30]\n        time_taken, ax = task_func(my_list)\n        print(f\"Time taken: {time_taken:.6f} seconds\")\n        plt.show()\n    except (TypeError, ValueError) as e:\n        print(f\"Error: {e}\")\n```\n\nThis Python function starts by ensuring that the input `my_list` is indeed a list and contains only numeric elements. It then appends the number 12 to `my_list`, sets a random seed for reproducibility, and calculates the sum of `my_list`. It limits the size of the random numbers list to the smaller of this sum or the specified `size` parameter. The function generates random integers between 1 and 100, measures the time taken for this process, and finally, plots a histogram of these numbers. The output is a tuple containing the time taken and the `matplotlib` Axes object, which you can display or manipulate further."}
{"task_id": "BigCodeBench/129", "solution": "Here is a self-contained Python function that meets your requirements for scraping the first table from a web page and extracting its data into a Pandas DataFrame. This function ensures that the DataFrame columns are named according to the table's header row, if present. It handles various exceptions to manage connection issues, HTTP errors, and data extraction problems:\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # This will raise an HTTPError for bad responses\n\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the URL\")\n\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP request failed: {e}\")\n\n    try:\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the first table in the HTML\n        table = soup.find('table')\n        if table is None:\n            raise ValueError(\"No table found on the page\")\n\n        # Extract headers (if present)\n        headers = []\n        header_row = table.find('tr')\n        if header_row:\n            headers = [th.text.strip() for th in header_row.find_all('th')]\n            if not headers:  # Check if <th> is absent and <td> is used instead in the header row\n                headers = [td.text.strip() for td in header_row.find_all('td')]\n\n        # Extract table data\n        data = []\n        for row in table.find_all('tr'):\n            row_data = [td.text.strip() for td in row.find_all('td')]\n            if row_data:\n                data.append(row_data)\n\n        if not data:\n            raise ValueError(\"No data found in the table\")\n\n        # Create DataFrame\n        df = pd.DataFrame(data, columns=headers if headers else None)\n        return df\n\n    except Exception as e:\n        raise ValueError(f\"Error parsing table data: {e}\")\n\n# Example of how to use the function\ntry:\n    url = 'https://example.com/page-with-table'\n    df = task_func(url)\n    print(df)\nexcept Exception as e:\n    print(e)\n```\n\n### Key Points of the Function:\n\n1. **HTTP Requests**: The function uses `requests.get` to fetch the page content. It checks for connection errors and HTTP errors.\n2. **HTML Parsing**: BeautifulSoup is used to parse the HTML content and extract data from the first table found.\n3. **Data Extraction**: Table headers are extracted from `<th>` elements or `<td>` elements if `<th>` is absent. The data rows are gathered from `<td>` elements.\n4. **DataFrame Construction**: The data is converted into a Pandas DataFrame. If headers are present, they are used as column names.\n5. **Error Handling**: The function handles and raises appropriate exceptions for connection issues, HTTP errors, and data parsing errors.\n6. **Usage**: The function is demonstrated with an example URL and error handling in the usage section.\n"}
{"task_id": "BigCodeBench/139", "solution": "Here is a self-contained Python function that meets your criteria for drawing histograms of numeric columns in a DataFrame and handling exceptions appropriately:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n    \n    # Check if there are any numeric columns in the DataFrame\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    if numeric_columns.empty:\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n    \n    # Create histograms for each numeric column and collect Axes objects\n    axes_list = []\n    for column in numeric_columns:\n        fig, ax = plt.subplots()\n        ax.hist(df[column].dropna(), bins='auto', edgecolor='black')\n        ax.set_title(column)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n        plt.close(fig)  # Close the figure to prevent display\n    \n    return axes_list\n```\n\nThis function starts by checking if the input is a valid, non-empty DataFrame and if it contains any numeric columns. It then creates a histogram for each numeric column, sets the title, x-axis, and y-axis labels, and stores each `Axes` object in a list which it returns. The figures are closed after creation to prevent them from displaying immediately. This allows you to manage how and when to display or further manipulate these plots."}
{"task_id": "BigCodeBench/147", "solution": "```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    def check_port(ip, port, results):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        results[str(ip)] = result == 0\n        sock.close()\n\n    results = {}\n    threads = []\n\n    for ip in IPv4Network(ip_range):\n        thread = Thread(target=check_port, args=(ip, port, results))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    return results\n\n# Example usage:\nif __name__ == \"__main__\":\n    ip_range = \"192.168.1.0/24\"  # Replace with your desired IP range\n    port = 80  # Replace with the port you want to check\n    result = task_func(ip_range, port)\n    print(result)\n```\n\nIn this function:\n\n1. **Socket Library**: Utilizes Python's `socket` library to handle TCP connections.\n2. **IPv4Network**: Manages IP ranges.\n3. **Threading**: Implements threading to improve the speed of scanning across multiple IPs.\n\nThe function `task_func` takes an IP range in CIDR notation and a port number as inputs. It checks each IP address in the range to see if the specified port is open, returning a dictionary where each key is an IP address and each value is a boolean indicating whether the port is open. Threads are used to perform the checks concurrently, thereby speeding up the scanning process."}
{"task_id": "BigCodeBench/161", "solution": "Here is a Python function that meets your requirements for extracting logging information from a log file and storing the data in a CSV format. This function processes logs in the specified format and ensures that each timestamp is valid. If any issues are found, such as an invalid timestamp or if no valid log entries exist, it raises a `ValueError`.\n\n```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Regular expression pattern to match log entries\n    pattern = r'(\\w+):\\s+\\[(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\\]\\s+-\\s+(.+)'\n    \n    log_data = []\n    \n    try:\n        with open(log_file, 'r') as file:\n            for line in file:\n                match = re.match(pattern, line.strip())\n                if match:\n                    log_type, timestamp, message = match.groups()\n                    \n                    # Validate timestamp\n                    try:\n                        datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    except ValueError:\n                        raise ValueError(f\"Invalid timestamp format in log entry: {line.strip()}\")\n                    \n                    log_data.append({\n                        'Type': log_type,\n                        'Timestamp': timestamp,\n                        'Message': message\n                    })\n    \n        if not log_data:\n            raise ValueError(\"No valid log entries found in the file.\")\n        \n        # Create a DataFrame from the log data\n        df = pd.DataFrame(log_data)\n        \n        # Generate output CSV file name\n        output_file = log_file.rsplit('.', 1)[0] + '_structured.csv'\n        \n        # Save DataFrame to CSV\n        df.to_csv(output_file, index=False)\n        \n        return output_file\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Log file not found: {log_file}\")\n    except PermissionError:\n        raise PermissionError(f\"Permission denied: Unable to read {log_file}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred while processing the log file: {str(e)}\")\n\n# Example usage:\n# try:\n#     result = task_func('path/to/your/logfile.log')\n#     print(f\"Structured log data saved to: {result}\")\n# except Exception as e:\n#     print(f\"Error: {str(e)}\")\n```\n\nThis function will output the path to the newly created CSV file containing structured log data if processed successfully. If any error occurs, such as a file not found or permission issues, it will raise an appropriate exception with a descriptive message."}
{"task_id": "BigCodeBench/162", "solution": "```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Clean the text and split into words using regex to match word boundaries\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Check if there are any words to plot\n    if word_lengths:\n        # Determine the range of word lengths\n        min_length = min(word_lengths)\n        max_length = max(word_lengths)\n        \n        # Create histogram of word lengths\n        n, bins, patches = ax.hist(word_lengths, bins=range(min_length, max_length + 2), \n                                   rwidth=rwidth, align='left', edgecolor='black')\n        \n        # Set labels and title\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths')\n        \n        # Set x-axis ticks to clearly show each word length\n        ax.set_xticks(range(min_length, max_length + 1))\n        \n        # Optionally, add value labels on top of each bar for better visualization\n        for i, v in enumerate(n):\n            ax.text(bins[i] + 0.5, v, str(int(v)), ha='center', va='bottom')\n        \n        # Adjust layout to prevent clipping of tick-labels\n        plt.tight_layout()\n    else:\n        # Display a message if no words are found in the text\n        ax.text(0.5, 0.5, 'No words found in the text', ha='center', va='center', fontsize=12)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    return ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_text = \"This is a sample text to demonstrate the word length distribution function.\"\n    ax = task_func(sample_text)\n    plt.show()\n```\n\nThis Python function `task_func` takes a text string and an optional relative width parameter for bars in the histogram. It analyzes the distribution of word lengths in the text, generating a histogram on a matplotlib Axes object. If no words are found, it displays a message stating \"No words found in the text\" and avoids creating any histogram bins. The function uses regular expressions to extract words and numpy with matplotlib to plot the histogram, offering visualization of word frequency by their lengths."}
{"task_id": "BigCodeBench/177", "solution": "```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef task_func(df):\n    # Check if DataFrame is empty or missing required columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Ensure the 'punkt' tokenizer from NLTK is available\n    try:\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    # Filter articles based on keywords in title\n    keywords = ['like', 'what']\n    filtered_df = df[df['Title'].str.lower().str.contains('|'.join(keywords), regex=True)]\n\n    # Combine all content from filtered articles and remove punctuation\n    all_content = ' '.join(filtered_df['Content'].astype(str))\n    all_content = re.sub(f'[{punctuation}]', '', all_content.lower())\n\n    # Tokenize the content into words\n    words = nltk.word_tokenize(all_content)\n\n    # Count word frequencies\n    word_freq = Counter(words)\n\n    return dict(word_freq)\n```\n\nThis function first checks if the provided DataFrame is empty or lacks the required columns. If so, it raises a `ValueError`. It then ensures that NLTK's 'punkt' tokenizer is available, downloading it if necessary. The function filters articles whose titles contain the keywords \"like\" or \"what\" (case-insensitive), combines their content, removes punctuation, tokenizes the content into words, and finally counts and returns the word frequencies as a dictionary. The output dictionary has words as keys and their frequencies as values, excluding any punctuation from the counts."}
{"task_id": "BigCodeBench/184", "solution": "```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    def preprocess_text(text):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n\n    # Apply preprocessing to the specified text column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Initialize and fit the CountVectorizer\n    vectorizer = CountVectorizer()\n    word_count_matrix = vectorizer.fit_transform(dataframe[text_column])\n\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Convert the matrix to a DataFrame\n    word_count_df = pd.DataFrame(word_count_matrix.toarray(), columns=feature_names)\n\n    return word_count_df\n\n# Example usage:\n# Assuming you have a DataFrame 'df' with a column 'text_column'\n# df = pd.DataFrame({'text_column': ['This is a sample text.', 'Another example sentence.']})\n# result = task_func(df, 'text_column')\n# print(result)\n```\nThis code defines a function `task_func` that preprocesses text data by converting text to lowercase, removing numbers, punctuation, and stopwords, and finally vectorizing the cleaned text using `CountVectorizer` from scikit-learn. The output is a DataFrame with words as columns and their counts as rows, providing a structured format for text analysis."}
{"task_id": "BigCodeBench/187", "solution": "Here's a self-contained Python function using GeoPandas to create a DataFrame for a list of cities with randomly generated coordinates within specified ranges. The function includes robust error handling for input validation:\n\n```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate dictionary for 'Lon' and 'Lat' keys and their tuple type\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Both 'Lon' and 'Lat' keys must be present in the dictionary.\")\n    if not (isinstance(dic['Lon'], tuple) and isinstance(dic['Lat'], tuple)):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples.\")\n    \n    # Generate random coordinates within the specified ranges\n    longitudes = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    latitudes = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n    \n    # Create Point objects for each city based on the generated coordinates\n    geometries = [Point(lon, lat) for lon, lat in zip(longitudes, latitudes)]\n    \n    # Create a GeoDataFrame with 'City' and 'Coordinates' columns\n    gdf = gpd.GeoDataFrame({\n        'City': cities,\n        'Coordinates': geometries\n    }, geometry='Coordinates')\n    \n    return gdf\n\n# Example usage:\ntry:\n    result = task_func()\n    print(result)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n```\n\nIn this function:\n- The dictionary `dic` should contain 'Lon' and 'Lat' keys with their values set as tuples representing the range for longitude and latitude.\n- The list `cities` contains the names of the cities for which coordinates will be generated.\n- The function checks for the presence of required keys and the correct type of their values, raising a `ValueError` if the checks fail.\n- Random coordinates are generated within the given ranges using `np.random.uniform`.\n- `Point` objects are created for each city, and a GeoDataFrame is constructed with city names and their corresponding coordinates.\n- This GeoDataFrame is then returned. If any error occurs, it is caught in the `try-except` block, and an appropriate error message is printed."}
{"task_id": "BigCodeBench/199", "solution": "```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Input validation\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    \n    # Set random seed\n    set_seed(seed)\n    \n    # Create a list to store weather data\n    weather_data = []\n    \n    # Generate weather report for each city\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone not provided for city: {city}\")\n        \n        # Get the timezone for the city\n        tz = pytz.timezone(timezones[city])\n        \n        # Convert UTC time to local time\n        local_time = utc_datetime.replace(tzinfo=pytz.UTC).astimezone(tz)\n        \n        # Format local time as string\n        local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Generate random weather condition\n        weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Add data to the list\n        weather_data.append({\n            'City': city,\n            'Local Time': local_time_str,\n            'Weather Condition': weather\n        })\n    \n    # Create DataFrame from the weather data\n    df = pd.DataFrame(weather_data)\n    \n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    utc_time = datetime(2023, 5, 17, 12, 0, 0)  # Example UTC time\n    result = task_func(utc_time)\n    print(result)\n```\nThis code implements the `task_func` function which generates a weather report for specified cities at a given UTC datetime. The function includes thorough input validation checks and utilizes a random seed for reproducibility. The output is a pandas DataFrame containing the city, local time formatted with the timezone abbreviation, and a randomly selected weather condition."}
{"task_id": "BigCodeBench/208", "solution": "Here is the complete implementation of the `task_func` function that generates and plots a random walk with the given number of steps, and returns the descriptive statistics for the sequence:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    # Input validation\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate cumulative sum for the walk\n    walk = np.cumsum(steps)\n\n    # Calculate descriptive statistics\n    stats = pd.Series(walk).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    stats_dict = {\n        'count': stats['count'],\n        'mean': stats['mean'],\n        'std': stats['std'],\n        'min': stats['min'],\n        '5%': stats['5%'],\n        '25%': stats['25%'],\n        'median': stats['50%'],\n        '75%': stats['75%'],\n        '95%': stats['95%'],\n        'max': stats['max']\n    }\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(range(elements + 1), np.insert(walk, 0, 0), color='blue')\n    ax.set_title(f'Random Walk of {elements} Steps')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.grid(True)\n\n    return stats_dict, ax\n\n# Example usage:\n# stats, ax = task_func(1000)\n# plt.show()\n# print(stats)\n```\n\nThis function starts with required imports and function signature. It validates the input to ensure that `elements` is a positive integer. It sets a random seed for reproducibility and generates a sequence of random steps (-1 or 1). The cumulative sum of these steps is calculated to create the random walk. The function uses pandas to compute descriptive statistics, including specified percentiles, and plots the random walk using matplotlib. Finally, it returns the statistics dictionary and the matplotlib Axes object for the plot. You can use this function by calling it with the number of steps as an argument, and then display the plot and print the statistics as shown in the example usage."}
{"task_id": "BigCodeBench/211", "solution": "Here's a self-contained Python function that meets your requirements to download a zip file from a URL, extract its contents to a specified directory, and return a list of filenames of the extracted files. The zip file is kept in the destination directory after extraction.\n\n```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    # Define the path for the downloaded zip file\n    zip_file_path = os.path.join(destination_directory, 'downloaded.zip')\n\n    try:\n        # Download the zip file\n        with requests.get(url, headers=headers, stream=True) as response:\n            response.raise_for_status()  # Check that the request was successful\n            with open(zip_file_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n        # Extract the contents of the zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n\n        # Create a list of filenames of the extracted files\n        extracted_files = []\n        for root, _, files in os.walk(destination_directory):\n            for file in files:\n                if file != 'downloaded.zip':  # Exclude the zip file itself\n                    extracted_files.append(os.path.join(root, file))\n\n        return extracted_files\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading the zip file: {e}\")\n        return []\n    except zipfile.BadZipFile:\n        print(\"The downloaded file is not a valid zip file.\")\n        return []\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []\n\n# Example usage:\nurl = \"https://example.com/sample.zip\"\ndestination = \"/path/to/extract\"\nextracted_files = task_func(url, destination)\nprint(extracted_files)\n```\n\nThis function checks if the destination directory exists and creates it if necessary, downloads the zip file to the specified directory, extracts its contents, and returns a list of the extracted file paths, excluding the zip file itself. It handles errors such as download issues, invalid zip files, and other exceptions."}
{"task_id": "BigCodeBench/214", "solution": "Here is a self-contained Python function that generates a random RGB image and displays it using `matplotlib`, while ensuring that it handles the exception if `range_low` is not less than `range_high`.\n\n```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Args:\n        seed (int): Random seed for reproducibility.\n        image_size (tuple): Size of the image (height, width, channels).\n        range_low (int): Lower bound of pixel values.\n        range_high (int): Upper bound of pixel values.\n\n    Returns:\n        tuple: (ax, image)\n            ax (matplotlib.axes.Axes): Axes object of the plot.\n            image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate random image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Hide axis details\n    plt.title(\"Random RGB Image\")\n\n    return ax, image\n\n# Example usage\ntry:\n    ax, image = task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255)\n    plt.show()\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\n# Example with invalid range (should raise ValueError)\ntry:\n    ax, image = task_func(seed=42, image_size=(100, 100, 3), range_low=200, range_high=100)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n```\n\nThis function is designed to be reproducible by setting the random seed. It generates a random RGB image within the specified range of pixel values, handles color space conversion using OpenCV, and properly displays the image using `matplotlib`. It also appropriately raises a `ValueError` if the given pixel range is invalid, as demonstrated in the provided usage examples."}
{"task_id": "BigCodeBench/227", "solution": "```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file '{audio_file}' does not exist.\")\n\n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the Sound Pressure Level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    if len(L) != M * N:\n        raise ValueError(\"The length of list L must be exactly M * N\")\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / np.max(np.abs(matrix)) * spl\n\n    # Generate the spectrogram\n    S = librosa.stft(normalized_matrix.flatten())\n    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n\n    # Create the figure and plot the spectrogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img = librosa.display.specshow(S_db, sr=samplerate, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return normalized_matrix, fig\n```\n\nThis implementation includes comprehensive functionality for:\n1. Verifying the existence of the specified audio file, raising a `FileNotFoundError` if it does not exist.\n2. Reading audio data from the file and calculating its SPL.\n3. Creating an MxN matrix from a list of values, ensuring the list length matches the required number of elements for the matrix.\n4. Normalizing this matrix based on the SPL calculated from the audio data.\n5. Generating and displaying a spectrogram from the normalized matrix, with appropriate scaling for frequency and time in the plot.\n\nMake sure the list `L` has exactly `M*N` elements and replace `'path_to_your_audio_file.wav'` with the actual path to your audio file when using the function."}
{"task_id": "BigCodeBench/239", "solution": "Here is a self-contained Python function that performs the described task of extracting numeric values from a list of tuples, computing basic statistics, and generating a histogram with an overlaid probability density function (PDF):\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([value for tup in original for value in tup if isinstance(value, (int, float))])\n\n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'standard_deviation': np.std(numeric_values),\n        'minimum': np.min(numeric_values),\n        'maximum': np.max(numeric_values)\n    }\n\n    # Create histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='skyblue', edgecolor='black')\n    \n    # Calculate and plot PDF\n    x = np.linspace(np.min(numeric_values), np.max(numeric_values), 100)\n    kde = stats.gaussian_kde(numeric_values)\n    ax.plot(x, kde(x), 'r-', lw=2)\n\n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with PDF')\n\n    # Add grid\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    return numeric_values, statistics, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example list of tuples\n    data = [(1, 2.5, 'a'), (3, 4.2, 5), ('b', 6.7, 7), (8.1, 9, 10.5)]\n\n    # Call the function\n    values, stats, axes = task_func(data)\n\n    # Print results\n    print(\"Extracted numeric values:\", values)\n    print(\"Statistics:\", stats)\n\n    # Show the plot\n    plt.show()\n```\n\nThis function extracts the numeric values from a list of mixed-type tuples, computes the basic statistics (mean, standard deviation, minimum, and maximum), plots a histogram of these values, and overlays a probability density function using kernel density estimation. The result is a numpy array of the numeric values, a dictionary containing the computed statistics, and a matplotlib `Axes` object displaying the histogram and PDF. The histogram is plotted with density set to True, alpha set to 0.6, and automatic bin selection (`bins='auto'`)."}
{"task_id": "BigCodeBench/241", "solution": "Certainly! Here's a self-contained Python function that takes a list of numerical values, converts it to a numpy array, normalizes this array, and then plots both the original and normalized arrays using matplotlib. The function also returns the numpy arrays and the matplotlib axes object with the plotted data.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the original array\n    ax.plot(original_array, label='Original', marker='o', color='blue')\n\n    # Plot the normalized array\n    ax.plot(normalized_array, label='Normalized', marker='s', color='green')\n\n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Comparison of Original and Normalized Arrays')\n\n    # Add legend\n    ax.legend()\n\n    # Add grid for better visibility\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    # Display the plot\n    plt.show()\n\n    # Return the original and normalized arrays, and the Axes object\n    return original_array, normalized_array, ax\n\n# Example usage:\n# original_list = [10, 20, 30, 40, 50]\n# original_array, normalized_array, ax = task_func(original_list)\n```\n\nThis function handles the conversion of a list to a numpy array, normalization of this array using `sklearn.preprocessing.normalize`, and plotting of both arrays on a matplotlib figure. Additionally, the axes are labeled, a title is set, a legend is included for clarity, and a grid is added to enhance the visual understanding of the plots. The function returns the original numpy array, the normalized numpy array, and the matplotlib Axes object containing the plotted data. You can test this function by uncommenting the example usage section and providing a list of numbers."}
{"task_id": "BigCodeBench/267", "solution": "Here is a self-contained Python function that meets the specified requirements using `numpy` for numerical operations, `scipy.fftpack` for computing the Fast Fourier Transform, and `matplotlib.pyplot` for plotting. This function will add a new key to the dictionary, generate a signal based on the dictionary's values, perform an FFT on this signal, and plot the FFT. It returns the FFT result and the plot axes as a tuple.\n\n```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # 1. Add a new key \"a\" with value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    # Assume the keys are frequencies and values are amplitudes (if applicable)\n    time = np.arange(0, 1, 1/sample_rate)  # Time vector for 1 second\n    signal = np.zeros_like(time)\n    for frequency, amplitude in data.items():\n        if type(frequency) in [int, float]:  # Ensure that the frequency is numeric\n            signal += amplitude * np.sin(2 * np.pi * frequency * time)\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), 1/sample_rate)\n\n    # 4. Plot the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(freqs[:len(freqs)//2], np.abs(fft_result)[:len(fft_result)//2])  # Plot only the positive frequencies\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('FFT of the Signal')\n\n    # Return the FFT result and the plot axes\n    return fft_result, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    sample_data = {'b': 2, 'c': 3, 'd': 4}\n    \n    # Call the function\n    fft_result, ax = task_func(sample_data)\n    \n    # Display the plot\n    plt.show()\n    \n    # Print the FFT result\n    print(\"FFT Result:\", fft_result)\n```\n\n### Explanation:\n1. **Dictionary Update**: The function begins by adding a key `'a'` with value `1` to the dictionary. This step modifies the input dictionary to include this new key-value pair.\n2. **Signal Generation**: The signal is generated as a sum of sine waves. The keys in the dictionary are treated as frequencies (in Hz), and the values are treated as amplitudes. This is a common practice in signal processing where different frequencies contribute to the overall signal with varying strengths.\n3. **FFT Computation**: The FFT is computed on the generated signal. This transformation is crucial for analyzing the frequency components of the signal.\n4. **Plotting**: The function plots the magnitude of the FFT result against the frequencies. Only positive frequencies are considered as FFT results are symmetric about the Nyquist frequency in real-valued signals.\n\nThe function's output includes both the FFT result and the plot, making it suitable for applications where both numerical and graphical analyses are required."}
{"task_id": "BigCodeBench/273", "solution": "```python\nimport cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    class DataHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            content_type = self.headers.get('Content-Type')\n\n            if content_type != 'application/json':\n                self.send_error(400, \"Content-Type header is not application/json\")\n                return\n\n            try:\n                post_data = self.rfile.read(content_length)\n                json_data = json.loads(post_data.decode('utf-8'))\n\n                if 'data' not in json_data:\n                    self.send_error(400, \"No data key in request\")\n                    return\n\n                # Process the data here (not implemented in this example)\n\n                self.send_response(200)\n                self.send_header('Content-Type', 'application/json')\n                response = json.dumps(SUCCESS_RESPONSE)\n                self.send_header('Content-Length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON\")\n\n    return DataHandler\n\n# Usage example:\nif __name__ == \"__main__\":\n    handler = task_func()\n    server = http.server.HTTPServer(('localhost', 8000), handler)\n    print(\"Server started on http://localhost:8000\")\n    server.serve_forever()\n```\n\nThis code snippet defines a `DataHandler` class within the `task_func` function, inheriting from `http.server.BaseHTTPRequestHandler`. The `do_POST` method is designed to handle POST requests:\n\n1. It initially checks if the `Content-Type` header is 'application/json'. If not, it sends a 400 error with a specific message.\n2. It attempts to parse the JSON from the request body. If it's not valid JSON, it sends a 400 error with a corresponding message.\n3. It checks if the JSON object contains the 'data' key. If absent, it sends a 400 error with a related message.\n4. If all checks are passed, it sends a 200 success response with the `SUCCESS_RESPONSE` message.\n\nThe server is set up to run on localhost port 8000 and will respond according to the conditions defined in the request handler."}
{"task_id": "BigCodeBench/274", "solution": "```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length).decode('utf-8')\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_error(400, \"Bad Request: Invalid JSON\")\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            if not all(key in email_data for key in required_keys):\n                self.send_error(400, \"Bad Request: Missing required keys\")\n                return\n\n            subject = email_data['subject']\n            message = email_data['message']\n            recipient = email_data['to']\n\n            msg = MIMEText(message)\n            msg['Subject'] = subject\n            msg['From'] = smtp_username\n            msg['To'] = recipient\n\n            try:\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n\n                response = \"Email sent successfully\"\n                self.send_response(200)\n                self.send_header('Content-type', 'text/plain')\n                self.send_header('Content-length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication Failed\")\n            except Exception as e:\n                self.send_error(500, f\"Internal Server Error: {str(e)}\")\n\n    return EmailHandler\n\n# Example usage:\nif __name__ == \"__main__\":\n    smtp_server = \"smtp.example.com\"\n    smtp_port = 587\n    smtp_username = \"your_username@example.com\"\n    smtp_password = \"your_secure_password\"\n\n    handler_class = task_func(smtp_server, smtp_port, smtp_username, smtp_password)\n    server = http.server.HTTPServer(('localhost', 8000), handler_class)\n    print(\"Server running on port 8000\")\n    server.serve_forever()\n```\nThis Python script creates a self-contained HTTP server that can process incoming email data through POST requests, validate the JSON payload, and send emails using the provided SMTP settings. The script handles various errors by sending appropriate HTTP response codes and ensures that the response type and content length are correctly set and sent back to the client."}
{"task_id": "BigCodeBench/287", "solution": "```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    # Initialize a Counter to store word counts for each file\n    word_counts = Counter()\n    \n    # Initialize total word count\n    total_words = 0\n    \n    # Iterate through files in the specified directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                \n                # Read the content of the file\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    \n                    # Split content into words and count them\n                    words = content.split()\n                    file_word_count = len(words)\n                    \n                    # Update the Counter and total word count\n                    word_counts[file_path] = file_word_count\n                    total_words += file_word_count\n    \n    # Export word counts to a JSON file\n    with open(os.path.join(directory, filename), 'w', encoding='utf-8') as json_file:\n        json.dump(dict(word_counts), json_file, indent=4)\n    \n    # Return the total number of words\n    return total_words\n```\n\nThis function will traverse all directories and subdirectories starting from the specified `directory`, count the words in each `.txt` file, store these counts in a `Counter` object, and then write the counts to a JSON file named `filename` in the specified `directory`. It returns the total number of words found in all `.txt` files combined."}
{"task_id": "BigCodeBench/302", "solution": "Here's a self-contained Python function that processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns, calculates the Pearson correlation coefficient between these columns, and optionally visualizes the correlation matrix using a heatmap:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Validate the input DataFrame\n    if df.empty or not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame is empty or missing required columns.\")\n\n    # Ensure that the 'Value' column contains only lists\n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"'Value' column must contain lists.\")\n\n    # Split lists in 'Value' column into separate columns\n    max_length = max(len(val) for val in df['Value'])\n    split_df = pd.DataFrame(df['Value'].tolist(), columns=[f'Value_{i+1}' for i in range(max_length)])\n    \n    # Calculate the Pearson correlation coefficient\n    correlation_matrix = split_df.corr(method='pearson')\n\n    # Visualize the correlation matrix if plot is True\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n        plt.title(\"Correlation Heatmap\")\n        plt.tight_layout()\n        plt.show()\n        return correlation_matrix, ax\n\n    return correlation_matrix\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Date': ['2023-01-01', '2023-01-02', '2023-01-03'],\n#     'Value': [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# })\n# result = task_func(df, plot=True)\n# print(result)\n```\n\nThis function:\n1. Checks if the DataFrame is empty or does not have the required columns.\n2. Verifies that each item in the 'Value' column is a list.\n3. Splits these lists into separate columns.\n4. Calculates the Pearson correlation coefficient for these columns.\n5. Optionally plots the correlation matrix with the title \"Correlation Heatmap\" if `plot=True`.\n\nThe function returns the correlation matrix as a DataFrame, and if plotting is enabled, it also returns the matplotlib Axes object containing the heatmap plot."}
{"task_id": "BigCodeBench/308", "solution": "```python\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with any additional fields provided\n    all_fields = FIELDS + additional_fields\n\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n\n    # Create a DataFrame with the generated data\n    df = pd.DataFrame(data, index=STUDENTS)\n\n    # Calculate the average grade for each student and add it as a new column\n    df['Average Grade'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject and add it as a new row\n    subject_averages = df.mean()\n    subject_averages.name = 'Average'\n\n    # Append the subject averages as a new row in the DataFrame\n    df = df.append(subject_averages)\n\n    return df\n\n# Example of how to use the function without additional fields\nresult = task_func()\nprint(result)\n\n# Example of how to use the function with additional fields\nresult_with_additional_fields = task_func(['Art', 'Music'])\nprint(result_with_additional_fields)\n```\nThis Python code defines a function `task_func` that can generate a report on students' grades across various subjects, calculate the average grade per student and per subject, and return the results in a pandas DataFrame. The function allows for the dynamic addition of subjects beyond the predefined list. The output DataFrame lists each student as an index with their grades and average grade, and includes a row for the average grade per subject."}
{"task_id": "BigCodeBench/310", "solution": "```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate simulated data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f\"Person_{i + 1}\"\n        age = random.randint(18, 80)\n        height = round(random.uniform(150, 200), 1)  # in cm\n        weight = round(random.uniform(45, 120), 1)  # in kg\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = mean([row[1] for row in data])\n    avg_height = mean([row[2] for row in data])\n    avg_weight = mean([row[3] for row in data])\n\n    # Write data to CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n        \n        # Append averages\n        writer.writerow(['Average', round(avg_age, 1), round(avg_height, 1), round(avg_weight, 1)])\n\n    return os.path.abspath(filename)\n\n# Example usage\noutput_file = task_func('people_data.csv')\nprint(f\"CSV file created at: {output_file}\")\n```\n\nThis Python script uses the `task_func` function to generate and write simulated data for 100 people to a CSV file. Each person's data includes a name, age, height, and weight. After generating the data, the function calculates and appends the averages for age, height, and weight to the end of the CSV file. The function returns the absolute path of the created CSV file, making it easy to locate the file after its creation."}
{"task_id": "BigCodeBench/313", "solution": "Here is a Python function designed to organize files in a directory based on the first text that is not enclosed in square brackets. Files matching the criteria are moved to subdirectories named after the found text. The function returns a tuple containing the directory path and a dictionary detailing the subdirectories and their files.\n\n```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Dictionary to store subdirectories and the files moved to them\n    organized_files = {}\n\n    # Regular expression to find the first text not enclosed in square brackets\n    pattern = re.compile(r'(?<!\\[)[^\\[\\]]+(?!\\])')\n\n    # Process each file in the directory\n    for filename in os.listdir(directory):\n        # Construct the full file path\n        filepath = os.path.join(directory, filename)\n\n        # Skip if it's not a file\n        if not os.path.isfile(filepath):\n            continue\n\n        # Use regex to find matching text\n        match = pattern.search(filename)\n\n        if match:\n            subdirectory_name = match.group().strip()\n\n            # Create subdirectory if it doesn't exist\n            subdirectory_path = os.path.join(directory, subdirectory_name)\n            if not os.path.exists(subdirectory_path):\n                os.makedirs(subdirectory_path)\n\n            # Move file to the subdirectory\n            new_filepath = os.path.join(subdirectory_path, filename)\n            shutil.move(filepath, new_filepath)\n\n            # Track moved files\n            if subdirectory_name not in organized_files:\n                organized_files[subdirectory_name] = []\n            organized_files[subdirectory_name].append(filename)\n\n    # Generate a summary path with a timestamp to ensure uniqueness\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    organized_directory = f\"{directory}_organized_{timestamp}\"\n\n    # Return the path and the organization details\n    return (organized_directory, organized_files)\n\n# Example usage:\n# result = task_func(\"/path/to/your/directory\")\n# print(result)\n```\n\nThis function iterates over each file in the specified directory, applies a regular expression to extract the first text not enclosed in square brackets, and moves the file to a corresponding subdirectory. If no matching text is found, the file remains in its original location. The function returns a tuple with the new directory path and a dictionary detailing the organization."}
{"task_id": "BigCodeBench/324", "solution": "Here's a self-contained Python function that runs each file in the given `file_list` as a separate subprocess simultaneously using threading. The function will output a list of exit codes corresponding to each subprocess. This implementation assumes that the files are Python scripts.\n\n```python\nimport subprocess\nimport threading\n\ndef task_func(file_list):\n    exit_codes = [None] * len(file_list)  # Initialize list to store exit codes\n    \n    def run_file(index, file_path):\n        # Run the file as a subprocess and capture the exit code\n        process = subprocess.Popen(['python', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()  # Wait for the subprocess to finish\n        exit_codes[index] = process.returncode  # Store the exit code at the corresponding index\n\n    threads = []\n    for i, file_path in enumerate(file_list):\n        # Create a new thread for each file in the list\n        thread = threading.Thread(target=run_file, args=(i, file_path))\n        threads.append(thread)\n        thread.start()  # Start the thread\n\n    for thread in threads:\n        thread.join()  # Wait for all threads to complete\n\n    return exit_codes\n\n# Example usage:\nif __name__ == \"__main__\":\n    files = ['script1.py', 'script2.py', 'script3.py']\n    exit_codes = task_func(files)\n    print(\"Exit codes:\", exit_codes)\n```\n\n### Explanation:\n1. **Initialization**: The `exit_codes` list is initialized to store the exit codes for each subprocess. Each element is initially set to `None` and corresponds to a file in the `file_list`.\n2. **Subprocess Execution**: The `run_file` function is defined to run a file using `subprocess.Popen`. It captures the exit code and stores it in the `exit_codes` list at the appropriate index.\n3. **Threading**: For each file in the `file_list`, a new thread is created to execute the `run_file` function. This allows multiple files to be run concurrently.\n4. **Thread Management**: The main thread waits for all the subprocess threads to complete execution by calling `join()` on each thread.\n5. **Return**: The function returns the list of exit codes once all threads have completed. \n\nThis function will correctly execute Python scripts concurrently and collect their exit codes. Adjustments may be needed if running scripts or executables that require different commands or parameters."}
{"task_id": "BigCodeBench/326", "solution": "```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    # Ensure the directory path exists\n    if not os.path.isdir(directory_path):\n        print(f\"Error: '{directory_path}' is not a valid directory.\")\n        return []\n\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    results = []\n\n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Run the .bat file\n            if sys.platform == \"win32\":\n                # On Windows, use shell=True to run .bat files\n                process = subprocess.run(bat_file, shell=True, check=True)\n            else:\n                # On non-Windows systems, use 'cmd /c' to run .bat files\n                process = subprocess.run([\"cmd\", \"/c\", bat_file], check=True)\n            \n            exit_code = process.returncode\n        except subprocess.CalledProcessError as e:\n            # If the process fails, capture the non-zero exit code\n            exit_code = e.returncode\n        except Exception:\n            # If the file couldn't be executed for any other reason, set exit_code to None\n            exit_code = None\n\n        results.append((file_name, exit_code))\n\n    return results\n\n# Example usage:\nif __name__ == \"__main__\":\n    directory = r\"C:\\path\\to\\your\\directory\"  # Replace with your directory path\n    result = task_func(directory)\n    for file_name, exit_code in result:\n        print(f\"File: {file_name}, Exit Code: {exit_code}\")\n```\n\nThis function checks if the provided directory path is valid, finds all `.bat` files within it, and attempts to run each file, capturing the exit code. If the file could not be executed, the exit code is set to `None`. The results are returned as a list of tuples, each containing the file name and its exit code. Note that running `.bat` files on non-Windows systems may require additional configuration."}
{"task_id": "BigCodeBench/341", "solution": "```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Validate input requirements\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' does not exist in the DataFrame\")\n\n    # Determine if the column data is numeric or categorical\n    is_numeric = pd.api.types.is_numeric_dtype(df[col])\n\n    # Prepare figure and subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n\n    # First subplot: Histogram with KDE for numeric data, Bar plot for categorical data\n    if is_numeric:\n        sns.histplot(data=df, x=col, kde=True, ax=ax1)\n        ax1.set_title(f'Histogram with KDE for {col}')\n    else:\n        sns.countplot(data=df, x=col, ax=ax1)\n        ax1.set_title(f'Bar Plot for {col}')\n    ax1.set_xlabel(col)\n    ax1.set_ylabel('Frequency')\n\n    # Second subplot: Box plot\n    sns.boxplot(data=df, x=col, ax=ax2)\n    ax2.set_title(f'Box Plot for {col}')\n\n    # Adjust layout for clear visualization\n    plt.tight_layout()\n\n    # Return the matplotlib figure object\n    return fig\n```\n\nThis function `task_func` is designed to validate the input DataFrame and column name, generate appropriate plots based on data type, and return a matplotlib figure. It effectively handles both numeric and categorical data, ensuring the plots are meaningful for the data type of the specified column. The function raises `ValueError` if the input conditions are not met, ensuring robust error handling."}
{"task_id": "BigCodeBench/346", "solution": "Here is a self-contained Python function that meets your specifications for running a Python script as a subprocess with predefined arguments. It includes error handling for missing files and subprocess exceptions, and it allows for both synchronous and asynchronous execution based on the `wait` parameter.\n\n```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    # Prepare the command to execute the Python script with provided arguments\n    command = [sys.executable, script_path] + list(args)\n\n    try:\n        # If wait is True, use subprocess.run to wait for the process to complete\n        if wait:\n            result = subprocess.run(command, check=True, capture_output=True, text=True)\n            return result.returncode  # Return the return code of the subprocess\n        else:\n            # If wait is False, use subprocess.Popen to start the process and return None\n            subprocess.Popen(command, start_new_session=True)\n            return None\n    except subprocess.CalledProcessError as e:\n        # Handle the exception raised if the subprocess exits with a non-zero status\n        print(f\"Script output (stdout):\\n{e.stdout}\")\n        print(f\"Script output (stderr):\\n{e.stderr}\")\n        raise  # Re-raise the exception to indicate failure\n\n# Example usage of the task_func\nif __name__ == \"__main__\":\n    # Path to a Python script and arguments\n    script_path = \"example_script.py\"\n    args = (\"arg1\", \"arg2\")\n\n    # Example: Running a script with wait=True\n    try:\n        return_code = task_func(script_path, True, *args)\n        print(f\"Return code: {return_code}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    # Example: Running a script with wait=False\n    try:\n        result = task_func(script_path, False, *args)\n        print(f\"Return result: {result}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```\n\n### Key Functionality:\n- **File Check**: The function first checks if the provided script path points to an existing file. If not, it raises a `ValueError`.\n- **Command Preparation**: It constructs a command list that includes the Python interpreter (from `sys.executable`), the script path, and any additional arguments.\n- **Subprocess Execution**: Depending on the `wait` parameter, it either waits for the process to complete using `subprocess.run` (and returns the process's return code) or starts the process with `subprocess.Popen` and returns `None`.\n- **Error Handling**: If the subprocess exits with a non-zero status, a `subprocess.CalledProcessError` is raised. The error details are printed before re-raising the exception to inform the caller of the function.\n\nThis function provides a flexible way to execute Python scripts as subprocesses, suitable for both blocking and non-blocking scenarios."}
{"task_id": "BigCodeBench/360", "solution": "Below is a self-contained Python function that meets your requirements. This function loads data from an Excel spreadsheet, calculates the mean and standard deviation of each column, and draws a bar chart to visualize these statistics. It handles exceptions for non-existent files and sheets, and returns both the statistics and the matplotlib figure object.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the Excel file exists at the specified location\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    # Attempt to read the specified sheet from the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\")\n    \n    # Calculate the mean and standard deviation for each column in the DataFrame\n    stats = {column: {'mean': df[column].mean(), 'std': df[column].std()} for column in df.columns}\n    \n    # Create a bar chart with the calculated statistics\n    fig, ax = plt.subplots(figsize=(10, 6))\n    columns = np.arange(len(df.columns))\n    bar_width = 0.35\n\n    # Plotting means and standard deviations\n    means = [stats[col]['mean'] for col in df.columns]\n    stds = [stats[col]['std'] for col in df.columns]\n\n    ax.bar(columns - bar_width/2, means, width=bar_width, label='Mean', color='b', yerr=stds, capsize=5)\n    ax.bar(columns + bar_width/2, stds, width=bar_width, label='Standard Deviation', color='r')\n\n    # Setting the labels and title for the chart\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(columns)\n    ax.set_xticklabels(df.columns, rotation=45, ha='right')\n    ax.legend()\n\n    plt.tight_layout()\n\n    return stats, fig\n```\n\n### Usage Example:\nTo use this function, you can call it with the path to your Excel file and the sheet name. Here's an example of how you might call the function and handle potential errors:\n\n```python\nfile_path = 'path/to/your/excel/file.xlsx'\nsheet_name = 'Sheet1'\n\ntry:\n    stats, fig = task_func(file_path, sheet_name)\n    print(stats)\n    plt.show()  # This will display the figure\nexcept (FileNotFoundError, ValueError) as e:\n    print(f\"An error occurred: {e}\")\n``` \n\nThis approach ensures that the function is robust, handles errors gracefully, and provides clear visual and data outputs."}
{"task_id": "BigCodeBench/367", "solution": "Here's a Python function that meets your requirements. The function, `task_func`, takes a list of `datetime` objects, verifies the type of each object, counts the activities per day, and then plots this data using `matplotlib`. If any item in the list is not a `datetime` object, it raises a `TypeError`.\n\n```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n\n    # Count activities for each day of the week\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_name = activity.strftime('%A')\n        day_counts[day_name] += 1\n\n    # Define the order of days\n    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    # Create lists for x-axis labels and y-axis values\n    days = [day for day in days_order]\n    counts = [day_counts[day] for day in days_order]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n\n    # Customize the chart\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n\n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n\n    return ax\n```\n\nThis function ensures that all elements in the `activities` list are `datetime` objects. It uses a `defaultdict` to count the number of activities for each day of the week. It then plots these counts on a bar chart with appropriate labels and a title. The x-axis labels are rotated for better readability, and the layout is adjusted to prevent cutting off labels."}
{"task_id": "BigCodeBench/368", "solution": "Here's a complete Python function that moves a random file from a specified source directory to a destination directory, and returns the name of the file moved:\n\n```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Check if the source directory exists\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n\n    # Check if the destination directory exists, create it if it doesn't\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n\n    # Get list of files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n\n    # Check if there are any files to move\n    if not files:\n        raise FileNotFoundError(f\"No files found in source directory '{src_dir}'.\")\n\n    # Select a random file\n    file_to_move = random.choice(files)\n\n    # Define source and destination paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the moved file\n    return file_to_move\n```\n\nThis function includes error handling for non-existent directories and empty source directories, ensuring that it behaves robustly under various conditions. The use of a seed for the random number generator allows for reproducibility of the file selection when needed."}
{"task_id": "BigCodeBench/374", "solution": "```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Ensure the directory path ends with a slash\n    if not directory_path.endswith('/'):\n        directory_path += '/'\n\n    # Get all .xlsx files in the specified directory\n    xlsx_files = glob.glob(directory_path + '*.xlsx')\n    processed_files = 0\n\n    for file_path in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file_path)\n\n        # Flag to check if any changes were made\n        changes_made = False\n\n        # Iterate through all sheets in the workbook\n        for sheet in workbook.sheetnames:\n            worksheet = workbook[sheet]\n\n            # Iterate through all cells in the worksheet\n            for row in worksheet.iter_rows():\n                for cell in row:\n                    if cell.data_type == 's':  # Check if cell contains a string\n                        # Use regex to find and replace unescaped double quotes\n                        new_value = re.sub(r'(?<!\\\\)\"', r'\\\\\"', cell.value)\n                        if new_value != cell.value:\n                            cell.value = new_value\n                            changes_made = True\n\n        # Save the workbook if changes were made\n        if changes_made:\n            workbook.save(file_path)\n            processed_files += 1\n\n    return processed_files\n```\n\nThis function searches through all `.xlsx` files in the specified directory, and for each file, it checks every string cell to see if it contains double quotes that are not already escaped. If such quotes are found, they are replaced with escaped double quotes (`\\\\\"`). The workbook is then saved only if at least one change was made. The function returns the count of processed files where changes have occurred."}
{"task_id": "BigCodeBench/399", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Create a diagram of a sine wave and cosine wave with a given frequency.\n\n    Parameters:\n    frequency (float): The frequency of the waves.\n    sample_size (int, optional): The number of points to sample. Defaults to 10000.\n\n    Returns:\n    tuple: (matplotlib.figure.Figure, matplotlib.axes.Axes)\n\n    Raises:\n    ValueError: If the frequency is negative or if the sample size is non-positive.\n    \"\"\"\n    # Validate input parameters\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    # Generate time values\n    t = np.linspace(0, 2 * np.pi, sample_size)\n    \n    # Calculate sine and cosine waves\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    \n    # Set labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves (Frequency: {frequency} Hz)')\n    \n    # Add legend\n    ax.legend()\n    \n    # Add grid\n    ax.grid(True)\n    \n    # Set y-axis limits\n    ax.set_ylim(-1.1, 1.1)\n    \n    # Return the figure and axes object\n    return fig, ax\n\n# Example usage:\n# fig, ax = task_func(1)  # Creates a plot with frequency 1 Hz\n# plt.show()  # Displays the plot\n```\nThis function generates and returns a plot of sine and cosine waves given a specific frequency and sample size. It ensures validity of the input parameters by checking for negative frequencies and non-positive sample sizes, raising appropriate exceptions if these conditions are met. The plot includes labels, a title, a legend, and a grid for clarity and visual appeal. Adjustments like setting y-axis limits are made to ensure the waves are fully visible on the plot."}
{"task_id": "BigCodeBench/401", "solution": "```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create Flask app\n    app = Flask(app_name)\n\n    # Configure mail settings\n    mail_config = {\n        'MAIL_SERVER': os.getenv('MAIL_SERVER', 'localhost'),\n        'MAIL_PORT': int(os.getenv('MAIL_PORT', 25)),\n        'MAIL_USE_TLS': os.getenv('MAIL_USE_TLS', 'False').lower() in ('true', '1', 't'),\n        'MAIL_USERNAME': os.getenv('MAIL_USERNAME', None),\n        'MAIL_PASSWORD': os.getenv('MAIL_PASSWORD', None)\n    }\n\n    # Update app config with mail settings\n    app.config.update(mail_config)\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Return a tuple containing the Flask-Mail instance and the app's mail configurations\n    return mail, app.config\n```"}
{"task_id": "BigCodeBench/409", "solution": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full file path\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The Excel file '{full_path}' does not exist.\")\n    \n    # Load the Excel file\n    df = pd.read_excel(full_path)\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Extract the data from the specified column\n    data = df[column_name]\n    \n    # Calculate mean, median, and standard deviation\n    mean = np.mean(data)\n    median = np.median(data)\n    std_dev = np.std(data)\n    \n    # Create and return the result dictionary\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"standard_deviation\": std_dev\n    }\n\n# Example usage:\n# try:\n#     result = task_func(\"/path/to/excel/folder\", \"data.xlsx\", \"Column1\")\n#     print(result)\n# except (FileNotFoundError, ValueError) as e:\n#     print(f\"Error: {str(e)}\")\n```\n\nThis function efficiently constructs a path, checks for file existence and column presence, and calculates statistical measures, handling potential errors gracefully."}
{"task_id": "BigCodeBench/417", "solution": "```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01, epochs=50):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Create the Sequential model with one hidden layer\n    model = Sequential([\n        Dense(4, activation='sigmoid', input_dim=2),  # Hidden layer with 4 neurons\n        Dense(1, activation='sigmoid')  # Output layer\n    ])\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer\n    model.compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy')\n\n    # Train the model on the training data, also using test set as validation data\n    history = model.fit(X_train, Y_train, epochs=epochs, verbose=0, validation_data=(X_test, Y_test))\n\n    # Plot the training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    # Return the trained model and the Axes object of the plot\n    return model, ax\n```\n\nThis function takes input data `X` and target labels `Y`, splits them into training and test sets, constructs a Sequential model with a hidden layer, compiles the model, trains it, and plots the training and validation loss. The function provides a straightforward way to train a neural network on binary classification problems where the input has an assumed dimension of 2. After training, it returns the model and the plot's Axes object for further customization."}
{"task_id": "BigCodeBench/418", "solution": "Here is a self-contained Python function that meets all the specified requirements for processing the input data, creating and training a Keras model, and plotting the ROC curve:\n\n```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer\n    model = keras.Sequential([\n        keras.layers.Dense(units=64, activation='sigmoid', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(units=1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    # Fit the model to the training data\n    model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n\n    # Make predictions on the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n\n    # Calculate ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.set_title('ROC Curve')\n    ax.legend(loc=\"lower right\")\n\n    return model, ax\n```\n\nThis function accomplishes the following:\n\n1. Splits the input data `X` and labels `Y` into training and test sets with a 70-30 split.\n2. Creates a Keras Sequential model with one hidden layer of 64 units, each using a sigmoid activation function.\n3. Compiles the model using an SGD optimizer with a specified learning rate of 0.01 and binary cross-entropy loss.\n4. Trains the model on the training data for 100 epochs with a batch size of 32, in non-verbose mode to suppress the training output.\n5. Makes predictions on the test set and calculates the ROC curve and the AUC score.\n6. Plots the ROC curve, sets the plot labels and title, and includes the AUC score in the legend.\n7. Returns the trained Keras model and the matplotlib Axes object for the ROC curve plot."}
{"task_id": "BigCodeBench/424", "solution": "```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Validate existence of the image file\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"Image file not found at {image_path}\")\n    \n    # Validate n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"Unable to read image at {image_path}\")\n\n    # Convert BGR to RGB as the function assumes image is in RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixels = image_rgb.reshape((-1, 3))\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n\n    # Get the labels and cluster centers\n    labels = kmeans.labels_\n    centers = np.uint8(kmeans.cluster_centers_)\n\n    # Create the segmented image\n    segmented = centers[labels].reshape(image_rgb.shape)\n\n    # Save each region as a separate image\n    for i in range(n_clusters):\n        mask = (labels == i).reshape(image_rgb.shape[:2])\n        region = np.zeros_like(image_rgb)\n        region[mask] = segmented[mask]\n        cv2.imwrite(f'region_{i + 1}.jpg', cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n\n    return (image_rgb, segmented)\n\n# Example usage:\n# original, segmented = task_func('path/to/your/image.jpg', n_clusters=5, random_seed=42)\n```\n\nThis code provides a detailed implementation of the specified task. It begins by checking the validity of the input parameters and the existence of the image file. It reads the image, ensures it is in the correct RGB format, and then performs K-means clustering on the pixels. The resultant clusters are used to segment the image, saving each distinct region as a separate file, and finally returning the original and segmented images as numpy arrays."}
{"task_id": "BigCodeBench/443", "solution": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Calculate the product of matrix P and tensor T using np.tensordot\n    result = np.tensordot(P, T, axes=([1], [0]))\n\n    # Flatten the result\n    flattened_result = result.flatten()\n\n    # Reshape the flattened result for KMeans (each sample should be a row)\n    X = flattened_result.reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(X)\n\n    # Visualize the clustering\n    fig, ax = plt.subplots(figsize=(10, 6))\n    scatter = ax.scatter(X, np.zeros_like(X), c=cluster_result, cmap='viridis', alpha=0.6)\n    \n    # Add colorbar\n    colorbar = plt.colorbar(scatter)\n    colorbar.set_label('Cluster')\n\n    # Set title and labels\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Flattened Data')\n    ax.set_yticks([])  # Remove y-axis ticks as they are not meaningful in this context\n\n    return cluster_result, ax\n\n# Example usage:\n# P = np.random.rand(3, 4)\n# T = np.random.rand(4, 5, 6)\n# cluster_result, ax = task_func(P, T)\n# plt.show()\n```\n\nThis function computes the product of matrix `P` and a 3D tensor `T` using `np.tensordot`, ensuring that dimensions align properly for multiplication. The result is then flattened and reshaped to fit the requirements of KMeans clustering. The clustering result is visualized on a scatter plot, where each point's color indicates its cluster, providing an intuitive visual representation of the data segmentation."}
{"task_id": "BigCodeBench/445", "solution": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Input validation\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input 'points' must be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input 'points' must be a 2D array with shape (n, 2)\")\n    if not isinstance(seed, int):\n        raise TypeError(\"Input 'seed' must be an integer\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Apply jittering to avoid colinear points and degenerate cases\n    jitter = np.random.normal(0, 1e-8, points.shape)\n    jittered_points = points + jitter\n\n    # Calculate Voronoi diagram\n    vor = Voronoi(jittered_points)\n\n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=2)\n    ax.plot(points[:, 0], points[:, 1], 'ko')\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_aspect('equal')\n\n    plt.show()\n\n    return vor, ax\n\n# Example usage:\n# points = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [0.5, 0.5]])\n# vor, ax = task_func(points)\n```\n\nThis Python function `task_func` calculates and plots the Voronoi diagram for a set of 2D points. It validates the input to ensure the points are in a suitable format and applies a small jitter to avoid degenerate cases in the Voronoi computation. The function plots the diagram and returns a tuple containing the Voronoi object and the axes of the plot."}
{"task_id": "BigCodeBench/454", "solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory not found: {src_dir}\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory not found: {dest_dir}\")\n\n    # Ensure the extension starts with a dot\n    if not ext.startswith('.'):\n        ext = '.' + ext\n\n    # Get all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f'*{ext}'))\n\n    moved_files = []\n\n    for file in files:\n        filename = os.path.basename(file)\n        dest_path = os.path.join(dest_dir, filename)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_path):\n            # Move the file\n            shutil.move(file, dest_path)\n            moved_files.append(dest_path)\n\n    return moved_files\n```\n\nThis function performs the following tasks:\n\n1. It checks if both the source and destination directories exist. If not, it raises `FileNotFoundError`.\n2. It ensures the file extension starts with a dot (`.`). If the passed extension does not start with a dot, it prepends one.\n3. It uses `glob.glob()` to find all files in the source directory that match the given extension.\n4. For each file found, it constructs the destination path and checks if a file with the same name already exists in the destination directory.\n5. If the file does not exist in the destination, it is moved there using `shutil.move()`, and its path is added to the `moved_files` list.\n6. Finally, the function returns a list of the full paths of files that were successfully moved. Files that were not moved because they already existed in the destination directory are not included in this list."}
{"task_id": "BigCodeBench/458", "solution": "```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Check if the input string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    try:\n        # Load JSON string into a dictionary\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        print(\"Invalid JSON string\")\n        return pd.DataFrame()\n\n    # Function to double numerical values, including those in strings\n    def double_value(value):\n        if isinstance(value, (int, float)):\n            return float(value * 2)\n        elif isinstance(value, str):\n            # Try to extract and double numerical values from strings\n            numbers = re.findall(r'-?\\d+\\.?\\d*', value)\n            if numbers:\n                doubled_numbers = [str(float(num) * 2) for num in numbers]\n                for old, new in zip(numbers, doubled_numbers):\n                    value = value.replace(old, new)\n            return value\n        elif isinstance(value, list):\n            return [double_value(item) for item in value]\n        else:\n            return value\n\n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {key: double_value(value) for key, value in data_dict.items()}\n\n    # Create a DataFrame from the normalized dictionary\n    df = pd.DataFrame([normalized_dict])\n\n    # Ensure numerical values are stored as floats\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            try:\n                df[column] = df[column].astype(float)\n            except ValueError:\n                pass  # Keep as object if conversion to float fails\n\n    return df\n```"}
{"task_id": "BigCodeBench/461", "solution": "```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script not found: {script_path}\")\n\n    # Start the subprocess using bash\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    start_time = time.time()\n    cpu_usage = 0\n    memory_usage = 0\n\n    try:\n        while process.poll() is None:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                raise TimeoutError(f\"Script execution timed out after {timeout} seconds\")\n\n            try:\n                p = psutil.Process(process.pid)\n                if p.status() == psutil.STATUS_ZOMBIE:\n                    break\n\n                cpu_percent = p.cpu_percent(interval=0.1)\n                cpu_usage += cpu_percent * 0.1  # Accumulate CPU usage\n\n                memory_info = p.memory_info()\n                memory_usage += max(0, memory_info.rss - memory_usage)  # Accumulate positive memory increments\n\n            except psutil.NoSuchProcess:\n                break\n\n            time.sleep(0.1)\n\n    finally:\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }\n```\n\nThis Python function `task_func` addresses all the requirements:\n\n1. **Script Check**: It first ensures that the specified script path exists.\n2. **Subprocess Execution**: It executes the bash script as a subprocess.\n3. **Monitoring**: The process is monitored using `psutil` for its CPU and memory usage.\n4. **CPU Usage**: CPU usage is accumulated over time, providing a cumulative measure rather than an average.\n5. **Memory Usage**: Memory usage is accumulated as the sum of positive RSS memory increments during the script's execution.\n6. **Timeout Management**: The function will terminate the process if it exceeds the specified timeout and raise a `TimeoutError`.\n7. **Zombie and Process Existence**: It handles cases where the process might turn into a zombie or disappear (no longer found).\n8. **Clean Termination**: Ensures the subprocess is properly terminated if still running at the end of monitoring.\n\nThis function can be used in environments where the `psutil` library is available, ensuring comprehensive monitoring and management of subprocesses."}
{"task_id": "BigCodeBench/477", "solution": "Here's a complete Python function that meets your requirements for generating a DataFrame with random values and categories, and plotting the data:\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Assign categories ensuring each category appears at least once if possible\n    if N >= len(CATEGORIES):\n        # Fill categories so each appears at least once\n        categories = CATEGORIES + list(np.random.choice(CATEGORIES, N - len(CATEGORIES)))\n        np.random.shuffle(categories)\n    else:\n        # Randomly sample categories without replacement\n        categories = list(np.random.choice(CATEGORIES, N, replace=False))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in CATEGORIES:\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category, alpha=0.7)\n    \n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Scatter Plot of X vs Y, Colored by Category')\n    ax.legend(title='Category')\n    \n    plt.show()\n    \n    return (df, ax)\n\n# Example usage of the function\ndf, ax = task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42)\n```\n\nThis function, `task_func`, starts by setting a random seed to ensure the results are reproducible. It generates two arrays of random numbers for the `x` and `y` columns. Depending on the number of rows (`N`) compared to the length of the `CATEGORIES` list, it either ensures each category is represented at least once or samples without replacement. It then creates a scatter plot of the `x` vs. `y` points, colored by the category, and returns the DataFrame along with the plot's Axes object."}
{"task_id": "BigCodeBench/486", "solution": "Here's a complete Python function to generate and plot a time series with the specifications you requested:\n\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Convert epoch times to datetime objects\n    start_time = datetime.fromtimestamp(start_time)\n    end_time = datetime.fromtimestamp(end_time)\n\n    # Generate a range of timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate values from a normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps))\n\n    # Apply a linear trend to the values\n    trend_values = np.linspace(0, trend * (len(timestamps) - 1), len(timestamps))\n    values += trend_values\n\n    # Create a DataFrame to store the time and values\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n\n    # Plotting the time series\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Time'], df['Value'], label='Time Series with Trend')\n\n    # Setting labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    # Format x-axis to display dates and times\n    ax.xaxis.set_major_locator(plt.AutoLocator())\n    ax.xaxis.set_major_formatter(plt.DateFormatter('%Y-%m-%d %H:%M:%S'))\n    plt.xticks(rotation=45)\n    plt.legend()\n\n    # Adjust layout to ensure no labels are cut off\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\nstart_epoch = 1672531200  # Corresponds to 2023-01-01 00:00:00 UTC\nend_epoch = 1672617600    # Corresponds to 2023-01-02 00:00:00 UTC\nstep = 'H'  # Hourly frequency\ntrend = 0.1  # Linear trend value\n\nax = task_func(start_epoch, end_epoch, step, trend)\nplt.show()\n```\n\n### Explanation:\n- **Epoch Conversion:** The function starts by converting the provided epoch start and end times into Python `datetime` objects.\n- **Timestamp Generation:** It uses `pd.date_range()` to create a series of timestamps from the start to the end time at the specified frequency (`step`).\n- **Value Generation:** Values are drawn from a normal distribution, and a linear trend is applied using a linearly spaced array multiplied by the `trend` parameter.\n- **Plotting:** The function then plots these values against the timestamps, formats the plot with labels and a title, and adjusts the layout to ensure a clean presentation.\n- **Return:** The function returns the `Axes` object for further manipulation or display outside the function, if necessary."}
{"task_id": "BigCodeBench/492", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Check if the input is a valid integer\n    if not isinstance(epoch_milliseconds, int):\n        raise ValueError(\"epoch_milliseconds must be an integer\")\n\n    # Convert epoch milliseconds to a datetime object\n    try:\n        start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    except OSError:\n        raise ValueError(\"Invalid epoch timestamp\")\n\n    # Ensure the start date is not in the future\n    if start_date > datetime.now():\n        raise ValueError(\"Start date cannot be in the future\")\n\n    # Generate a date range from the start date to the current date\n    end_date = datetime.now()\n    date_range = pd.date_range(start=start_date.date(), end=end_date.date(), freq='D')\n\n    # Generate sales data\n    data = []\n    for current_date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append({\n                'Product': product,\n                'Date': current_date,\n                'Sales': sales\n            })\n\n    # Create and return the DataFrame\n    df = pd.DataFrame(data)\n    df = df.sort_values(['Date', 'Product'])\n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example epoch time (e.g., January 1, 2023)\n    example_epoch = 1672531200000\n\n    # Generate sales data\n    sales_data = task_func(example_epoch)\n\n    # Display the first few rows of the generated data\n    print(sales_data.head(10))\n```\n\nThis Python function, `task_func`, generates sales data for five products from a specified epoch time to the current date. It validates the input `epoch_milliseconds`, ensures it is an integer, and converts it to a `datetime` object. Sales data is randomly generated for each product on each day within the range [10, 50]. The resulting DataFrame is sorted by 'Date' and 'Product'. The function is designed to handle errors related to invalid timestamps and future start dates."}
{"task_id": "BigCodeBench/501", "solution": "Here is a Python function that meets your requirements for converting JSON strings to an Excel file, handling various data and exception scenarios:\n\n```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if the input is of the correct type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    try:\n        # Parse the JSON string\n        data = json.loads(json_str)\n    \n        # Check if the parsed data is a list (which JSON arrays should be)\n        if not isinstance(data, list):\n            raise ValueError(\"JSON must represent an array\")\n    \n        # Convert the JSON data to a pandas DataFrame\n        df = pd.DataFrame(data)\n    \n        # Create an Excel workbook and worksheet using xlwt\n        workbook = xlwt.Workbook()\n        worksheet = workbook.add_sheet(sheet_name)\n    \n        # Write the headers and data to the Excel worksheet if DataFrame is not empty\n        if not df.empty:\n            for col, header in enumerate(df.columns):\n                worksheet.write(0, col, header)\n            for row in range(len(df)):\n                for col, value in enumerate(df.iloc[row]):\n                    worksheet.write(row + 1, col, value)\n    \n        # Save the workbook to the specified filename and ensure .xls extension\n        if not filename.endswith('.xls'):\n            filename += '.xls'\n        abs_path = os.path.abspath(filename)\n        workbook.save(abs_path)\n    \n        return abs_path\n\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n    except Exception as e:\n        raise Exception(f\"Error writing to Excel file: {str(e)}\")\n```\n\n### Function Behavior Explanation:\n1. **Type Checking**: The function first checks if the provided `json_str` is of type string, bytes, or bytearray, raising a `TypeError` if not.\n2. **JSON Parsing**: It attempts to parse the JSON string. If parsing fails, it raises a `ValueError` indicating the JSON is invalid.\n3. **Data Handling**: If the JSON is parsed successfully, it checks whether the parsed object is a list (the expected format for a JSON array). The data is then loaded into a pandas DataFrame.\n4. **Excel File Creation**: Using `xlwt`, it creates an Excel workbook and a worksheet. If the DataFrame is not empty, it writes headers and data to the Excel file.\n5. **File Saving**: The workbook is saved to the specified filename, adding the '.xls' extension if not present, and the absolute path of the file is returned.\n6. **Exception Handling**: The function handles exceptions related to file writing and other general errors, providing a clear message regarding the issue encountered."}
{"task_id": "BigCodeBench/502", "solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Calculate the date range for the past specified number of days\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    dates = [start_date + timedelta(days=i) for i in range(days_in_past)]\n    \n    # Generate random data for each activity on each date\n    data = []\n    for date in dates:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append({'Date': date, 'Activity': activity, 'Duration': duration})\n    \n    # Create DataFrame from the generated data\n    df = pd.DataFrame(data)\n    \n    # Create a Seaborn line plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity')\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.legend(title='Activity', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    \n    # Return the plot axis and the DataFrame\n    return ax, df\n\n# Example usage:\n# ax, df = task_func(days_in_past=10, random_seed=42)\n# plt.show()\n```\n\nThis function `task_func` takes two parameters: `days_in_past`, which specifies how many days of data to generate, and `random_seed`, which ensures reproducibility of the random number generation. It generates a DataFrame with columns for date, activity, and duration, where durations are randomly set between 0 and 120 minutes for each activity. The function then uses Seaborn to create a line plot visualizing these durations over the specified date range and returns the plot axis and the DataFrame. You can call this function with different days and seeds to generate and visualize activity data for different scenarios."}
{"task_id": "BigCodeBench/503", "solution": "Certainly! Here's a Python function that creates a DataFrame of random stock prices for a specified number of days in the past. The prices are floats in the range [0.0, 1.0).\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Calculate the date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past-1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices\n    stock_prices = np.random.random((len(date_range), len(stock_names)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(stock_prices, columns=stock_names, index=date_range)\n    \n    # Round prices to 4 decimal places for better readability\n    df = df.round(4)\n    \n    return df\n\n# Example usage:\nresult = task_func(days_in_past=10, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\"], random_seed=42)\nprint(result)\n```\n\nThis function uses `numpy` to generate random numbers and `pandas` to create a DataFrame with these numbers as stock prices. The prices are rounded to four decimal places for better readability. You can modify the number of days, stock names, and the random seed to get different outputs."}
{"task_id": "BigCodeBench/509", "solution": "```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the contents of both CSV files\n        with open(file_path1, 'r', newline='', encoding='utf-8') as file1, open(file_path2, 'r', newline='', encoding='utf-8') as file2:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            lines1 = [line for line in reader1]\n            lines2 = [line for line in reader2]\n\n        # Check if either file is empty\n        if not lines1:\n            raise ValueError(f\"File '{file_path1}' is empty.\")\n        if not lines2:\n            raise ValueError(f\"File '{file_path2}' is empty.\")\n\n        # Compare the files using ndiff\n        diff = list(ndiff(lines1, lines2))\n\n        # Process the differences to format them into the required output\n        differences = []\n        line_number = 0\n        for line in diff:\n            status = line[0]\n            content = ','.join(line[2:]) if isinstance(line[2:], tuple) else line[2:]\n            if status in ['-', '+', ' ']:\n                if status != ' ':\n                    line_number += 1\n                differences.append({\n                    'Line Number': line_number,\n                    'Status': status,\n                    'Content': content\n                })\n\n        # Create a DataFrame from the differences\n        df = pd.DataFrame(differences, columns=['Line Number', 'Status', 'Content'])\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except ValueError as ve:\n        raise ve\n    except Exception as e:\n        raise Exception(f\"An IO-related error occurred: {str(e)}\")\n```\n\nThis function is designed to compare two CSV files line by line and identify differences between them. The differences are indicated by a status symbol ('-', '+', ' '), representing lines unique to the first file, unique to the second file, or common between both files, respectively. The function outputs these differences in a structured pandas DataFrame, providing clear visibility into the content changes across the files. It includes robust error handling to manage common issues such as file not found errors and empty file scenarios, ensuring that the function behaves predictably and informatively under various error conditions."}
{"task_id": "BigCodeBench/511", "solution": "Here is a complete implementation of the `task_func` that meets your requirements:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data)\n    \n    # Initialize the dictionary to store statistics\n    stats = {\n        'sum': 0 if df.empty else df[column].sum(),\n        'mean': np.nan if df.empty else df[column].mean(),\n        'min': np.nan if df.empty else df[column].min(),\n        'max': np.nan if df.empty else df[column].max()\n    }\n    \n    # Initialize the plot\n    fig, ax = plt.subplots()\n    \n    if not df.empty and 'Age' in df.columns:\n        # Group data by 'Age' and count each age's occurrence\n        age_counts = df['Age'].value_counts()\n        \n        # Plot a pie chart\n        ax.pie(age_counts, labels=age_counts.index, autopct='%1.1f%%', startangle=90)\n        ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n        ax.set_title(f'Distribution of Ages for {column}')\n    else:\n        # Display a message if no data is available\n        ax.text(0.5, 0.5, 'No Age data available' if not df.empty else 'No data available', \n                horizontalalignment='center', verticalalignment='center', fontsize=12)\n        ax.axis('off')\n    \n    return stats, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    employee_data = [\n        {'Name': 'John', 'Age': 30, 'Salary': 50000},\n        {'Name': 'Alice', 'Age': 25, 'Salary': 60000},\n        {'Name': 'Bob', 'Age': 35, 'Salary': 75000},\n        {'Name': 'Emma', 'Age': 28, 'Salary': 55000},\n        {'Name': 'David', 'Age': 40, 'Salary': 80000}\n    ]\n\n    result_stats, result_ax = task_func('Salary', employee_data)\n    print(\"Statistics:\", result_stats)\n    plt.show()\n```\n\nThis function starts by creating a pandas DataFrame from the input data list. It then checks if the DataFrame is empty and calculates the sum, mean, minimum, and maximum values for the specified column. The function also checks if the 'Age' column is present in the DataFrame and uses it to create a pie chart that shows the distribution of ages. If the DataFrame is empty or the 'Age' column is missing, it displays an appropriate message. The function returns a tuple containing the dictionary with the computed statistics and the Axes object used for the pie chart."}
{"task_id": "BigCodeBench/513", "solution": "Here's a self-contained Python function that analyzes a list of fitness data, calculates the sum, mean, minimum, and maximum of a certain column, and draws a line chart. It includes validation checks for non-negative numeric values and raises exceptions appropriately:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Check if data is empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Validate column existence\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' not found in data\")\n\n    # Validate non-negative values for numeric columns\n    for col in ['steps', 'calories_burned', 'distance_walked']:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in '{col}' column\")\n\n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['date'], df[column], marker='o', linestyle='-')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column.capitalize())\n    ax.set_title(f'Line Chart of {column.capitalize()}')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return stats, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [\n        {'date': '2023-05-01', 'steps': 8000, 'calories_burned': 300, 'distance_walked': 5.2},\n        {'date': '2023-05-02', 'steps': 10000, 'calories_burned': 400, 'distance_walked': 6.5},\n        {'date': '2023-05-03', 'steps': 7500, 'calories_burned': 280, 'distance_walked': 4.8},\n        {'date': '2023-05-04', 'steps': 9000, 'calories_burned': 350, 'distance_walked': 5.8},\n        {'date': '2023-05-05', 'steps': 11000, 'calories_burned': 450, 'distance_walked': 7.1},\n    ]\n\n    try:\n        stats, ax = task_func('steps', data)\n        print(\"Statistics:\", stats)\n        plt.show()\n    except (KeyError, ValueError) as e:\n        print(f\"Error: {e}\")\n```\n\nThis code defines a function `task_func` that performs the necessary calculations and validations before plotting the results. It checks if the data list is empty, validates the existence of the specified column, and ensures that the numeric values are non-negative. It then calculates the required statistics and plots a line chart using matplotlib. The function returns a tuple containing the statistics and the Axes object of the plotted chart. The example usage demonstrates how to call the function and handle exceptions."}
{"task_id": "BigCodeBench/526", "solution": "```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Read JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize defaultdict to store numeric values for each key\n    numeric_values = defaultdict(list)\n    \n    # Iterate through each dictionary in the list\n    for item in data:\n        for key, value in item.items():\n            # Check if value is numeric (int or float)\n            if isinstance(value, (int, float)):\n                numeric_values[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = []\n    for key, values in numeric_values.items():\n        mean = np.mean(values)\n        median = np.median(values)\n        results.append({'variable': key, 'mean': mean, 'median': median})\n    \n    # Create DataFrame from results\n    df = pd.DataFrame(results)\n    \n    # Set 'variable' as index and sort\n    df.set_index('variable', inplace=True)\n    df.sort_index(inplace=True)\n    \n    return df\n\n# Example usage:\n# df = task_func(\"data.json\")\n# print(df)\n```\n\nThis Python function `task_func` reads data from a JSON file, processes each dictionary within the list to extract numeric values, and calculates the mean and median for each key. It then converts these statistics into a Pandas DataFrame, which is indexed and sorted by the variable names. This DataFrame includes columns for both the mean and median values of each variable."}
{"task_id": "BigCodeBench/528", "solution": "```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.lower().endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension\")\n    \n    # Read the CSV file and identify duplicate rows\n    try:\n        with open(file_path, 'r', newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            rows = [tuple(row) for row in reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found\")\n    except csv.Error as e:\n        raise ValueError(f\"Error reading CSV file: {e}\")\n\n    # Count duplicate rows\n    row_counts = Counter(rows)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to DataFrame\n    df = pd.DataFrame.from_dict(duplicates, orient='index', columns=['Count'])\n    df.index = [', '.join(map(str, idx)) for idx in df.index]\n    df = df.sort_values('Count', ascending=False)\n\n    # Plot the results\n    fig, ax = plt.subplots(figsize=(12, 6))\n    df.plot(kind='bar', ax=ax)\n    ax.set_title('Duplicate Rows in CSV File')\n    ax.set_xlabel('Row Content')\n    ax.set_ylabel('Count')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return duplicates, ax\n\n# Example usage:\n# duplicates, ax = task_func('your_file.csv')\n# plt.show()\n```\n\nThis function verifies that the given file is a CSV file and reads it to detect and count duplicate rows. It then visualizes these duplicates using a bar chart and returns both the dictionary of duplicates and the plot's Axes object. Adjustments are made to ensure it handles file errors and formatting issues properly."}
{"task_id": "BigCodeBench/530", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure 'name' and 'age' columns are present\n    if 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns\")\n    \n    # Convert age to integer, rounding down, and check for negative values\n    df['age'] = df['age'].apply(np.floor).astype(int)\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated('name', keep=False)]\n    \n    if duplicates.empty:\n        return Counter(), None\n    \n    # Calculate age distribution for duplicates\n    age_distribution = Counter(duplicates['age'])\n    \n    # Plot histogram\n    plt.figure(figsize=(10, 6))\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)  # Adjust bins to center integer ages\n    \n    ax = sns.histplot(duplicates, x='age', bins=bins, kde=False)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution for Duplicate Names')\n    \n    # Adjust x-axis ticks to show integer values\n    ax.set_xticks(np.arange(min_age, max_age + 1))\n    \n    plt.tight_layout()\n    \n    return age_distribution, ax\n\n# Example usage:\n# df = pd.DataFrame({'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n#                    'age': [25.4, 30.7, 25.9, 22, 30.1]})\n# age_dist, plot = task_func(df)\n# print(age_dist)\n# if plot:\n#     plt.show()\n```\n\nThis implementation checks for empty data, ensures necessary columns are present, handles ages as integers, checks for negative values, identifies duplicates, computes their age distribution, and plots the histogram. It handles edge cases and returns the appropriate plot or `None` if there are no duplicates."}
{"task_id": "BigCodeBench/532", "solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates = {k: v for k, v in value_counts.items() if v > 1}\n\n    # Create histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    n, bins, patches = ax.hist(df['value'], bins=bins, edgecolor='black', color='green', alpha=0.6)\n\n    # Fit normal distribution to the data\n    mu, std = norm.fit(df['value'])\n\n    # Generate points for the normal distribution curve\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n\n    # Scale the normal distribution curve to match the histogram height\n    p = p * (n.max() / p.max())\n\n    # Plot the normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n\n    return Counter(duplicates), ax\n```\n\nThis function `task_func` accepts a DataFrame `df` and an optional `bins` parameter specifying the number of bins for the histogram. It identifies and counts duplicate values in the `value` column of the DataFrame, creates a histogram of the values, and overlays a normal distribution curve fit to the data. The histogram bars are styled green with 60% opacity, and the normal distribution curve is plotted in black with a linewidth of 2. The plot is titled \"Distribution\" and includes labels for both the x-axis (\"Value\") and the y-axis (\"Frequency\"). The function returns a tuple containing a `Counter` object with counts of duplicate values and a `matplotlib.axes.Axes` object representing the plotted histogram and normal distribution curve."}
{"task_id": "BigCodeBench/553", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Determine the number of rows from list 'a' and the number of columns from the length of list 'b'\n    num_rows = len(a)\n    num_cols = min(len(b), len(COLUMNS))  # Limit the number of columns to the size of COLUMNS\n\n    # Generate random data for the DataFrame\n    data = np.random.rand(num_rows, num_cols)\n    \n    # Create the DataFrame with 'a' as row indices and the appropriate number of columns from COLUMNS\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_cols])\n    \n    # Plotting the DataFrame\n    fig, ax = plt.subplots(figsize=(10, 6))  # Create a matplotlib figure and axis object\n    df.plot(kind='bar', ax=ax)  # Plot the DataFrame as a bar chart on the axis\n\n    # Customize the plot with labels and title\n    ax.set_title('Bar Chart of Random Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    ax.legend(title='Columns')\n    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout\n\n    return ax  # Return the matplotlib Axes object\n\n# Example usage:\na = ['X', 'Y', 'Z']\nb = [1, 2, 3, 4]\n\nresult_ax = task_func(a, b)\nplt.show()  # Display the plot\n```\n\nThis code snippet defines a function `task_func` that generates a pandas DataFrame with random values determined by the lists `a` (for row indices) and `b` (to determine the number of columns). The DataFrame is then plotted as a bar chart, and the function returns the Axes object for the chart. This implementation ensures that the number of columns does not exceed the number of predefined column names in `COLUMNS`. The example usage at the end demonstrates how to use this function and display the resulting bar chart."}
{"task_id": "BigCodeBench/560", "solution": "```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Ensure the 'month' column is in a proper datetime format\n    if data['month'].dtype == 'int64':\n        data['month'] = data['month'].apply(lambda x: datetime.strptime(str(x), '%m').strftime('%B'))\n    elif data['month'].dtype == 'object':\n        data['month'] = data['month'].apply(lambda x: datetime.strptime(x, '%B').strftime('%B'))\n    \n    # Sort the data by month\n    month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n                   'July', 'August', 'September', 'October', 'November', 'December']\n    data['month'] = pd.Categorical(data['month'], categories=month_order, ordered=True)\n    data = data.sort_values('month')\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(data['month'], data['value'])\n\n    # Set the title and labels\n    ax.set_title('Monthly Data for {}'.format(datetime.now().year))\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n    \n    return ax\n```\nThis function converts the 'month' column to a format that can be sorted by month order, regardless of whether the months are initially provided as integers or names. It then plots a bar chart with the months on the x-axis and the corresponding values on the y-axis, and formats the plot with appropriate labels and title. The function returns the `matplotlib.axes.Axes` object representing the plot."}
{"task_id": "BigCodeBench/567", "solution": "Here is a self-contained Python function that creates a histogram based on numeric values provided in a string format. The function takes care of converting the string data into numeric values, calculating bins, drawing the histogram, and returning the relevant Axes object.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Split the input data string and convert to numeric, handling non-numeric values\n    numeric_data = pd.to_numeric(data.split(), errors='coerce')\n    \n    # Remove any NaN values that arise from non-numeric entries\n    numeric_data = numeric_data.dropna()\n    \n    # Calculate the histogram bins\n    bins = np.arange(numeric_data.min(), numeric_data.max() + 2) - 0.5\n    \n    # Create the histogram plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.hist(numeric_data, bins=bins, edgecolor='black')\n    \n    # Set the x-axis and y-axis labels, and the title of the histogram\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Adjust the layout to ensure everything fits without overlap\n    plt.tight_layout()\n    \n    # Return the Axes object for further manipulation if necessary\n    return ax\n\n# Example usage:\n# data_string = \"1 2 3 4 5 2 3 4 3 4 5 6 7 5 4 3 2 1\"\n# ax = task_func(data_string)\n# plt.show()  # To display the histogram\n```\n\nThis function takes a string input, cleans and processes it into a numeric format suitable for histogram plotting, and then configures and displays the histogram with appropriate labels and titles. Use the example usage to test the function or integrate it into a larger application."}
{"task_id": "BigCodeBench/574", "solution": "```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate noisy sine wave\n    y_noisy = np.sin(x) + np.random.normal(0, noise_level, array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Initial guess for the parameters\n    initial_params = [1, 1, 0, 0]\n    \n    # Perform curve fitting\n    popt, _ = curve_fit(sine_func, x, y_noisy, p0=initial_params)\n    \n    # Generate fitted curve based on the optimized parameters\n    y_fit = sine_func(x, *popt)\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot noisy data\n    ax.scatter(x, y_noisy, color='blue', alpha=0.5, label='Noisy data')\n    \n    # Plot fitted curve\n    ax.plot(x, y_fit, color='red', label='Fitted curve')\n    \n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy Sine Wave with Fitted Curve')\n    \n    # Add legend\n    ax.legend()\n    \n    # Set y-axis limits\n    ax.set_ylim(-1.5, 1.5)\n    \n    # Add grid\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    return ax\n\n# Example usage:\n# ax = task_func(array_length=200, noise_level=0.3)\n# plt.show()\n```\n\nThis function `task_func` generates a noisy sine wave with the specified `array_length` and `noise_level`, fits a sine function to the noisy data using `curve_fit` from `scipy.optimize`, and plots both the noisy data and the fitted curve. The function returns the `Axes` object of the plot for further manipulation or display. To see the plot, you can call the function with your desired parameters and use `plt.show()` after the function call. Adjust `array_length` and `noise_level` to see different behaviors of the sine wave and fitting process."}
{"task_id": "BigCodeBench/579", "solution": "Here's a complete Python function that meets all your specified requirements. It reads a CSV file, normalizes the text to ASCII, counts the words, and returns the 10 most common words along with their frequencies as a matplotlib bar plot and a list of tuples. The function also appropriately handles exceptions for file not found and I/O errors.\n\n```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        # Read the CSV file\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join([' '.join(row) for row in reader])\n\n        # Normalize text to ASCII\n        normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n\n        # Count words\n        words = normalized_text.lower().split()\n        word_counts = Counter(words)\n\n        # Get the 10 most common words\n        top_10 = word_counts.most_common(10)\n\n        # Create a bar plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        words, counts = zip(*top_10)\n        ax.bar(words, counts)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        ax.set_title('10 Most Common Words')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n\n        # Return the plot's Axes object and the list of top words\n        return ax, top_10\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{csv_file}' was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file '{csv_file}'.\")\n\n# Example usage:\n# ax, top_words = task_func('path/to/your/csv_file.csv')\n# plt.show()\n# print(top_words)\n```\n\nThis function does the following:\n1. Opens and reads the CSV file, concatenating all rows into a single string.\n2. Normalizes the concatenated string to ASCII to ensure compatibility.\n3. Splits the normalized text into words and counts the frequency of each word.\n4. Identifies the 10 most common words and their frequencies.\n5. Creates a bar plot using matplotlib to visualize the data.\n6. Returns a tuple containing the matplotlib.axes.Axes object for the plot and a list of the 10 most common words with their frequencies.\n\nIf the file is not found at the specified path, it will raise a `FileNotFoundError`. If there's an error reading the file, it will raise an `IOError`."}
{"task_id": "BigCodeBench/582", "solution": "Here is a self-contained Python function that generates a list of normally distributed random numbers, and plots both their histogram and the corresponding probability density function (PDF). The function returns a `matplotlib.figure.Figure` object containing the combined plot.\n\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n\n    # Create a figure and an axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot the histogram of the data\n    n, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black', label='Histogram')\n    \n    # Calculate the PDF from theoretical normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, loc=0, scale=1)\n    \n    # Plot the PDF\n    ax.plot(x, p, 'r-', lw=2, label='Normal PDF')\n    \n    # Customize the plot\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title(f'Histogram and PDF of {size} Normally Distributed Random Numbers')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Return the figure object\n    return fig\n\n# Example usage:\n# fig = task_func(size=1000)\n# plt.show()\n```\n\nYou can call `task_func(size=1000)` to generate and return the plot with 1000 normally distributed random numbers. Adjust the `size` parameter to change the number of data points in the plot. Use `plt.show()` to display the plot if you are running in a script or an interactive Python environment."}
{"task_id": "BigCodeBench/583", "solution": "```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Convert private key to PEM format\n    privkey_pem = privkey.save_pkcs1().decode('utf-8')\n    \n    # Generate a random password and nonce for AES encryption\n    password = get_random_bytes(32)  # 256-bit key\n    nonce = get_random_bytes(16)  # 128-bit nonce\n\n    # Encrypt the private key\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, tag = cipher.encrypt_and_digest(privkey_pem.encode('utf-8'))\n\n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Combine nonce, ciphertext, and tag for storage\n    encrypted_data = nonce + ciphertext + tag\n\n    # Save the encrypted private key to the file\n    with open(filename, 'wb') as f:\n        f.write(encrypted_data)\n\n    return pubkey, filename, password, nonce\n\n# Example usage:\npublic_key, private_key_filename, encryption_password, encryption_nonce = task_func()\n\nprint(f\"Public Key: {public_key}\")\nprint(f\"Encrypted Private Key Filename: {private_key_filename}\")\nprint(f\"Encryption Password: {encryption_password.hex()}\")\nprint(f\"Encryption Nonce: {encryption_nonce.hex()}\")\n\n# Note: The decryption process would require reversing the encryption steps, \n# using the saved password and nonce to decrypt the data from the file.\n```\n\nThis Python function generates an RSA key pair, encrypts the private key using AES encryption with a random password and nonce, and saves the encrypted data to a file named in the format \"private_key_<hex>.txt\". It returns the public key, the filename of the encrypted private key, and the encryption details (password and nonce) for potential testing of decryption. The code uses the `rsa` library for RSA operations and `pycryptodome` for AES encryption."}
{"task_id": "BigCodeBench/587", "solution": "```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256-bit key\n\n    # Read the file content\n    with open(file_path, 'rb') as file:\n        file_content = file.read()\n\n    # Pad the file content\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(file_content) + padder.finalize()\n\n    # Generate a random IV\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Encrypt the file content\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save the encrypted file\n    encrypted_file_name = file_path + \".encrypted\"\n    with open(encrypted_file_name, 'wb') as encrypted_file:\n        encrypted_file.write(iv + encrypted_data)\n\n    # Save the encrypted AES key\n    encrypted_key_file_name = file_path + \".key\"\n    with open(encrypted_key_file_name, 'wb') as key_file:\n        key_file.write(encrypted_aes_key)\n\n    return pubkey, encrypted_file_name, encrypted_key_file_name\n\n# Example usage:\n# public_key, encrypted_file, encrypted_key_file = task_func(\"example.txt\")\n# print(f\"Public Key: {public_key}\")\n# print(f\"Encrypted File: {encrypted_file}\")\n# print(f\"Encrypted Key File: {encrypted_key_file}\")\n```\n\nThis Python function demonstrates a hybrid encryption scheme, where a file is encrypted using AES encryption with a randomly generated key and initialization vector. The AES key is then encrypted using an RSA public key. Both the encrypted file and the encrypted AES key are saved to new files with specified extensions. This approach ensures that the file data is securely encrypted, and only someone with the corresponding RSA private key can decrypt the AES key and, subsequently, the file."}
{"task_id": "BigCodeBench/590", "solution": "```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nfrom urllib.error import URLError\n\ndef task_func(url):\n    # Validate URL: Check if URL is empty or invalid\n    if not url or not url.startswith(('http://', 'https://')):\n        raise ValueError(\"Invalid or empty URL provided\")\n\n    try:\n        # Fetch HTML content\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n        \n        # Parse HTML using PyQuery\n        doc = pq(html)\n        \n        # Extract text and href from anchor tags\n        anchors = doc('a')\n        texts = [anchor.text() for anchor in anchors.items()]\n        hrefs = [anchor.attr('href') for anchor in anchors.items()]\n        \n        # Get current timestamp for the 'fetch_time' column\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        \n        # Create DataFrame with extracted data and a timestamp\n        df = pd.DataFrame({\n            'text': texts,\n            'href': hrefs,\n            'fetch_time': [fetch_time] * len(texts)\n        })\n        \n        return df\n\n    except URLError as e:\n        # Handle errors related to network issues\n        raise URLError(f\"Network connectivity issue or server error: {str(e)}\")\n    except Exception as e:\n        # Handle other unexpected errors\n        raise Exception(f\"An unexpected error occurred: {str(e)}\")\n```\n\nThis function performs the following tasks:\n\n1. Validates the URL to ensure it is not empty and starts with 'http://' or 'https://'. If it does not meet these criteria, a `ValueError` is raised.\n2. Fetches the HTML content from the specified URL using the `urllib.request.urlopen()` method.\n3. Parses the fetched HTML content using the `PyQuery` library to extract text and href attributes from all anchor tags.\n4. Constructs a pandas DataFrame that includes three columns: 'text', 'href', and 'fetch_time'. Each row in the DataFrame corresponds to an anchor tag found in the HTML, where 'text' and 'href' contain the text and the hyperlink reference of the anchor tag, respectively. The 'fetch_time' column contains the timestamp of when the data was fetched, formatted as 'YYYY-MM-DD HH:MM:SS'.\n5. Handles `URLError` to catch any issues with network connectivity or server errors, raising a descriptive error message.\n6. Catches and raises any other unexpected errors during execution.\n\nBy following these steps, the function ensures robust error handling while efficiently extracting and organizing the required information from a webpage."}
{"task_id": "BigCodeBench/592", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint, uniform\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate filename with current timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"sensor_data_{timestamp}.csv\"\n    filepath = os.path.join(output_dir, filename)\n\n    # Generate data\n    data = []\n    start_time = datetime.now()\n    for i in range(hours * 60):  # Generate data for every minute\n        current_time = start_time + timedelta(minutes=i)\n        temperature = round(uniform(20, 30), 1)  # Temperature between 20\u00b0C and 30\u00b0C\n        humidity = randint(30, 70)  # Humidity between 30% and 70%\n        pressure = round(uniform(990, 1030), 1)  # Pressure between 990 hPa and 1030 hPa\n        data.append([current_time.strftime(\"%Y-%m-%d %H:%M:%S\"), temperature, humidity, pressure])\n\n    # Write data to CSV file\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writerows(data)\n\n    print(f\"Sensor data for {hours} hours has been generated and saved to {filepath}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    task_func(24)  # Generate data for 24 hours\n```"}
{"task_id": "BigCodeBench/593", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate data\n    start_time = datetime.now().replace(minute=0, second=0, microsecond=0)\n    data = []\n    for i in range(hours):\n        current_time = start_time + timedelta(hours=i)\n        row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n        row.extend([randint(0, 100) for _ in VEHICLE_TYPES])\n        data.append(row)\n\n    # Save data to CSV\n    csv_filename = os.path.join(output_dir, f'traffic_data_{start_time.strftime(\"%Y%m%d_%H%M%S\")}.csv')\n    with open(csv_filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n\n    # Read CSV and create plot\n    df = pd.read_csv(csv_filename, parse_dates=['Time'])\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Save plot\n    plot_filename = os.path.join(output_dir, f'traffic_plot_{start_time.strftime(\"%Y%m%d_%H%M%S\")}.png')\n    plt.savefig(plot_filename)\n\n    return csv_filename, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    csv_path, plot_axes = task_func(24)\n    print(f\"CSV file saved at: {csv_path}\")\n    plt.show()\n```\n\nThis script generates traffic data for different vehicle types over a specified number of hours, saves the data to a CSV file with columns 'Time', 'Car', 'Bus', 'Truck', and 'Bike', and plots the data in a line chart with 'Time' on the x-axis and 'Vehicle Count' on the y-axis. The function returns a tuple containing the path to the CSV file and the matplotlib axes object of the line plot. The example usage in the script demonstrates how to call this function and display the plot."}
{"task_id": "BigCodeBench/594", "solution": "Here's a complete Python script that meets your specifications to generate weather data for a specified number of hours, save it to a CSV file, and back up the file in a backup directory. The function returns the path of the generated CSV file.\n\n```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR, backup_dir=BACKUP_DIR):\n    # Ensure output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(backup_dir, exist_ok=True)\n\n    # Generate a timestamped filename for uniqueness\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"weather_data_{timestamp}.csv\"\n    file_path = os.path.join(output_dir, filename)\n    \n    # Generate weather data\n    current_time = datetime.now()\n    weather_data = []\n    for _ in range(hours):\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append([current_time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n        current_time += timedelta(hours=1)\n\n    # Write data to CSV file\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Path for the backup file\n    backup_file_path = os.path.join(backup_dir, filename)\n    shutil.copy(file_path, backup_file_path)\n\n    return file_path\n\n# Example usage\nif __name__ == \"__main__\":\n    result = task_func(24)\n    print(f\"CSV file generated at: {result}\")\n```\n\n### Explanation:\n1. **Directory Management**: The function ensures that both the output and backup directories exist before proceeding. If they don't, they are created using `os.makedirs` with `exist_ok=True` to avoid throwing an error if the directory already exists.\n\n2. **File Naming**: A unique filename is generated using the current timestamp. This prevents overwriting existing files and helps in identifying when the data was generated.\n\n3. **Data Generation**: For the specified number of hours, the function generates weather conditions from a predefined list. Each condition is paired with a timestamp, incremented hourly from the current time.\n\n4. **CSV Writing**: This data is then written to a CSV file located in the output directory. The file has two columns: \"Time\" and \"Condition\".\n\n5. **Backup**: After writing the CSV file, it is copied to the backup directory to ensure data durability.\n\n6. **Return Value**: Finally, the path to the created CSV file is returned, allowing the user to know where the file has been saved.\n\nThis script can be directly used or integrated into larger applications as needed."}
{"task_id": "BigCodeBench/618", "solution": "```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generate DataFrame with random data\n    data = []\n    for team in TEAMS:\n        for _ in range(goals):\n            goals_scored = randint(0, 5)\n            penalties_received = randint(0, 3)\n            fine = penalties_received * PENALTY_COST\n            data.append({\n                'Team': team,\n                'Goals': goals_scored,\n                'Penalties': penalties_received,\n                'Penalty Cost': fine\n            })\n\n    df = pd.DataFrame(data)\n\n    # Create visualizations\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Goals visualization\n    sns.barplot(x='Team', y='Goals', data=df, ax=ax1)\n    ax1.set_title('Goals Scored by Team')\n    ax1.set_xlabel('Team')\n    ax1.set_ylabel('Goals')\n\n    # Penalty costs visualization\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=ax2)\n    ax2.set_title('Total Penalty Costs by Team')\n    ax2.set_xlabel('Team')\n    ax2.set_ylabel('Penalty Cost (USD)')\n\n    plt.tight_layout()\n    \n    return df, [ax1, ax2]\n\n# Example usage\nresult_df, plot_axes = task_func(10, 5)\nprint(result_df)\nplt.show()\n```\n\nThis Python code defines a function `task_func` that simulates the results of football matches for multiple teams, generating random data for goals scored and penalties received. Penalties are converted into monetary fines based on a predefined cost. The function outputs a DataFrame containing the match results and a list of two plot objects visualizing the goals and penalty costs using Seaborn. The example usage of the function demonstrates how to generate and visualize the data."}
{"task_id": "BigCodeBench/636", "solution": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.astype(bool).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    non_zero_counts.plot(kind='bar', ax=ax)\n    \n    # Customize the plot\n    ax.set_title('Non-Zero Value Counts by Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    ax.set_ylim(0, rows)  # Set y-axis limit to the number of rows\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(non_zero_counts):\n        ax.text(i, v + 1, str(v), ha='center', va='bottom')\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    \n    return df, ax\n\n# Example usage:\n# df, ax = task_func(100)\n# plt.show()\n```\n\nThis function `task_func` takes the number of rows as input and performs several operations:\n1. Generates a DataFrame with random integers between 0 and 9.\n2. Counts the non-zero values in each column.\n3. Creates and customizes a bar plot to visualize the count of non-zero values in each column.\n4. Returns a tuple consisting of the DataFrame and the matplotlib Axes object for the plot."}
{"task_id": "BigCodeBench/637", "solution": "```python\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define the courses offered\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    \n    # Generate random student IDs\n    student_ids = ['Student_' + str(i) for i in sample(range(1000, 9999), num_students)]\n\n    # Create random grades for each student in each course\n    data = {course: np.random.randint(0, 101, num_students) for course in courses}\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate average grade and passing counts for each course\n    averages = df.mean()\n    passing_counts = (df >= 60).sum()\n\n    # Prepare DataFrame for visualization\n    visualization_data = pd.DataFrame({\n        'Average Grade': averages,\n        'Passing Count': passing_counts\n    })\n\n    # Plotting the data\n    ax = visualization_data.plot(kind='bar', color=['skyblue', 'lightgreen'], figsize=(10, 6))\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Counts / Grades')\n    ax.legend(['Average Grade', 'Passing Count'])\n\n    return df, ax\n\n# Example usage:\ndf, ax = task_func(30)\nplt.show()\n```"}
{"task_id": "BigCodeBench/654", "solution": "Here is a self-contained Python function that fits an exponential decay function to the indices in the array where the first column matches the target value. The function returns a tuple containing the optimized parameters of the fitting function (`popt`) and the `matplotlib` Axes object for visualization:\n\n```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Find indices where the first column matches the target value\n    target_indices = np.where(array[:, 0] == target_value)[0]\n    \n    # If no matching indices found, return None\n    if len(target_indices) == 0:\n        return None\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Prepare data for fitting\n    x_data = np.arange(len(target_indices))\n    y_data = array[target_indices, 1]  # Assuming we want to fit the second column values\n    \n    # Perform the curve fitting\n    try:\n        popt, _ = optimize.curve_fit(exp_decay, x_data, y_data)\n    except RuntimeError:\n        return None  # Return None if curve fitting fails\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Data')\n    \n    # Generate points for the fitted curve\n    x_fit = np.linspace(0, len(target_indices) - 1, 100)\n    y_fit = exp_decay(x_fit, *popt)\n    \n    # Plot the fitted curve\n    ax.plot(x_fit, y_fit, 'r-', label='Fitted Curve')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Exponential Decay Fit for Target Value: {target_value}')\n    ax.legend()\n    \n    return (popt, ax)\n\n# Example usage:\n# array = np.array([[1, 2], [1, 3], [2, 4], [1, 5], [2, 6]])\n# target_value = 1\n# result = task_func(array, target_value)\n# if result is not None:\n#     popt, ax = result\n#     print(\"Optimized parameters:\", popt)\n#     plt.show()\n# else:\n#     print(\"Fitting failed or no matching indices found.\")\n```\n\nThis function efficiently filters the given array to find the rows where the first column equals the target value, extracts values for fitting from these rows, fits an exponential decay curve to these values, and plots both the data points and the fitted curve. The function handles cases where no matching rows are found or where curve fitting fails, returning `None` in such cases."}
{"task_id": "BigCodeBench/655", "solution": "Here is a complete and self-contained Python function that performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF). The function preprocesses the input texts by removing non-alphanumeric characters (excluding spaces), converting all characters to lowercase, and removing stopwords. It then vectorizes these texts using TF-IDF and applies NMF to extract the specified number of topics. Each topic is represented as a list of the most significant words based on the NMF component weights.\n\n```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords', quiet=True)\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    def preprocess(text):\n        text = ALPHANUMERIC.sub(' ', text).lower()\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n\n    processed_texts = [preprocess(text) for text in texts]\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=5000)\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract topics\n    nmf_model = NMF(n_components=num_topics, init='nndsvda', random_state=42)\n    nmf_output = nmf_model.fit_transform(tfidf_matrix)\n\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n        top_words = [feature_names[i] for i in top_words_idx]\n        topics.append(top_words)\n\n    return topics\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Machine learning is a subset of artificial intelligence.\",\n        \"Python is a popular programming language for data science.\",\n        \"Natural language processing deals with the interaction between computers and humans using natural language.\"\n    ]\n    num_topics = 2\n    extracted_topics = task_func(sample_texts, num_topics)\n    for i, topic in enumerate(extracted_topics):\n        print(f\"Topic {i + 1}: {', '.join(topic)}\")\n```\n\nThis function includes all necessary steps from preprocessing the texts to extracting the topics with NMF. The `preprocess` inner function handles text cleaning and stopwords removal. The vectorization step limits the number of features to 5000, which can be adjusted based on your corpus size for better performance. The NMF model is initialized with `nndsvda` for reproducibility and potentially better convergence in some cases. Note that the exact output may vary depending on text data and random initialization of NMF."}
{"task_id": "BigCodeBench/657", "solution": "Here is a self-contained Python function that processes a list of texts, cleans them by removing non-alphanumeric characters (except spaces), converts them to lowercase, removes stopwords, and then trains a Word2Vec model using the cleaned text. The function will return the trained Word2Vec model:\n\n```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords as nltk_stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # Ensure the NLTK stopwords are downloaded\n    nltk.download('stopwords', quiet=True)\n    \n    # Use English stopwords from NLTK if not provided\n    if stopwords is None:\n        stopwords = set(nltk_stopwords.words('english'))\n    \n    # Function to clean and tokenize text\n    def clean_and_tokenize(text):\n        # Remove non-alphanumeric characters except space, and convert to lowercase\n        cleaned_text = ALPHANUMERIC.sub(' ', text.lower())\n        # Tokenize the text\n        tokens = cleaned_text.split()\n        # Filter out stopwords\n        filtered_tokens = [token for token in tokens if token not in stopwords]\n        return filtered_tokens\n    \n    # Process all texts\n    processed_texts = [clean_and_tokenize(text) for text in texts]\n\n    # Train the Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Hello world! This is a test sentence.\",\n        \"Natural language processing is fascinating.\",\n    ]\n    \n    word2vec_model = task_func(sample_texts)\n    \n    # Test the model\n    print(word2vec_model.wv.most_similar(\"quick\"))\n```\n\nThis code does the following:\n1. Checks and downloads NLTK stopwords if they are not already present.\n2. Uses a custom function `clean_and_tokenize` to clean each text by removing non-alphanumeric characters (except spaces), converting to lowercase, tokenizing, and removing stopwords.\n3. Processes all provided texts using the `clean_and_tokenize` function.\n4. Trains a Word2Vec model on the processed texts with specified model parameters.\n5. Returns the trained Word2Vec model, which can be used for various NLP tasks like finding similar words."}
{"task_id": "BigCodeBench/678", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Check and create the directory if it doesn't exist\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    # Create the 'processed' subdirectory if it doesn't exist\n    processed_dir = os.path.join(path, 'processed')\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    # Get a list of JSON files in the directory, sorted alphabetically\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()\n\n    # Initialize an empty list to store DataFrames\n    data_frames = []\n\n    # Process each JSON file\n    for file in json_files:\n        file_path = os.path.join(path, file)\n        \n        # Read JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n\n        # Convert to DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add 'Source' column\n        df['Source'] = file\n        \n        # Append to list of DataFrames\n        data_frames.append(df)\n        \n        # Move file to 'processed' subdirectory\n        shutil.move(file_path, os.path.join(processed_dir, file))\n\n    # Concatenate all DataFrames\n    if data_frames:\n        final_df = pd.concat(data_frames, ignore_index=True)\n    else:\n        final_df = pd.DataFrame()\n\n    return final_df\n\n# Example usage:\n# df = task_func('/path/to/directory')\n```\n\nThis function ensures the target directory and its 'processed' subdirectory exist, processes each JSON file by reading it into a DataFrame, adds a \"Source\" column indicating the file's origin, and then moves the processed file to the 'processed' subdirectory. All individual DataFrames are concatenated into a single DataFrame which is then returned. If no JSON files are found, an empty DataFrame is returned."}
{"task_id": "BigCodeBench/720", "solution": "```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    \n    # Generate random sensor data\n    data = [\n        {\n            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'temperature': round(random.uniform(20, 30), 2),\n            'humidity': round(random.uniform(30, 70), 2)\n        }\n        for _ in range(10)  # Generate 10 data points\n    ]\n    \n    # Write data to CSV file in append mode\n    with open(file_path, 'a', newline='') as csvfile:\n        fieldnames = ['timestamp', 'temperature', 'humidity']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header if file is empty\n        if os.stat(file_path).st_size == 0:\n            writer.writeheader()\n        \n        for row in data:\n            writer.writerow(row)\n    \n    # Store the file path before deletion\n    result = file_path\n    \n    # Delete the file\n    os.remove(file_path)\n    \n    # Return the path to the CSV file before deletion\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    print(task_func())\n```\n\nThis function performs the following steps:\n1. It sets up the file path for the CSV file.\n2. It ensures the necessary directory exists, creating it if necessary.\n3. It generates a list of dictionaries, each representing a set of sensor data (temperature and humidity) with a timestamp.\n4. It writes this data to the CSV file in append mode, ensuring to write a header if the file is new.\n5. It returns the file path to the user.\n6. Before the function exits, it deletes the CSV file, thereby cleaning up after its usage."}
{"task_id": "BigCodeBench/723", "solution": "Here's a self-contained Python function that scrapes data from a given URL and saves the scraped data to a CSV file. The function uses `urllib.request` to fetch the webpage, `BeautifulSoup` from the `bs4` library to parse the HTML, and Python's built-in `csv` module to write the data to a CSV file.\n\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    try:\n        # Fetch the HTML content from the URL\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n\n        # Parse the HTML content\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Extract data from the parsed HTML\n        # Example: extracting <a> tags with href attributes\n        data = []\n        for a in soup.find_all('a', href=True):\n            data.append([a['href'], a.text.strip()])\n\n        # Save the scraped data to a CSV file\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['URL', 'Text'])  # Header row\n            writer.writerows(data)\n\n        return CSV_FILE_PATH\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"https://example.com\"  # Replace with the actual URL you want to scrape\n    result = task_func(url)\n    if result:\n        print(f\"Scraped data saved to: {result}\")\n    else:\n        print(\"Failed to scrape data\")\n```\n\nThis function assumes that you want to scrape `<a>` tags with `href` attributes from the webpage. You can modify the `soup.find_all` line to target different HTML elements or attributes based on the specific data you want to scrape. The scraped data is then written to a CSV file with columns for the URL and the text content of the `<a>` tags. Adjust the column headers and data extraction logic as needed for your specific use case."}
{"task_id": "BigCodeBench/752", "solution": "```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validate that the input data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a DataFrame.\")\n    \n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"Data must not be empty.\")\n    \n    # Ensure the target column is part of the DataFrame\n    if target_column not in data.columns:\n        raise ValueError(f\"Target column '{target_column}' is not a column of the DataFrame.\")\n    \n    # Confirm that all values in the DataFrame are numeric\n    if not data.select_dtypes(include=[np.number]).columns.tolist() == list(data.columns):\n        raise ValueError(\"All columns in the DataFrame must contain numeric values.\")\n    \n    # Validate that random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be an integer.\")\n    \n    # Validate that test_size is a float in the range (0, 1)\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size must be between 0 and 1.\")\n    \n    # Split the data into training and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Calculate and return the model's score on the test set\n    return model.score(X_test, y_test)\n```\n\nThis function ensures that all necessary input validations are performed before proceeding with data splitting and model training. It raises the appropriate `ValueError` exceptions for invalid inputs, as specified. When used correctly, this function will return the model's score on the test set as a float."}
{"task_id": "BigCodeBench/760", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Generate random data\n    num_records = 100\n    ids = np.arange(1, num_records + 1)\n    \n    # Combine and correct names\n    all_names = latin_names + other_names\n    all_names = [codecs.decode(name.encode('latin-1'), 'utf-8') for name in all_names]\n    names = np.random.choice(all_names, num_records)\n    \n    # Generate random dates\n    start_date = datetime.datetime(start_year, 1, 1)\n    end_date = datetime.datetime(end_year, 12, 31)\n    date_range = (end_date - start_date).days\n    random_days = np.random.randint(0, date_range, num_records)\n    dates_of_birth = [start_date + datetime.timedelta(days=day) for day in random_days]\n    \n    # Generate emails\n    def create_email(name, dob):\n        name_part = re.sub(r'[^\\w]', '', name.lower())\n        year_part = str(dob.year)\n        return f\"{name_part}{year_part}@{email_domain}\"\n    \n    emails = [create_email(name, dob) for name, dob in zip(names, dates_of_birth)]\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n    \n    return df\n\n# Example usage\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\nprint(df)\n```"}
{"task_id": "BigCodeBench/763", "solution": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Validate the input data format\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"Input JSON must be a list of dictionaries\")\n\n    # Initialize defaultdict to store values for each key\n    values_dict = defaultdict(list)\n\n    # Collect values for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, (int, float)):  # Ensure the value is numeric\n                values_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    result = {}\n    for key, values in values_dict.items():\n        result[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key, stats in result.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n\n    return result\n\n# Example usage:\n# result = task_func('input.json', 'output.csv')\n# print(result)\n```\n\nThis code snippet defines a function `task_func` that takes an input JSON file and an output CSV file as parameters. It reads the JSON file into a list of dictionaries, verifies that the data is in the correct format, and collects all numeric values for each key. It then computes the mean and median for these values using NumPy and writes these statistics to a specified CSV file. The function finally returns a dictionary containing the mean and median for each key.\n\nTo use this function in a real scenario, you just need to provide the paths to your input JSON and output CSV files."}
{"task_id": "BigCodeBench/765", "solution": "```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    \n    # Ensure the target directory exists\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    for file_path in kwargs.values():\n        file_path = Path(file_path)\n        \n        # Check if the file exists and is not empty\n        if file_path.exists() and file_path.stat().st_size > 0:\n            # Copy the file to the target directory\n            dest_path = target_path / file_path.name\n            shutil.copy2(file_path, dest_path)\n            copied_files.append(str(dest_path.resolve()))\n    \n    return {\"copied_files\": copied_files}\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dictionary of files\n    files_dict = {\n        \"file1\": \"path/to/file1.txt\",\n        \"file2\": \"path/to/file2.txt\",\n        \"file3\": \"path/to/file3.txt\"\n    }\n    \n    # Call the function\n    result = task_func(files_dict)\n    \n    # Print the copied files\n    print(\"Copied files:\")\n    for file in result[\"copied_files\"]:\n        print(file)\n```\n\nThis Python script defines a function `task_func` that processes a dictionary of file paths (`kwargs`), checks if each file exists and has content, and copies them to a specified directory (`target_dir`). It outputs a dictionary containing a list of full paths to the files that were successfully copied. The example usage demonstrates how to call this function with a sample dictionary and print the results."}
{"task_id": "BigCodeBench/771", "solution": "Here's a comprehensive Python function that processes CSV files in a directory based on a specified pattern, creates new files with altered names while preserving the content, and returns a list of the new filenames:\n\n```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    directory_path = Path(directory)\n    \n    # Ensure the directory exists\n    if not directory_path.is_dir():\n        raise ValueError(f\"The directory {directory} does not exist.\")\n    \n    # Compile the regex pattern\n    regex = re.compile(pattern)\n    \n    # Iterate over all files in the directory\n    for file_path in directory_path.glob('*.csv'):\n        match = regex.match(file_path.name)\n        if match:\n            # Extract the base name from the pattern\n            base_name = match.group(1)\n            # Create the new file name\n            new_file_name = f\"{base_name}_new.csv\"\n            new_file_path = directory_path / new_file_name\n            \n            # Copy the content to the new file\n            with open(file_path, 'r', newline='') as old_file, \\\n                 open(new_file_path, 'w', newline='') as new_file:\n                reader = csv.reader(old_file)\n                writer = csv.writer(new_file)\n                for row in reader:\n                    writer.writerow(row)\n            \n            # Append the new file name to the list\n            new_files.append(new_file_name)\n    \n    return new_files\n\n# Example usage:\nif __name__ == \"__main__\":\n    directory = \"path/to/your/directory\"\n    result = task_func(directory)\n    print(f\"New files created: {result}\")\n```\n\n### Explanation:\n1. **Imports**: Necessary Python modules are imported, including `re` for regex operations, `os` and `Path` for filesystem navigation, and `csv` for handling CSV files.\n2. **Function Definition**: The function `task_func` is defined to take a directory path and an optional regex pattern.\n3. **Directory Check**: Checks if the specified directory exists. If not, it raises a `ValueError`.\n4. **Regex Compilation**: The regex pattern is compiled to match CSV filenames with a specific format.\n5. **File Processing**: Iterates over all CSV files in the directory:\n   - Uses the regex to match and extract the base name of the file.\n   - Creates a new filename by appending `_new` to the base name.\n   - Copies the content of the original file to the new file using `csv.reader` and `csv.writer`.\n   - Adds the new filename to the list of new files.\n6. **Return**: Returns a list of new filenames created during the process.\n\nThis function ensures that all CSV files matching the specified pattern are processed, and their contents are preserved in new files with altered names, which are then returned in a list."}
{"task_id": "BigCodeBench/777", "solution": "```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = set()\n\n    # Ensure the input directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The specified directory '{directory}' does not exist.\")\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Iterate through all files in the directory\n    for filename in os.listdir(directory):\n        match = regex.match(filename)\n        if match:\n            # Get the full path of the zip file\n            zip_path = os.path.join(directory, filename)\n            \n            # Extract the prefix (directory name) from the filename using the last occurrence of \"-\"\n            prefix = filename.rsplit('-', 1)[0]\n            \n            # Create the extraction directory\n            extract_dir = os.path.join(directory, prefix)\n            os.makedirs(extract_dir, exist_ok=True)\n            \n            # Unzip the file\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n            \n            # Add the extraction directory to the set\n            extracted_dirs.add(extract_dir)\n    \n    # Convert the set to a list and return it\n    return list(extracted_dirs)\n```\n\nThis function searches for zip files in the directory that match the specified pattern. It splits the filename at the last occurrence of \"-\" to determine the prefix, which is used as the directory name for extraction. The function then extracts each matching zip file into its respective directory and returns a list of these directories. The use of a set ensures that each directory is listed only once, even if multiple files are extracted into the same directory."}
{"task_id": "BigCodeBench/785", "solution": "```python\nimport subprocess\nimport os\nimport glob\nfrom datetime import datetime\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    if not files:\n        print(f\"No files found matching the pattern: {pattern}\")\n        return None\n\n    # Create a unique archive filename using the current timestamp for uniqueness\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    archive_filename = f\"archive_{timestamp}.tar.gz\"\n    archive_path = os.path.join(ARCHIVE_DIR, archive_filename)\n\n    # Create the archive using tar command\n    try:\n        subprocess.run(['tar', '-czf', archive_path] + files, check=True)\n        # Delete original files\n        for file in files:\n            os.remove(file)\n\n        return archive_path\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating archive: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage:\narchive_file = task_func(\"/path/to/files/*.txt\")\nif archive_file:\n    print(f\"Archive created: {archive_file}\")\nelse:\n    print(\"Archive creation failed\")\n```\n\nThis script:\n- Ensures the archive directory exists.\n- Finds all files matching the given pattern.\n- If no files are found, it prints a message and returns `None`.\n- It creates a unique archive filename using the current timestamp.\n- Uses the `tar` command (via `subprocess.run()`) to create a gzip-compressed tar archive of the matching files.\n- If the archive is successfully created, it deletes the original files.\n- Returns the path of the created archive file. If an error occurs, it handles it gracefully and returns `None`."}
{"task_id": "BigCodeBench/800", "solution": "Here's the complete code for the function `task_func` which reads data from a CSV file, counts the total number of goals and penalties, updates them with the given goals and penalties, and returns a Counter object with the updated counts:\n\n```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    count = Counter()\n\n    # Read existing data from CSV file\n    try:\n        with open(csv_file_path, 'r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                count['goals'] += int(row['goals'])\n                count['penalties'] += int(row['penalties'])\n    except FileNotFoundError:\n        # If the file doesn't exist, start with empty counts\n        print(\"File not found. Starting with empty counts.\")\n\n    # Update counts with new goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    return count\n\n# Example usage:\nif __name__ == \"__main__\":\n    os.makedirs('test_data', exist_ok=True)\n    setup_csv()\n    \n    # Call the function\n    result = task_func(2, 1, 'test_data/test_case_2.csv')\n    print(result)\n```\n\nThis implementation first checks for the existence of the CSV file, reads the current count of goals and penalties if available, and updates those counts with the new values provided as function arguments. If the CSV file does not exist, it starts with zero counts. The function returns a `Counter` object containing the total counts of goals and penalties after the update. The example usage section demonstrates how to use this function in a practical scenario, including setting up a sample CSV file for testing purposes."}
{"task_id": "BigCodeBench/826", "solution": "Here's a Python function that accomplishes the task of moving files from a source directory to the target directory based on a specified pattern. The function ensures the creation of the target directory if it doesn't exist, compiles the regex pattern for file matching, and handles exceptions during the file-moving process, providing robust error handling.\n\n```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Compile the regex pattern\n    pattern = re.compile(file_pattern)\n\n    moved_files_count = 0\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the filename matches the pattern\n        if pattern.match(filename):\n            source_path = os.path.join(source_dir, filename)\n            target_path = os.path.join(target_dir, filename)\n\n            try:\n                # Move the file\n                shutil.move(source_path, target_path)\n                moved_files_count += 1\n            except Exception as e:\n                print(f\"Error moving file {filename}: {str(e)}\")\n\n    return moved_files_count\n\n# Example usage:\n# source_dir = \"/path/to/source\"\n# target_dir = \"/path/to/target\"\n# moved_count = task_func(source_dir, target_dir)\n# print(f\"Moved {moved_count} files.\")\n```\n\nThis function does the following:\n1. Ensures that the target directory exists and creates it if necessary.\n2. Compiles the specified regex pattern for matching file names.\n3. Iterates through all files in the source directory, checking if each file's name matches the regex pattern.\n4. If a match is found, the function attempts to move the file from the source to the target directory. If an error occurs during the move, it catches the exception and prints an error message.\n5. It counts and returns the number of files successfully moved.\n\nThis code is robust and handles exceptions, ensuring that the process does not break unexpectedly due to file-related errors."}
{"task_id": "BigCodeBench/845", "solution": "```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Clean texts\n    clean_text1 = ALPHANUMERIC.sub(' ', text1.lower()).strip()\n    clean_text2 = ALPHANUMERIC.sub(' ', text2.lower()).strip()\n    \n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(clean_text1, clean_text2)\n    \n    # Tokenize texts\n    tokens1 = clean_text1.split()\n    tokens2 = clean_text2.split()\n    \n    # Create term frequency dictionaries\n    tf1 = Counter(tokens1)\n    tf2 = Counter(tokens2)\n    \n    # Get unique words from both texts\n    all_words = set(tf1.keys()).union(tf2.keys())\n    \n    # Create term frequency vectors\n    vec1 = np.array([tf1.get(word, 0) for word in all_words])\n    vec2 = np.array([tf2.get(word, 0) for word in all_words])\n    \n    # Calculate cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n    \n    # Calculate cosine similarity, handle division by zero in case of empty vectors\n    if norm1 == 0 or norm2 == 0:\n        cosine_similarity = 0.0\n    else:\n        cosine_similarity = dot_product / (norm1 * norm2)\n    \n    return (cosine_similarity, levenshtein_ratio)\n```\n\nThis code defines a function `task_func` that computes the cosine similarity and Levenshtein ratio for two given texts. It starts by cleaning the texts to remove non-alphanumeric characters except spaces and converts them to lowercase. The function then computes the Levenshtein ratio directly from these cleaned texts. For the cosine similarity, it tokenizes the texts, calculates term frequencies, creates corresponding vectors, and finally computes the cosine similarity using the dot product and norms of these vectors. The function returns both similarity measures as a tuple of floats, ensuring that the cosine similarity calculation handles potential division by zero gracefully."}
{"task_id": "BigCodeBench/854", "solution": "Here's a Python function that satisfies your requirements:\n\n```python\nfrom functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Check if input is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all elements are non-negative integers\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"Input must contain non-negative integers only\")\n    \n    # If the input list is empty, return empty lists\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations\n    perms = list(permutations(numbers))\n    \n    # Calculate factorial sums for each permutation\n    factorial_sums = [\n        sum(math.factorial(num) for num in perm)\n        for perm in perms\n    ]\n    \n    # Convert permutations from tuples to lists\n    perms_as_lists = [list(perm) for perm in perms]\n    \n    return factorial_sums, perms_as_lists\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        fac, perm = task_func([0, 4])\n        print(\"Factorial sums:\", fac)\n        print(\"Permutations:\", perm)\n        \n        # Test with empty list\n        fac, perm = task_func([])\n        print(\"Empty list result:\")\n        print(\"Factorial sums:\", fac)\n        print(\"Permutations:\", perm)\n        \n        # These will raise exceptions:\n        # task_func(\"not a list\")  # TypeError\n        # task_func([1, -2, 3])    # ValueError\n        \n    except (TypeError, ValueError) as e:\n        print(f\"Error: {e}\")\n```\n\nThis implementation checks if the input is a list of non-negative integers and raises the appropriate exceptions if the conditions are not met. It then calculates the factorial sums of all permutations of the list. If the list is empty, it returns empty lists for both the sums and permutations. The function converts permutations from tuples to lists before returning to match the specified output format."}
{"task_id": "BigCodeBench/857", "solution": "```python\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure the destination directory exists\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Normalize extensions and create file patterns\n    file_patterns = [os.path.join(SOURCE_DIR, f\"*{ext}\") for ext in EXTENSIONS]\n\n    # Iterate through all matching files\n    for pattern in file_patterns:\n        for file_path in glob.glob(pattern):\n            file_name = os.path.basename(file_path)\n            dest_path = os.path.join(DEST_DIR, file_name)\n\n            try:\n                # Check if the file already exists in the destination\n                if os.path.exists(dest_path):\n                    # If it exists, add a timestamp to the filename\n                    base_name, ext = os.path.splitext(file_name)\n                    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n                    new_file_name = f\"{base_name}_{timestamp}{ext}\"\n                    dest_path = os.path.join(DEST_DIR, new_file_name)\n\n                # Copy the file to the destination\n                shutil.copy2(file_path, dest_path)\n                transferred_files.append(file_name)\n\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file_name}: {str(e)}\")\n\n    return transferred_files\n\n# Example usage:\nsource_dir = \"/path/to/source\"\ndest_dir = \"/path/to/destination\"\nextensions = [\".txt\", \".pdf\", \".docx\"]\n\ntransferred = task_func(source_dir, dest_dir, extensions)\nprint(f\"Successfully transferred files: {transferred}\")\n```\n\nThis function will transfer files from the specified `SOURCE_DIR` to `DEST_DIR` based on their extensions listed in `EXTENSIONS`. It checks for existing files in the destination directory and avoids overwriting by renaming with a timestamp. It issues warnings for any files that cannot be transferred due to errors and returns a list of successfully transferred file names."}
{"task_id": "BigCodeBench/865", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items, counts, weights = zip(*data)\n\n    # Convert counts and weights to numpy arrays for normalization\n    counts_array = np.array(counts)\n    weights_array = np.array(weights)\n\n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts_array)\n\n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights_array.reshape(-1, 1)).flatten()\n\n    # Create a pandas DataFrame with the normalized data\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n\n    return df\n\n# Example usage:\n# data = [('item1', 10, 0.5), ('item2', 20, 0.8), ('item3', 30, 1.2)]\n# result = task_func(data)\n# print(result)\n```\n\nThis function takes a list of tuples (`data`) where each tuple contains an item name, its count, and its weight. It normalizes the counts using z-score and the weights using min-max scaling. Finally, it returns a pandas DataFrame with columns for the item names, normalized counts, and normalized weights. Each row in the DataFrame corresponds to one of the entries from the input data."}
{"task_id": "BigCodeBench/870", "solution": "Here's a self-contained implementation of the function `task_func` that calculates the mean of numerical values in each position across tuples in a list, while ignoring non-numeric values:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Determine the maximum length of tuples in the list\n    max_length = max(len(t) for t in data_list)\n\n    # Initialize lists to store numeric values for each position\n    numeric_values = [[] for _ in range(max_length)]\n\n    # Iterate through each tuple in the list and each value in the tuple\n    for tuple_item in data_list:\n        for i, value in enumerate(tuple_item):\n            # Check if the value is numeric (int or float)\n            if isinstance(value, (int, float)):\n                if i < max_length:\n                    numeric_values[i].append(value)\n\n    # Calculate mean for each position, ignoring non-numeric values\n    means = [np.mean(values) if values else np.nan for values in numeric_values]\n\n    # Create a DataFrame with the results\n    df = pd.DataFrame({'Mean Value': means}, index=[f'Position {i}' for i in range(max_length)])\n\n    return df\n\n# Example usage\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)\nprint(df)\n```\n\nThis function checks for the maximum length of tuples to ensure that all positions are considered. It iterates through each tuple and value, checking if the value is numeric before appending it to the appropriate list. Means are then calculated using `np.mean()`, and a pandas DataFrame is created with these means. The DataFrame's index labels are formatted as 'Position i', where i is the current position. If an empty list is passed, an empty DataFrame is returned, ensuring robust handling of various input scenarios."}
{"task_id": "BigCodeBench/879", "solution": "Here's a Python function that performs a chi-square test of independence on two categorical variables within a given DataFrame. The function `task_func` takes a DataFrame and the names of two columns as input, constructs a contingency table, performs the chi-square test, and returns the p-value. It also includes various checks to validate the input data, ensuring that the assumptions for conducting a chi-square test are met.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the specified columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both columns ({col1}, {col2}) not found in the DataFrame.\")\n    \n    # Check if columns contain categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not pd.api.types.is_object_dtype(data[col1]):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not pd.api.types.is_object_dtype(data[col2]):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data.\")\n    \n    # Create the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if both columns have multiple categories\n    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Check if any category has less than 5 observations, which violates chi-square test assumptions\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating the chi-square test assumptions.\")\n    \n    # Perform the chi-square test of independence\n    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    return p_value\n\n# Example usage:\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})\ndata['a'] = data['a'].astype('category')\ndata['b'] = data['b'].astype('category')\n\nprint(task_func(data, 'a', 'b'))  # Output: 1.0\n```\nThis function first verifies that the DataFrame and the specified columns are appropriate for conducting a chi-square test. It then constructs a contingency table from the two specified columns, checks the assumptions necessary for a valid chi-square test, performs the test, and returns the p-value. The example demonstrates how to use the function with a sample DataFrame, where columns 'a' and 'b' are categorical."}
{"task_id": "BigCodeBench/897", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    dice_rolls = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(6, dtype=int)\n    for roll in dice_rolls:\n        frequency[roll - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(NUMBERS, frequency, color='blue', edgecolor='black')\n    ax.set_xlabel('Dice Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Dice Rolls')\n    ax.set_xticks(NUMBERS)\n    \n    # Add frequency labels on top of each bar\n    for i, v in enumerate(frequency):\n        ax.text(i + 1, v, str(v), ha='center', va='bottom')\n    \n    # Adjust y-axis to show all frequency labels\n    ax.set_ylim(0, max(frequency) * 1.1)\n    \n    return frequency, ax\n\n# Example usage:\nrolls = 1000\nfreq, ax = task_func(rolls, seed=42)\nprint(\"Frequency array:\", freq)\nplt.show()\n```\n\nThis Python code snippet defines a function `task_func` that simulates a specified number of dice rolls, calculates the frequency of each possible outcome (from 1 to 6), and generates a histogram of these results. The function returns a tuple containing a numpy array of frequencies and a matplotlib `Axes` object for the histogram. The histogram bar chart is enhanced with labels indicating the frequency on top of each bar, and the y-axis is adjusted to ensure all labels fit within the figure. You can customize the number of rolls and the seed for reproducibility in the example usage section."}
{"task_id": "BigCodeBench/906", "solution": "```python\nimport zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        # Walk through the source directory\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                # Check if the file has '_processed' suffix\n                if re.search(r'_processed\\.[^.]+$', file):\n                    file_path = os.path.join(root, file)\n                    # Add file to the zip archive\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n\n    # If the archive is empty (no processed files found), remove it\n    if os.path.getsize(archive_path) == 0:\n        os.remove(archive_path)\n        return \"No processed files found to archive.\"\n\n    return archive_path\n```"}
{"task_id": "BigCodeBench/914", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure the DataFrame has the required columns\n    if not 'Date' in df.columns or not 'Close' in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Close' columns\")\n    \n    # Convert 'Date' to datetime if it's not already\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values('Date')\n    \n    # Create features (X) based on the seconds since the minimum date\n    X = (df['Date'] - df['Date'].min()).dt.total_seconds().values.reshape(-1, 1)\n    y = df['Close'].values\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future dates using a constant time step of 24*60*60 seconds\n    last_date = df['Date'].max()\n    future_dates = [last_date + pd.Timedelta(seconds=24*60*60*i) for i in range(1, 8)]\n    \n    # Create future X values based on the seconds since the minimum date\n    future_X = np.array([(date - df['Date'].min()).total_seconds() for date in future_dates]).reshape(-1, 1)\n    \n    # Predict future prices\n    future_prices = model.predict(future_X)\n    \n    # Plot the data\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    ax.scatter(df['Date'], df['Close'], color='blue', label='Historical Data')\n    \n    # Plot regression line\n    ax.plot(df['Date'], model.predict(X), color='red', label='Regression Line')\n    \n    # Plot future predictions\n    ax.scatter(future_dates, future_prices, color='green', label='Future Predictions')\n    \n    # Customize the plot\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Price Prediction')\n    ax.legend()\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return (future_prices.tolist(), ax)\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Date': ['2023-01-01 00:00:00', '2023-01-02 00:00:00', '2023-01-03 00:00:00'],\n#     'Close': [100, 102, 101]\n# })\n# predicted_prices, axes = task_func(df)\n# print(predicted_prices)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/915", "solution": "Here's a self-contained Python function that identifies and plots outliers in the 'closing_price' column of a DataFrame using the Z-Score method. The function allows customization of the Z-score threshold via the `z_threshold` parameter, and it returns a tuple containing a DataFrame of the outliers and the plot object. The plot is properly labeled and tailored to highlight the outliers distinctly.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\n\ndef task_func(df, z_threshold=2):\n    # Calculate Z-scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-score threshold\n    outliers = df[np.abs(z_scores) > z_threshold]\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot all data points\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Normal')\n    \n    # Highlight outliers\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outlier')\n    \n    # Set plot labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    # Add horizontal lines for upper and lower thresholds\n    mean = df['closing_price'].mean()\n    std = df['closing_price'].std()\n    ax.axhline(y=mean + z_threshold * std, color='green', linestyle='--', label='Upper Threshold')\n    ax.axhline(y=mean - z_threshold * std, color='green', linestyle='--', label='Lower Threshold')\n    \n    # Update legend\n    ax.legend()\n    \n    # Display the plot\n    plt.show()\n    \n    # Return the outliers DataFrame and the plot object\n    return outliers, ax\n\n# Example usage:\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\n```\n\nThis function does the following:\n1. It calculates Z-scores for the 'closing_price' column.\n2. It identifies outliers by selecting data points where the absolute Z-score exceeds the specified threshold.\n3. It plots all data points and highlights the outliers in red.\n4. It adds horizontal lines to visually indicate the upper and lower thresholds based on the Z-score.\n5. It customizes the plot with appropriate labels and a title.\n6. The function returns a tuple containing the DataFrame of outliers and the plot object, which can be used or displayed as needed."}
{"task_id": "BigCodeBench/916", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Check if the DataFrame has the necessary 'Close' column\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column\")\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), gridspec_kw={'height_ratios': [1, 2]})\n\n    # Box plot for closing prices\n    sns.boxplot(x=df['Close'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n    ax1.set_xlabel('Closing Price')\n\n    # Histogram for closing prices\n    sns.histplot(df['Close'], kde=True, ax=ax2)\n    ax2.set_title('Histogram of Closing Prices')\n    ax2.set_xlabel('Closing Price')\n    ax2.set_ylabel('Frequency')\n\n    # Layout adjustment\n    plt.tight_layout()\n\n    return (ax1, ax2)\n\n# Example usage:\n# Assuming you have a DataFrame 'df' with a 'Close' column\n# df = pd.read_csv('your_stock_data.csv')\n# ax1, ax2 = task_func(df)\n# plt.show()\n```\n\nThis code provides a Python function named `task_func` that visualizes stock closing prices with a box plot and a histogram in a single figure. The function checks for the presence of a 'Close' column in the DataFrame, constructs the visualizations using `seaborn` and `matplotlib`, and returns the axes for further customization or inspection. To utilize this function, ensure your DataFrame has a 'Close' column and call `task_func` passing the DataFrame as an argument. Use `plt.show()` to display the plot."}
{"task_id": "BigCodeBench/917", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame is sorted by date and has a datetime index\n    df = df.sort_index()\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    \n    # Extract the 'Close' prices\n    close_prices = df['Close']\n\n    # Fit ARIMA model, order can be adjusted based on the data characteristics\n    model = ARIMA(close_prices, order=(1, 1, 1))  # Example order\n    results = model.fit()\n\n    # Forecast next 7 days\n    forecast = results.forecast(steps=7)\n    forecast_values = forecast.tolist()\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Plot historical data\n    ax.plot(close_prices.index, close_prices.values, label='Historical')\n\n    # Plot forecast\n    last_date = close_prices.index[-1]\n    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=7)\n    ax.plot(forecast_dates, forecast_values, color='red', label='Forecast')\n\n    # Fill the area between the historical data and the forecast\n    ax.fill_between(forecast_dates, forecast_values, color='red', alpha=0.3)\n\n    # Customize the plot\n    ax.set_title('Stock Price Forecast (ARIMA)')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Close Price')\n    ax.legend()\n    ax.grid(True)\n\n    # Rotate and align the tick labels so they look better\n    fig.autofmt_xdate()\n\n    # Use tight layout to prevent clipping of labels\n    plt.tight_layout()\n\n    return forecast_values, ax\n```"}
{"task_id": "BigCodeBench/928", "solution": "Here is a self-contained Python function that creates a dictionary containing all possible two-letter combinations of the lowercase English alphabets and counts their frequency in a given word. If a combination does not appear in the word, its value will be set to 0:\n\n```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of lowercase English alphabets\n    all_combinations = [''.join(combo) for combo in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    result = {combo: 0 for combo in all_combinations}\n    \n    # Count the occurrences of each two-letter combination in the word\n    word = word.lower()  # Convert the word to lowercase for case-insensitive processing\n    for i in range(len(word) - 1):\n        combo = word[i:i+2]\n        if combo in result:\n            result[combo] += 1\n    \n    return result\n\n# Example of how to use the function:\n# result = task_func(\"hello\")\n# print(result)\n```\n\nThis function:\n1. Imports necessary modules: `Counter`, `itertools`, and `string`.\n2. Uses `itertools.product()` combined with `string.ascii_lowercase` to generate all possible two-letter combinations of lowercase English letters.\n3. Initializes a dictionary `result` with each two-letter combination as a key and an initial count of 0.\n4. Converts the input `word` to lowercase to ensure that the counting is case-insensitive.\n5. Iterates over the word, examining each pair of adjacent letters, checks if the pair is in the dictionary, and increments the count if found.\n6. Returns the `result` dictionary containing the counts of two-letter combinations found in the word, with all other combinations remaining 0. \n\nYou can use this function by calling it with a string argument, and it will return the dictionary as described."}
{"task_id": "BigCodeBench/942", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create empty list to store data\n    data = []\n    \n    # Generate random sales data for each category\n    for category in categories:\n        base_sales = np.random.randint(1000, 5000)\n        trend = np.random.uniform(-0.1, 0.1)\n        noise = np.random.normal(0, 200, periods)\n        sales = base_sales + np.arange(periods) * trend * base_sales + noise\n        sales = np.maximum(sales, 0)  # Ensure non-negative sales\n        \n        for date, sale in zip(dates, sales):\n            data.append({'Date': date, 'Category': category, 'Sales': sale})\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    ax.grid(True)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func()\nprint(df.head())\nplt.show()\n```\n\nThis code defines a `task_func` function that generates a sales report for different product categories over a specified period and visualizes the sales data using a line plot. The function returns a DataFrame with sales data and the Matplotlib Axes object for further customization or analysis. The plot displays sales trends for each category over the specified dates, with each category differentiated by color and markers."}
{"task_id": "BigCodeBench/945", "solution": "Certainly! Below is a self-contained Python function that generates a time series of sales data starting from a specified date, uses linear regression to forecast future sales based on the provided or generated sales data, and outputs a numpy array containing the forecasted future sales for the same number of periods as the input data.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # If sales_data is not provided, generate random sales data\n    if sales_data is None:\n        np.random.seed(42)  # for reproducibility\n        sales_data = np.random.randint(1000, 5000, size=periods)\n\n    # Create a DataFrame with dates and sales data\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n\n    # Create feature matrix X (days since start) and target vector y (sales)\n    X = (df['Date'] - df['Date'].min()).dt.days.values.reshape(-1, 1)\n    y = df['Sales'].values\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future dates for forecasting\n    future_dates = pd.date_range(start=df['Date'].max() + pd.Timedelta(days=1), \n                                 periods=periods, \n                                 freq=freq)\n\n    # Create feature matrix for future dates\n    X_future = (future_dates - df['Date'].min()).days.values.reshape(-1, 1)\n\n    # Forecast future sales\n    future_sales = model.predict(X_future)\n\n    return future_sales.astype(int)\n\n# Example usage\nforecasted_sales = task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI')\nprint(forecasted_sales)\n```\n\nThis function does the following:\n1. Generates a date range based on the specified `start_date`, `periods`, and `freq`.\n2. If `sales_data` is not provided, it generates random sales data for reproducibility.\n3. Creates a DataFrame with the dates and sales data.\n4. Prepares the feature matrix `X` from the days since the start date and the target vector `y` from the sales data.\n5. Fits a linear regression model to the data.\n6. Generates future dates for forecasting, covering the same number of periods as the input data.\n7. Creates a feature matrix for these future dates.\n8. Uses the fitted model to predict future sales.\n9. Returns the forecasted future sales as a numpy array of integers."}
{"task_id": "BigCodeBench/952", "solution": "```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Validate the number of tasks\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n\n    # Set the random seed for reproducibility if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Ensure that the number of tasks requested does not exceed the list provided\n    if n_tasks > len(sanitized_tasks):\n        raise ValueError(\"n_tasks exceeds the number of available tasks\")\n\n    # Randomly select tasks\n    selected_tasks = random.sample(sanitized_tasks, n_tasks)\n\n    # Get the current date\n    current_date = datetime.now().date()\n\n    # Assign tasks to employees and set due dates\n    task_assignments = [{\n        'Task Name': task,\n        'Assigned To': random.choice(employees),\n        'Due Date': current_date\n    } for task in selected_tasks]\n\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(task_assignments)\n\n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    task_list = [\"Clean office\", \"Update website\", \"Prepare report\", \"Call clients\", \"Organize meeting\"]\n    n_tasks = 3\n    result = task_func(task_list, n_tasks)\n    print(result)\n```\n\nThis Python function efficiently assigns a specified number of tasks to employees, ensuring that the task names are sanitized and due dates are set to the current system date. It raises a `ValueError` if the number of tasks requested is negative or exceeds the number of tasks provided. The resulting DataFrame neatly organizes the task assignments."}
{"task_id": "BigCodeBench/955", "solution": "Here is a self-contained Python function that adheres to your specifications. It replaces spaces in given words (from the `mystrings` list) with underscores, then counts the frequency of each unique word in the input text, and finally plots these frequencies. The function raises a `ValueError` if the input text is empty.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text.strip():\n        raise ValueError(\"Input text is empty\")\n\n    # Convert text to lowercase and split into words\n    words = text.lower().split()\n\n    # Replace spaces with underscores in given words and convert to lowercase\n    modified_words = [word.replace(' ', '_').lower() for word in mystrings]\n\n    # Replace occurrences of given words in the text with their modified versions\n    modified_text = text.lower()\n    for original, modified in zip(mystrings, modified_words):\n        pattern = re.compile(r'\\b' + re.escape(original.lower()) + r'\\b')\n        modified_text = pattern.sub(modified, modified_text)\n\n    # Split the modified text into words and count frequencies\n    words = modified_text.split()\n    word_counts = Counter(words)\n\n    # Extract words and their frequencies\n    unique_words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(range(len(unique_words)), frequencies, tick_label=unique_words)\n    ax.set_xticks(range(len(unique_words)))\n    ax.set_xticklabels(unique_words, rotation=45, ha='right')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency')\n\n    plt.tight_layout()\n    return ax\n\n# Example usage:\nmystrings = [\"New York\", \"San Francisco\"]\ntext = \"I love New York and San Francisco. New York is a big city.\"\nax = task_func(mystrings, text)\nplt.show()\n```\n\nThis function processes the input text and the given words list (`mystrings`) in a case-insensitive manner. It modifies the given words by replacing spaces with underscores, applies these modifications to the text, counts the occurrences of each word, and then plots these frequencies. The plot displays each unique word on the x-axis in the order they appear after modification, with its corresponding frequency on the y-axis. The function returns the `Axes` object for further manipulation or display."}
{"task_id": "BigCodeBench/963", "solution": "```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Ensure the target directory exists\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n\n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create the full path for the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        # Iterate through all files in the source directory by extension\n        for ext in valid_extensions:\n            for file in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add file to the zip, preserving only the base filename\n                zipf.write(file, os.path.basename(file))\n\n    # Return the full path to the created zip file\n    return zip_path\n\n# Example usage:\n# result = task_func(\"/path/to/source\", \"/path/to/target\", \"my_zip\")\n# print(result)\n```\n\nThis Python function `task_func` efficiently zips files with the specified extensions from a source directory and saves the zip file in a target directory, returning the full path to the created zip file. It checks for the existence of the source directory and ensures the target directory is prepared before proceeding with file compression. The function adheres to the requirement of raising an `OSError` if the source directory is not found, ensuring robust error handling."}
{"task_id": "BigCodeBench/964", "solution": "Here's a self-contained Python function that meets your requirements for converting files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files, and saving them in a target directory. The function captures each file's text content and stores it in a CSV with a single 'Text' column, overwriting existing files if necessary. It also appropriately handles the case where the source directory does not exist by raising a `FileNotFoundError`.\n\n```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl\nimport csv\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    converted_count = 0\n    # Set of allowed file extensions\n    allowed_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_directory):\n        file_path = os.path.join(source_directory, filename)\n        file_extension = Path(file_path).suffix.lower()\n        \n        if file_extension in allowed_extensions:\n            text_content = \"\"\n            \n            # Process files based on their extension\n            if file_extension == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    text_content = file.read()\n            elif file_extension == '.docx':\n                doc = docx.Document(file_path)\n                text_content = \"\\n\".join([para.text for para in doc.paragraphs])\n            elif file_extension == '.xlsx':\n                wb = openpyxl.load_workbook(file_path)\n                sheet = wb.active\n                text_content = \"\\n\".join([\" \".join([str(cell.value) if cell.value is not None else \"\" for cell in row]) for row in sheet.iter_rows()])\n            elif file_extension == '.csv':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    csv_reader = csv.reader(file)\n                    text_content = \"\\n\".join([\" \".join(row) for row in csv_reader])\n\n            # Create a DataFrame with a single 'Text' column\n            df = pd.DataFrame({'Text': [text_content]})\n            \n            # Save as CSV in the target directory\n            csv_filename = Path(filename).stem + '.csv'\n            csv_path = os.path.join(target_directory, csv_filename)\n            df.to_csv(csv_path, index=False)\n            \n            # Increment the counter for converted files\n            converted_count += 1\n\n    return converted_count\n```\n\nThis function systematically checks for the existence of the source directory, reads files based on their extension, converts their content to a single column CSV format, and counts the number of files processed. Dependency `openpyxl` needs to be installed for handling `.xlsx` files."}
{"task_id": "BigCodeBench/969", "solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    \n    # Check for non-numeric data types\n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    # Check for NaN values\n    if df.isna().any().any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Create a new DataFrame to store the results\n    result_df = pd.DataFrame()\n    \n    # Process each column\n    for column in df.columns:\n        # Compute cumulative sum\n        cumsum = df[column].cumsum()\n        \n        # Reshape for MinMaxScaler\n        cumsum_reshaped = cumsum.values.reshape(-1, 1)\n        \n        # Apply MinMaxScaler\n        normalized_cumsum = scaler.fit_transform(cumsum_reshaped)\n        \n        # Add to result DataFrame\n        result_df[column] = normalized_cumsum.flatten()\n    \n    return result_df\n```\n\nThis function checks for empty data, non-numeric types, and NaN values in a DataFrame, then computes the MinMax-normalized cumulative sum for each numeric column. It raises appropriate exceptions for invalid conditions and returns a DataFrame maintaining the original column names."}
{"task_id": "BigCodeBench/971", "solution": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Check if the directory exists\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n\n    result = []\n    \n    # Use os.scandir() to efficiently iterate over entries in the directory\n    for entry in os.scandir(directory_path):\n        if entry.is_file():  # Process only files, ignore subdirectories\n            # Retrieve file statistics\n            stats = entry.stat()\n            \n            # Extract the file size\n            size = stats.st_size\n            \n            # Attempt to use the creation time or fallback to the last metadata change time\n            try:\n                creation_time = datetime.fromtimestamp(stats.st_ctime, tz=timezone.utc)\n            except AttributeError:\n                creation_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc)\n            \n            # Extract the last modification time\n            modification_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc)\n            \n            # Format times in ISO format and append the result as a tuple\n            result.append((\n                entry.name,\n                size,\n                creation_time.isoformat(),\n                modification_time.isoformat()\n            ))\n    \n    return result\n```\n\nThis implementation ensures:\n\n1. The function checks if the provided directory exists and raises a `ValueError` if it does not.\n2. Only files within the directory are processed, and subdirectories are ignored.\n3. File information including the name, size, creation time, and modification time is retrieved. If the creation time is not available, the last metadata change time is used instead.\n4. All times are reported in UTC and formatted in ISO format.\n5. The function returns a list of tuples containing the required file information. If the directory is empty, an empty list is returned."}
{"task_id": "BigCodeBench/985", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Validate and parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    \n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n    \n    # Prepare data for DataFrame\n    countries = []\n    populations = []\n\n    for country, population in data.items():\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name encountered\")\n        \n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Invalid population value encountered\")\n        \n        countries.append(country)\n        populations.append(math.floor(population))  # Round down float populations\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Country\": countries,\n        \"Population\": populations\n    })\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(f\"Unable to write to file: {file_path}\")\n\n    return file_path, df\n```\n\nThis function starts by attempting to parse the JSON input data. It checks for common issues such as malformed JSON or empty data, raising a `ValueError` if any issues are found. It then iterates through each item in the parsed JSON, validating the types and values of each country-population pair, and rounds down any floating-point population values.\n\nThe function proceeds to create a pandas DataFrame from the validated data, ensuring that the output directory exists before attempting to write the DataFrame to a CSV file. If the CSV file cannot be written due to an I/O error, the function raises an `IOError`.\n\nFinally, the function returns the file path of the generated CSV and the DataFrame itself. This ensures that users of the function can both retrieve the data programmatically and access the CSV file directly."}
{"task_id": "BigCodeBench/988", "solution": "Here is a self-contained Python function that meets your requirements:\n\n```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if the directory exists and is a directory\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n\n    # Validate and deduplicate predicates\n    valid_predicates = list(set(pred for pred in predicates if callable(pred)))\n\n    if not valid_predicates:\n        raise ValueError(\"No valid predicates provided.\")\n\n    result = {}\n\n    # Iterate through items in the directory\n    for item in os.listdir(dir_path):\n        # Evaluate each predicate for the current item name\n        predicate_results = [pred(item) for pred in valid_predicates]\n        \n        # Store the results in the dictionary\n        result[item] = predicate_results\n\n    return result\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    # Define some example predicates\n    def starts_with_a(name):\n        return name.startswith('a')\n\n    def is_python_file(name):\n        return name.endswith('.py')\n\n    def contains_number(name):\n        return bool(re.search(r'\\d', name))\n\n    # Test case 1: Valid directory and predicates\n    try:\n        result = task_func('.', [starts_with_a, is_python_file, contains_number])\n        print(\"Test case 1 result:\", result)\n    except Exception as e:\n        print(\"Test case 1 error:\", str(e))\n\n    # Test case 2: Invalid directory\n    try:\n        result = task_func('/nonexistent/directory', [starts_with_a])\n        print(\"Test case 2 result:\", result)\n    except Exception as e:\n        print(\"Test case 2 error:\", str(e))\n\n    # Test case 3: No valid predicates\n    try:\n        result = task_func('.', [\"not a function\"])\n        print(\"Test case 3 result:\", result)\n    except Exception as e:\n        print(\"Test case 3 error:\", str(e))\n\n    # Test case 4: Duplicate predicates\n    try:\n        result = task_func('.', [starts_with_a, starts_with_a, is_python_file])\n        print(\"Test case 4 result:\", result)\n    except Exception as e:\n        print(\"Test case 4 error:\", str(e))\n```\n\nThis function checks if each item (file or directory) in the specified directory matches the given conditions (predicates). It handles errors such as non-existent directories and invalid or no predicates. The function returns a dictionary where each key is an item from the directory, and the corresponding value is a list of boolean results from evaluating the predicates against the item name."}
{"task_id": "BigCodeBench/990", "solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    try:\n        # Decode the hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to UTF-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Prepare the result dictionary\n        result = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('ascii'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        # Try ASCII encoding, if not possible, set to 'Not representable in ASCII'\n        try:\n            result['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n        except UnicodeEncodeError:\n            result['ASCII'] = 'Not representable in ASCII'\n        \n        return result\n    \n    except (binascii.Error, UnicodeDecodeError):\n        # Return an error message if the input is not a valid hexadecimal string\n        return \"Invalid hexadecimal input\"\n\n# Example usage:\nprint(task_func(\"68656c6c6f\"))\n```\n\nThis code defines a function `task_func` that takes a hexadecimal string as input and decodes it to various formats. It comprehensively handles potential errors, such as invalid hex inputs or non-ASCII characters in the UTF-8 decoded string. If the input is valid, the function returns a dictionary with the string encoded in multiple formats including its original hexadecimal, base64, UTF-8, UTF-16, UTF-32, URL encoding, ROT13, and ASCII (or a note if ASCII representation is not possible)."}
{"task_id": "BigCodeBench/998", "solution": "```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_checksum = md5_hash.hexdigest()\n\n        # Validate the checksum\n        if calculated_checksum != EXPECTED_MD5_CHECKSUM:\n            os.remove(TARGET_TAR_FILE)\n            return False\n\n        # Extract the contents of the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n            tar.extractall()\n        return True\n\n    except Exception as e:\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        print(f\"An error occurred: {str(e)}\")\n        return False\n\n# Example usage:\nresult = task_func(\"http://example.com/sample.tar.gz\")\nprint(result)\n```\n\nThis function performs the following tasks:\n\n1. Downloads a tar.gz file from a specified URL.\n2. Calculates its MD5 checksum.\n3. Compares the calculated checksum with the expected value.\n4. If the checksums match, it extracts the contents of the tar.gz file.\n5. If the checksums do not match, it deletes the downloaded file.\n6. Handles exceptions and ensures the downloaded file is deleted if an error occurs.\n7. Returns `True` if the file is successfully downloaded, its checksum matches the expected value, and it is extracted. Returns `False` otherwise."}
{"task_id": "BigCodeBench/999", "solution": "```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Attempt to download the CSV file from the specified URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a counter to keep track of value occurrences in the specified column\n        value_counts = collections.Counter()\n        \n        # Open and process the CSV file\n        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n            csv_reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists in the CSV file\n            if column_name not in csv_reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in csv_reader:\n                value = row[column_name]\n                value_counts[value] += 1\n\n        # Return the dictionary with counts of each value\n        return dict(value_counts)\n\n    except urllib.error.URLError as e:\n        # Handle errors that occur during the file download\n        raise Exception(f\"Error downloading the file: {e}\")\n    \n    except csv.Error as e:\n        # Handle errors that occur during the processing of the CSV file\n        raise Exception(f\"Error reading the CSV file: {e}\")\n    \n    except Exception as e:\n        # Handle any other unexpected errors\n        raise Exception(f\"An unexpected error occurred: {e}\")\n    \n    finally:\n        # Ensure the downloaded CSV file is deleted after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n```\n\nThis function is structured to address multiple error scenarios effectively:\n1. It uses `urllib.request.urlretrieve` to download the CSV from a URL to a specified local file path.\n2. It employs `csv.DictReader` to read the CSV file and check the existence of the specified column, raising a `ValueError` if the column is not present.\n3. It counts the occurrences of each value in the specified column using a `Counter`, which simplifies collecting the count of each unique entry.\n4. The function handles file download errors, CSV reading errors, and other exceptions by raising exceptions with appropriate messages.\n5. The CSV file is deleted after its contents have been processed, regardless of whether an error occurred, using a `finally` block to ensure cleanup."}
{"task_id": "BigCodeBench/1003", "solution": "Here's a complete Python function that fetches and parses an XML file from a specified URL and converts it into a Pandas DataFrame. The function is designed to raise a `ValueError` in various error scenarios, including invalid URLs, XML syntax errors, or non-conforming XML structures.\n\n```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML content from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Invalid URL or unable to fetch XML file: {e}\")\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the XML structure conforms to the expected format\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    # Extract data from each 'item' element\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    # Ensure there is data to create a DataFrame\n    if not data:\n        raise ValueError(\"No data found in the XML file\")\n\n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage:\n# try:\n#     df = task_func('http://example.com/valid_structure.xml')\n#     print(df)\n# except ValueError as e:\n#     print(e)\n```\n\n### Explanation:\n1. **Fetching XML Content:** The function starts by attempting to fetch the XML file from the provided URL using `urllib.request.urlopen()`. If this operation fails due to an invalid URL or network issue, it raises a `ValueError` with an appropriate message.\n\n2. **Parsing XML Content:** Next, the XML content is parsed with `lxml.etree.fromstring()`. If the XML content has syntax errors, a `ValueError` is raised detailing the syntax issue.\n\n3. **Validating XML Structure:** The function checks for the presence of `item` elements, which are expected to be the main data-carrying elements of the XML structure. If none are found, it raises a `ValueError` indicating that the XML structure does not match the expected format.\n\n4. **Extracting Data:** For each `item` element, it extracts child elements and their text content, creating a dictionary for each item. These dictionaries serve as rows for the resulting DataFrame.\n\n5. **Creating DataFrame:** The extracted data is used to create a Pandas DataFrame. If no data is extracted, a `ValueError` is raised noting the absence of data in the XML file.\n\n6. **Return Value:** Finally, if all steps are successful and the DataFrame is created, it is returned.\n\nThis function effectively handles various potential errors and ensures that the resulting DataFrame accurately represents the structured data from the XML file, provided the XML is well-formed and matches the expected schema."}
{"task_id": "BigCodeBench/1004", "solution": "```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Attempt to download the text file from the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            text = response.read().decode('utf-8')\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n        return None, None\n\n    # Use a regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Create a Counter object to count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    top_ten = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_ten)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(words, counts)\n    ax.set_title('Ten Most Frequently Occurring Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    # Return the Counter object and the Axes object\n    return word_counts, ax\n```\n\nThis Python function accomplishes the task as outlined:\n1. It attempts to download the text file from the specified URL, handling possible exceptions due to issues like network errors or incorrect URL formats.\n2. It reads the content and decodes it assuming UTF-8 text encoding.\n3. It uses a regular expression (`\\b\\w+\\b`) to find all words in the text. This pattern matches sequences of alphanumeric characters and underscores.\n4. It counts the frequency of each word using a `Counter` from the `collections` module.\n5. It identifies and extracts the ten most common words for visualization.\n6. It plots these words and their corresponding frequencies in a bar chart using `matplotlib`.\n7. The function returns a tuple containing the `Counter` object with all word frequencies and the `Axes` object of the plot, allowing further manipulation or display of the plot outside the function.\n\nTo use this function, provide the URL of a plain text file as an argument, and optionally display the plot or analyze the frequency data:\n\n```python\nurl = \"http://example.com/sample_text.txt\"\nword_counts, ax = task_func(url)\n\nif word_counts and ax:\n    print(word_counts)\n    plt.show()  # This line displays the plot when running in a script or interactive environment\n```"}
{"task_id": "BigCodeBench/1006", "solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Attempt to download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for bad HTTP status codes\n\n        # Validate if the content type is 'application/zip'\n        content_type = response.headers.get('Content-Type', '').lower()\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Save the downloaded ZIP file\n        file_name = os.path.join(download_path, 'downloaded_file.zip')\n        with open(file_name, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Create a directory for extraction\n        extract_path = os.path.join(download_path, 'extracted')\n        os.makedirs(extract_path, exist_ok=True)\n        \n        # Try extracting the ZIP file\n        try:\n            with ZipFile(file_name, 'r') as zip_ref:\n                zip_ref.extractall(extract_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Optionally, remove the downloaded ZIP file after extraction\n        os.remove(file_name)\n\n        # Return the path to the extracted contents\n        return extract_path\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage:\n# result = task_func(\"https://example.com/sample.zip\")\n# print(result)\n```\n\nThis Python function `task_func` is designed to download and extract a ZIP file from a given URL to a specified directory. It handles various error conditions such as network issues, incorrect file type, and corrupt ZIP files. The function will return either the path to the directory containing the extracted contents or a descriptive error message, depending on the outcome of the operation."}
{"task_id": "BigCodeBench/1008", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the table with the specified ID\n        table = soup.find('table', id=table_id)\n        if table is None:\n            raise ValueError(\"Table with the specified ID not found.\")\n\n        # Check if the table has any rows\n        rows = table.find_all('tr')\n        if not rows:\n            return pd.DataFrame()  # Return an empty DataFrame if no rows are found\n\n        # Extract table headers\n        headers = [header.text.strip() for header in rows[0].find_all(['th', 'td'])]\n\n        # Extract table data\n        data = []\n        for row in rows[1:]:\n            data.append([cell.text.strip() for cell in row.find_all(['td', 'th'])])\n\n        # Create a DataFrame\n        df = pd.DataFrame(data, columns=headers)\n\n        return df\n\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {e}\")\n    except ValueError as e:\n        raise ValueError(str(e))\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {e}\")\n\n# Example usage:\n# try:\n#     df = task_func('https://example.com/data.html', 'table_id')\n#     print(df)\n# except Exception as e:\n#     print(e)\n```\n\nThis function sends an HTTP GET request to the specified URL and parses the HTML content using BeautifulSoup to locate and extract data from a table identified by `table_id`. It raises appropriate exceptions for HTTP errors and when the table is not found. If the table is found but contains no data rows, it returns an empty DataFrame. The function is encapsulated to handle exceptions gracefully and provide clear feedback on error conditions."}
{"task_id": "BigCodeBench/1012", "solution": "```python\nimport requests\nfrom pathlib import Path\nimport zipfile\nimport shutil\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure the download directory exists\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    \n    # Download the file\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Failed to download file due to network issues. {str(e)}\", []\n\n    # Extract the zip file\n    try:\n        # Remove existing ZIP_DIR if it exists and create a new one\n        if ZIP_DIR.exists():\n            shutil.rmtree(ZIP_DIR)\n        ZIP_DIR.mkdir(exist_ok=True)\n        \n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        \n        # Get list of extracted files\n        extracted_files = [f.name for f in ZIP_DIR.glob('**/*') if f.is_file()]\n        return \"Success: File downloaded and extracted successfully.\", extracted_files\n    except zipfile.BadZipFile:\n        return \"Error: The downloaded file is not a valid zip file.\", []\n    except PermissionError:\n        return \"Error: Permission denied when extracting files.\", []\n    except Exception as e:\n        return f\"Error: An unexpected error occurred during extraction. {str(e)}\", []\n```"}
{"task_id": "BigCodeBench/1013", "solution": "```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure the request was successful\n\n        # Parse the HTML content of the webpage\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Find all <a> tags with href attributes and convert them to absolute URLs\n        links = set()\n        for a_tag in soup.find_all('a', href=True):\n            absolute_url = urljoin(base_url, a_tag['href'])\n            links.add(absolute_url)\n\n        # Write the unique absolute URLs to a CSV file\n        with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['URL'])\n            for link in links:\n                writer.writerow([link])\n\n        # Return the number of unique absolute links\n        return len(links)\n\n    except requests.RequestException as e:\n        print(f\"Error fetching {url}: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return 0\n\n# Example usage\nif __name__ == \"__main__\":\n    num_links = task_func(\"https://www.example.com\")\n    print(f\"Number of unique links scraped: {num_links}\")\n```\n\nThis function fetches a webpage, extracts all hyperlinks, converts them into absolute URLs, and saves these URLs into a CSV file. It also handles exceptions from network errors and returns the total count of unique absolute URLs extracted. You can replace `\"https://www.example.com\"` with any other URL you wish to scrape."}
{"task_id": "BigCodeBench/1015", "solution": "```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Determine the content source and fetch the content\n        if webpage_url.startswith('http'):\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Will raise HTTPError for bad responses\n            content = response.content\n        else:\n            with open(webpage_url, 'rb') as file:\n                content = file.read()\n\n        # Parse the HTML content\n        tree = html.fromstring(content)\n        tables = tree.xpath('//table')\n        \n        if not tables:\n            return 0  # No tables found\n\n        # Extract and parse the first table data\n        df = pd.read_html(html.tostring(tables[0]))[0]\n        if df.empty:\n            return 0  # Table is empty\n\n        # Connect to SQLite database and manage table\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n        \n        # Drop the existing table if it exists and create a new one\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Commit the changes and close the database connection\n        conn.commit()\n        conn.close()\n\n        # Return the number of rows in the DataFrame\n        return len(df)\n\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Failed to fetch the webpage: {str(e)}\")\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database operation failed: {str(e)}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {str(e)}\")\n\n# Example usage:\n# rows = task_func('http://example.com/table_page.html')\n# print(f\"Number of rows parsed and stored: {rows}\")\n```\n\nThis function integrates the handling of both web URLs and local file paths for HTML content input, parses the first HTML table found, and manages the SQLite database interactions by replacing the existing table named \"my_table\" each time the function is called. It correctly handles exceptions related to network issues and database operations and returns the number of rows successfully parsed and stored in the database."}
{"task_id": "BigCodeBench/1019", "solution": "```python\nfrom PIL import Image\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Try OCR text extraction\n            text = pytesseract.image_to_string(img)\n            try:\n                # Convert the extracted text\n                converted_text = text.encode(from_encoding).decode(to_encoding)\n                return converted_text\n            except (UnicodeDecodeError, LookupError) as e:\n                # If encoding conversion fails, attempt to process the image comment\n                if 'comment' in img.info:\n                    try:\n                        comment = img.info['comment'].encode(from_encoding).decode(to_encoding)\n                        return comment\n                    except (UnicodeDecodeError, LookupError) as e:\n                        raise ValueError(f\"Error converting image comment: {str(e)}\")\n                else:\n                    return \"\"  # No comment available, return empty string\n    except IOError as e:\n        raise ValueError(f\"Error opening image file: {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Unexpected error: {str(e)}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        result = task_func()\n        print(\"Extracted and converted text:\", result)\n    except ValueError as e:\n        print(\"Error:\", e)\n```\n\nThis Python function opens an image file, attempts to extract text using OCR, and converts the text encoding. If the OCR extraction fails or the encoding conversion throws an error, it tries to process any available image comment. The function is designed to handle various errors robustly, including incorrect encodings, and will raise a `ValueError` with an appropriate message if necessary. If both OCR and comment processing fail, it returns an empty string."}
{"task_id": "BigCodeBench/1020", "solution": "```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Send a GET request with a 5-second timeout\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Checks if the request was successful\n\n        # Retrieve the response content in raw bytes\n        raw_content = response.content\n\n        # Handle cases when the content is empty\n        if not raw_content:\n            return {}\n\n        # Detect the encoding if not provided\n        if from_encoding is None:\n            detected = chardet.detect(raw_content)\n            from_encoding = detected['encoding']\n\n            # If encoding cannot be detected, raise a ValueError\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the content using the detected or provided encoding\n        decoded_content = raw_content.decode(from_encoding)\n\n        # Re-encode the content to the desired encoding\n        re_encoded_content = decoded_content.encode(to_encoding)\n\n        # Parse and return the JSON data\n        return json.loads(re_encoded_content.decode(to_encoding))\n\n    except requests.RequestException as e:\n        # Handle any request-related exceptions\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        print(f\"JSON parsing failed: {e}\")\n        return {}\n    except ValueError as e:\n        # Handle value errors, specifically related to encoding detection\n        print(f\"Value error: {e}\")\n        raise\n\n# Example usage\nif __name__ == \"__main__\":\n    result = task_func()\n    print(result)\n```\n\nThis Python function, `task_func`, efficiently fetches data from a specified REST API, handling the intricacies of content encoding and ensuring the data is correctly parsed into JSON format. It sets a 5-second timeout for the GET request, checks for empty response content, and dynamically detects the encoding of the data if not specified. In cases where encoding cannot be detected, it raises a `ValueError`. The function is robust against various common errors such as request failures and JSON parsing issues, making it reliable for handling API responses in different encoding formats."}
{"task_id": "BigCodeBench/1022", "solution": "```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Handle the case of an empty CSV file\n        if df.empty:\n            return pd.DataFrame()\n\n        # Check if the specified column exists in the DataFrame\n        if column_name not in df.columns:\n            raise ValueError(f\"The column '{column_name}' is not present in the CSV file.\")\n\n        # Convert date strings to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n        # Get today's date\n        current_date = datetime.now()\n\n        # Filter rows based on the current date\n        df = df[df[column_name] <= current_date]\n\n        # Sort the DataFrame by the date column\n        df = df.sort_values(by=column_name)\n\n        return df\n\n    except EmptyDataError:\n        # Return an empty DataFrame if the CSV file is empty\n        return pd.DataFrame()\n\n    except pd.errors.ParserError as e:\n        # Handle CSV parsing errors\n        raise ValueError(f\"Error parsing CSV file: {str(e)}\")\n\n    except Exception as e:\n        # Handle any other unexpected errors\n        raise Exception(f\"An unexpected error occurred: {str(e)}\")\n\n# Example usage:\n# result = task_func('path_to_csv_file.csv', 'date_column_name')\n# print(result)\n```\n\nThis function ensures robust CSV file handling, including checking for file existence, verifying column presence, and parsing dates. It properly handles various error scenarios, making it reliable for processing date-related data from CSV files."}
{"task_id": "BigCodeBench/1039", "solution": "```python\nimport ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Initialize SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode().strip()\n\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            secure_socket.sendall(\"File not found\".encode())\n            return \"File not found\"\n\n        # Compute the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as file:\n            for byte_block in iter(lambda: file.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        file_hash = sha256_hash.hexdigest()\n\n        # Send the hash back to the client\n        secure_socket.sendall(file_hash.encode())\n\n        return file_hash\n\n    except ssl.SSLError as e:\n        error_message = f\"SSL Error: {str(e)}\"\n        secure_socket.sendall(error_message.encode())\n        return error_message\n    except IOError as e:\n        error_message = f\"I/O Error: {str(e)}\"\n        secure_socket.sendall(error_message.encode())\n        return error_message\n    except Exception as e:\n        error_message = f\"Unexpected Error: {str(e)}\"\n        secure_socket.sendall(error_message.encode())\n        return error_message\n```\n\n### Function Details:\n\n- **SSL/TLS Setup**: The function sets up SSL/TLS security using the provided certificate and key files.\n- **Receiving File Path**: It receives the file path request from the client through the secure socket.\n- **File Existence Check**: If the file does not exist, it sends a \"File not found\" message back to the client and returns the same message.\n- **Hash Calculation**: If the file exists, it computes its SHA256 hash.\n- **Sending Hash**: The computed hash is sent back to the client.\n- **Error Handling**: The function handles SSL errors, I/O errors, and other unexpected exceptions by sending an appropriate error message to the client and returning it.\n\nThe function assumes the `client_socket` is already open and does not handle its opening or closing."}
{"task_id": "BigCodeBench/1040", "solution": "Here's a self-contained Python script that implements a non-blocking echo server. This server appends the current server time to the received data and sends it back to the client. It handles exceptional conditions for each socket and provides a status message indicating the server's operation and run duration.\n\n```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setblocking(False)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # Sockets from which we expect to read\n    inputs = [server_socket]\n    # Sockets to which we expect to write\n    outputs = []\n    # Outgoing message queues (socket:Queue)\n    message_queues = {}\n\n    # Start time and duration\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    while datetime.now() < end_time:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n        for s in readable:\n            if s is server_socket:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    response = f\"{data.decode()} (Server Time: {current_time})\"\n                    message_queues[s].put(response.encode())\n                    # Add output channel for response\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    # Remove message queue\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                # No messages waiting so stop checking for writability.\n                outputs.remove(s)\n            else:\n                s.send(next_msg)\n\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Clean up the server socket\n    server_socket.close()\n    \n    duration = datetime.now() - start_time\n    return f\"Server ran for {duration.total_seconds():.2f} seconds, handling connections on {server_address}:{server_port}\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    result = task_func()\n    print(result)\n```\n\nThis script sets up a non-blocking server socket and uses `select` to handle multiple connections efficiently. It appends the server's current time to the received data and then sends it back to the client. The server runs for a specified duration and then shuts down, returning a status message indicating its operation and run duration."}
{"task_id": "BigCodeBench/1042", "solution": "```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    try:\n        # Receive message from client socket\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n        # Get email details from user\n        sender_email = input(\"Enter sender's email: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        sender_password = getpass.getpass(\"Enter sender's email password: \")\n\n        # Create email message\n        email = EmailMessage()\n        email['From'] = sender_email\n        email['To'] = recipient_email\n        email['Subject'] = \"Message from Client Socket\"\n        email.set_content(message)\n\n        # Send email using SMTP\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()  # Start TLS encryption\n            server.login(sender_email, sender_password)\n            server.send_message(email)\n\n        print(\"Email sent successfully.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\n    finally:\n        # Close the client socket\n        client_socket.close()\n\n    # Function returns None implicitly\n```\n\nThis Python function `task_func` provides a comprehensive solution for receiving a message from a client socket and sending it as an email via an SMTP server, specifically tailored to use Gmail's SMTP services. It includes user prompts for the necessary email details and handles the email creation and sending. The function also includes error handling to manage exceptions and ensures the client socket is closed after the operation."}
{"task_id": "BigCodeBench/1053", "solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file and check for 'Text' column or assume first column is 'Text' if header is missing\n        df = pd.read_csv(file_path, header=None, names=['Text'] if pd.read_csv(file_path, nrows=1).columns[0] != 'Text' else None)\n        \n        # Combine all text into a single string\n        combined_text = ' '.join(df['Text'].astype(str))\n        \n        # Initialize CountVectorizer with stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the combined text\n        word_counts = vectorizer.fit_transform([combined_text])\n        \n        # Sum the word counts to get frequencies and convert to a list of tuples (word, frequency)\n        sum_words = word_counts.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        \n        # Sort words by frequency and get the top 10\n        top_words = sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]\n        \n        # Extract words and their frequencies for plotting\n        words, frequencies = zip(*top_words)\n        \n        # Create the plot\n        plt.figure(figsize=(10, 5))\n        plt.bar(words, frequencies, color='blue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        \n        # Save or show the plot based on save_path\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.show()\n            return plt.gca()\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage:\n# task_func('path_to_csv_file.csv', 'path_to_save_histogram.png')\n```\n\nThis function effectively handles reading a CSV file with or without a 'Text' header, processes the text to exclude specified stopwords, and plots a histogram of the ten most common words. It provides options to either display the plot directly or save it to a file, and includes robust error handling."}
{"task_id": "BigCodeBench/1057", "solution": "```python\nimport pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Default lists if not provided\n    default_animals = ['cat', 'dog', 'rabbit', 'hamster', 'parrot']\n    default_foods = ['apple', 'banana', 'carrot', 'lettuce', 'seed']\n\n    # Use default lists if not provided or if provided lists are empty\n    animals = animals if animals else default_animals\n    foods = foods if foods else default_foods\n\n    # If both lists are empty, return an empty DataFrame\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    # Generate all combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle the combinations to ensure variety\n    np.random.shuffle(combinations)\n\n    # Create the DataFrame with animals as rows and foods as columns\n    df = pd.DataFrame(index=animals, columns=foods)\n\n    # Fill the DataFrame with the combinations in 'animal:food' format\n    for animal, food in combinations:\n        df.at[animal, food] = f\"{animal}:{food}\"\n\n    return df\n\n# Example usage:\nresult = task_func()\nprint(result)\n\n# Custom lists\ncustom_animals = ['elephant', 'giraffe', 'lion']\ncustom_foods = ['grass', 'leaves', 'meat']\ncustom_result = task_func(custom_animals, custom_foods)\nprint(\"\\nCustom lists:\")\nprint(custom_result)\n\n# Empty lists\nempty_result = task_func([], [])\nprint(\"\\nEmpty lists:\")\nprint(empty_result)\n\n# One empty list\none_empty_result = task_func([], ['pizza', 'pasta'])\nprint(\"\\nOne empty list:\")\nprint(one_empty_result)\n```\n\nThis function provides a robust solution to generating a DataFrame of animal-food combinations. It handles cases where the input lists may be empty or not provided, using default lists in such scenarios. The DataFrame is dynamically created based on the input lists and filled with formatted strings representing each unique combination. The function demonstrates flexibility by returning an empty DataFrame when both input lists are empty, ensuring that the function's behavior is predictable and consistent across different scenarios."}
{"task_id": "BigCodeBench/1077", "solution": "```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert timezone string to pytz timezone object\n    tz = pytz.timezone(timezone)\n\n    # Convert timestamps to datetime objects in the specified timezone\n    datetimes = []\n    for time_string in time_strings:\n        dt = datetime.fromisoformat(time_string)\n        dt = dt.astimezone(tz)\n        datetimes.append(dt)\n\n    # Calculate time differences in seconds\n    time_diffs = []\n    for i in range(1, len(datetimes)):\n        diff = abs((datetimes[i] - datetimes[i-1]).total_seconds())\n        time_diffs.append(diff)\n\n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n\n    # Calculate and return the mean time difference\n    return float(np.mean(time_diffs))\n\n# Example usage:\n# time_strings = [\"2023-10-01 12:00:00\", \"2023-10-01 12:01:00\", \"2023-10-01 12:02:30\"]\n# timezone = \"America/New_York\"\n# print(task_func(time_strings, timezone))  # Outputs the average time difference in seconds\n```\n\nThis function efficiently handles the task by first ensuring there are enough timestamps to calculate differences, converting each timestamp to the specified timezone, calculating the absolute time differences between each consecutive pair, and finally using numpy to compute the mean of these differences. If the conditions for calculating differences aren't met, it returns 0.0 as specified."}
{"task_id": "BigCodeBench/1085", "solution": "```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text and remove punctuation\n    processed_text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split the text into words\n    words = processed_text.split()\n    \n    # Count the frequency of each word using Counter\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10_words)\n    \n    # Create a bar plot using matplotlib\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    \n    # Set titles and labels\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cut-off labels\n    plt.tight_layout()\n    \n    # Return the list of tuples and the Axes object\n    return top_10_words, ax\n```\n\nThis code defines a function `task_func` that:\n1. Processes the input text by converting it to lowercase and removing punctuation.\n2. Splits the cleaned text into words.\n3. Counts the frequency of each word using a `Counter`.\n4. Extracts the top 10 most common words and their counts.\n5. Plots these top 10 words in a bar chart using `matplotlib`.\n6. Returns the list of these top 10 words along with the `Axes` object for the plot. \n\nYou can use this function to analyze word frequencies and visualize them in a bar chart by passing a text string to `task_func`. After calling the function, it will return both the data and the plot, which can be displayed using `plt.show()` if running in an interactive environment."}
{"task_id": "BigCodeBench/1124", "solution": "Here's a self-contained Python function that extracts a URL from the provided string, retrieves the title of the webpage from that URL, and handles various error cases as specified:\n\n```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    \n    # Find all URLs in the string\n    urls = re.findall(url_pattern, myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    # Take the first URL found\n    url = urls[0]\n    \n    # Validate the URL\n    try:\n        result = urlparse(url)\n        if all([result.scheme, result.netloc]):\n            # URL is valid, proceed with fetching the content\n            try:\n                response = requests.get(url, timeout=10)\n                response.raise_for_status()  # Raises an HTTPError for bad responses\n                \n                # Parse the HTML content\n                soup = BeautifulSoup(response.text, 'html.parser')\n                \n                # Find the title tag\n                title_tag = soup.find('title')\n                \n                if title_tag:\n                    return title_tag.string.strip()\n                else:\n                    return \"No title tag found in the webpage.\"\n            \n            except requests.RequestException as e:\n                return f\"Unable to fetch the content of the URL: {url}\"\n        else:\n            return \"No valid URL found in the provided string.\"\n    \n    except ValueError:\n        return \"No valid URL found in the provided string.\"\n```\n\nThis function checks for URLs in a given string and attempts to fetch the web page title of the first valid URL. It handles errors such as no URL found, URL fetch failures, and missing title tags on the web page."}
{"task_id": "BigCodeBench/1129", "solution": "```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n        \n        # Check if the specified key exists in the JSON data\n        if unknown_key not in data:\n            raise KeyError(f\"Key '{unknown_key}' not found in JSON data\")\n        \n        # Retrieve the URL associated with the specified key\n        url = data[unknown_key]\n        \n        # Download the file from the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raises HTTPError for bad requests (4XX or 5XX)\n        \n        # Generate a timestamp for the filename\n        timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n        \n        # Create the filename using the key and timestamp\n        filename = f\"{unknown_key}_{timestamp}.txt\"\n        \n        # Determine the directory to save the file, default is current working directory\n        if save_dir is None:\n            save_dir = os.getcwd()\n        else:\n            save_dir = os.path.abspath(save_dir)\n            os.makedirs(save_dir, exist_ok=True)  # Create the directory if it does not exist\n        \n        # Construct the full path where the file will be saved\n        file_path = os.path.join(save_dir, filename)\n        \n        # Write the downloaded content to the file\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Return the absolute path of the saved file\n        return os.path.abspath(file_path)\n    \n    except json.JSONDecodeError:\n        print(\"Error: Invalid JSON data\")\n    except KeyError as e:\n        print(f\"Error: {str(e)}\")\n    except requests.RequestException as e:\n        print(f\"Error downloading file: {str(e)}\")\n    except IOError as e:\n        print(f\"Error saving file: {str(e)}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n    \n    return None\n```\n\nThis function encapsulates the process of parsing JSON data, retrieving a URL based on a specific key, downloading the file from that URL, and saving it to a specified directory with a timestamped filename. The function is equipped with error handling to manage issues that might arise during each step of the process, ensuring robustness and reliability."}
{"task_id": "BigCodeBench/1137", "solution": "```python\nimport bs4\nimport requests\nimport re\nimport json\nimport os\n\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern for phone numbers\n    phone_pattern = r'\\b(?:\\+?1[-.\\s]?)?(?:\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n    \n    # Check if the input is a URL or a local file path\n    if url.startswith('http://') or url.startswith('https://'):\n        # Fetch the content from the URL\n        response = requests.get(url)\n        content = response.text\n    elif os.path.isfile(url):\n        # Read the content from a local file\n        with open(url, 'r', encoding='utf-8') as file:\n            content = file.read()\n    else:\n        raise ValueError(\"Invalid input. Please provide a valid URL or local file path.\")\n\n    # Parse the content with BeautifulSoup\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    \n    # Extract text content from the parsed HTML\n    text_content = soup.get_text()\n    \n    # Find all phone numbers in the text content\n    phone_numbers = re.findall(phone_pattern, text_content)\n    \n    # Remove duplicates and sort the list\n    phone_numbers = sorted(list(set(phone_numbers)))\n    \n    # Save the phone numbers to the specified output file in JSON format\n    with open(output_path, 'w') as outfile:\n        json.dump(phone_numbers, outfile, indent=2)\n    \n    return phone_numbers\n\n# Example usage:\n# url = 'https://example.com'  # or 'path/to/local/file.html'\n# output_path = 'phone_numbers.json'\n# result = task_func(url, output_path)\n# print(result)\n```\n\nThis Python function efficiently extracts phone numbers from either a URL or a local HTML file, then saves the found numbers in a JSON file at the specified path. It handles both web and local data sources, ensures data uniqueness, and provides an output list of phone numbers."}
