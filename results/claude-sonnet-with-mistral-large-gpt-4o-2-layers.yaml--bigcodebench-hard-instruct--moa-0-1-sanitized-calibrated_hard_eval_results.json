{
  "date": "2024-08-18 19:38",
  "eval": {
    "BigCodeBench/19": [
      {
        "task_id": "BigCodeBench/19",
        "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n\n    pass\nimport os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n\n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n\n    # If there are no files, return None\n    if not files:\n        return None\n\n    # Define the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n\n    # Create the zip file and add all files\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n\n    # Return the path to the created zip file\n    return zip_path",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/17": [
      {
        "task_id": "BigCodeBench/17",
        "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n\n    pass\nimport subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    try:\n        # Check if the process is running\n        for proc in psutil.process_iter(['name']):\n            if proc.info['name'] == process_name:\n                # Process found, terminate it\n                proc.terminate()\n                proc.wait(timeout=5)  # Wait up to 5 seconds for the process to terminate\n                time.sleep(1)  # Short delay to ensure process has fully terminated\n                \n                # Restart the process\n                subprocess.Popen(process_name)\n                return f\"Process found. Restarting {process_name}.\"\n        \n        # Process not found, start it\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    \n    except psutil.NoSuchProcess:\n        # Process terminated between finding and attempting to terminate it\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    except psutil.TimeoutExpired:\n        # Process didn't terminate in time, force kill it\n        for proc in psutil.process_iter(['name']):\n            if proc.info['name'] == process_name:\n                proc.kill()\n        subprocess.Popen(process_name)\n        return f\"Process found but unresponsive. Forcefully restarting {process_name}.\"\n    except subprocess.SubprocessError as e:\n        return f\"Error starting process {process_name}: {str(e)}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {str(e)}\"",
        "status": "fail",
        "details": {
          "test_process_found_restarts_process": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 62, in test_process_found_restarts_process\nAssertionError: 'Process not found. Starting notepad.' != 'Process found. Restarting notepad.'\n- Process not found. Starting notepad.\n?         ----       ^\n+ Process found. Restarting notepad.\n?                ^^^\n\n",
          "test_process_terminates_and_restarts_multiple_instances": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 76, in test_process_terminates_and_restarts_multiple_instances\nAssertionError: 'Process not found. Starting multi_instance.' != 'Process found. Restarting multi_instance.'\n- Process not found. Starting multi_instance.\n?         ----       ^\n+ Process found. Restarting multi_instance.\n?                ^^^\n\n"
        }
      }
    ],
    "BigCodeBench/13": [
      {
        "task_id": "BigCodeBench/13",
        "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n\n    pass\nimport subprocess\nimport ftplib\nimport os\nfrom typing import List\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test') -> List[str]:\n    try:\n        # Connect to the FTP server\n        with ftplib.FTP(ftp_server) as ftp:\n            try:\n                # Login to the FTP server\n                ftp.login(user=ftp_user, passwd=ftp_password)\n            except ftplib.error_perm as e:\n                raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n            try:\n                # Change to the specified directory\n                ftp.cwd(ftp_dir)\n            except ftplib.error_perm as e:\n                raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n            # List files in the directory\n            filenames = ftp.nlst()\n\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    # Download each file using wget in a subprocess\n    for filename in filenames:\n        file_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        try:\n            subprocess.run(['wget', '-q', file_url], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download file {filename}: {str(e)}\")\n\n    return filenames",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 63, in test_case_1\nAssertionError: 0 != 2\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 73, in test_case_2\nAssertionError: 'Fail[13 chars] to FTP server' != 'Fail[13 chars] to FTP server invalid_server: Failed to connect to FTP server'\n- Failed to connect to FTP server\n+ Failed to connect to FTP server invalid_server: Failed to connect to FTP server\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 79, in test_case_3\nAssertionError: Exception not raised\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 87, in test_case_4\nAssertionError: Exception not raised\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 95, in test_case_5\nAssertionError: Exception not raised\n"
        }
      }
    ],
    "BigCodeBench/82": [
      {
        "task_id": "BigCodeBench/82",
        "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n\n    pass\nfrom flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\nusers = {\n    'testuser': User(1, 'testuser', 'password123')\n}\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = users.get(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n            return 'Invalid username or password'\n        return render_template('login.html', form=form)\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! This is a protected page.'\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('home'))\n\n    return app",
        "status": "fail",
        "details": {
          "test_login_page_accessibility": "Traceback (most recent call last):\n  File \"__test__.py\", line 133, in test_login_page_accessibility\nAssertionError: 500 != 200 : The login page should be accessible.\n",
          "test_logout_route_redirects_to_login": "Traceback (most recent call last):\n  File \"__test__.py\", line 153, in test_logout_route_redirects_to_login\nAssertionError: 'Logged in as: testuser' not found in \"Hello, <MagicMock name='_get_user().username' id='46916763144288'>! This is a protected page.\"\n"
        }
      }
    ],
    "BigCodeBench/89": [
      {
        "task_id": "BigCodeBench/89",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Extract the specified column\n    col_data = data[column].values.reshape(-1, 1)\n    \n    # Standardize the column\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(col_data)\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n    \n    # Create a mask to remove outliers\n    mask = np.ones(len(data), dtype=bool)\n    mask[outliers] = False\n    \n    # Remove outliers\n    data_without_outliers = data[mask].copy()\n    \n    # Visualize the data before and after outlier removal\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Data with Outliers\n    ax1.scatter(range(len(data)), data[column], alpha=0.7)\n    ax1.set_title('Data with Outliers')\n    ax1.set_xlabel('Index')\n    ax1.set_ylabel(column)\n    \n    # Data without Outliers\n    ax2.scatter(range(len(data_without_outliers)), data_without_outliers[column], alpha=0.7)\n    ax2.set_title('Data without Outliers')\n    ax2.set_xlabel('Index')\n    ax2.set_ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_without_outliers, outliers.tolist()",
        "status": "fail",
        "details": {
          "test_data_without_outliers": "Traceback (most recent call last):\n  File \"__test__.py\", line 67, in test_data_without_outliers\n  File \"__test__.py\", line 14, in task_func\nAttributeError: 'numpy.ndarray' object has no attribute 'values'\n",
          "test_no_change_in_data_dimension": "Traceback (most recent call last):\n  File \"__test__.py\", line 83, in test_no_change_in_data_dimension\n  File \"__test__.py\", line 14, in task_func\nAttributeError: 'numpy.ndarray' object has no attribute 'values'\n",
          "test_no_plotting": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 79, in test_no_plotting\n  File \"__test__.py\", line 14, in task_func\nAttributeError: 'numpy.ndarray' object has no attribute 'values'\n",
          "test_original_data_unchanged": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_original_data_unchanged\n  File \"__test__.py\", line 14, in task_func\nAttributeError: 'numpy.ndarray' object has no attribute 'values'\n",
          "test_plot_titles": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 88, in test_plot_titles\n  File \"__test__.py\", line 14, in task_func\nAttributeError: 'numpy.ndarray' object has no attribute 'values'\n",
          "test_return_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 71, in test_return_type\n  File \"__test__.py\", line 14, in task_func\nAttributeError: 'numpy.ndarray' object has no attribute 'values'\n"
        }
      }
    ],
    "BigCodeBench/34": [
      {
        "task_id": "BigCodeBench/34",
        "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n\n    pass\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    cleaned_text = re.sub(r'http[s]?://\\S+', '', text)\n    \n    # Remove any remaining whitespace and check if there are words left\n    cleaned_text = cleaned_text.strip()\n    if not cleaned_text:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n    return wordcloud",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/15": [
      {
        "task_id": "BigCodeBench/15",
        "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n\n    pass\nimport subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Ensure the output directory exists\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    # Read commands from the CSV file\n    with open(commands_file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader):\n            if not row:\n                continue\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n            output_files.append(output_file_path)\n\n            try:\n                # Execute the command and capture the output\n                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                output_content = result.stdout\n            except subprocess.CalledProcessError as e:\n                # If the command fails, capture the error message and exit code\n                output_content = f\"Command failed with exit code {e.returncode}\\nError message:\\n{e.stderr}\"\n\n            # Write the output or error message to the output file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(output_content)\n\n    return output_files",
        "status": "fail",
        "details": {
          "test_command_failure_with_specific_exit_code": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_command_failure_with_specific_exit_code\nAssertionError: 'Error executing command' not found in 'Command failed with exit code 1\\nError message:\\n'\n"
        }
      }
    ],
    "BigCodeBench/92": [
      {
        "task_id": "BigCodeBench/92",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pd.DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1.\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    centroids = kmeans.cluster_centers_\n\n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot data points\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis')\n    \n    # Plot centroids\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n    \n    # Customize plot\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    ax.set_title(f'K-means Clustering (k={n_clusters})')\n    ax.legend()\n\n    # Add colorbar\n    plt.colorbar(scatter, label='Cluster')\n\n    return cluster_labels, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/93": [
      {
        "task_id": "BigCodeBench/93",
        "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n\n    pass\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Create DataFrame with transformed data\n    columns = [f\"PC{i+1}\" for i in range(n_components)]\n    df_transformed = pd.DataFrame(transformed_data, columns=columns)\n\n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(df_transformed['PC1'], df_transformed['PC2'])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_title('PCA Scatter Plot (First 2 Components)')\n        \n        if n_components > 2:\n            plt.figtext(0.5, 0.01, f\"Note: Only showing first 2 of {n_components} components\", \n                        ha=\"center\", fontsize=9, style='italic')\n    else:\n        ax.text(0.5, 0.5, \"Scatter plot requires at least 2 components\", \n                ha='center', va='center', fontsize=12)\n        ax.axis('off')\n\n    return df_transformed, ax",
        "status": "fail",
        "details": {
          "test_values": "Traceback (most recent call last):\n  File \"__test__.py\", line 77, in test_values\nAssertionError: 0.572203679961561 != -0.36270132751314693 within 7 places (0.9349050074747078 difference) : DataFrame contents should match the expected output\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 80, in test_values\nAssertionError: 0.572203679961561 != 0.36270132751314693 within 7 places (0.20950235244841403 difference) : DataFrame contents should match the expected output\n"
        }
      }
    ],
    "BigCodeBench/37": [
      {
        "task_id": "BigCodeBench/37",
        "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n\n    pass\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Train Random Forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n    \n    # Get feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n    \n    # Create and sort DataFrame of feature importances\n    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n    \n    # Create bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='importance', y='feature', data=feature_importance_df, palette='viridis')\n    \n    # Set labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return clf, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/120": [
      {
        "task_id": "BigCodeBench/120",
        "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n\n    pass\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Input validation\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' cannot be later than 'end_date'.\")\n\n    # Set seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the number of days in the range (inclusive)\n    num_days = (end_date - start_date).days + 1\n\n    # Generate random dates\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    # Create and return a pandas Series\n    return pd.Series(random_dates)",
        "status": "fail",
        "details": {
          "test_reproducibility_with_seed": "Traceback (most recent call last):\n  File \"__test__.py\", line 42, in test_reproducibility_with_seed\nAssertionError: Lists differ: ['202[5056 chars]11-26', '2020-06-02', '2020-08-22', '2020-06-10', '2020-02-07'] != ['202[5056 chars]11-26', '2020-06-02', '2020-08-22', '2020-06-10']\n\nFirst list contains 1 additional elements.\nFirst extra element 365:\n'2020-02-07'\n\nDiff is 6275 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n",
          "test_series_length": "Traceback (most recent call last):\n  File \"__test__.py\", line 48, in test_series_length\nAssertionError: 10 != 9\n"
        }
      }
    ],
    "BigCodeBench/108": [
      {
        "task_id": "BigCodeBench/108",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n\n    required_columns = {'date', 'value'}\n    if not required_columns.issubset(df.columns):\n        raise ValueError(f\"Input 'df' must contain the columns: {required_columns}\")\n\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column must be of datetime type.\")\n\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must be numeric.\")\n\n    # Validate frequency\n    valid_freqs = {'B', 'C', 'D', 'W', 'M', 'Q', 'Y', 'H', 'T', 'S', 'L', 'U'}\n    if freq not in valid_freqs:\n        raise ValueError(f\"Invalid frequency string '{freq}'. Valid options are {valid_freqs}.\")\n\n    # Validate decomposition model\n    if decomposition_model not in {'additive', 'multiplicative'}:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n\n    # Set the date column as the index\n    df = df.set_index('date')\n\n    # Perform the decomposition\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.Timedelta(freq).days)\n\n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n    \n    decomposition_result.observed.plot(ax=axes[0])\n    axes[0].set_ylabel('Observed')\n    axes[0].set_title('Time Series Decomposition')\n    \n    decomposition_result.trend.plot(ax=axes[1])\n    axes[1].set_ylabel('Trend')\n    \n    decomposition_result.seasonal.plot(ax=axes[2])\n    axes[2].set_ylabel('Seasonal')\n    \n    decomposition_result.resid.plot(ax=axes[3])\n    axes[3].set_ylabel('Residual')\n    \n    plt.tight_layout()\n\n    return (decomposition_result, axes)",
        "status": "fail",
        "details": {
          "test_return_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_return_type\n  File \"__test__.py\", line 38, in task_func\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 1766, in pandas._libs.tslibs.timedeltas.Timedelta.__new__\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 649, in pandas._libs.tslibs.timedeltas.parse_timedelta_string\nValueError: unit abbreviation w/o a number\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 76, in test_return_type\nAssertionError: Unexpected ValueError raised: unit abbreviation w/o a number\n",
          "test_additive_model": "Traceback (most recent call last):\n  File \"__test__.py\", line 112, in test_additive_model\n  File \"__test__.py\", line 38, in task_func\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 1766, in pandas._libs.tslibs.timedeltas.Timedelta.__new__\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 649, in pandas._libs.tslibs.timedeltas.parse_timedelta_string\nValueError: unit abbreviation w/o a number\n",
          "test_component_shapes": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_component_shapes\n  File \"__test__.py\", line 38, in task_func\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 1766, in pandas._libs.tslibs.timedeltas.Timedelta.__new__\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 649, in pandas._libs.tslibs.timedeltas.parse_timedelta_string\nValueError: unit abbreviation w/o a number\n",
          "test_components_existence": "Traceback (most recent call last):\n  File \"__test__.py\", line 100, in test_components_existence\n  File \"__test__.py\", line 38, in task_func\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 1766, in pandas._libs.tslibs.timedeltas.Timedelta.__new__\n  File \"pandas/_libs/tslibs/timedeltas.pyx\", line 649, in pandas._libs.tslibs.timedeltas.parse_timedelta_string\nValueError: unit abbreviation w/o a number\n"
        }
      }
    ],
    "BigCodeBench/100": [
      {
        "task_id": "BigCodeBench/100",
        "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n\n    pass\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=29)\n        dates = pd.date_range(start=start_date, end=end_date)\n\n        # Generate random time series data\n        data = [random.uniform(0, 100) for _ in range(len(dates))]\n        time_series = pd.Series(data, index=dates)\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        ax = time_series.plot(kind='line', marker='o', linestyle='-', color='b')\n\n        # Styling the plot\n        plt.xlabel('Date', fontname='Arial', fontsize=12)\n        plt.ylabel('Value', fontname='Arial', fontsize=12)\n        plt.title('Random Time Series Data', fontname='Arial', fontsize=14)\n        plt.xticks(rotation=45, ha='right', fontname='Arial', fontsize=10)\n        plt.yticks(fontname='Arial', fontsize=10)\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.tight_layout()\n\n        # Return the Axes object\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the data or plot: {e}\")",
        "status": "fail",
        "details": {
          "test_value": "Traceback (most recent call last):\n  File \"__test__.py\", line 80, in test_value\nAssertionError: Lists differ: [81, 14, 3, 94, 35, 31, 28, 17, 94, 13, 86[69 chars], 69] != [63.942679845788376, 2.5010755222666936, 2[531 chars]8911]\n\nFirst differing element 0:\n81\n63.942679845788376\n\nDiff is 874 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n"
        }
      }
    ],
    "BigCodeBench/123": [
      {
        "task_id": "BigCodeBench/123",
        "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n\n    pass\nimport pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The input 'my_list' must be a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append('12')\n    \n    # Calculate the sum of numeric elements in the list\n    total_files = sum(int(x) for x in my_list if str(x).isdigit())\n    \n    # Find all CSV files in the specified directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    csv_files = glob.glob(file_pattern)\n    \n    # Raise FileNotFoundError if no files are found\n    if not csv_files:\n        raise FileNotFoundError(f\"No {file_ext} files found in the directory: {file_dir}\")\n    \n    # Select the number of files to concatenate\n    files_to_concat = csv_files[:total_files]\n    \n    # Read and concatenate the selected CSV files\n    dataframes = [pd.read_csv(file) for file in files_to_concat]\n    concatenated_df = pd.concat(dataframes, ignore_index=True)\n    \n    return concatenated_df",
        "status": "fail",
        "details": {
          "test_list_modification": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in test_list_modification\nAssertionError: 12 not found in [1, 2, 3, '12']\n"
        }
      }
    ],
    "BigCodeBench/129": [
      {
        "task_id": "BigCodeBench/129",
        "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n\n    pass\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        # Send GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find the first table in the HTML\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page.\")\n        \n        # Extract table headers\n        headers = [th.get_text(strip=True) for th in table.find_all('th')]\n        \n        # Extract table rows\n        rows = []\n        for tr in table.find_all('tr'):\n            row = [td.get_text(strip=True) for td in tr.find_all('td')]\n            if row:  # Ignore empty rows\n                rows.append(row)\n        \n        if not rows:\n            raise ValueError(\"No data found in the table.\")\n        \n        # Create DataFrame\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n        return df\n\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the URL.\")\n    except requests.HTTPError as http_err:\n        raise requests.HTTPError(f\"HTTP request failed: {http_err}\")\n    except Exception as e:\n        raise ValueError(f\"Failed to parse the page content: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_return_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 17, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 57, in test_return_type\n  File \"__test__.py\", line 46, in task_func\nValueError: Failed to parse the page content: expected string or bytes-like object\n",
          "test_return_type_with_complex_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 17, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 105, in test_return_type_with_complex_data\n  File \"__test__.py\", line 46, in task_func\nValueError: Failed to parse the page content: expected string or bytes-like object\n"
        }
      }
    ],
    "BigCodeBench/124": [
      {
        "task_id": "BigCodeBench/124",
        "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n\n    pass\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Input validation\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in 'my_list' must be numeric (int or float).\")\n\n    # Enhance my_list\n    my_list.append(12)\n\n    # Calculate sum and determine size of random numbers list\n    total_sum = sum(my_list)\n    num_count = min(total_sum, size)\n\n    # Set random seed\n    random_seed(seed)\n\n    # Generate random numbers and measure time\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(num_count)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black', align='left')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Generated Random Numbers')\n    ax.set_xlim(0, 101)\n    ax.set_xticks(range(0, 101, 10))\n\n    return (time_taken, ax)",
        "status": "fail",
        "details": {
          "test_output_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 53, in test_output_types\nAssertionError: Lists differ: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0[453 chars] 0.0] != [2.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0[53 chars] 2.0]\n\nFirst differing element 0:\n0.0\n2.0\n\nFirst list contains 80 additional elements.\nFirst extra element 20:\n0.0\n\nDiff is 928 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n",
          "test_plot_title": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_plot_title\nAssertionError: 'Distribution of Generated Random Numbers' != 'Histogram of Random Numbers'\n- Distribution of Generated Random Numbers\n+ Histogram of Random Numbers\n\n"
        }
      }
    ],
    "BigCodeBench/161": [
      {
        "task_id": "BigCodeBench/161",
        "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n\n    pass\nimport re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Define regex pattern to extract log information\n    log_pattern = re.compile(r'^(INFO|DEBUG|WARNING|ERROR|CRITICAL): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)$')\n    \n    log_entries = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line.strip())\n            if match:\n                log_type, timestamp_str, message = match.groups()\n                try:\n                    # Validate timestamp\n                    datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                    log_entries.append((log_type, timestamp_str, message))\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp: {timestamp_str} in log entry: {line.strip()}\")\n\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Create DataFrame from log entries\n    df = pd.DataFrame(log_entries, columns=['Type', 'Timestamp', 'Message'])\n\n    # Save DataFrame to CSV\n    csv_file_path = 'structured_logs.csv'\n    df.to_csv(csv_file_path, index=False)\n\n    return csv_file_path",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/147": [
      {
        "task_id": "BigCodeBench/147",
        "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n\n    pass\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    def scan_ip(ip, port, results):\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                sock.settimeout(1)\n                result = sock.connect_ex((str(ip), port))\n                results[str(ip)] = (result == 0)\n        except:\n            results[str(ip)] = False\n\n    network = IPv4Network(ip_range)\n    results = {}\n    threads = []\n\n    for ip in network:\n        thread = Thread(target=scan_ip, args=(ip, port, results))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results",
        "status": "fail",
        "details": {
          "test_open_port": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 49, in test_open_port\nAssertionError: False is not true : At least one port should be open for the test range.\n"
        }
      }
    ],
    "BigCodeBench/139": [
      {
        "task_id": "BigCodeBench/139",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n\n    # Select only the numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number])\n\n    # Raise an exception if there are no numeric columns\n    if numeric_cols.empty:\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    # Create a list to store the Axes objects\n    axes_list = []\n\n    # Plot histograms for each numeric column\n    for col in numeric_cols.columns:\n        fig, ax = plt.subplots()\n        ax.hist(df[col].dropna(), bins=30, edgecolor='black')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n\n    return axes_list",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/101": [
      {
        "task_id": "BigCodeBench/101",
        "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n\n    pass\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Set random seed for reproducibility\n        np.random.seed(seed)\n\n        # Load the dataset\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n        \n        columns = [\n            'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n            'TAX', 'PTRATIO', 'B', 'LSTAT'\n        ]\n        \n        # Create DataFrame\n        df = pd.DataFrame(data, columns=columns)\n        df['MEDV'] = target\n        \n        # Compute correlation matrix\n        corr_matrix = df.corr()\n        \n        # Create figure and axes\n        fig, ax = plt.subplots(figsize=(12, 10))\n        \n        # Plot heatmap\n        sns.heatmap(corr_matrix, ax=ax, annot=True, fmt=\".2f\", cmap='coolwarm',\n                    cbar=True, square=True, linewidths=.5, annot_kws={\"size\": 8})\n        \n        plt.title(\"Correlation Heatmap of Boston Housing Dataset\")\n        plt.tight_layout()\n        \n        # Save the plot if save_path is provided\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating or saving the plot: {e}\")",
        "status": "fail",
        "details": {
          "test_heatmap_features": "Traceback (most recent call last):\n  File \"__test__.py\", line 61, in test_heatmap_features\nAssertionError: Tuples differ: (196,) != (169,)\n\nFirst differing element 0:\n196\n169\n\n- (196,)\n?    -\n\n+ (169,)\n?   +\n\n",
          "test_plot_appearance": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_plot_appearance\nAssertionError: 'Correlation Heatmap of Boston Housing Dataset' != ''\n- Correlation Heatmap of Boston Housing Dataset\n+ \n\n",
          "test_heatmap_values": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_heatmap_values\n  File \"/usr/local/lib/python3.10/unittest/case.py\", line 876, in assertAlmostEqual\n    diff = abs(first - second)\nTypeError: unsupported operand type(s) for -: 'list' and 'list'\n"
        }
      }
    ],
    "BigCodeBench/162": [
      {
        "task_id": "BigCodeBench/162",
        "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n\n    pass\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Tokenize the text to extract words\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    if not word_lengths:\n        # If there are no words, return an empty plot\n        ax.set_title(\"Distribution of Word Lengths\")\n        ax.set_xlabel(\"Word Length\")\n        ax.set_ylabel(\"Frequency\")\n        ax.text(0.5, 0.5, \"No words found\", ha='center', va='center', transform=ax.transAxes)\n        return ax\n    \n    # Calculate the range for bins\n    max_length = max(word_lengths)\n    bins = range(1, max_length + 2)  # +2 to include the max length in its own bin\n    \n    # Create the histogram\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, align='left', edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel(\"Word Length\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(\"Distribution of Word Lengths\")\n    \n    # Set x-axis ticks to show only integer values\n    ax.set_xticks(range(1, max_length + 1))\n    \n    # Add grid for better readability\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n    \n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n    \n    return ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/184": [
      {
        "task_id": "BigCodeBench/184",
        "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n\n    pass\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\ndef task_func(dataframe, text_column):\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    \n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the preprocessed text\n    text_matrix = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the resulting matrix to a DataFrame\n    word_count_df = pd.DataFrame(text_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return word_count_df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/199": [
      {
        "task_id": "BigCodeBench/199",
        "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n\n    pass\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate input parameters\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n\n    # Set random seed for reproducibility\n    set_seed(seed)\n\n    # Initialize list to store weather data\n    weather_data = []\n\n    # Generate weather report for each city\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone not provided for city: {city}\")\n\n        # Convert UTC time to local time\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n\n        # Format local time as required\n        formatted_time = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n\n        # Randomly select weather condition\n        weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Add data to list\n        weather_data.append({\n            'City': city,\n            'Local Time': formatted_time,\n            'Weather Condition': weather\n        })\n\n    # Create and return DataFrame\n    return pd.DataFrame(weather_data)",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/177": [
      {
        "task_id": "BigCodeBench/177",
        "solution": "import re\nimport nltk\nfrom string import punctuation\ndef task_func(df):\n\n    pass\nimport re\nimport nltk\nfrom string import punctuation\nimport pandas as pd\nfrom collections import defaultdict\ndef task_func(df):\n    # Check if DataFrame is empty or does not contain the necessary columns\n    if df.empty or not {'Title', 'Content'}.issubset(df.columns):\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Define case-insensitive keywords\n    keywords = [\"like\", \"what\"]\n    pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n\n    # Filter the DataFrame to extract articles whose titles contain the specific keywords\n    filtered_df = df[df['Title'].apply(lambda title: bool(pattern.search(title)))]\n\n    # Initialize a dictionary to store word frequencies\n    word_freq = defaultdict(int)\n\n    # Define a regex pattern to remove punctuation\n    punc_pattern = re.compile(f\"[{re.escape(punctuation)}]\")\n\n    # Download the 'punkt' tokenizer if not already available\n    nltk.download('punkt', quiet=True)\n\n    # Tokenize and count word frequencies for the filtered articles\n    for content in filtered_df['Content']:\n        # Remove punctuation from the content\n        content_wo_punctuation = punc_pattern.sub('', content)\n        # Tokenize the content into words\n        words = nltk.word_tokenize(content_wo_punctuation)\n        # Update the word frequency dictionary\n        for word in words:\n            # Convert word to lowercase to ensure case-insensitivity\n            word = word.lower()\n            if word:  # Ensure the word is not empty\n                word_freq[word] += 1\n\n    # Convert the defaultdict to a regular dictionary for the final output\n    return dict(word_freq)",
        "status": "fail",
        "details": {
          "test_case_sensitive_handling": "Traceback (most recent call last):\n  File \"__test__.py\", line 115, in test_case_sensitive_handling\nAssertionError: {'technological': 1, 'growth': 1, 'is': 1, [85 chars]': 1} != {'Technological': 1, 'growth': 1, 'is': 1, [85 chars]': 1}\n+ {'Technological': 1,\n+  'These': 1,\n- {'advancements': 1,\n? ^\n\n+  'advancements': 1,\n? ^\n\n   'are': 1,\n   'exponential': 1,\n   'growth': 1,\n   'is': 1,\n   'like': 1,\n   'no': 1,\n-  'other': 1,\n?            ^\n\n+  'other': 1}\n?            ^\n\n-  'technological': 1,\n-  'these': 1} : Case sensitivity handling is faulty.\n",
          "test_word_frequencies": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_word_frequencies\nAssertionError: {'data': 2, 'science': 1, 'is': 2, 'about':[132 chars]': 1} != {'Data': 1, 'Science': 1, 'is': 2, 'about':[154 chars]': 1}\n+ {'Data': 1,\n+  'Like': 1,\n+  'Python': 1,\n+  'Science': 1,\n+  'What': 1,\n- {'about': 1,\n? ^\n\n+  'about': 1,\n? ^\n\n   'among': 1,\n   'analysis': 1,\n-  'data': 2,\n?          ^\n\n+  'data': 1,\n?          ^\n\n   'developers': 1,\n   'do': 1,\n   'is': 2,\n-  'like': 1,\n   'popular': 1,\n-  'python': 1,\n-  'science': 1,\n   'see': 1,\n   'think': 1,\n-  'what': 2,\n?          ^\n\n+  'what': 1,\n?          ^\n\n   'you': 2} : The word frequencies do not match the expected output.\n",
          "test_empty_dataframe": "Traceback (most recent call last):\n  File \"__test__.py\", line 97, in test_empty_dataframe\n  File \"__test__.py\", line 15, in task_func\nValueError: The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\n"
        }
      }
    ],
    "BigCodeBench/211": [
      {
        "task_id": "BigCodeBench/211",
        "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n\n    pass\nimport requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers, stream=True)\n    response.raise_for_status()\n\n    # Get the filename from the URL or Content-Disposition header\n    filename = os.path.basename(url)\n    if 'Content-Disposition' in response.headers:\n        content_disposition = response.headers['Content-Disposition']\n        if 'filename=' in content_disposition:\n            filename = content_disposition.split('filename=')[1].strip('\"')\n\n    # Save the zip file\n    zip_path = os.path.join(destination_directory, filename)\n    with open(zip_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    # Extract the contents\n    extracted_files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        for file in zip_ref.namelist():\n            zip_ref.extract(file, destination_directory)\n            extracted_files.append(file)\n\n    return extracted_files",
        "status": "fail",
        "details": {
          "test_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 92, in test_2\n  File \"__test__.py\", line 12, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 116, in test_3\n  File \"__test__.py\", line 12, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 140, in test_4\n  File \"__test__.py\", line 12, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 164, in test_5\n  File \"__test__.py\", line 12, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n",
          "test_download_and_extract": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 68, in test_download_and_extract\n  File \"__test__.py\", line 12, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/path'\n"
        }
      }
    ],
    "BigCodeBench/208": [
      {
        "task_id": "BigCodeBench/208",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    # Input validation\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate the random walk\n    walk = np.cumsum(steps)\n\n    # Calculate descriptive statistics\n    stats = {\n        \"count\": elements,\n        \"mean\": np.mean(walk),\n        \"standard deviation\": np.std(walk),\n        \"minimum\": np.min(walk),\n        \"5th percentile\": np.percentile(walk, 5),\n        \"25th percentile\": np.percentile(walk, 25),\n        \"median\": np.median(walk),\n        \"75th percentile\": np.percentile(walk, 75),\n        \"95th percentile\": np.percentile(walk, 95),\n        \"maximum\": np.max(walk)\n    }\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(range(elements), walk)\n    ax.set_title(\"Random Walk\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Position\")\n    ax.grid(True)\n\n    # Show the plot (can be commented out if not needed)\n    plt.show()\n\n    return stats, ax",
        "status": "fail",
        "details": {
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 90, in test_case_4\nAssertionError: 'std' not found in {'count': 10, 'mean': 2.1, 'standard deviation': 2.211334438749598, 'minimum': -1, '5th percentile': -0.55, '25th percentile': 0.25, 'median': 1.5, '75th percentile': 3.75, '95th percentile': 5.549999999999999, 'maximum': 6}\n",
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 71, in test_case_1\nKeyError: 'std'\n"
        }
      }
    ],
    "BigCodeBench/227": [
      {
        "task_id": "BigCodeBench/227",
        "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n\n    pass\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Convert list L to MxN matrix\n    if len(L) != M * N:\n        raise ValueError(\"The length of list L is not equal to M * N\")\n    matrix = np.array(L).reshape(M, N)\n\n    # Read audio data from the file\n    data, sample_rate = sf.read(audio_file)\n\n    # Ensure data is mono\n    if data.ndim > 1:\n        data = data.mean(axis=1)\n\n    # Calculate SPL\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Normalize the matrix\n    normalized_matrix = matrix / np.linalg.norm(matrix)\n\n    # Adjust the amplitude of the normalized matrix using SPL\n    adjusted_matrix = normalized_matrix * 10**(spl / 20)\n\n    # Generate the spectrogram\n    S = librosa.stft(adjusted_matrix.ravel())\n    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n\n    # Create the spectrogram plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img = librosa.display.specshow(S_db, x_axis='time', y_axis='hz', ax=ax, sr=sample_rate)\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    ax.set_title('Spectrogram')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency (Hz)')\n\n    return normalized_matrix, fig",
        "status": "fail",
        "details": {
          "test_docstring_examples": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 86, in test_docstring_examples\n  File \"__test__.py\", line 17, in task_func\nFileNotFoundError: The specified audio file audio.wav does not exist.\n",
          "test_empty_list_input": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 66, in test_empty_list_input\n  File \"__test__.py\", line 17, in task_func\nFileNotFoundError: The specified audio file audio.wav does not exist.\n",
          "test_spl_calculation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 96, in test_spl_calculation\n  File \"__test__.py\", line 17, in task_func\nFileNotFoundError: The specified audio file audio.wav does not exist.\n",
          "test_successful_matrix_creation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 74, in test_successful_matrix_creation\n  File \"__test__.py\", line 17, in task_func\nFileNotFoundError: The specified audio file audio.wav does not exist.\n"
        }
      }
    ],
    "BigCodeBench/239": [
      {
        "task_id": "BigCodeBench/239",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([value for tup in original for value in tup if isinstance(value, (int, float))])\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='skyblue', edgecolor='black')\n    \n    # Calculate and plot PDF\n    kde = stats.gaussian_kde(numeric_values)\n    x_range = np.linspace(numeric_values.min(), numeric_values.max(), 100)\n    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='PDF')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with Overlaid PDF')\n    ax.legend()\n    \n    return numeric_values, statistics, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 48, in test_case_1\nAssertionError: {'mean': 2.5, 'std_dev': 1.118033988749895, 'min': 1, 'max': 4} != {'mean': 2.5, 'std': 1.118033988749895, 'min': 1, 'max': 4}\n- {'max': 4, 'mean': 2.5, 'min': 1, 'std_dev': 1.118033988749895}\n?                                       ----\n\n+ {'max': 4, 'mean': 2.5, 'min': 1, 'std': 1.118033988749895}\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 55, in test_case_2\nAssertionError: {'mean': 15.0, 'std_dev': 5.0, 'min': 10, 'max': 20} != {'mean': 15.0, 'std': 5.0, 'min': 10, 'max': 20}\n- {'max': 20, 'mean': 15.0, 'min': 10, 'std_dev': 5.0}\n?                                          ----\n\n+ {'max': 20, 'mean': 15.0, 'min': 10, 'std': 5.0}\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 62, in test_case_3\nAssertionError: {'mean': -10.0, 'std_dev': 4.08248290463863, 'min': -15, 'max': -5} != {'mean': -10.0, 'std': 4.08248290463863, 'min': -15, 'max': -5}\n- {'max': -5, 'mean': -10.0, 'min': -15, 'std_dev': 4.08248290463863}\n?                                            ----\n\n+ {'max': -5, 'mean': -10.0, 'min': -15, 'std': 4.08248290463863}\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 76, in test_case_5\nAssertionError: {'mean': 6.5, 'std_dev': 0.816496580927726, 'min': 5.5, 'max': 7.5} != {'mean': 6.5, 'std': 0.816496580927726, 'min': 5.5, 'max': 7.5}\n- {'max': 7.5, 'mean': 6.5, 'min': 5.5, 'std_dev': 0.816496580927726}\n?                                           ----\n\n+ {'max': 7.5, 'mean': 6.5, 'min': 5.5, 'std': 0.816496580927726}\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_case_4\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/stats/kde.py\", line 206, in __init__\n    self.set_bandwidth(bw_method=bw_method)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/stats/kde.py\", line 554, in set_bandwidth\n    self._compute_covariance()\n  File \"/usr/local/lib/python3.10/site-packages/scipy/stats/kde.py\", line 566, in _compute_covariance\n    self._data_inv_cov = linalg.inv(self._data_covariance)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/linalg/basic.py\", line 968, in inv\n    raise LinAlgError(\"singular matrix\")\nnumpy.linalg.LinAlgError: singular matrix\n"
        }
      }
    ],
    "BigCodeBench/241": [
      {
        "task_id": "BigCodeBench/241",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using Min-Max scaling\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot original data\n    ax.plot(original_array, marker='o', linestyle='-', label='Original Data')\n    \n    # Plot normalized data\n    ax.plot(normalized_array, marker='s', linestyle='--', label='Normalized Data')\n    \n    # Customize the plot\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original vs Normalized Data')\n    ax.legend()\n    ax.grid(True, linestyle=':', alpha=0.7)\n    \n    # Add value annotations\n    for i, (orig, norm) in enumerate(zip(original_array, normalized_array)):\n        ax.annotate(f'{orig:.2f}', (i, orig), textcoords=\"offset points\", xytext=(0,10), ha='center')\n        ax.annotate(f'{norm:.2f}', (i, norm), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return original_array, normalized_array, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 49, in test_case_1\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 62, in test_case_2\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_case_3\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 91, in test_case_4\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'a'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_case_5\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 916, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 435, in fit\n    return self.partial_fit(X, y)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 473, in partial_fit\n    X = self._validate_data(\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 605, in _validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 967, in check_array\n    raise ValueError(\nValueError: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.\n"
        }
      }
    ],
    "BigCodeBench/267": [
      {
        "task_id": "BigCodeBench/267",
        "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n\n    pass\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    time = np.linspace(0, 1, sample_rate, endpoint=False)\n    signal = np.zeros(sample_rate)\n    \n    for key, value in data.items():\n        if isinstance(value, (int, float)):\n            try:\n                freq = float(key) if key != 'a' else 1.0\n                signal += value * np.sin(2 * np.pi * freq * time)\n            except ValueError:\n                pass  # Skip non-numeric keys\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    frequencies = fftpack.fftfreq(sample_rate, 1/sample_rate)\n\n    # 4. Plot the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(frequencies[:sample_rate//2], np.abs(fft_result)[:sample_rate//2])\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('FFT of the Signal')\n    plt.tight_layout()\n\n    return fft_result, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in test_case_1\nAssertionError: 'Frequency (Hz)' != 'Frequency [Hz]'\n- Frequency (Hz)\n?           ^  ^\n+ Frequency [Hz]\n?           ^  ^\n\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_case_2\nAssertionError: 'Frequency (Hz)' != 'Frequency [Hz]'\n- Frequency (Hz)\n?           ^  ^\n+ Frequency [Hz]\n?           ^  ^\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 82, in test_case_3\nAssertionError: 'Frequency (Hz)' != 'Frequency [Hz]'\n- Frequency (Hz)\n?           ^  ^\n+ Frequency [Hz]\n?           ^  ^\n\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 97, in test_case_4\nAssertionError: 'Frequency (Hz)' != 'Frequency [Hz]'\n- Frequency (Hz)\n?           ^  ^\n+ Frequency [Hz]\n?           ^  ^\n\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 112, in test_case_5\nAssertionError: 'Frequency (Hz)' != 'Frequency [Hz]'\n- Frequency (Hz)\n?           ^  ^\n+ Frequency [Hz]\n?           ^  ^\n\n"
        }
      }
    ],
    "BigCodeBench/273": [
      {
        "task_id": "BigCodeBench/273",
        "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\ndef task_func():\n\n    pass\nimport cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\ndef task_func():\n    class CustomHTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n        def _send_json_response(self, status_code, response_data):\n            self.send_response(status_code)\n            self.send_header('Content-Type', 'application/json')\n            response_json = json.dumps(response_data)\n            self.send_header('Content-Length', len(response_json))\n            self.end_headers()\n            self.wfile.write(response_json.encode('utf-8'))\n\n        def do_POST(self):\n            content_type = self.headers.get('Content-Type')\n            \n            if content_type != 'application/json':\n                self._send_json_response(400, {\n                    'status': 'error',\n                    'message': 'Content-Type header is not application/json'\n                })\n                return\n\n            content_length = int(self.headers.get('Content-Length', 0))\n            \n            try:\n                post_data = self.rfile.read(content_length)\n                json_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self._send_json_response(400, {\n                    'status': 'error',\n                    'message': 'Invalid JSON'\n                })\n                return\n\n            if 'data' not in json_data:\n                self._send_json_response(400, {\n                    'status': 'error',\n                    'message': 'No data key in request'\n                })\n                return\n\n            self._send_json_response(200, SUCCESS_RESPONSE)\n\n    return CustomHTTPRequestHandler",
        "status": "fail",
        "details": {
          "test_valid_json_data": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 100, in test_valid_json_data\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: mock(200)\nActual: mock(400)\n",
          "test_invalid_content_type": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 78, in test_invalid_content_type\n  File \"__test__.py\", line 36, in do_POST\n  File \"__test__.py\", line 25, in _send_json_response\n  File \"/usr/local/lib/python3.10/http/server.py\", line 498, in send_response\n    self.log_request(code)\n  File \"/usr/local/lib/python3.10/http/server.py\", line 551, in log_request\n    self.requestline, str(code), str(size))\nAttributeError: 'CustomHTTPRequestHandler' object has no attribute 'requestline'\n",
          "test_invalid_json": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 111, in test_invalid_json\n  File \"__test__.py\", line 36, in do_POST\n  File \"__test__.py\", line 25, in _send_json_response\n  File \"/usr/local/lib/python3.10/http/server.py\", line 498, in send_response\n    self.log_request(code)\n  File \"/usr/local/lib/python3.10/http/server.py\", line 551, in log_request\n    self.requestline, str(code), str(size))\nAttributeError: 'CustomHTTPRequestHandler' object has no attribute 'requestline'\n",
          "test_missing_data_key": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 122, in test_missing_data_key\n  File \"__test__.py\", line 36, in do_POST\n  File \"__test__.py\", line 25, in _send_json_response\n  File \"/usr/local/lib/python3.10/http/server.py\", line 498, in send_response\n    self.log_request(code)\n  File \"/usr/local/lib/python3.10/http/server.py\", line 551, in log_request\n    self.requestline, str(code), str(size))\nAttributeError: 'CustomHTTPRequestHandler' object has no attribute 'requestline'\n"
        }
      }
    ],
    "BigCodeBench/274": [
      {
        "task_id": "BigCodeBench/274",
        "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n\n    pass\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers.get('Content-Length', 0))\n            \n            try:\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data.decode('utf-8'))\n\n                if not all(key in email_data for key in ['subject', 'message', 'to']):\n                    raise ValueError(\"Missing required fields: 'subject', 'message', or 'to'\")\n\n                subject = email_data['subject']\n                message = email_data['message']\n                to_address = email_data['to']\n\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_address\n\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n\n                response = json.dumps({'status': 'success', 'message': 'Email sent successfully'})\n                self.send_response(200)\n                self.send_header('Content-type', 'application/json')\n                self.send_header('Content-length', len(response))\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Bad Request: Invalid JSON\")\n            except ValueError as ve:\n                self.send_error(400, f\"Bad Request: {str(ve)}\")\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication Failed\")\n            except Exception as e:\n                self.send_error(500, f\"Internal Server Error: {str(e)}\")\n\n    return EmailHandler",
        "status": "fail",
        "details": {
          "test_email_sending_exception": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 121, in test_email_sending_exception\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: mock(535)\nActual: not called.\n",
          "test_empty_json_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 106, in test_empty_json_data\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: mock(400)\nActual: not called.\n",
          "test_invalid_content_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_invalid_content_type\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: mock(400)\nActual: not called.\n",
          "test_invalid_json_format": "Traceback (most recent call last):\n  File \"__test__.py\", line 100, in test_invalid_json_format\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: mock(400)\nActual: not called.\n",
          "test_missing_key_in_json_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 84, in test_missing_key_in_json_data\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: mock(400)\nActual: not called.\n",
          "test_valid_json_request": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 91, in test_valid_json_request\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: SMTP('smtp.example.com', 587)\nActual: not called.\n"
        }
      }
    ],
    "BigCodeBench/308": [
      {
        "task_id": "BigCodeBench/308",
        "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n\n    pass\nimport pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n    \n    # Create DataFrame from the data dictionary\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # Calculate Average Grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate average grade for each subject\n    subject_averages = df[all_fields].mean()\n    \n    # Add 'Average' row to the DataFrame\n    df.loc['Average'] = subject_averages.tolist() + [df['Average Grade'].mean()]\n    \n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/287": [
      {
        "task_id": "BigCodeBench/287",
        "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n\n    pass\nfrom collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counts = Counter()\n    total_word_count = 0\n\n    # Traverse the directory and process .txt files\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    words = content.split()\n                    word_count = len(words)\n                    word_counts[file_path] = word_count\n                    total_word_count += word_count\n\n    # Export word counts to JSON file\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(dict(word_counts), f, ensure_ascii=False, indent=4)\n\n    return total_word_count",
        "status": "fail",
        "details": {
          "test_empty_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 80, in test_empty_files\nAssertionError: {'./testdir/empty_file.txt': 0} != {}\n- {'./testdir/empty_file.txt': 0}\n+ {}\n",
          "test_files_with_special_characters": "Traceback (most recent call last):\n  File \"__test__.py\", line 93, in test_files_with_special_characters\nAssertionError: {'./testdir/special_chars.txt': 2} != {'hello-world': 1, 'hello_python': 1}\n- {'./testdir/special_chars.txt': 2}\n+ {'hello-world': 1, 'hello_python': 1}\n",
          "test_multiple_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 69, in test_multiple_files\nAssertionError: {'./testdir/second.txt': 3, './testdir/third.txt': 2, './testdir/first.txt': 2} != {'hello': 2, 'world': 2, 'python': 2, 'coding': 1}\n- {'./testdir/first.txt': 2, './testdir/second.txt': 3, './testdir/third.txt': 2}\n+ {'coding': 1, 'hello': 2, 'python': 2, 'world': 2}\n",
          "test_nested_directories": "Traceback (most recent call last):\n  File \"__test__.py\", line 107, in test_nested_directories\nAssertionError: {'./testdir/nested_dir/nested_file.txt': 3} != {'hello': 2, 'world': 1}\n- {'./testdir/nested_dir/nested_file.txt': 3}\n+ {'hello': 2, 'world': 1}\n",
          "test_single_file_few_words": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in test_single_file_few_words\nAssertionError: {'./testdir/single_file.txt': 3} != {'hello': 2, 'world': 1}\n- {'./testdir/single_file.txt': 3}\n+ {'hello': 2, 'world': 1}\n"
        }
      }
    ],
    "BigCodeBench/214": [
      {
        "task_id": "BigCodeBench/214",
        "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n\n    pass\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set random seeds for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate random RGB image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis labels\n\n    # Show the plot\n    plt.show()\n\n    return ax, image",
        "status": "fail",
        "details": {
          "test_image_size_and_type": "Traceback (most recent call last):\n  File \"__test__.py\", line 50, in test_image_size_and_type\nAssertionError: Lists differ: [[[102, 220, 225], [95, 179, 61], [234, 203,[6230 chars]88]]] != [[[57, 12, 140], [125, 114, 71], [52, 44, 21[6305 chars]45]]]\n\nFirst differing element 0:\n[[102, 220, 225], [95, 179, 61], [234, 203,[269 chars]221]]\n[[57, 12, 140], [125, 114, 71], [52, 44, 21[266 chars] 36]]\n\nDiff is 16643 characters long. Set self.maxDiff to None to see it. : DataFrame contents should match the expected output\n"
        }
      }
    ],
    "BigCodeBench/310": [
      {
        "task_id": "BigCodeBench/310",
        "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n\n    pass\nimport os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Ensure directory exists\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    \n    # Generate data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{_ + 1}\"\n        age = random.randint(18, 80)\n        height = round(random.uniform(150, 200), 2)  # cm\n        weight = round(random.uniform(45, 120), 2)  # kg\n        data.append([name, age, height, weight])\n    \n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    \n    # Write to CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n        writer.writerow(['Average', avg_age, avg_height, avg_weight])\n    \n    return os.path.abspath(filename)",
        "status": "fail",
        "details": {
          "test_average_row_label": "Traceback (most recent call last):\n  File \"__test__.py\", line 50, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'test_average_row_label.csv'\n",
          "test_averages_calculation": "Traceback (most recent call last):\n  File \"__test__.py\", line 50, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'test_averages_calculation.csv'\n",
          "test_file_content_rows": "Traceback (most recent call last):\n  File \"__test__.py\", line 50, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'test_file_content_rows.csv'\n",
          "test_file_creation": "Traceback (most recent call last):\n  File \"__test__.py\", line 50, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'test_file_creation.csv'\n",
          "test_header": "Traceback (most recent call last):\n  File \"__test__.py\", line 50, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'test_header.csv'\n"
        }
      }
    ],
    "BigCodeBench/302": [
      {
        "task_id": "BigCodeBench/302",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Constants\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndef task_func(df, plot=False):\n    # Validate input DataFrame\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    if 'Value' not in df.columns:\n        raise ValueError(\"The input DataFrame does not contain the 'Value' column.\")\n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"The 'Value' column contains invalid data. All entries must be lists.\")\n\n    # Split lists in 'Value' column into separate columns\n    expanded_df = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Calculate Pearson correlation coefficient\n    corr_matrix = expanded_df.corr(method='pearson')\n    \n    if plot:\n        # Create heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n        ax.set_title(\"Correlation Heatmap\")\n        plt.tight_layout()\n        return corr_matrix, ax\n    \n    return corr_matrix",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/313": [
      {
        "task_id": "BigCodeBench/313",
        "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n\n    pass\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The provided path '{directory}' is not a valid directory.\")\n\n    moved_files = {}\n    pattern = re.compile(r'^([^[\\]]+)')\n\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            match = pattern.search(filename)\n            if match:\n                subdirectory_name = match.group(1).strip()\n                if subdirectory_name:\n                    subdirectory_path = os.path.join(directory, subdirectory_name)\n                    os.makedirs(subdirectory_path, exist_ok=True)\n                    \n                    new_file_path = os.path.join(subdirectory_path, filename)\n                    shutil.move(file_path, new_file_path)\n                    \n                    if subdirectory_name not in moved_files:\n                        moved_files[subdirectory_name] = []\n                    moved_files[subdirectory_name].append(filename)\n\n    return directory, moved_files",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 67, in test_case_1\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/tmp/tmpc5adiyo0/test/test_file1.txt'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 77, in test_case_2\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/tmp/tmpvbpmz9l1/test/test_file1.txt'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in test_case_3\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/tmp/tmp4nsl26lo/test/test_file1.txt'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 90, in test_case_4\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/tmp/tmp2no5r_k0/test/test_file1.txt'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_case_5\n  File \"__test__.py\", line 27, in task_func\n  File \"/usr/local/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/tmp/tmp9kgl3rtx/test/test_file1.txt'\n"
        }
      }
    ],
    "BigCodeBench/324": [
      {
        "task_id": "BigCodeBench/324",
        "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n\n    pass\nimport subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    def run_subprocess(file, results, index):\n        try:\n            process = subprocess.Popen(['python', file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            process.wait()\n            results[index] = process.returncode\n        except Exception as e:\n            print(f\"Error running {file}: {str(e)}\")\n            results[index] = -1\n\n    threads = []\n    results = [None] * len(file_list)\n\n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(file, results, i))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results",
        "status": "fail",
        "details": {
          "test_multiple_processes_with_different_exit_codes": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 68, in test_multiple_processes_with_different_exit_codes\nAssertionError: Lists differ: [<MagicMock name='Popen().returncode' id='[130 chars]96'>] != [0, 1, None]\n\nFirst differing element 0:\n<MagicMock name='Popen().returncode' id='46916724593696'>\n0\n\n+ [0, 1, None]\n- [<MagicMock name='Popen().returncode' id='46916724593696'>,\n-  <MagicMock name='Popen().returncode' id='46916724593696'>,\n-  <MagicMock name='Popen().returncode' id='46916724593696'>]\n",
          "test_process_still_running": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 62, in test_process_still_running\nAssertionError: Lists differ: [<MagicMock name='Popen().returncode' id='46916725001232'>] != [None]\n\nFirst differing element 0:\n<MagicMock name='Popen().returncode' id='46916725001232'>\nNone\n\n- [<MagicMock name='Popen().returncode' id='46916725001232'>]\n+ [None]\n",
          "test_valid_directory_and_files": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 55, in test_valid_directory_and_files\nAssertionError: Lists differ: [<MagicMock name='Popen().returncode' id='[71 chars]60'>] != [0, 0]\n\nFirst differing element 0:\n<MagicMock name='Popen().returncode' id='46916725113760'>\n0\n\n+ [0, 0]\n- [<MagicMock name='Popen().returncode' id='46916725113760'>,\n-  <MagicMock name='Popen().returncode' id='46916725113760'>]\n",
          "test_valid_files": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 47, in test_valid_files\nAssertionError: Lists differ: [<MagicMock name='Popen().returncode' id='[71 chars]56'>] != [0, 0]\n\nFirst differing element 0:\n<MagicMock name='Popen().returncode' id='46916725226656'>\n0\n\n+ [0, 0]\n- [<MagicMock name='Popen().returncode' id='46916725226656'>,\n-  <MagicMock name='Popen().returncode' id='46916725226656'>]\n"
        }
      }
    ],
    "BigCodeBench/326": [
      {
        "task_id": "BigCodeBench/326",
        "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n\n    pass\nimport subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    results = []\n    \n    # Find all .bat files in the given directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Run the .bat file and capture the exit code\n            process = subprocess.run([bat_file], shell=True, check=False, capture_output=True)\n            exit_code = process.returncode\n        except Exception:\n            # If the file couldn't be executed, set exit code to None\n            exit_code = None\n        \n        results.append((file_name, exit_code))\n    \n    return results",
        "status": "fail",
        "details": {
          "test_multiple_bat_files_mixed_results": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 71, in test_multiple_bat_files_mixed_results\nAssertionError: Lists differ: [('file1.bat', None), ('file2.bat', None), ('file3.bat', None)] != [('file1.bat', 0), ('file2.bat', 1), ('file3.bat', None)]\n\nFirst differing element 0:\n('file1.bat', None)\n('file1.bat', 0)\n\n- [('file1.bat', None), ('file2.bat', None), ('file3.bat', None)]\n?                ^^^^                 ^^^^\n\n+ [('file1.bat', 0), ('file2.bat', 1), ('file3.bat', None)]\n?                ^                 ^\n\n",
          "test_single_bat_file_failure": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 58, in test_single_bat_file_failure\nAssertionError: Lists differ: [('file1.bat', None)] != [('file1.bat', 1)]\n\nFirst differing element 0:\n('file1.bat', None)\n('file1.bat', 1)\n\n- [('file1.bat', None)]\n?                ^^^^\n\n+ [('file1.bat', 1)]\n?                ^\n\n",
          "test_single_bat_file_success": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 49, in test_single_bat_file_success\nAssertionError: Lists differ: [('file1.bat', None)] != [('file1.bat', 0)]\n\nFirst differing element 0:\n('file1.bat', None)\n('file1.bat', 0)\n\n- [('file1.bat', None)]\n?                ^^^^\n\n+ [('file1.bat', 0)]\n?                ^\n\n"
        }
      }
    ],
    "BigCodeBench/341": [
      {
        "task_id": "BigCodeBench/341",
        "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n\n    pass\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input DataFrame must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' does not exist in the DataFrame\")\n\n    # Create figure and subplots\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n\n    # Histogram with KDE for numerical data\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(data=df, x=col, kde=True, ax=ax1)\n    else:\n        sns.histplot(data=df, x=col, ax=ax1)\n    ax1.set_title(f'Histogram of {col}')\n    ax1.set_xlabel(col)\n    ax1.set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(data=df, x=col, ax=ax2)\n    ax2.set_title(f'Box Plot of {col}')\n    ax2.set_xlabel(col)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return fig",
        "status": "fail",
        "details": {
          "test_categorical_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_categorical_data\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/360": [
      {
        "task_id": "BigCodeBench/360",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n\n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation for each column\n    stats = {}\n    for column in df.columns:\n        if pd.api.types.is_numeric_dtype(df[column]):\n            stats[column] = {\n                'mean': df[column].mean(),\n                'std': df[column].std()\n            }\n\n    # Prepare data for the bar chart\n    columns = list(stats.keys())\n    means = [stats[col]['mean'] for col in columns]\n    stds = [stats[col]['std'] for col in columns]\n\n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    x = np.arange(len(columns))\n    width = 0.35\n\n    ax.bar(x - width/2, means, width, label='Mean', color='skyblue')\n    ax.bar(x + width/2, stds, width, label='Standard Deviation', color='lightgreen')\n\n    # Customize the chart\n    ax.set_xlabel('Columns', fontsize=12)\n    ax.set_ylabel('Values', fontsize=12)\n    ax.set_title('Mean and Standard Deviation', fontsize=14)\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns, rotation=45, ha='right')\n    ax.legend()\n\n    # Adjust layout and display\n    fig.tight_layout()\n\n    return stats, fig",
        "status": "fail",
        "details": {
          "test_correct_mean_and_std_values": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_correct_mean_and_std_values\nAssertionError: {'A':[19 chars]d': 14.142135623730951}, 'B': {'mean': 30.0, '[21 chars]951}} != {'A':[19 chars]d': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}}\n+ {'A': {'mean': 20.0, 'std': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}}\n- {'A': {'mean': 20.0, 'std': 14.142135623730951},\n-  'B': {'mean': 30.0, 'std': 14.142135623730951}}\n",
          "test_value": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_value\nAssertionError: {'A':[19 chars]d': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}} != {'A':[19 chars]d': 14.142135623730951}, 'B': {'mean': 30.0, '[21 chars]951}}\n- {'A': {'mean': 20.0, 'std': 10.0}, 'B': {'mean': 30.0, 'std': 10.0}}\n+ {'A': {'mean': 20.0, 'std': 14.142135623730951},\n+  'B': {'mean': 30.0, 'std': 14.142135623730951}}\n"
        }
      }
    ],
    "BigCodeBench/367": [
      {
        "task_id": "BigCodeBench/367",
        "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n\n    pass\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Check if all elements in activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n    \n    # Count activities for each day of the week\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n    \n    # Prepare data for plotting\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_counts[day] for day in days]\n    \n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(days, counts, color='skyblue', edgecolor='navy')\n    \n    # Customize the chart\n    ax.set_xlabel('Day of the Week', fontsize=12)\n    ax.set_ylabel('Number of Activities', fontsize=12)\n    ax.set_title('Weekly Activity', fontsize=14, fontweight='bold')\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    \n    # Add value labels on top of each bar\n    for i, count in enumerate(counts):\n        ax.text(i, count, str(count), ha='center', va='bottom')\n    \n    # Adjust layout and improve appearance\n    plt.tight_layout()\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    \n    return ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/368": [
      {
        "task_id": "BigCodeBench/368",
        "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n\n    pass\nimport os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Get list of files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n\n    # Check if there are files in the source directory\n    if not files:\n        raise ValueError(\"No files found in the source directory.\")\n\n    # Choose a random file\n    random_file = random.choice(files)\n\n    # Construct full paths\n    src_path = os.path.join(src_dir, random_file)\n    dest_path = os.path.join(dest_dir, random_file)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the moved file\n    return random_file",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 84, in test_case_3\n  File \"__test__.py\", line 19, in task_func\nValueError: No files found in the source directory.\n"
        }
      }
    ],
    "BigCodeBench/346": [
      {
        "task_id": "BigCodeBench/346",
        "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n\n    pass\nimport subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script '{script_path}' does not exist.\")\n\n    # Construct the command to run the script\n    command = [sys.executable, script_path] + list(args)\n\n    try:\n        # Start the subprocess\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        if wait:\n            # Wait for the process to complete\n            stdout, stderr = process.communicate()\n            return_code = process.returncode\n\n            if return_code != 0:\n                raise subprocess.CalledProcessError(return_code, command, output=stdout, stderr=stderr)\n\n            return return_code\n        else:\n            # Do not wait for the process to complete\n            return None\n\n    except subprocess.CalledProcessError:\n        # Re-raise the CalledProcessError\n        raise\n    except Exception as e:\n        # For any other exception, raise a CalledProcessError\n        raise subprocess.CalledProcessError(-1, command, str(e))",
        "status": "fail",
        "details": {
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_case_2\n  File \"__test__.py\", line 30, in task_func\nsubprocess.CalledProcessError: Command '['/usr/local/bin/python3', '/tmp/tmpxt8tzccw/test/script2.py']' returned non-zero exit status 1.\n"
        }
      }
    ],
    "BigCodeBench/399": [
      {
        "task_id": "BigCodeBench/399",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    # Validate input parameters\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Generate time values\n    t = np.linspace(0, 1, sample_size)\n\n    # Calculate sine and cosine waves\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave', color='blue')\n    ax.plot(t, cosine_wave, label='Cosine Wave', color='red')\n\n    # Set title and labels\n    ax.set_title(f'Sine and Cosine Waves (Frequency: {frequency} Hz)')\n    ax.set_xlabel('Time (s)')\n    ax.set_ylabel('Amplitude')\n\n    # Add legend and grid\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    # Set y-axis limits for better visualization\n    ax.set_ylim(-1.1, 1.1)\n\n    # Return the figure and axes objects\n    return fig, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 49, in test_case_1\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/374": [
      {
        "task_id": "BigCodeBench/374",
        "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n\n    pass\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Counter for processed files\n    processed_files = 0\n    \n    # Compile regex pattern for double quotes\n    quote_pattern = re.compile(r'\"')\n    \n    for file_path in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(filename=file_path)\n        \n        # Process each sheet in the workbook\n        for sheet in workbook.worksheets:\n            for row in sheet.iter_rows():\n                for cell in row:\n                    if isinstance(cell.value, str):\n                        # Replace double quotes with escaped double quotes\n                        cell.value = quote_pattern.sub(r'\\\\\"', cell.value)\n        \n        # Save the modified workbook\n        workbook.save(file_path)\n        processed_files += 1\n    \n    return processed_files",
        "status": "fail",
        "details": {
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 126, in test_case_4\nAssertionError: FileNotFoundError not raised\n"
        }
      }
    ],
    "BigCodeBench/401": [
      {
        "task_id": "BigCodeBench/401",
        "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n\n    pass\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables or use defaults\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    mail_port = int(os.environ.get('MAIL_PORT', 25))\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', '').lower() in ('true', '1', 'yes', 'on')\n    mail_username = os.environ.get('MAIL_USERNAME')\n    mail_password = os.environ.get('MAIL_PASSWORD')\n\n    # Configure Flask app with mail settings\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Return tuple with Flask-Mail instance and mail configurations\n    return mail, app.config",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/409": [
      {
        "task_id": "BigCodeBench/409",
        "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n\n    pass\nimport os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The Excel file does not exist at the specified path: {full_path}\")\n    \n    # Read the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise FileNotFoundError(f\"Error reading the Excel file: {e}\")\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not found in the Excel file.\")\n    \n    # Calculate mean, median, and standard deviation\n    mean_value = df[column_name].mean()\n    median_value = df[column_name].median()\n    std_dev_value = df[column_name].std()\n    \n    # Return the results in a dictionary\n    return {\n        \"mean\": mean_value,\n        \"median\": median_value,\n        \"standard_deviation\": std_dev_value\n    }",
        "status": "fail",
        "details": {
          "test_normal_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_normal_functionality\nKeyError: 'std_dev'\n"
        }
      }
    ],
    "BigCodeBench/424": [
      {
        "task_id": "BigCodeBench/424",
        "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n\n    pass\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Failed to read the image from the path: {image_path}\")\n\n    # Convert the image from BGR to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixels = image_rgb.reshape((-1, 3))\n\n    # Convert to float type for K-means\n    pixels_float = np.float32(pixels)\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels_float)\n\n    # Get the labels and cluster centers\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n\n    # Convert back to 8-bit values\n    centers = np.uint8(centers)\n\n    # Map each pixel to its corresponding cluster center\n    segmented_image = centers[labels.flatten()]\n\n    # Reshape the segmented image back to the original shape\n    segmented_image = segmented_image.reshape(image_rgb.shape)\n\n    # Save each region as a separate image\n    for i in range(n_clusters):\n        mask = labels == i\n        region = np.zeros_like(image_rgb)\n        region[mask.reshape(image_rgb.shape[:2])] = image_rgb[mask.reshape(image_rgb.shape[:2])]\n        cv2.imwrite(f'region_{i}.png', cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n\n    return image_rgb, segmented_image",
        "status": "fail",
        "details": {
          "test_normal_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 94, in test_normal_functionality\nAssertionError: False is not true : The arrays should be equal\n",
          "test_single_cluster_returns_original_image": "Traceback (most recent call last):\n  File \"__test__.py\", line 119, in test_single_cluster_returns_original_image\n  File \"/usr/local/lib/python3.10/site-packages/numpy/testing/_private/utils.py\", line 934, in assert_array_equal\n    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\n  File \"/usr/local/lib/python3.10/site-packages/numpy/testing/_private/utils.py\", line 844, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not equal\nThe original and segmented images should be identical when n_clusters is set to 1.\nMismatched elements: 296 / 300 (98.7%)\nMax absolute difference: 255\nMax relative difference: 1.04508197\n x: array([[[255, 252, 253],\n        [251, 246, 243],\n        [255, 254, 248],...\n y: array([[[247, 247, 244],\n        [247, 247, 244],\n        [247, 247, 244],...\n"
        }
      }
    ],
    "BigCodeBench/443": [
      {
        "task_id": "BigCodeBench/443",
        "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n\n    pass\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Validate input dimensions\n    if P.ndim != 2 or T.ndim != 3:\n        raise ValueError(\"P must be a 2D matrix and T must be a 3D tensor.\")\n    if P.shape[1] != T.shape[2]:\n        raise ValueError(\"The number of columns in P must match the last dimension of T.\")\n\n    # Calculate the product of P and T\n    result = np.dot(T.reshape(-1, T.shape[2]), P.T).reshape(T.shape[0], T.shape[1], P.shape[0])\n\n    # Flatten the result\n    flattened_result = result.reshape(-1, P.shape[0])\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_result)\n\n    # Visualize the clustering result\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis')\n    legend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n    ax.add_artist(legend1)\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\n    return cluster_result, ax",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_case_3\nAssertionError: 9 != 12\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 84, in test_case_2\nAttributeError: 'bool' object has no attribute 'all'\n"
        }
      }
    ],
    "BigCodeBench/445": [
      {
        "task_id": "BigCodeBench/445",
        "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n\n    pass\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Input validation\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input points must be a numpy array\")\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Points must be a 2D array with shape (n, 2)\")\n    \n    if points.size == 0:\n        raise ValueError(\"Points array must not be empty\")\n\n    # Set random seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Apply jittering to the points\n    jitter = rng.normal(scale=1e-6, size=points.shape)\n    jittered_points = points + jitter\n    \n    # Compute the Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=2)\n    \n    # Plot the original points\n    ax.plot(points[:, 0], points[:, 1], 'ko', markersize=5)\n    \n    # Set plot limits and labels\n    ax.set_xlim(points[:, 0].min() - 0.1, points[:, 0].max() + 0.1)\n    ax.set_ylim(points[:, 1].min() - 0.1, points[:, 1].max() + 0.1)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Voronoi Diagram')\n    \n    # Return the Voronoi object and the plot axes\n    return vor, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/454": [
      {
        "task_id": "BigCodeBench/454",
        "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n\n    pass\nimport os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if the source directory exists\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    \n    # Check if the destination directory exists\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n    \n    # Ensure the extension starts with a dot\n    if not ext.startswith('.'):\n        ext = '.' + ext\n    \n    # Create the search pattern for the specified extension\n    search_pattern = os.path.join(src_dir, f\"*{ext}\")\n    \n    # Find all files in the source directory that match the specified extension\n    files_to_move = glob.glob(search_pattern)\n    \n    # List to store the full paths of files that were successfully moved\n    moved_files = []\n    \n    for file_path in files_to_move:\n        # Extract the file name from the full path\n        file_name = os.path.basename(file_path)\n        \n        # Create the destination path for the file\n        dest_path = os.path.join(dest_dir, file_name)\n        \n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_path):\n            # Move the file to the destination directory\n            shutil.move(file_path, dest_path)\n            # Add the destination path to the list of moved files\n            moved_files.append(dest_path)\n    \n    return moved_files",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/458": [
      {
        "task_id": "BigCodeBench/458",
        "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n\n    pass\nimport json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    def double_numeric(value):\n        if isinstance(value, (int, float)):\n            return float(value * 2)\n        elif isinstance(value, str):\n            match = re.match(r'^-?\\d+(\\.\\d+)?$', value)\n            if match:\n                return float(float(value) * 2)\n        elif isinstance(value, list):\n            return [double_numeric(item) for item in value]\n        return value\n\n    try:\n        data_dict = json.loads(json_str)\n        if not isinstance(data_dict, dict):\n            return pd.DataFrame()\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    normalized_dict = {k: double_numeric(v) for k, v in data_dict.items()}\n    \n    try:\n        df = pd.DataFrame([normalized_dict])\n        return df\n    except ValueError:\n        return pd.DataFrame()",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 44, in test_case_1\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1163, in assert_frame_equal\n    raise_assert_detail(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (1, 3)\n[right]: (3, 3)\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 48, in test_case_2\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1163, in assert_frame_equal\n    raise_assert_detail(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (1, 0)\n[right]: (0, 0)\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 59, in test_case_3\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1163, in assert_frame_equal\n    raise_assert_detail(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (1, 4)\n[right]: (3, 4)\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 67, in test_case_5\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1163, in assert_frame_equal\n    raise_assert_detail(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame are different\n\nDataFrame shape mismatch\n[left]:  (1, 2)\n[right]: (3, 2)\n"
        }
      }
    ],
    "BigCodeBench/477": [
      {
        "task_id": "BigCodeBench/477",
        "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n\n    pass\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate categories\n    if N >= len(CATEGORIES):\n        # Ensure each category appears at least once\n        categories = np.array(CATEGORIES + list(np.random.choice(CATEGORIES, N - len(CATEGORIES))))\n        np.random.shuffle(categories)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=pd.Categorical(df['category']).codes, cmap='viridis')\n    \n    # Add legend\n    legend = ax.legend(*scatter.legend_elements(), title=\"Category\")\n    ax.add_artist(legend)\n    \n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter plot of x vs y, colored by category')\n    \n    return df, ax",
        "status": "fail",
        "details": {
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 87, in test_case_6\n  File \"__test__.py\", line 37, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/collections.py\", line 1094, in legend_elements\n    fmt.axis.set_view_interval(fu.min(), fu.max())\n  File \"/usr/local/lib/python3.10/site-packages/numpy/ma/core.py\", line 5700, in min\n    result = self.filled(fill_value).min(\n  File \"/usr/local/lib/python3.10/site-packages/numpy/core/_methods.py\", line 44, in _amin\n    return umr_minimum(a, axis, None, out, keepdims, initial, where)\nValueError: zero-size array to reduction operation minimum which has no identity\n"
        }
      }
    ],
    "BigCodeBench/486": [
      {
        "task_id": "BigCodeBench/486",
        "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\ndef task_func(start_time, end_time, step, trend, seed=42):\n\n    pass\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generate and plot a time series with specified parameters.\n\n    Parameters:\n    start_time (str): Start time in 'YYYY-MM-DD HH:MM:SS' format.\n    end_time (str): End time in 'YYYY-MM-DD HH:MM:SS' format.\n    step (str): Time step (e.g., '1H' for hourly, '1D' for daily).\n    trend (float): Linear trend to add per time step.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    matplotlib.pyplot.Axes: The Axes object of the generated plot.\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Create date range\n    date_range = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate random values from normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(date_range))\n\n    # Add linear trend\n    trend_component = np.arange(len(date_range)) * trend\n    values += trend_component\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': date_range, 'Value': values})\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n\n    # Customize plot\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n\n    # Adjust layout to prevent cutoff\n    plt.tight_layout()\n\n    return ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_case_1\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py\", line 945, in date_range\n    dtarr = DatetimeArray._generate_range(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\", line 398, in _generate_range\n    freq = to_offset(freq)\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4102, in pandas._libs.tslibs.offsets.to_offset\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4203, in pandas._libs.tslibs.offsets.to_offset\nValueError: Invalid frequency: 100\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 76, in test_case_2\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py\", line 945, in date_range\n    dtarr = DatetimeArray._generate_range(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\", line 398, in _generate_range\n    freq = to_offset(freq)\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4102, in pandas._libs.tslibs.offsets.to_offset\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4203, in pandas._libs.tslibs.offsets.to_offset\nValueError: Invalid frequency: 100\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 96, in test_case_3\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py\", line 945, in date_range\n    dtarr = DatetimeArray._generate_range(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\", line 398, in _generate_range\n    freq = to_offset(freq)\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4102, in pandas._libs.tslibs.offsets.to_offset\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4203, in pandas._libs.tslibs.offsets.to_offset\nValueError: Invalid frequency: 100\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_case_4\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py\", line 945, in date_range\n    dtarr = DatetimeArray._generate_range(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\", line 398, in _generate_range\n    freq = to_offset(freq)\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4102, in pandas._libs.tslibs.offsets.to_offset\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4203, in pandas._libs.tslibs.offsets.to_offset\nValueError: Invalid frequency: 100\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_case_5\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py\", line 945, in date_range\n    dtarr = DatetimeArray._generate_range(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\", line 398, in _generate_range\n    freq = to_offset(freq)\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4102, in pandas._libs.tslibs.offsets.to_offset\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4203, in pandas._libs.tslibs.offsets.to_offset\nValueError: Invalid frequency: 100\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 132, in test_case_8\n  File \"__test__.py\", line 29, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py\", line 945, in date_range\n    dtarr = DatetimeArray._generate_range(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\", line 398, in _generate_range\n    freq = to_offset(freq)\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4102, in pandas._libs.tslibs.offsets.to_offset\n  File \"pandas/_libs/tslibs/offsets.pyx\", line 4203, in pandas._libs.tslibs.offsets.to_offset\nValueError: Invalid frequency: 100\n"
        }
      }
    ],
    "BigCodeBench/492": [
      {
        "task_id": "BigCodeBench/492",
        "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n\n    pass\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Input validation\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    end_date = datetime.now()\n\n    # Initialize list to store sales data\n    sales_data = []\n\n    # Generate sales data for each day and product\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({\n                'Product': product,\n                'Date': current_date,\n                'Sales': sales\n            })\n        current_date += timedelta(days=1)\n\n    # Create DataFrame from sales data\n    df = pd.DataFrame(sales_data)\n\n    return df",
        "status": "fail",
        "details": {
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 100, in test_case_5\nAssertionError: ValueError not raised\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 104, in test_case_6\nAssertionError: ValueError not raised\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_case_7\nAssertionError: ValueError not raised\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_case_2\n  File \"__test__.py\", line 21, in task_func\nValueError: epoch_milliseconds must be a non-negative integer\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_case_3\n  File \"__test__.py\", line 21, in task_func\nValueError: epoch_milliseconds must be a non-negative integer\n"
        }
      }
    ],
    "BigCodeBench/501": [
      {
        "task_id": "BigCodeBench/501",
        "solution": "import xlwt\nimport os\nimport pandas as pd\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n\n    pass\nimport xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is a valid type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n    \n    # Convert JSON data to a DataFrame\n    if isinstance(data, list) and not data:\n        # Handle empty JSON array\n        df = pd.DataFrame()\n    else:\n        df = pd.DataFrame(data)\n    \n    # Write DataFrame to Excel file\n    try:\n        with pd.ExcelWriter(filename, engine='xlwt') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(f\"Error writing Excel file: {e}\")\n    \n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)",
        "status": "fail",
        "details": {
          "test_custom_sheet_name": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 91, in get_writer\n    return _writers[engine_name]\nKeyError: 'xlwt'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 31, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_base.py\", line 1119, in __new__\n    cls = get_writer(engine)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 93, in get_writer\n    raise ValueError(f\"No Excel writer '{engine_name}'\") from err\nValueError: No Excel writer 'xlwt'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 63, in test_custom_sheet_name\n  File \"__test__.py\", line 34, in task_func\nException: Error writing Excel file: No Excel writer 'xlwt'\n",
          "test_empty_json": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 91, in get_writer\n    return _writers[engine_name]\nKeyError: 'xlwt'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 31, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_base.py\", line 1119, in __new__\n    cls = get_writer(engine)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 93, in get_writer\n    raise ValueError(f\"No Excel writer '{engine_name}'\") from err\nValueError: No Excel writer 'xlwt'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 51, in test_empty_json\n  File \"__test__.py\", line 34, in task_func\nException: Error writing Excel file: No Excel writer 'xlwt'\n",
          "test_file_content": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 91, in get_writer\n    return _writers[engine_name]\nKeyError: 'xlwt'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 31, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_base.py\", line 1119, in __new__\n    cls = get_writer(engine)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 93, in get_writer\n    raise ValueError(f\"No Excel writer '{engine_name}'\") from err\nValueError: No Excel writer 'xlwt'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 69, in test_file_content\n  File \"__test__.py\", line 34, in task_func\nException: Error writing Excel file: No Excel writer 'xlwt'\n",
          "test_valid_json": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 91, in get_writer\n    return _writers[engine_name]\nKeyError: 'xlwt'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 31, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_base.py\", line 1119, in __new__\n    cls = get_writer(engine)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/excel/_util.py\", line 93, in get_writer\n    raise ValueError(f\"No Excel writer '{engine_name}'\") from err\nValueError: No Excel writer 'xlwt'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 44, in test_valid_json\n  File \"__test__.py\", line 34, in task_func\nException: Error writing Excel file: No Excel writer 'xlwt'\n"
        }
      }
    ],
    "BigCodeBench/502": [
      {
        "task_id": "BigCodeBench/502",
        "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\ndef task_func(days_in_past=7, random_seed=0):\n\n    pass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random data\n    data = []\n    for date in date_range:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append({\n                'Date': date.strftime('%Y-%m-%d'),\n                'Activity': activity,\n                'Duration': duration\n            })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Customize plot\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax, df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/418": [
      {
        "task_id": "BigCodeBench/418",
        "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n\n    pass\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(10, activation='sigmoid', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer specifying a learning rate\n    model.compile(loss='binary_crossentropy', \n                  optimizer=keras.optimizers.SGD(learning_rate=0.01), \n                  metrics=['accuracy'])\n\n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_pred = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.set_title('ROC curve')\n    ax.legend(loc=\"lower right\")\n\n    return model, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/503": [
      {
        "task_id": "BigCodeBench/503",
        "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n\n    pass\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices\n    stock_data = {stock: np.random.random(days_in_past) for stock in stock_names}\n    \n    # Create DataFrame\n    df = pd.DataFrame(stock_data, index=date_range)\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 92, in test_case_4\nAssertionError: ValueError not raised\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_case_5\nAssertionError: ValueError not raised\n"
        }
      }
    ],
    "BigCodeBench/509": [
      {
        "task_id": "BigCodeBench/509",
        "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n\n    pass\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the contents of both files\n        with open(file_path1, 'r', newline='', encoding='utf-8') as file1:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            lines1 = list(reader1)\n\n        with open(file_path2, 'r', newline='', encoding='utf-8') as file2:\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            lines2 = list(reader2)\n\n        # Check if either file is empty\n        if not lines1:\n            raise ValueError(f\"File {file_path1} is empty.\")\n        if not lines2:\n            raise ValueError(f\"File {file_path2} is empty.\")\n\n        # Convert lines to strings for comparison\n        lines1_str = [delimiter.join(line) for line in lines1]\n        lines2_str = [delimiter.join(line) for line in lines2]\n\n        # Generate the diff\n        diff = list(ndiff(lines1_str, lines2_str))\n\n        # Prepare the difference report\n        differences = []\n        for line_num, line in enumerate(diff, start=1):\n            status = line[0]\n            content = line[2:]\n            if status in ('-', '+', ' '):\n                differences.append({\n                    'Line Number': line_num,\n                    'Status': status,\n                    'Content': content.strip()\n                })\n\n        # Create and return the DataFrame\n        return pd.DataFrame(differences)\n\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\")\n    except ValueError as e:\n        raise e\n    except Exception as e:\n        raise Exception(f\"An error occurred: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_difference_report": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_difference_report\nAssertionError: Lists differ: ['1, ,name,age', '2,-,Alice,30', '4,+,Alic[15 chars],25'] != [\"1, ,('name', 'age')\", \"2,-,('Alice', '30[43 chars]5')\"]\n\nFirst differing element 0:\n'1, ,name,age'\n\"1, ,('name', 'age')\"\n\n- ['1, ,name,age', '2,-,Alice,30', '4,+,Alice,31', '6, ,Bob,25']\n+ [\"1, ,('name', 'age')\",\n+  \"2,-,('Alice', '30')\",\n+  \"3,+,('Alice', '31')\",\n+  \"4, ,('Bob', '25')\"]\n"
        }
      }
    ],
    "BigCodeBench/511": [
      {
        "task_id": "BigCodeBench/511",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Handle empty data list\n    if not data:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([1], labels=['No Data'])\n        ax.set_title('Empty Dataset')\n        return stats, ax\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in the dataset.\")\n\n    # Calculate statistics\n    col_data = df[column]\n    stats = {\n        'sum': col_data.sum(),\n        'mean': col_data.mean(),\n        'min': col_data.min(),\n        'max': col_data.max()\n    }\n\n    # Create pie chart\n    fig, ax = plt.subplots()\n    if 'Age' in df.columns:\n        age_counts = df['Age'].value_counts().sort_index()\n        ax.pie(age_counts.values, labels=age_counts.index, autopct='%1.1f%%', startangle=90)\n        ax.set_title('Age Distribution')\n    else:\n        unique_values = col_data.value_counts().sort_index()\n        ax.pie(unique_values.values, labels=unique_values.index, autopct='%1.1f%%', startangle=90)\n        ax.set_title(f'{column} Distribution')\n\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n\n    return stats, ax\ndata = [\n    {'Name': 'Alice', 'Age': 30, 'Salary': 50000},\n    {'Name': 'Bob', 'Age': 35, 'Salary': 60000},\n    {'Name': 'Charlie', 'Age': 30, 'Salary': 55000},\n    {'Name': 'David', 'Age': 40, 'Salary': 70000},\n]",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_case_1\n  File \"__test__.py\", line 24, in task_func\nValueError: Column 'Salary' not found in the dataset.\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 81, in test_case_2\n  File \"__test__.py\", line 24, in task_func\nValueError: Column 'Experience' not found in the dataset.\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 92, in test_case_3\n  File \"__test__.py\", line 24, in task_func\nValueError: Column 'Age' not found in the dataset.\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 104, in test_case_5\n  File \"__test__.py\", line 24, in task_func\nValueError: Column 'Age' not found in the dataset.\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 114, in test_case_6\n  File \"__test__.py\", line 24, in task_func\nValueError: Column 'InvalidColumn' not found in the dataset.\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 124, in test_case_7\n  File \"__test__.py\", line 24, in task_func\nValueError: Column 'Salary' not found in the dataset.\n"
        }
      }
    ],
    "BigCodeBench/513": [
      {
        "task_id": "BigCodeBench/513",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if the data list is empty\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n\n    # Convert the data list to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate non-negative values for steps, calories burned, and distance walked\n    numeric_columns = ['steps', 'calories_burned', 'distance_walked']\n    for col in numeric_columns:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in '{col}' column.\")\n\n    # Calculate statistics for the specified column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create the line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df[column], marker='o', linestyle='-', linewidth=2, markersize=6)\n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel(column.capitalize(), fontsize=12)\n    ax.set_title(f'Line Chart of {column.capitalize()}', fontsize=14)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Add grid for better readability\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    # Adjust layout to prevent cutoff of labels\n    plt.tight_layout()\n\n    return stats, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_case_1\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Steps' is not valid.\"\n",
          "test_case_11": "Traceback (most recent call last):\n  File \"__test__.py\", line 145, in test_case_11\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Steps' is not valid.\"\n",
          "test_case_12": "Traceback (most recent call last):\n  File \"__test__.py\", line 155, in test_case_12\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Distance Walked' is not valid.\"\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 74, in test_case_2\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Calories Burned' is not valid.\"\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 82, in test_case_3\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Distance Walked' is not valid.\"\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 92, in test_case_4\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Steps' is not valid.\"\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 102, in test_case_5\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Calories Burned' is not valid.\"\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 116, in test_case_7\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Steps' is not valid.\"\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_case_8\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Steps' is not valid.\"\n",
          "test_case_9": "Traceback (most recent call last):\n  File \"__test__.py\", line 129, in test_case_9\n  File \"__test__.py\", line 20, in task_func\nKeyError: \"The specified column 'Steps' is not valid.\"\n"
        }
      }
    ],
    "BigCodeBench/526": [
      {
        "task_id": "BigCodeBench/526",
        "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n\n    pass\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Load JSON data from the file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Dictionary to store numeric values for each key\n    values_dict = defaultdict(list)\n    \n    # Collect numeric values for each key\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                values_dict[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = {}\n    for key, values in values_dict.items():\n        if values:\n            results[key] = {\n                'mean': np.mean(values),\n                'median': np.median(values)\n            }\n    \n    # Convert results to a Pandas DataFrame\n    df = pd.DataFrame.from_dict(results, orient='index')\n    \n    # Sort the DataFrame by index (variable names)\n    df = df.sort_index()\n    \n    return df",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 77, in test_case_3\nAssertionError: Lists differ: ['a'] != ['a', 'b']\n\nSecond list contains 1 additional elements.\nFirst extra element 1:\n'b'\n\n- ['a']\n+ ['a', 'b']\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_case_5\nAssertionError: Lists differ: ['a', 'c'] != ['a', 'b', 'c']\n\nFirst differing element 1:\n'c'\n'b'\n\nSecond list contains 1 additional elements.\nFirst extra element 2:\n'c'\n\n- ['a', 'c']\n+ ['a', 'b', 'c']\n?       +++++\n\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 128, in test_case_8\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1096, in __getitem__\n    return self.obj._get_value(*key, takeable=self._takeable)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3870, in _get_value\n    series = self._get_item_cache(col)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 4255, in _get_item_cache\n    loc = self.columns.get_loc(item)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/range.py\", line 349, in get_loc\n    raise KeyError(key)\nKeyError: 'mean'\n"
        }
      }
    ],
    "BigCodeBench/461": [
      {
        "task_id": "BigCodeBench/461",
        "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n\n    pass\nimport subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path '{script_path}' does not exist.\")\n\n    accumulated_cpu_usage = 0.0\n    accumulated_memory_usage = 0\n\n    try:\n        process = subprocess.Popen(['bash', script_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        start_time = time.time()\n\n        while process.poll() is None:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                process.wait()\n                raise TimeoutError(f\"Script execution exceeded the {timeout} second timeout.\")\n\n            try:\n                proc = psutil.Process(process.pid)\n                cpu_percent = proc.cpu_percent(interval=1)\n                memory_info = proc.memory_info()\n\n                accumulated_cpu_usage += cpu_percent\n                accumulated_memory_usage += memory_info.rss\n\n            except (psutil.NoSuchProcess, psutil.ZombieProcess):\n                break\n\n    except Exception as e:\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n        raise e\n\n    finally:\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n\n    return {\n        'CPU Usage': accumulated_cpu_usage,\n        'Memory Usage': accumulated_memory_usage\n    }",
        "status": "fail",
        "details": {
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 103, in test_case_7\n  File \"__test__.py\", line 44, in task_func\n  File \"__test__.py\", line 27, in task_func\nTimeoutError: Script execution exceeded the 0 second timeout.\n"
        }
      }
    ],
    "BigCodeBench/528": [
      {
        "task_id": "BigCodeBench/528",
        "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n\n    pass\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension.\")\n\n    # Read the CSV file and identify duplicate rows\n    with open(file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        header = next(reader)  # Read the header\n        rows = [tuple(row) for row in reader]  # Convert rows to tuples for hashability\n\n    # Count the occurrences of each row\n    row_counts = Counter(rows)\n\n    # Filter out duplicates (rows with count > 1)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to a pandas DataFrame\n    df_data = []\n    for row, count in duplicates.items():\n        df_data.extend([row] * count)  # Add each duplicate row 'count' times\n    df = pd.DataFrame(df_data, columns=header)\n\n    # Plot the duplicate rows using matplotlib\n    fig, ax = plt.subplots(figsize=(12, 6))\n    bars = ax.bar(range(len(duplicates)), list(duplicates.values()))\n    ax.set_xticks(range(len(duplicates)))\n    ax.set_xticklabels([str(row) for row in duplicates.keys()], rotation=90, ha='right')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_title('Count of Duplicate Rows')\n\n    # Add value labels on top of each bar\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height}', ha='center', va='bottom')\n\n    plt.tight_layout()\n\n    return duplicates, ax",
        "status": "fail",
        "details": {
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 87, in test_case_2\nAssertionError: 'Count of Duplicate Rows' != 'Duplicate Entries'\n- Count of Duplicate Rows\n+ Duplicate Entries\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_case_3\nAssertionError: <Axes: title={'center': 'Count of Duplicate Rows'}, xlabel='Duplicate Rows', ylabel='Count'> is not None\n"
        }
      }
    ],
    "BigCodeBench/530": [
      {
        "task_id": "BigCodeBench/530",
        "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if required columns exist\n    if 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns.\")\n    \n    # Round down ages to nearest integer\n    df['age'] = np.floor(df['age']).astype(int)\n    \n    # Check for negative ages\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative.\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    # If no duplicates, return empty Counter and None\n    if duplicates.empty:\n        return Counter(), None\n    \n    # Calculate age distribution for duplicates\n    age_distribution = Counter(duplicates['age'])\n    \n    # Create histogram plot\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5)\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.histplot(data=duplicates, x='age', bins=bins, ax=ax)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution for Duplicate Names')\n    \n    return age_distribution, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/532": [
      {
        "task_id": "BigCodeBench/532",
        "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n\n    pass\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = df['value'].value_counts()\n    duplicates = value_counts[value_counts > 1]\n    duplicate_counter = Counter(duplicates.to_dict())\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    values = df['value']\n    ax.hist(values, bins=bins, color='green', alpha=0.6, density=True)\n\n    # Fit and overlay normal distribution curve\n    mu, std = norm.fit(values)\n    x = np.linspace(values.min(), values.max(), 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Customize plot\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return duplicate_counter, ax",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/553": [
      {
        "task_id": "BigCodeBench/553",
        "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n\n    pass\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    # Determine the number of columns based on the length of list 'b'\n    num_columns = len(b)\n    \n    # Generate random values for the DataFrame\n    data = np.random.rand(len(a), num_columns)\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_columns])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    \n    # Customize the plot\n    plt.title('Bar Chart of Random Data')\n    plt.xlabel('Row Indices')\n    plt.ylabel('Values')\n    plt.legend(title='Columns', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\na = ['Row1', 'Row2', 'Row3', 'Row4']\nb = [1, 2, 3]",
        "status": "fail",
        "details": {
          "test_both_lists_empty": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_both_lists_empty\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py\", line 975, in __call__\n    return plot_backend.plot(data, kind=kind, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 71, in plot\n    plot_obj.generate()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 446, in generate\n    self._compute_plot_data()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 632, in _compute_plot_data\n    raise TypeError(\"no numeric data to plot\")\nTypeError: no numeric data to plot\n",
          "test_empty_a_list": "Traceback (most recent call last):\n  File \"__test__.py\", line 48, in test_empty_a_list\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py\", line 975, in __call__\n    return plot_backend.plot(data, kind=kind, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 71, in plot\n    plot_obj.generate()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 455, in generate\n    self._post_plot_logic(ax, self.data)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 1750, in _post_plot_logic\n    s_edge = self.ax_pos[0] - 0.25 + self.lim_offset\nIndexError: index 0 is out of bounds for axis 0 with size 0\n",
          "test_empty_b_list": "Traceback (most recent call last):\n  File \"__test__.py\", line 52, in test_empty_b_list\n  File \"__test__.py\", line 24, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py\", line 975, in __call__\n    return plot_backend.plot(data, kind=kind, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 71, in plot\n    plot_obj.generate()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 446, in generate\n    self._compute_plot_data()\n  File \"/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py\", line 632, in _compute_plot_data\n    raise TypeError(\"no numeric data to plot\")\nTypeError: no numeric data to plot\n"
        }
      }
    ],
    "BigCodeBench/560": [
      {
        "task_id": "BigCodeBench/560",
        "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n\n    pass\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): DataFrame with columns 'month' and 'value'.\n\n    Returns:\n    matplotlib.axes.Axes: Axes object representing the plot.\n    \"\"\"\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if not {'month', 'value'}.issubset(data.columns):\n        raise ValueError(\"DataFrame must contain 'month' and 'value' columns\")\n\n    # Ensure 'month' is datetime type and sort the data\n    data['month'] = pd.to_datetime(data['month'])\n    data = data.sort_values('month')\n\n    # Extract the year from the first month entry\n    year = data['month'].iloc[0].year\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(data['month'], data['value'], width=20, align='center')\n\n    # Set title and labels\n    ax.set_title(f'Monthly Data for {year}', fontsize=16)\n    ax.set_xlabel('Month', fontsize=12)\n    ax.set_ylabel('Value', fontsize=12)\n\n    # Format x-axis\n    ax.xaxis.set_major_locator(plt.MonthLocator())\n    ax.xaxis.set_major_formatter(plt.DateFormatter('%b'))\n\n    # Rotate and align the tick labels so they look better\n    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n\n    # Use a tight layout to prevent label clipping\n    plt.tight_layout()\n\n    return ax",
        "status": "fail",
        "details": {
          "test_basic_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 57, in test_basic_functionality\n  File \"__test__.py\", line 22, in task_func\nTypeError: Input must be a pandas DataFrame\n",
          "test_empty_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 77, in test_empty_data\n  File \"__test__.py\", line 22, in task_func\nTypeError: Input must be a pandas DataFrame\n",
          "test_full_year_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 64, in test_full_year_data\n  File \"__test__.py\", line 22, in task_func\nTypeError: Input must be a pandas DataFrame\n",
          "test_incorrect_data_format": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_incorrect_data_format\n  File \"__test__.py\", line 22, in task_func\nTypeError: Input must be a pandas DataFrame\n",
          "test_partial_year_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_partial_year_data\n  File \"__test__.py\", line 22, in task_func\nTypeError: Input must be a pandas DataFrame\n"
        }
      }
    ],
    "BigCodeBench/567": [
      {
        "task_id": "BigCodeBench/567",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws a histogram to visualize the frequency distribution of numeric values \n    provided in a string format.\n\n    Parameters:\n    data (str): A string of numeric values separated by spaces.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): The Axes object of the created histogram.\n    \"\"\"\n    # Convert the input string to a list of floats\n    numeric_values = [float(value) for value in data.split()]\n    \n    # Create a Pandas Series from the numeric values\n    data_series = pd.Series(numeric_values)\n    \n    # Calculate the bins\n    bins = np.arange(data_series.min(), data_series.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_series, bins=bins, edgecolor='black')\n    \n    # Set title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 62, in test_case_3\nAssertionError: Lists differ: [6.4, 6.6000000000000005, 6.800000000000001, 7.0, 7.2, 7.4, 7.6000000000000005] != [7]\n\nFirst differing element 0:\n6.4\n7\n\nFirst list contains 6 additional elements.\nFirst extra element 1:\n6.6000000000000005\n\n- [6.4, 6.6000000000000005, 6.800000000000001, 7.0, 7.2, 7.4, 7.6000000000000005]\n+ [7]\n",
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 44, in test_case_1\n  File \"__test__.py\", line 22, in task_func\n  File \"__test__.py\", line 22, in <listcomp>\nValueError: could not convert string to float: '1-2-3-4-5'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 51, in test_case_2\n  File \"__test__.py\", line 22, in task_func\n  File \"__test__.py\", line 22, in <listcomp>\nValueError: could not convert string to float: '5-5-5-5-5'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 65, in test_case_4\n  File \"__test__.py\", line 22, in task_func\n  File \"__test__.py\", line 22, in <listcomp>\nValueError: could not convert string to float: '2-8-4-10-1'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_case_5\n  File \"__test__.py\", line 22, in task_func\n  File \"__test__.py\", line 22, in <listcomp>\nValueError: could not convert string to float: '1-50-100-150'\n"
        }
      }
    ],
    "BigCodeBench/579": [
      {
        "task_id": "BigCodeBench/579",
        "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n\n    pass\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        # Read the CSV file\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = ' '.join([' '.join(row) for row in reader])\n\n        # Normalize text to ASCII\n        normalized_text = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('ascii')\n\n        # Tokenize and count words\n        words = normalized_text.lower().split()\n        word_counts = Counter(words)\n\n        # Get the 10 most common words\n        common_words = word_counts.most_common(10)\n\n        # Create bar plot\n        words, counts = zip(*common_words)\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.bar(words, counts)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('10 Most Common Words')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n\n        return ax, common_words\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The CSV file '{csv_file}' cannot be found at the specified path.\")\n    except IOError as e:\n        raise IOError(f\"An error occurred while reading the file: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_normalized_text": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_normalized_text\n  File \"__test__.py\", line 37, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 2322, in tight_layout\n    return gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/figure.py\", line 3494, in tight_layout\n    engine.execute(self)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/layout_engine.py\", line 175, in execute\n    kwargs = get_tight_layout_figure(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 266, in get_tight_layout_figure\n    kwargs = _auto_adjust_subplotpars(fig, renderer,\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 82, in _auto_adjust_subplotpars\n    bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 4387, in get_tightbbox\n    ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1320, in get_tightbbox\n    self._update_label_position(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2308, in _update_label_position\n    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2104, in _get_tick_boxes_siblings\n    tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in _get_ticklabel_bboxes\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in <listcomp>\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 959, in get_window_extent\n    bbox, info, descent = self._get_layout(self._renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 378, in _get_layout\n    _, lp_h, lp_d = _get_text_metrics_with_cache(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 97, in _get_text_metrics_with_cache\n    return _get_text_metrics_with_cache_impl(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 105, in _get_text_metrics_with_cache_impl\n    return renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 233, in get_text_width_height_descent\n    font = self._prepare_font(prop)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 267, in _prepare_font\n    font = get_font(_fontManager._find_fonts_by_props(font_prop))\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 297, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1520, in get_font\n    return _get_font(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1461, in _get_font\n    return ft2font.FT2Font(\nRuntimeError: In FT2Font: Can not load face (invalid stream operation; error code 0x55)\n",
          "test_plot_output": "Traceback (most recent call last):\n  File \"__test__.py\", line 79, in test_plot_output\n  File \"__test__.py\", line 37, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 2322, in tight_layout\n    return gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/figure.py\", line 3494, in tight_layout\n    engine.execute(self)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/layout_engine.py\", line 175, in execute\n    kwargs = get_tight_layout_figure(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 266, in get_tight_layout_figure\n    kwargs = _auto_adjust_subplotpars(fig, renderer,\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 82, in _auto_adjust_subplotpars\n    bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 4387, in get_tightbbox\n    ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1320, in get_tightbbox\n    self._update_label_position(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2308, in _update_label_position\n    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2104, in _get_tick_boxes_siblings\n    tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in _get_ticklabel_bboxes\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in <listcomp>\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 959, in get_window_extent\n    bbox, info, descent = self._get_layout(self._renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 378, in _get_layout\n    _, lp_h, lp_d = _get_text_metrics_with_cache(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 97, in _get_text_metrics_with_cache\n    return _get_text_metrics_with_cache_impl(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 105, in _get_text_metrics_with_cache_impl\n    return renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 233, in get_text_width_height_descent\n    font = self._prepare_font(prop)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 267, in _prepare_font\n    font = get_font(_fontManager._find_fonts_by_props(font_prop))\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 297, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1520, in get_font\n    return _get_font(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1461, in _get_font\n    return ft2font.FT2Font(\nRuntimeError: In FT2Font: Can not load face (invalid stream operation; error code 0x55)\n",
          "test_valid_csv_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_valid_csv_file\n  File \"__test__.py\", line 37, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 2322, in tight_layout\n    return gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/figure.py\", line 3494, in tight_layout\n    engine.execute(self)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/layout_engine.py\", line 175, in execute\n    kwargs = get_tight_layout_figure(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 266, in get_tight_layout_figure\n    kwargs = _auto_adjust_subplotpars(fig, renderer,\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_tight_layout.py\", line 82, in _auto_adjust_subplotpars\n    bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 4387, in get_tightbbox\n    ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/artist.py\", line 1415, in _get_tightbbox_for_layout_only\n    return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1320, in get_tightbbox\n    self._update_label_position(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2308, in _update_label_position\n    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 2104, in _get_tick_boxes_siblings\n    tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in _get_ticklabel_bboxes\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/axis.py\", line 1299, in <listcomp>\n    return ([tick.label1.get_window_extent(renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 959, in get_window_extent\n    bbox, info, descent = self._get_layout(self._renderer)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 378, in _get_layout\n    _, lp_h, lp_d = _get_text_metrics_with_cache(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 97, in _get_text_metrics_with_cache\n    return _get_text_metrics_with_cache_impl(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/text.py\", line 105, in _get_text_metrics_with_cache_impl\n    return renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 233, in get_text_width_height_descent\n    font = self._prepare_font(prop)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 267, in _prepare_font\n    font = get_font(_fontManager._find_fonts_by_props(font_prop))\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 297, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1520, in get_font\n    return _get_font(\n  File \"/usr/local/lib/python3.10/site-packages/matplotlib/font_manager.py\", line 1461, in _get_font\n    return ft2font.FT2Font(\nRuntimeError: In FT2Font: Can not load face (invalid stream operation; error code 0x55)\n"
        }
      }
    ],
    "BigCodeBench/574": [
      {
        "task_id": "BigCodeBench/574",
        "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n\n    pass\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2*np.pi, array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Generate the original sine wave\n    true_params = [1.0, 1.0, 0.0, 0.0]  # [amplitude, frequency, phase, offset]\n    y_true = sine_func(x, *true_params)\n    \n    # Add noise to the sine wave\n    noise = noise_level * np.random.normal(size=array_length)\n    y_noisy = y_true + noise\n    \n    # Fit the curve\n    initial_guess = [1.0, 1.0, 0.0, 0.0]\n    fitted_params, _ = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n    \n    # Generate the fitted curve\n    y_fitted = sine_func(x, *fitted_params)\n    \n    # Plot the noisy sine wave and the fitted curve\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(x, y_noisy, 'b.', label='Noisy Data', alpha=0.5)\n    ax.plot(x, y_fitted, 'r-', label='Fitted Curve', linewidth=2)\n    ax.plot(x, y_true, 'g--', label='True Sine Wave', linewidth=1)\n    ax.legend()\n    ax.set_title(f\"Noisy Sine Wave and Fitted Curve (Noise Level: {noise_level})\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in test_case_1\nAssertionError: 3 != 2\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 70, in test_case_5\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/582": [
      {
        "task_id": "BigCodeBench/582",
        "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n\n    pass\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot histogram\n    ax.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n    \n    # Calculate mean and standard deviation\n    mu, std = np.mean(data), np.std(data)\n    \n    # Generate x values for PDF\n    x = np.linspace(mu - 4*std, mu + 4*std, 100)\n    \n    # Calculate PDF\n    pdf = stats.norm.pdf(x, mu, std)\n    \n    # Plot PDF\n    ax.plot(x, pdf, 'r-', lw=2, label='Normal PDF')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    \n    # Add legend\n    ax.legend()\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return fig",
        "status": "fail",
        "details": {
          "test_histogram_pdf_overlay": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_histogram_pdf_overlay\nAssertionError: 30 != 28 : Number of histogram bins does not match expected\n"
        }
      }
    ],
    "BigCodeBench/590": [
      {
        "task_id": "BigCodeBench/590",
        "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n\n    pass\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nfrom urllib.error import URLError\ndef task_func(url):\n    # Validate URL\n    if not url or not isinstance(url, str):\n        raise ValueError(\"The provided URL is invalid or empty.\")\n\n    try:\n        # Fetch HTML content\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n\n        # Parse HTML\n        doc = pq(html_content)\n\n        # Extract anchor tags\n        anchors = doc('a')\n\n        # Prepare data\n        data = []\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n        for anchor in anchors.items():\n            data.append({\n                'text': anchor.text(),\n                'href': anchor.attr('href'),\n                'fetch_time': fetch_time\n            })\n\n        # Create DataFrame\n        return pd.DataFrame(data)\n\n    except URLError as e:\n        raise URLError(f\"There is an issue with network connectivity or the server: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_dynamic_comparison": "Traceback (most recent call last):\n  File \"__test__.py\", line 87, in test_dynamic_comparison\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1224, in assert_frame_equal\n    assert_series_equal(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 1014, in assert_series_equal\n    _testing.assert_almost_equal(\n  File \"pandas/_libs/testing.pyx\", line 52, in pandas._libs.testing.assert_almost_equal\n  File \"pandas/_libs/testing.pyx\", line 172, in pandas._libs.testing.assert_almost_equal\n  File \"/usr/local/lib/python3.10/site-packages/pandas/_testing/asserters.py\", line 599, in raise_assert_detail\n    raise AssertionError(msg)\nAssertionError: DataFrame.iloc[:, 0] (column name=\"text\") are different\n\nDataFrame.iloc[:, 0] (column name=\"text\") values are different (64.79401 %)\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n[left]:  [, Jump to navigation, Jump to search, Featured books, Wikijunior, Cookbook, Browse books, Help, Forum, Using Wikibooks, Welcome, Wikibooks, anyone can edit, 3,375 books, 97,985 pages, Computing, Engineering, Humanities, Languages, Mathematics, Miscellaneous, Recreational activities, Science, Social sciences, Standard curricula, Wikijunior, All subjects, More featured books..., , Guitar, Image credit, More featured children's books..., , Solar System, Image credit, More featured recipes..., , Glazed Lemon Poppy Seed Muffins, Image credit, More promoted shelves..., , Shelf:Games, athletic games, board games, card games, electronic games, game design, Image credit, More promoted shelves..., , Shelf:Computer programming, LaTeX, OpenSCAD User Manual, Python Programming, Java Programming, Image credit, More promoted shelves..., , Shelf:Languages of Asia, Marathi, Bengali, Kannada, Hindi, Sanskrit, Image credit, Wikimedia Foundation, , Wikiversity, , Wikipedia, , Wiktionary, , Wikiquote, , Wikisource, , Wikispecies, , Wikinews, , Wikivoyage, , Wikidata, , Commons, , Meta, , MediaWiki, , Phabricator, , Wikimedia Labs, Wikimedia Foundation, voluntarism, donations, other projects, server equipment, projects, ...]\n[right]: [None, Jump to navigation, Jump to search, Featured books, Wikijunior, Cookbook, Browse books, Help, Forum, Using Wikibooks, Welcome, Wikibooks, anyone can edit, 3,375 books, 97,985 pages, Computing, Engineering, Humanities, Languages, Mathematics, Miscellaneous, Recreational activities, Science, Social sciences, Standard curricula, Wikijunior, All subjects, More featured books..., None, Guitar, Image credit, More featured children's books..., None, Solar System, Image credit, More featured recipes..., None, Glazed Lemon Poppy Seed Muffins, Image credit, More promoted shelves..., None, Shelf:Games, athletic games, board games, card games, electronic games, game design, Image credit, More promoted shelves..., None, Shelf:Computer programming, LaTeX, OpenSCAD User Manual, Python Programming, Java Programming, Image credit, More promoted shelves..., None, Shelf:Languages of Asia, Marathi, Bengali, Kannada, Hindi, Sanskrit, Image credit, Wikimedia Foundation, None, Wikiversity, None, Wikipedia, None, Wiktionary, None, Wikiquote, None, Wikisource, None, Wikispecies, None, Wikinews, None, Wikivoyage, None, Wikidata, None, Commons, None, Meta, None, MediaWiki, None, Phabricator, None, Wikimedia Labs, Wikimedia Foundation, voluntarism, donations, other projects, server equipment, projects, ...]\nAt positional index 0, first diff:  != None\n"
        }
      }
    ],
    "BigCodeBench/592": [
      {
        "task_id": "BigCodeBench/592",
        "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n\n    pass\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate filename with current timestamp\n    current_time = datetime.now()\n    filename = f\"sensor_data_{current_time.strftime('%Y%m%d_%H%M%S')}.csv\"\n    filepath = os.path.join(output_dir, filename)\n\n    # Generate data\n    data = []\n    for minute in range(hours * 60):\n        timestamp = current_time + timedelta(minutes=minute)\n        temperature = randint(-10, 40)  # Temperature range: -10\u00b0C to 40\u00b0C\n        humidity = randint(0, 100)      # Humidity range: 0% to 100%\n        pressure = randint(950, 1050)   # Pressure range: 950 hPa to 1050 hPa\n        data.append([timestamp.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n\n    # Write data to CSV file\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writerows(data)\n\n    print(f\"Sensor data for {hours} hours has been saved to {filepath}\")",
        "status": "fail",
        "details": {
          "test_csv_file_creation": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_csv_file_creation\nAssertionError: False is not true\n",
          "test_file_path_return": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_file_path_return\nAssertionError: None != './output/sensor_data.csv'\n",
          "test_csv_file_header": "Traceback (most recent call last):\n  File \"__test__.py\", line 65, in test_csv_file_header\nFileNotFoundError: [Errno 2] No such file or directory: './output/sensor_data.csv'\n",
          "test_csv_file_rows": "Traceback (most recent call last):\n  File \"__test__.py\", line 60, in test_csv_file_rows\nFileNotFoundError: [Errno 2] No such file or directory: './output/sensor_data.csv'\n",
          "test_no_hours_data": "Traceback (most recent call last):\n  File \"__test__.py\", line 76, in test_no_hours_data\nFileNotFoundError: [Errno 2] No such file or directory: './output/sensor_data.csv'\n"
        }
      }
    ],
    "BigCodeBench/99": [
      {
        "task_id": "BigCodeBench/99",
        "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n\n    pass\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate a pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species', height=2.5)\n\n    # Set the plot title\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16, y=1.02)\n\n    # Customize axis labels\n    for i, feature in enumerate(iris.feature_names):\n        for ax in pair_plot.axes[i]:\n            ax.set_xlabel(feature)\n            ax.set_ylabel(feature)\n\n    # Adjust layout to prevent clipping of title\n    plt.tight_layout()\n\n    return pair_plot.fig",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/594": [
      {
        "task_id": "BigCodeBench/594",
        "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n\n    pass\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(BACKUP_DIR, exist_ok=True)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = start_time + timedelta(hours=i)\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append((time.strftime('%Y-%m-%d %H:%M:%S'), condition))\n\n    # Create CSV file with timestamp in filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_filename = f'weather_data_{timestamp}.csv'\n    csv_file_path = os.path.join(output_dir, csv_filename)\n\n    # Write data to CSV file\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file\n    backup_file_path = os.path.join(BACKUP_DIR, csv_filename)\n    shutil.copy(csv_file_path, backup_file_path)\n\n    return csv_file_path",
        "status": "fail",
        "details": {
          "test_backup_file_creation": "Traceback (most recent call last):\n  File \"__test__.py\", line 109, in test_backup_file_creation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 941, in assert_called_once_with\n    return self.assert_called_with(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: copy('./output/weather_data.csv', './output/backup/')\nActual: copy('./output/weather_data_20240818_194010.csv', './backup/weather_data_20240818_194010.csv')\n",
          "test_task_func_checks_backup_directory_exists": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 79, in test_task_func_checks_backup_directory_exists\nAssertionError: 'output/backup' != '.'\n- output/backup\n+ .\n : Expected output/backup, got .\n",
          "test_task_func_copies_to_backup_directory": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 89, in test_task_func_copies_to_backup_directory\nAssertionError: 'output/backup' != 'backup'\n- output/backup\n+ backup\n : The backup directory path does not match the expected directory path.\n"
        }
      }
    ],
    "BigCodeBench/593": [
      {
        "task_id": "BigCodeBench/593",
        "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n\n    pass\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate traffic data\n    data = []\n    start_time = datetime.now()\n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour)\n        row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n        row.extend([randint(0, 100) for _ in VEHICLE_TYPES])\n        data.append(row)\n\n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n\n    # Read CSV and plot data\n    df = pd.read_csv(csv_file_path)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return csv_file_path, ax",
        "status": "fail",
        "details": {
          "test_dataframe_content": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 86, in test_dataframe_content\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 898, in assert_called\n    raise AssertionError(msg)\nAssertionError: Expected 'show' to have been called.\n",
          "test_directory_creation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 99, in test_directory_creation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 929, in assert_called_with\n    raise AssertionError(_error_message()) from cause\nAssertionError: expected call not found.\nExpected: makedirs('./output')\nActual: makedirs('./output', exist_ok=True)\n",
          "test_empty_dataframe_on_zero_hours": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 91, in test_empty_dataframe_on_zero_hours\nAssertionError: <Axes: title={'center': 'Traffic Data Over Time'}, xlabel='Time', ylabel='Vehicle Count'> is not None\n",
          "test_plot_generation": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 104, in test_plot_generation\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 898, in assert_called\n    raise AssertionError(msg)\nAssertionError: Expected 'show' to have been called.\n"
        }
      }
    ],
    "BigCodeBench/618": [
      {
        "task_id": "BigCodeBench/618",
        "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\ndef task_func(goals, penalties):\n\n    pass\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Ensure the input lists have the same length as TEAMS\n    assert len(goals) == len(penalties) == len(TEAMS), \"Input lists must have the same length as TEAMS\"\n\n    # Create the DataFrame\n    df = pd.DataFrame({\n        'Team': TEAMS,\n        'Goals': goals,\n        'Penalties': penalties,\n        'Penalty Cost': [p * PENALTY_COST for p in penalties]\n    })\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Plot goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=ax1)\n    ax1.set_title('Goals Scored by Each Team')\n    ax1.set_ylabel('Number of Goals')\n\n    # Plot penalty costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=ax2)\n    ax2.set_title('Penalty Costs for Each Team')\n    ax2.set_ylabel('Penalty Cost ($)')\n\n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n    return df, [ax1, ax2]\ngoals = [randint(0, 5) for _ in TEAMS]\npenalties = [randint(0, 3) for _ in TEAMS]",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_case_1\n  File \"__test__.py\", line 19, in task_func\nTypeError: object of type 'int' has no len()\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 71, in test_case_2\n  File \"__test__.py\", line 19, in task_func\nTypeError: object of type 'int' has no len()\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 80, in test_case_3\n  File \"__test__.py\", line 19, in task_func\nTypeError: object of type 'int' has no len()\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 90, in test_case_4\n  File \"__test__.py\", line 19, in task_func\nTypeError: object of type 'int' has no len()\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_case_5\n  File \"__test__.py\", line 19, in task_func\nTypeError: object of type 'int' has no len()\n"
        }
      }
    ],
    "BigCodeBench/636": [
      {
        "task_id": "BigCodeBench/636",
        "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n\n    pass\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Generate random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create the bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    non_zero_counts.plot(kind='bar', ax=ax, color='skyblue')\n    \n    # Customize the plot\n    ax.set_title('Non-Zero Value Counts per Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-Zero Counts')\n    ax.set_ylim(0, rows)  # Set y-axis limit to the number of rows\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(non_zero_counts):\n        ax.text(i, v, str(v), ha='center', va='bottom')\n    \n    plt.tight_layout()\n    \n    return df, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in test_case_1\nAssertionError: 5 != 0\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 60, in test_case_4\n  File \"__test__.py\", line 15, in task_func\n  File \"mtrand.pyx\", line 748, in numpy.random.mtrand.RandomState.randint\n  File \"_bounded_integers.pyx\", line 1256, in numpy.random._bounded_integers._rand_int64\nValueError: negative dimensions are not allowed\n"
        }
      }
    ],
    "BigCodeBench/637": [
      {
        "task_id": "BigCodeBench/637",
        "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n\n    pass\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate random grades for each student in each course\n    np.random.seed(42)  # For reproducibility\n    data = {course: np.random.randint(0, 101, num_students) for course in courses}\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate average grade for each course\n    avg_grades = df.mean()\n    \n    # Calculate number of students with passing grade (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot data\n    x = np.arange(len(courses))\n    width = 0.35\n    \n    ax.bar(x - width/2, avg_grades, width, label='Average Grade', color='skyblue')\n    ax.bar(x + width/2, passing_counts, width, label='Passing Students', color='lightgreen')\n    \n    # Customize plot\n    ax.set_ylabel('Values')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xticks(x)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(avg_grades):\n        ax.text(i - width/2, v, f'{v:.1f}', ha='center', va='bottom')\n    for i, v in enumerate(passing_counts):\n        ax.text(i + width/2, v, str(v), ha='center', va='bottom')\n    \n    plt.tight_layout()\n    \n    return df, ax\nnum_students = 30",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/654": [
      {
        "task_id": "BigCodeBench/654",
        "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n\n    pass\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Find indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n    \n    if len(indices) == 0:\n        raise ValueError(f\"No matching values found for target_value {target_value}\")\n    \n    # Extract corresponding values from the second column\n    values = array[indices, 1]\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Perform curve fitting\n    initial_guess = [max(values), 0.1, min(values)]\n    popt, _ = optimize.curve_fit(exp_decay, indices, values, p0=initial_guess)\n    \n    # Create plot\n    fig, ax = plt.subplots()\n    ax.scatter(indices, values, label='Data')\n    \n    # Generate smooth curve for plotting\n    x_smooth = np.linspace(min(indices), max(indices), 100)\n    y_smooth = exp_decay(x_smooth, *popt)\n    ax.plot(x_smooth, y_smooth, 'r-', label='Fitted Curve')\n    \n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Exponential Decay Fit (Target Value: {target_value})')\n    ax.legend()\n    \n    return popt, ax",
        "status": "fail",
        "details": {
          "test_functionality": "Traceback (most recent call last):\n  File \"__test__.py\", line 74, in test_functionality\n  File \"__test__.py\", line 26, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 789, in curve_fit\n    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 410, in leastsq\n    shape, dtype = _check_func('leastsq', 'func', func, x0, args, n)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 24, in _check_func\n    res = atleast_1d(thefunc(*((x0[:numinputs],) + args)))\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 485, in func_wrapped\n    return func(xdata, *params) - ydata\n  File \"__test__.py\", line 22, in exp_decay\nTypeError: bad operand type for unary -: 'numpy.str_'\n",
          "test_not_enough_points": "Traceback (most recent call last):\n  File \"__test__.py\", line 71, in test_not_enough_points\n  File \"__test__.py\", line 18, in task_func\nIndexError: index 1 is out of bounds for axis 1 with size 1\n",
          "test_return_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_return_types\n  File \"__test__.py\", line 26, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 789, in curve_fit\n    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 410, in leastsq\n    shape, dtype = _check_func('leastsq', 'func', func, x0, args, n)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 24, in _check_func\n    res = atleast_1d(thefunc(*((x0[:numinputs],) + args)))\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 485, in func_wrapped\n    return func(xdata, *params) - ydata\n  File \"__test__.py\", line 22, in exp_decay\nTypeError: bad operand type for unary -: 'numpy.str_'\n",
          "test_target_value_found": "Traceback (most recent call last):\n  File \"__test__.py\", line 61, in test_target_value_found\n  File \"__test__.py\", line 26, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 789, in curve_fit\n    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 410, in leastsq\n    shape, dtype = _check_func('leastsq', 'func', func, x0, args, n)\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 24, in _check_func\n    res = atleast_1d(thefunc(*((x0[:numinputs],) + args)))\n  File \"/usr/local/lib/python3.10/site-packages/scipy/optimize/minpack.py\", line 485, in func_wrapped\n    return func(xdata, *params) - ydata\n  File \"__test__.py\", line 22, in exp_decay\nTypeError: bad operand type for unary -: 'numpy.str_'\n"
        }
      }
    ],
    "BigCodeBench/655": [
      {
        "task_id": "BigCodeBench/655",
        "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n\n    pass\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    def preprocess_text(text):\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n\n    # Preprocess texts\n    processed_texts = [preprocess_text(text) for text in texts]\n\n    # Vectorize using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=1000)\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42, init='nndsvda')\n    nmf.fit(tfidf_matrix)\n\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract topics\n    topics = []\n    for topic in nmf.components_:\n        top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n        top_words = [feature_names[i] for i in top_words_idx]\n        topics.append(top_words)\n\n    return topics",
        "status": "fail",
        "details": {
          "test_all_stopwords": "Traceback (most recent call last):\n  File \"__test__.py\", line 79, in test_all_stopwords\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2139, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1389, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1295, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n",
          "test_empty_texts": "Traceback (most recent call last):\n  File \"__test__.py\", line 69, in test_empty_texts\n  File \"__test__.py\", line 33, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2139, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1389, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1295, in _count_vocab\n    raise ValueError(\nValueError: empty vocabulary; perhaps the documents only contain stop words\n"
        }
      }
    ],
    "BigCodeBench/657": [
      {
        "task_id": "BigCodeBench/657",
        "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n\n    pass\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Download NLTK stopwords if not already downloaded\n    nltk.download('stopwords', quiet=True)\n    \n    # Use NLTK's English stopwords if none are provided\n    if stopwords is None:\n        stopwords = set(stopwords.words('english'))\n    \n    def preprocess_text(text):\n        # Remove non-alphanumeric characters (except space)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase the text\n        text = text.lower()\n        # Split into words and remove stopwords\n        words = [word for word in text.split() if word not in stopwords]\n        return words\n    \n    # Preprocess all texts\n    processed_texts = [preprocess_text(text) for text in texts]\n    \n    # Train Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_case_3\n  File \"__test__.py\", line 35, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/gensim/models/word2vec.py\", line 430, in __init__\n    self.train(\n  File \"/usr/local/lib/python3.10/site-packages/gensim/models/word2vec.py\", line 1045, in train\n    self._check_training_sanity(epochs=epochs, total_examples=total_examples, total_words=total_words)\n  File \"/usr/local/lib/python3.10/site-packages/gensim/models/word2vec.py\", line 1554, in _check_training_sanity\n    raise RuntimeError(\"you must first build vocabulary before training the model\")\nRuntimeError: you must first build vocabulary before training the model\n"
        }
      }
    ],
    "BigCodeBench/678": [
      {
        "task_id": "BigCodeBench/678",
        "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n\n    pass\nimport pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Create the directory if it doesn't exist\n    os.makedirs(path, exist_ok=True)\n\n    # Create the processed subdirectory\n    processed_dir = os.path.join(path, 'processed')\n    os.makedirs(processed_dir, exist_ok=True)\n\n    # Get a list of JSON files in the directory, sorted alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n\n    # List to store individual DataFrames\n    dataframes = []\n\n    for json_file in json_files:\n        file_path = os.path.join(path, json_file)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # Convert JSON to DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add Source column\n        df['Source'] = json_file\n        \n        # Append to list of DataFrames\n        dataframes.append(df)\n        \n        # Move processed file to the processed subdirectory\n        shutil.move(file_path, os.path.join(processed_dir, json_file))\n\n    # Combine all DataFrames\n    if dataframes:\n        combined_df = pd.concat(dataframes, ignore_index=True)\n    else:\n        combined_df = pd.DataFrame()\n\n    return combined_df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/720": [
      {
        "task_id": "BigCodeBench/720",
        "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n\n    pass\nimport os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    # Generate random sensor data\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    temperature = round(random.uniform(-10.0, 40.0), 2)\n    humidity = round(random.uniform(0.0, 100.0), 2)\n\n    # Write data to CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        if file.tell() == 0:  # Write header if file is empty\n            writer.writerow([\"Timestamp\", \"Temperature(C)\", \"Humidity(%)\"])\n        writer.writerow([timestamp, temperature, humidity])\n\n    # Store the file path before deletion\n    return_path = file_path\n\n    # Delete the file after use\n    os.remove(file_path)\n\n    # Return the path to the CSV file before deletion\n    return return_path",
        "status": "fail",
        "details": {
          "test_file_existence": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_data_appending": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_file_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_headers_only_once": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n",
          "test_return_value": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in tearDown\nFileNotFoundError: [Errno 2] No such file or directory: 'task_func_data/Output.txt'\n"
        }
      }
    ],
    "BigCodeBench/723": [
      {
        "task_id": "BigCodeBench/723",
        "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n\n    pass\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a request to the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            web_content = response.read()\n    except urllib.error.URLError as e:\n        print(f\"Error accessing the URL: {e}\")\n        return None\n\n    # Parse the webpage content\n    soup = BeautifulSoup(web_content, 'html.parser')\n\n    # Extract data from the page\n    # This example extracts all text from paragraph tags\n    # Modify this section based on the structure of the target website\n    data = []\n    paragraphs = soup.find_all('p')\n    for p in paragraphs:\n        data.append([p.text.strip()])\n\n    # If no data was found, return None\n    if not data:\n        print(\"No data found on the page.\")\n        return None\n\n    # Write data to a CSV file\n    try:\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csv_file:\n            writer = csv.writer(csv_file)\n            writer.writerow(['Content'])  # Header\n            writer.writerows(data)\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return None\n\n    return CSV_FILE_PATH",
        "status": "fail",
        "details": {
          "test_correct_scraping": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 67, in test_correct_scraping\n  File \"__test__.py\", line 25, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n",
          "test_empty_table": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 80, in test_empty_table\n  File \"__test__.py\", line 25, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n",
          "test_overwrite_existing_csv": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 101, in test_overwrite_existing_csv\n  File \"__test__.py\", line 25, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n"
        }
      }
    ],
    "BigCodeBench/760": [
      {
        "task_id": "BigCodeBench/760",
        "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    # Set random seed for reproducibility\n    np.random.seed(rng_seed)\n\n    # Generate 100 random records\n    num_records = 100\n\n    # Generate IDs\n    ids = np.arange(1, num_records + 1)\n\n    # Combine and select names\n    all_names = latin_names + other_names\n    names = np.random.choice(all_names, num_records)\n\n    # Generate random dates of birth\n    start_date = datetime.date(start_year, 1, 1)\n    end_date = datetime.date(end_year, 12, 31)\n    days_range = (end_date - start_date).days\n    random_days = np.random.randint(0, days_range + 1, num_records)\n    dates_of_birth = [start_date + datetime.timedelta(days=int(day)) for day in random_days]\n\n    # Generate emails\n    def create_email(name, dob):\n        # Correct encoding for Latin characters\n        name = codecs.decode(name.encode('latin1'), 'utf-8', 'ignore')\n        # Remove any remaining non-ASCII characters\n        name = re.sub(r'[^\\x00-\\x7F]+', '', name)\n        # Create email\n        return f\"{name.lower()}{dob.year}@{email_domain}\"\n\n    emails = [create_email(name, dob) for name, dob in zip(names, dates_of_birth)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n\n    return df",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/752": [
      {
        "task_id": "BigCodeBench/752",
        "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n\n    pass\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If data is not a DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"If data is empty.\")\n    \n    # Check if target_column is in data\n    if target_column not in data.columns:\n        raise ValueError(\"If target_column ist not a column of data.\")\n    \n    # Check if all values in data are numeric\n    if not data.select_dtypes(include=[np.number]).columns.tolist() == data.columns.tolist():\n        raise ValueError(\"If data contains values that are not numeric.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"If random_state is not an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"If test_size is not between 0 and 1.\")\n    \n    # Split the data into features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    # Train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Calculate and return the model score on the test set\n    return model.score(X_test, y_test)",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/763": [
      {
        "task_id": "BigCodeBench/763",
        "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n# Constants\ndef task_func(input_file, output_file):\n\n    pass\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"Input JSON must be a list of dictionaries\")\n\n    # Collect all values for each key\n    values_by_key = defaultdict(list)\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                values_by_key[key].append(value)\n\n    # Calculate mean and median for each key\n    result = {}\n    for key, values in values_by_key.items():\n        result[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, stats in result.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n\n    return result",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 99, in test_case_1\n  File \"__test__.py\", line 93, in validate_data\nKeyError: 'key'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 102, in test_case_2\n  File \"__test__.py\", line 93, in validate_data\nKeyError: 'key'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 105, in test_case_3\n  File \"__test__.py\", line 93, in validate_data\nKeyError: 'key'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_case_4\n  File \"__test__.py\", line 93, in validate_data\nKeyError: 'key'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 111, in test_case_5\n  File \"__test__.py\", line 93, in validate_data\nKeyError: 'key'\n"
        }
      }
    ],
    "BigCodeBench/765": [
      {
        "task_id": "BigCodeBench/765",
        "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n\n    pass\nimport os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    for file_path in kwargs.values():\n        source_file = Path(file_path)\n        if source_file.is_file() and source_file.stat().st_size > 0:\n            try:\n                dest_file = target_path / source_file.name\n                shutil.copy2(source_file, dest_file)\n                copied_files.append(str(dest_file.resolve()))\n            except Exception as e:\n                print(f\"Error copying {source_file}: {e}\")\n\n    return copied_files",
        "status": "fail",
        "details": {
          "test_files_with_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in test_files_with_content\nAssertionError: 0 != 2\n",
          "test_files_with_no_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 58, in test_files_with_no_content\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/pathlib.py\", line 960, in __new__\n    self = cls._from_parts(args)\n  File \"/usr/local/lib/python3.10/pathlib.py\", line 594, in _from_parts\n    drv, root, parts = self._parse_args(args)\n  File \"/usr/local/lib/python3.10/pathlib.py\", line 578, in _parse_args\n    a = os.fspath(a)\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n",
          "test_mixed_case": "Traceback (most recent call last):\n  File \"__test__.py\", line 74, in test_mixed_case\n  File \"__test__.py\", line 16, in task_func\n  File \"/usr/local/lib/python3.10/pathlib.py\", line 960, in __new__\n    self = cls._from_parts(args)\n  File \"/usr/local/lib/python3.10/pathlib.py\", line 594, in _from_parts\n    drv, root, parts = self._parse_args(args)\n  File \"/usr/local/lib/python3.10/pathlib.py\", line 578, in _parse_args\n    a = os.fspath(a)\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n"
        }
      }
    ],
    "BigCodeBench/771": [
      {
        "task_id": "BigCodeBench/771",
        "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    pass\nimport re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Ensure the directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # List to hold the names of new files\n    new_files = []\n\n    # Iterate over all CSV files in the specified directory\n    for file_path in Path(directory).glob('*.csv'):\n        file_name = file_path.name\n\n        # Check if the file name matches the pattern\n        match = regex.match(file_name)\n        if match:\n            # Extract the base name from the regex match\n            base_name = match.group(1)\n\n            # Create the new file name\n            new_file_name = f\"{base_name}-processed.csv\"\n            new_file_path = file_path.with_name(new_file_name)\n\n            # Copy the content from the old file to the new file\n            try:\n                with open(file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n                     open(new_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n                    reader = csv.reader(infile)\n                    writer = csv.writer(outfile)\n\n                    # Write the content to the new file\n                    for row in reader:\n                        writer.writerow(row)\n\n                # Add the new file name to the list\n                new_files.append(new_file_name)\n            except IOError as e:\n                print(f\"Error processing file {file_name}: {e}\")\n                continue\n\n    return new_files",
        "status": "fail",
        "details": {
          "test_empty_directory": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in test_empty_directory\n  File \"__test__.py\", line 15, in task_func\nValueError: The directory /mnt/data/test_data does not exist.\n",
          "test_exception_handling": "Traceback (most recent call last):\n  File \"__test__.py\", line 105, in test_exception_handling\n  File \"__test__.py\", line 15, in task_func\nValueError: The directory /mnt/data/test_data does not exist.\n",
          "test_file_creation_and_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 71, in test_file_creation_and_content\n  File \"__test__.py\", line 15, in task_func\nValueError: The directory /mnt/data/test_data does not exist.\n",
          "test_mixed_file_types": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_mixed_file_types\n  File \"__test__.py\", line 15, in task_func\nValueError: The directory /mnt/data/test_data does not exist.\n",
          "test_non_matching_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_non_matching_files\n  File \"__test__.py\", line 15, in task_func\nValueError: The directory /mnt/data/test_data does not exist.\n"
        }
      }
    ],
    "BigCodeBench/777": [
      {
        "task_id": "BigCodeBench/777",
        "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n\n    pass\nimport re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzips all zip files in a directory whose name matches a certain pattern by splitting\n    the filename the last time \"-\" occurs and using the prefix part of the filename as\n    the directory to extract.\n\n    Args:\n    directory (str): The directory containing the zip files.\n    pattern (str): The regex pattern to match the zip file names.\n\n    Returns:\n    list: A list of directories where the files were extracted.\n    \"\"\"\n    extracted_dirs = []\n    regex = re.compile(pattern)\n\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            match = regex.match(filename)\n            if match:\n                full_path = os.path.join(root, filename)\n                prefix = match.group(1)\n                extract_dir = os.path.join(root, prefix)\n\n                os.makedirs(extract_dir, exist_ok=True)\n\n                with zipfile.ZipFile(full_path, 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n\n                if extract_dir not in extracted_dirs:\n                    extracted_dirs.append(extract_dir)\n\n    return extracted_dirs",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 69, in test_case_1\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 982, in assert_has_calls\n    raise AssertionError(\nAssertionError: 'ZipFile' does not contain all of (call('/fake/test_zip_dir/sample-123.zip', 'r'), call('/fake/test_zip_dir/test_data-456.zip', 'r'), call('/fake/test_zip_dir/data_test-789.zip', 'r')) in its call list, found [] instead\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 80, in test_case_2\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'makedirs' to be called once. Called 0 times.\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 90, in test_case_3\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'makedirs' to be called once. Called 0 times.\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 113, in test_case_5\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 940, in assert_called_once_with\n    raise AssertionError(msg)\nAssertionError: Expected 'ZipFile' to be called once. Called 0 times.\n"
        }
      }
    ],
    "BigCodeBench/785": [
      {
        "task_id": "BigCodeBench/785",
        "solution": "import subprocess\nimport os\nimport glob\n# Constants\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n\n    pass\nimport subprocess\nimport os\nimport glob\nimport tarfile\nfrom datetime import datetime\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archive all files matching the given pattern and delete the originals.\n\n    Args:\n        pattern (str): The file pattern to match.\n\n    Returns:\n        str: The archive file path.\n\n    Raises:\n        FileNotFoundError: If no files match the given pattern.\n    \"\"\"\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files that match the pattern\n    matching_files = glob.glob(pattern)\n\n    if not matching_files:\n        raise FileNotFoundError(f\"No files found matching pattern: {pattern}\")\n\n    # Create a timestamped archive file name\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    archive_file = os.path.join(ARCHIVE_DIR, f\"archive_{timestamp}.tar.gz\")\n\n    # Create the archive\n    with tarfile.open(archive_file, \"w:gz\") as tar:\n        for file in matching_files:\n            tar.add(file, arcname=os.path.basename(file))\n\n    # Delete the original files\n    for file in matching_files:\n        os.remove(file)\n\n    return archive_file",
        "status": "fail",
        "details": {
          "test_archiving_in_nonexistent_directory": "Traceback (most recent call last):\n  File \"__test__.py\", line 101, in test_archiving_in_nonexistent_directory\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: No files found matching pattern: /tmp/archive/*.txt\n",
          "test_multiple_archiving_unique_naming": "Traceback (most recent call last):\n  File \"__test__.py\", line 94, in test_multiple_archiving_unique_naming\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: No files found matching pattern: ./test_files/*.txt\n",
          "test_no_matching_files": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_no_matching_files\n  File \"__test__.py\", line 35, in task_func\nFileNotFoundError: No files found matching pattern: ./test_files/*.pdf\n"
        }
      }
    ],
    "BigCodeBench/800": [
      {
        "task_id": "BigCodeBench/800",
        "solution": "import csv\nimport os\nfrom collections import Counter\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n\n    pass\nimport csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    count = Counter()\n\n    if os.path.exists(csv_file_path):\n        with open(csv_file_path, 'r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                count['goals'] += int(row['goals'])\n                count['penalties'] += int(row['penalties'])\n\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    with open(csv_file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['New Entry', str(goals), str(penalties)])\n\n    return count",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 58, in test_case_1\n  File \"__test__.py\", line 36, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 68, in test_case_2\n  File \"__test__.py\", line 36, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_case_3\n  File \"__test__.py\", line 36, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_case_4\n  File \"__test__.py\", line 36, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 99, in test_case_5\n  File \"__test__.py\", line 36, in task_func\nTypeError: unsupported operand type(s) for +=: 'int' and 'dict'\n"
        }
      }
    ],
    "BigCodeBench/826": [
      {
        "task_id": "BigCodeBench/826",
        "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n\n    pass\nimport re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from the source directory to the target directory based on a specified pattern.\n\n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        file_pattern (str, optional): The regex pattern to match file names. \n                                      Defaults to r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'.\n\n    Returns:\n        int: The number of files that were successfully moved from the source directory \n             to the target directory.\n    \"\"\"\n    # Ensure the source directory exists\n    if not os.path.exists(source_dir):\n        raise ValueError(f\"Source directory '{source_dir}' does not exist.\")\n\n    # Ensure the target directory exists, create it if it doesn't\n    os.makedirs(target_dir, exist_ok=True)\n\n    moved_files_count = 0\n    pattern = re.compile(file_pattern)\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        if pattern.match(filename):\n            source_file = os.path.join(source_dir, filename)\n            target_file = os.path.join(target_dir, filename)\n\n            try:\n                # Move the file to the target directory\n                shutil.move(source_file, target_file)\n                moved_files_count += 1\n            except (IOError, OSError) as e:\n                print(f\"Error moving file {filename}: {str(e)}\")\n\n    return moved_files_count",
        "status": "fail",
        "details": {
          "test_directory_does_not_exist": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_directory_does_not_exist\n  File \"__test__.py\", line 26, in task_func\nValueError: Source directory '/tmp/tmp1l_4k6z5/source' does not exist.\n"
        }
      }
    ],
    "BigCodeBench/845": [
      {
        "task_id": "BigCodeBench/845",
        "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n\n    pass\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    def clean_text(text):\n        return ALPHANUMERIC.sub(' ', text).lower()\n\n    def cosine_similarity(text1, text2):\n        vec1 = Counter(text1.split())\n        vec2 = Counter(text2.split())\n        \n        words = set(vec1.keys()) | set(vec2.keys())\n        \n        vec1_array = np.array([vec1.get(word, 0) for word in words])\n        vec2_array = np.array([vec2.get(word, 0) for word in words])\n        \n        dot_product = np.dot(vec1_array, vec2_array)\n        norm1 = np.linalg.norm(vec1_array)\n        norm2 = np.linalg.norm(vec2_array)\n        \n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n        return dot_product / (norm1 * norm2)\n\n    cleaned_text1 = clean_text(text1)\n    cleaned_text2 = clean_text(text2)\n\n    cosine_sim = cosine_similarity(cleaned_text1, cleaned_text2)\n    levenshtein_ratio = ratio(cleaned_text1, cleaned_text2)\n\n    return (cosine_sim, levenshtein_ratio)\ntext1 = \"Hello, this is a sample text!\"\ntext2 = \"Hello there, this is another example.\"",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/854": [
      {
        "task_id": "BigCodeBench/854",
        "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n\n    pass\nfrom functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Check if numbers is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n\n    # Check if any number is negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"Input numbers cannot be negative\")\n\n    # Handle empty list case\n    if not numbers:\n        return [], []\n\n    # Generate all permutations\n    all_permutations = list(permutations(numbers))\n\n    # Calculate sum of factorials for each permutation\n    factorial_sums = [sum(math.factorial(num) for num in perm) for perm in all_permutations]\n\n    # Convert permutations to list of lists\n    permutations_list = [list(perm) for perm in all_permutations]\n\n    return factorial_sums, permutations_list",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 40, in test_case_1\nAssertionError: Element counts were not equal:\nFirst has 1, Second has 0:  [1, 2]\nFirst has 1, Second has 0:  [2, 1]\nFirst has 0, Second has 1:  (2, 1)\nFirst has 0, Second has 1:  (1, 2)\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 46, in test_case_2\nAssertionError: Element counts were not equal:\nFirst has 1, Second has 0:  [1, 2, 3]\nFirst has 1, Second has 0:  [1, 3, 2]\nFirst has 1, Second has 0:  [2, 1, 3]\nFirst has 1, Second has 0:  [2, 3, 1]\nFirst has 1, Second has 0:  [3, 1, 2]\nFirst has 1, Second has 0:  [3, 2, 1]\nFirst has 0, Second has 1:  (1, 2, 3)\nFirst has 0, Second has 1:  (1, 3, 2)\nFirst has 0, Second has 1:  (2, 1, 3)\nFirst has 0, Second has 1:  (2, 3, 1)\nFirst has 0, Second has 1:  (3, 1, 2)\nFirst has 0, Second has 1:  (3, 2, 1)\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 52, in test_case_3\nAssertionError: Element counts were not equal:\nFirst has 1, Second has 0:  [1]\nFirst has 0, Second has 1:  (1,)\n"
        }
      }
    ],
    "BigCodeBench/857": [
      {
        "task_id": "BigCodeBench/857",
        "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n\n    pass\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure the destination directory exists\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Iterate over each file extension\n    for ext in EXTENSIONS:\n        # Create a glob pattern for the current extension\n        pattern = os.path.join(SOURCE_DIR, f'*{ext}')\n        \n        # Find all files matching the pattern\n        for file_path in glob.glob(pattern):\n            file_name = os.path.basename(file_path)\n            dest_path = os.path.join(DEST_DIR, file_name)\n            \n            try:\n                # Attempt to move the file\n                shutil.move(file_path, dest_path)\n                transferred_files.append(file_name)\n                print(f\"Transferred: {file_name}\")\n                \n                # Small delay to prevent potential issues with rapid file operations\n                time.sleep(0.1)\n                \n            except Exception as e:\n                warnings.warn(f\"Could not transfer {file_name}: {str(e)}\")\n\n    return transferred_files",
        "status": "fail",
        "details": {
          "test_exception_handling": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 92, in test_exception_handling\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/870": [
      {
        "task_id": "BigCodeBench/870",
        "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Determine the maximum length of tuples in the data list\n    max_length = max(len(t) for t in data_list)\n\n    # Initialize lists to hold numeric values for each position\n    numeric_values = [[] for _ in range(max_length)]\n\n    # Collect numeric values for each position\n    for tup in data_list:\n        for i, value in enumerate(tup):\n            if isinstance(value, (int, float)):\n                numeric_values[i].append(value)\n\n    # Calculate the mean value for each position\n    mean_values = [np.mean(values) if values else np.nan for values in numeric_values]\n\n    # Create a DataFrame with the results\n    index = [f'Position {i}' for i in range(max_length)]\n    df = pd.DataFrame({'Mean Value': mean_values}, index=index)\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/865": [
      {
        "task_id": "BigCodeBench/865",
        "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items, counts, weights = zip(*data)\n    \n    # Convert counts and weights to numpy arrays\n    counts_array = np.array(counts)\n    weights_array = np.array(weights)\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts_array)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights_array.reshape(-1, 1)).flatten()\n    \n    # Create and return the DataFrame\n    return pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/879": [
      {
        "task_id": "BigCodeBench/879",
        "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n\n    # Check if the specified columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns '{col1}' and/or '{col2}' are not in the DataFrame.\")\n\n    # Check if the columns contain categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not isinstance(data[col1].dtype, pd.CategoricalDtype):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not isinstance(data[col2].dtype, pd.CategoricalDtype):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data.\")\n\n    # Check if the columns have multiple categories\n    if data[col1].nunique() < 2:\n        raise ValueError(f\"Column '{col1}' does not have multiple categories.\")\n    if data[col2].nunique() < 2:\n        raise ValueError(f\"Column '{col2}' does not have multiple categories.\")\n\n    # Create the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Check if each cell in the contingency table has at least 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating chi-square test assumptions.\")\n\n    # Perform the chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 52, in test_case_1\n  File \"__test__.py\", line 21, in task_func\nTypeError: Column 'Var1' does not contain categorical data.\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 59, in test_case_2\n  File \"__test__.py\", line 21, in task_func\nTypeError: Column 'Var1' does not contain categorical data.\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_case_5\n  File \"__test__.py\", line 21, in task_func\nTypeError: Column 'Var1' does not contain categorical data.\n",
          "test_edge_case_large_categories_small_counts": "Traceback (most recent call last):\n  File \"__test__.py\", line 93, in test_edge_case_large_categories_small_counts\n  File \"__test__.py\", line 21, in task_func\nTypeError: Column 'Var1' does not contain categorical data.\n",
          "test_edge_case_single_category": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in test_edge_case_single_category\n  File \"__test__.py\", line 21, in task_func\nTypeError: Column 'Var1' does not contain categorical data.\n"
        }
      }
    ],
    "BigCodeBench/897": [
      {
        "task_id": "BigCodeBench/897",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\ndef task_func(rolls, seed=None):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    results = np.random.choice(NUMBERS, size=rolls)\n    \n    # Calculate frequency of each outcome\n    frequency = np.bincount(results, minlength=7)[1:]\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(0.5, 7.5, 1), align='mid', rwidth=0.8)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    ax.set_xlim(0.5, 6.5)\n    \n    return frequency, ax",
        "status": "fail",
        "details": {
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 54, in test_case_3\nAssertionError: Lists differ: [16586, 16721, 16530, 16792, 16820, 16551] != [16607, 16689, 16800, 16625, 16640, 16639]\n\nFirst differing element 0:\n16586\n16607\n\n- [16586, 16721, 16530, 16792, 16820, 16551]\n+ [16607, 16689, 16800, 16625, 16640, 16639]\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 61, in test_case_4\nAssertionError: Lists differ: [0, 0, 1, 0, 0, 0] != [0, 1, 0, 0, 0, 0]\n\nFirst differing element 1:\n0\n1\n\n- [0, 0, 1, 0, 0, 0]\n?        ---\n\n+ [0, 1, 0, 0, 0, 0]\n?     +++\n\n"
        }
      }
    ],
    "BigCodeBench/906": [
      {
        "task_id": "BigCodeBench/906",
        "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n\n    pass\nimport zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Full path to the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a new ZIP file\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source directory\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                if file.endswith('_processed'):\n                    file_path = os.path.join(root, file)\n                    # Add file to the ZIP archive with its relative path\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n\n    return archive_path",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 64, in test_case_1\nAssertionError: 'file2_processed.txt' not found in []\n"
        }
      }
    ],
    "BigCodeBench/914": [
      {
        "task_id": "BigCodeBench/914",
        "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n\n    pass\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the DataFrame contains 'Date' and 'Close' columns\n    if 'Date' not in df.columns or 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Close' columns\")\n\n    # Convert 'Date' to datetime and sort the DataFrame\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.sort_values('Date')\n\n    # Convert dates to timestamps (seconds since epoch)\n    df['Timestamp'] = df['Date'].astype(np.int64) // 10**9\n\n    # Prepare data for linear regression\n    X = df[['Timestamp']]\n    y = df['Close']\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps for the next 7 days\n    time_step = 24 * 60 * 60  # 1 day in seconds\n    last_timestamp = df['Timestamp'].iloc[-1]\n    future_timestamps = np.array([last_timestamp + i * time_step for i in range(1, 8)])\n\n    # Predict closing prices for the next 7 days\n    future_prices = model.predict(future_timestamps.reshape(-1, 1))\n\n    # Convert future timestamps to dates for plotting\n    future_dates = pd.to_datetime(future_timestamps, unit='s')\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Date'], df['Close'], label='Historical Prices')\n    ax.plot(future_dates, future_prices, 'r--', label='Predicted Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Price Prediction')\n    ax.legend()\n    ax.grid(True)\n\n    # Format x-axis dates\n    plt.gcf().autofmt_xdate()\n\n    return future_prices.tolist(), ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 65, in test_case_1\n  File \"__test__.py\", line 15, in task_func\nValueError: DataFrame must contain 'Date' and 'Close' columns\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_case_2\n  File \"__test__.py\", line 15, in task_func\nValueError: DataFrame must contain 'Date' and 'Close' columns\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 81, in test_case_3\n  File \"__test__.py\", line 15, in task_func\nValueError: DataFrame must contain 'Date' and 'Close' columns\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 89, in test_case_4\n  File \"__test__.py\", line 15, in task_func\nValueError: DataFrame must contain 'Date' and 'Close' columns\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 97, in test_case_5\n  File \"__test__.py\", line 15, in task_func\nValueError: DataFrame must contain 'Date' and 'Close' columns\n"
        }
      }
    ],
    "BigCodeBench/915": [
      {
        "task_id": "BigCodeBench/915",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    # Calculate Z-scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-score threshold\n    outliers = df[np.abs(z_scores) > z_threshold].copy()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot all data points\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Closing Prices')\n    \n    # Highlight outliers\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', marker='x', s=100, label='Outliers')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    # Add legend\n    ax.legend()\n    \n    # Add grid for better readability\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    return outliers[['closing_price']], ax",
        "status": "fail",
        "details": {
          "test_case_2": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 59, in test_case_2\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 67, in test_case_3\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 74, in test_case_4\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Z_score'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 82, in test_case_5\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'Z_score'\n"
        }
      }
    ],
    "BigCodeBench/916": [
      {
        "task_id": "BigCodeBench/916",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Check if 'Close' column exists in the DataFrame\n    if 'Close' not in df.columns:\n        raise ValueError(\"The DataFrame must contain a 'Close' column with closing prices.\")\n\n    # Create a figure with two subplots\n    fig, (ax_box, ax_hist) = plt.subplots(nrows=2, ncols=1, figsize=(10, 8), gridspec_kw={'height_ratios': [1, 2]})\n\n    # Box plot\n    sns.boxplot(x=df['Close'], ax=ax_box)\n    ax_box.set_title('Box Plot of Closing Prices')\n    ax_box.set_xlabel('')  # Remove x-label for cleaner look\n\n    # Histogram\n    sns.histplot(df['Close'], kde=True, ax=ax_hist)\n    ax_hist.set_title('Histogram of Closing Prices')\n    ax_hist.set_xlabel('Closing Price')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return (ax_box, ax_hist)",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 43, in test_case_1\n  File \"__test__.py\", line 13, in task_func\nValueError: The DataFrame must contain a 'Close' column with closing prices.\n",
          "test_empty_df": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_empty_df\n  File \"__test__.py\", line 13, in task_func\nValueError: The DataFrame must contain a 'Close' column with closing prices.\n",
          "test_invalid_column": "Traceback (most recent call last):\n  File \"__test__.py\", line 66, in test_invalid_column\n  File \"__test__.py\", line 13, in task_func\nValueError: The DataFrame must contain a 'Close' column with closing prices.\n",
          "test_large_values_df": "Traceback (most recent call last):\n  File \"__test__.py\", line 77, in test_large_values_df\n  File \"__test__.py\", line 13, in task_func\nValueError: The DataFrame must contain a 'Close' column with closing prices.\n",
          "test_single_value_df": "Traceback (most recent call last):\n  File \"__test__.py\", line 69, in test_single_value_df\n  File \"__test__.py\", line 13, in task_func\nValueError: The DataFrame must contain a 'Close' column with closing prices.\n"
        }
      }
    ],
    "BigCodeBench/917": [
      {
        "task_id": "BigCodeBench/917",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has a 'Close' column\n    if 'Close' not in df.columns:\n        raise ValueError(\"The DataFrame must contain a 'Close' column.\")\n\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(1,1,1))  # You may need to adjust these parameters\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecasted_prices = forecast.tolist()\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    df['Close'].plot(ax=ax, label='Historical Closing Prices', color='blue')\n    \n    # Plot forecasted data\n    forecast_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7)\n    ax.plot(forecast_dates, forecasted_prices, color='red', label='Forecasted Closing Prices')\n    \n    # Customize the plot\n    ax.set_title('Share Closing Prices Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return forecasted_prices, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 60, in test_case_1\n  File \"__test__.py\", line 17, in task_func\nValueError: The DataFrame must contain a 'Close' column.\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 81, in test_case_2\n  File \"__test__.py\", line 17, in task_func\nValueError: The DataFrame must contain a 'Close' column.\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 101, in test_case_3\n  File \"__test__.py\", line 17, in task_func\nValueError: The DataFrame must contain a 'Close' column.\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 122, in test_case_4\n  File \"__test__.py\", line 17, in task_func\nValueError: The DataFrame must contain a 'Close' column.\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 142, in test_case_5\n  File \"__test__.py\", line 17, in task_func\nValueError: The DataFrame must contain a 'Close' column.\n"
        }
      }
    ],
    "BigCodeBench/928": [
      {
        "task_id": "BigCodeBench/928",
        "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n\n    pass\nfrom collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations\n    all_combinations = [''.join(comb) for comb in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    result = {comb: 0 for comb in all_combinations}\n    \n    # Count the occurrences of two-letter combinations in the word\n    word_combinations = Counter(word[i:i+2].lower() for i in range(len(word) - 1))\n    \n    # Update the result dictionary with the counts from the word\n    for combo, count in word_combinations.items():\n        if combo in result:\n            result[combo] = count\n    \n    return result\nword = \"exampleword\"",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/945": [
      {
        "task_id": "BigCodeBench/945",
        "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n\n    pass\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create or use provided sales data\n    if sales_data is None:\n        sales_data = np.random.randint(100, 200, size=periods)\n    else:\n        sales_data = np.array(sales_data)\n    \n    # Prepare data for regression\n    X = np.arange(periods).reshape(-1, 1)\n    y = sales_data\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Forecast future sales\n    future_X = np.arange(periods, 2*periods).reshape(-1, 1)\n    forecasted_sales = model.predict(future_X)\n    \n    return forecasted_sales",
        "status": "fail",
        "details": {
          "test_forecasted_values_increasing": "Traceback (most recent call last):\n  File \"__test__.py\", line 58, in test_forecasted_values_increasing\nAssertionError: True is not false\n"
        }
      }
    ],
    "BigCodeBench/942": [
      {
        "task_id": "BigCodeBench/942",
        "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n\n    pass\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create DataFrame\n    data = []\n    for category in categories:\n        sales = np.random.randint(1000, 5000, size=periods)\n        for date, sale in zip(date_range, sales):\n            data.append({'Date': date, 'Category': category, 'Sales': sale})\n    \n    df = pd.DataFrame(data)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 57, in test_case_1\nAssertionError: 'Sales Report by Category' != 'Category-wise Sales Trends'\n- Sales Report by Category\n+ Category-wise Sales Trends\n\n"
        }
      }
    ],
    "BigCodeBench/952": [
      {
        "task_id": "BigCodeBench/952",
        "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n\n    pass\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Ensure there are enough tasks to assign\n    if n_tasks > len(sanitized_tasks):\n        raise ValueError(\"n_tasks exceeds the number of available tasks\")\n\n    # Randomly select tasks\n    selected_tasks = random.sample(sanitized_tasks, n_tasks)\n\n    # Get the current date\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Create the assignments\n    assignments = []\n    for task in selected_tasks:\n        assigned_employee = random.choice(employees)\n        assignments.append({\n            \"Task Name\": task,\n            \"Assigned To\": assigned_employee,\n            \"Due Date\": current_date\n        })\n\n    # Create and return the DataFrame\n    return pd.DataFrame(assignments)",
        "status": "fail",
        "details": {
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 94, in test_case_5\nAssertionError: Items in the second set but not the first:\n'Due Date'\n'Assigned To'\n'Task Name'\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 82, in test_case_3\n  File \"__test__.py\", line 34, in task_func\nValueError: n_tasks exceeds the number of available tasks\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 98, in test_case_6\n  File \"__test__.py\", line 34, in task_func\nValueError: n_tasks exceeds the number of available tasks\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 104, in test_case_7\n  File \"__test__.py\", line 34, in task_func\nValueError: n_tasks exceeds the number of available tasks\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 112, in test_case_8\n  File \"__test__.py\", line 34, in task_func\nValueError: n_tasks exceeds the number of available tasks\n"
        }
      }
    ],
    "BigCodeBench/963": [
      {
        "task_id": "BigCodeBench/963",
        "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n\n    pass\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n\n    # Valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Ensure the target directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Create the full path for the zip file\n    zip_path = Path(target_directory) / f\"{zip_name}.zip\"\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for ext in valid_extensions:\n            # Find all files with the current extension in the source directory\n            for file_path in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add the file to the zip archive\n                zipf.write(file_path, os.path.basename(file_path))\n\n    # Return the full path to the created zip file\n    return str(zip_path)",
        "status": "fail",
        "details": {
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_case_5\nAssertionError: 0 != 2\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 136, in test_case_7\nAssertionError: 0 != 4\n"
        }
      }
    ],
    "BigCodeBench/955": [
      {
        "task_id": "BigCodeBench/955",
        "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n\n    pass\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text.strip():\n        raise ValueError(\"The input text is empty.\")\n\n    # Convert text to lowercase for case-insensitive operations\n    text = text.lower()\n\n    # Replace spaces with underscores in given words and convert to lowercase\n    modified_words = [word.lower().replace(' ', '_') for word in mystrings]\n\n    # Find all words in the text\n    words_in_text = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_counts = Counter(words_in_text)\n\n    # Get frequencies for the modified words in the order they appear\n    frequencies = [word_counts[word] for word in modified_words]\n\n    # Create the frequency plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(modified_words, frequencies)\n\n    # Set labels and title\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n\n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    # Return the Axes object\n    return ax",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 61, in test_case_1\nAssertionError: False is not true\n",
          "test_case_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 67, in test_case_2\nAssertionError: 'hello_world' != 'hello_world!'\n- hello_world\n+ hello_world!\n?            +\n\n",
          "test_case_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 74, in test_case_3\nAssertionError: False is not true\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 83, in test_case_4\nAssertionError: False is not true\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 88, in test_case_5\nAssertionError: 'hello_world!' not found in ['hello_world']\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_case_6\nAssertionError: 'Hello_World!' not found in ['hello_world']\n",
          "test_case_7": "Traceback (most recent call last):\n  File \"__test__.py\", line 102, in test_case_7\nAssertionError: 'not_in_text' unexpectedly found in ['not_in_text']\n"
        }
      }
    ],
    "BigCodeBench/964": [
      {
        "task_id": "BigCodeBench/964",
        "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n\n    pass\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n\n    os.makedirs(target_directory, exist_ok=True)\n\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    converted_count = 0\n\n    for file_path in Path(source_directory).rglob('*'):\n        if file_path.suffix.lower() in extensions:\n            try:\n                if file_path.suffix.lower() == '.txt':\n                    with open(file_path, 'r', encoding='utf-8') as file:\n                        content = file.read()\n                elif file_path.suffix.lower() == '.docx':\n                    doc = docx.Document(file_path)\n                    content = \"\\n\".join(para.text for para in doc.paragraphs)\n                elif file_path.suffix.lower() == '.xlsx':\n                    df = pd.read_excel(file_path, header=None)\n                    content = df.to_csv(header=False, index=False)\n                else:  # .csv\n                    df = pd.read_csv(file_path, header=None)\n                    content = df.to_csv(header=False, index=False)\n\n                target_file_path = Path(target_directory) / f\"{file_path.stem}.csv\"\n                pd.DataFrame({'Text': [content]}).to_csv(target_file_path, index=False)\n                converted_count += 1\n\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n\n    return converted_count",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/969": [
      {
        "task_id": "BigCodeBench/969",
        "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n\n    pass\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Check for non-numeric data types\n    if not np.issubdtype(df.dtypes, np.number).all():\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    # Check for NaN values\n    if df.isna().any().any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n\n    # Compute the cumulative sum for each column\n    cumsum_df = df.cumsum()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the cumulative sum DataFrame\n    normalized_array = scaler.fit_transform(cumsum_df)\n\n    # Convert the normalized array back to a DataFrame with original column names\n    normalized_df = pd.DataFrame(normalized_array, columns=df.columns, index=df.index)\n\n    return normalized_df",
        "status": "fail",
        "details": {
          "test_all_zeros": "Traceback (most recent call last):\n  File \"__test__.py\", line 58, in test_all_zeros\n  File \"__test__.py\", line 41, in check_cumsum_and_scaling\n  File \"__test__.py\", line 16, in task_func\nAttributeError: 'bool' object has no attribute 'all'\n",
          "test_incremental_values": "Traceback (most recent call last):\n  File \"__test__.py\", line 48, in test_incremental_values\n  File \"__test__.py\", line 41, in check_cumsum_and_scaling\n  File \"__test__.py\", line 16, in task_func\nAttributeError: 'bool' object has no attribute 'all'\n",
          "test_nan_values_raise": "Traceback (most recent call last):\n  File \"__test__.py\", line 70, in test_nan_values_raise\n  File \"__test__.py\", line 16, in task_func\nAttributeError: 'bool' object has no attribute 'all'\n",
          "test_negative_numbers": "Traceback (most recent call last):\n  File \"__test__.py\", line 53, in test_negative_numbers\n  File \"__test__.py\", line 41, in check_cumsum_and_scaling\n  File \"__test__.py\", line 16, in task_func\nAttributeError: 'bool' object has no attribute 'all'\n",
          "test_non_numeric_data_raises": "Traceback (most recent call last):\n  File \"__test__.py\", line 67, in test_non_numeric_data_raises\n  File \"__test__.py\", line 16, in task_func\nAttributeError: 'bool' object has no attribute 'all'\n",
          "test_same_numbers": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_same_numbers\n  File \"__test__.py\", line 41, in check_cumsum_and_scaling\n  File \"__test__.py\", line 16, in task_func\nAttributeError: 'bool' object has no attribute 'all'\n"
        }
      }
    ],
    "BigCodeBench/971": [
      {
        "task_id": "BigCodeBench/971",
        "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n\n    pass\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    dir_path = Path(directory_path)\n    \n    if not dir_path.is_dir():\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    file_info = []\n    \n    for entry in dir_path.iterdir():\n        if entry.is_file():\n            stats = entry.stat()\n            \n            # Get file size\n            size = stats.st_size\n            \n            # Get creation time (or fallback to metadata change time)\n            try:\n                creation_time = datetime.fromtimestamp(stats.st_ctime, tz=timezone.utc).isoformat()\n            except AttributeError:\n                creation_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc).isoformat()\n            \n            # Get modification time\n            modification_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc).isoformat()\n            \n            file_info.append((entry.name, size, creation_time, modification_time))\n    \n    return file_info",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/985": [
      {
        "task_id": "BigCodeBench/985",
        "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n\n    pass\nimport pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        # Parse JSON data\n        data = json.loads(json_data)\n        \n        # Check if data is a non-empty list\n        if not isinstance(data, list) or len(data) == 0:\n            raise ValueError(\"The JSON data is empty or not a list.\")\n        \n        countries = []\n        populations = []\n        \n        for item in data:\n            if not isinstance(item, dict):\n                raise ValueError(\"Invalid item format in JSON data.\")\n            \n            country = item.get(\"Country\")\n            population = item.get(\"Population\")\n            \n            # Validate country name\n            if not isinstance(country, str):\n                raise ValueError(\"Non-string country name found.\")\n            \n            # Validate population\n            if not isinstance(population, (int, float)):\n                raise ValueError(\"Non-numeric population found.\")\n            if population < 0:\n                raise ValueError(\"Negative population found.\")\n            \n            # Round down float populations\n            population = math.floor(population)\n            \n            countries.append(country)\n            populations.append(population)\n        \n        # Create DataFrame\n        df = pd.DataFrame({\"Country\": countries, \"Population\": populations})\n        \n        # Ensure output directory exists\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Generate full file path\n        file_path = os.path.join(output_dir, file_name)\n        \n        # Write DataFrame to CSV\n        df.to_csv(file_path, index=False)\n        \n        return file_path, df\n    \n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    except IOError as e:\n        raise IOError(f\"The file cannot be written to the specified directory: {str(e)}\")\n    except Exception as e:\n        raise ValueError(str(e))",
        "status": "fail",
        "details": {
          "test_case_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 19, in task_func\nValueError: The JSON data is empty or not a list.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 85, in test_case_1\n  File \"__test__.py\", line 66, in task_func\nValueError: The JSON data is empty or not a list.\n",
          "test_case_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 19, in task_func\nValueError: The JSON data is empty or not a list.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 114, in test_case_4\n  File \"__test__.py\", line 66, in task_func\nValueError: The JSON data is empty or not a list.\n",
          "test_case_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 19, in task_func\nValueError: The JSON data is empty or not a list.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 122, in test_case_5\n  File \"__test__.py\", line 66, in task_func\nValueError: The JSON data is empty or not a list.\n",
          "test_case_6": "Traceback (most recent call last):\n  File \"__test__.py\", line 19, in task_func\nValueError: The JSON data is empty or not a list.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 130, in test_case_6\n  File \"__test__.py\", line 66, in task_func\nValueError: The JSON data is empty or not a list.\n",
          "test_case_8": "Traceback (most recent call last):\n  File \"__test__.py\", line 19, in task_func\nValueError: The JSON data is empty or not a list.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 154, in test_case_8\n  File \"__test__.py\", line 66, in task_func\nValueError: The JSON data is empty or not a list.\n",
          "test_case_9": "Traceback (most recent call last):\n  File \"__test__.py\", line 19, in task_func\nValueError: The JSON data is empty or not a list.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"__test__.py\", line 164, in test_case_9\n  File \"__test__.py\", line 66, in task_func\nValueError: The JSON data is empty or not a list.\n"
        }
      }
    ],
    "BigCodeBench/988": [
      {
        "task_id": "BigCodeBench/988",
        "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n\n    pass\nimport os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Validate the directory path\n    if not Path(dir_path).is_dir():\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Deduplicate predicates\n    predicates = list(set(predicates))\n    \n    # Validate predicates\n    if not predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n    \n    results = {}\n    \n    for item in os.listdir(dir_path):\n        results[item] = {}\n        for predicate in predicates:\n            try:\n                # Evaluate the predicate against the item name\n                result = eval(predicate, {\"re\": re}, {\"item_name\": item})\n                results[item][predicate] = bool(result)\n            except Exception as e:\n                # If there's an error evaluating the predicate, store the error message\n                results[item][predicate] = f\"Error: {str(e)}\"\n    \n    return results",
        "status": "fail",
        "details": {
          "test_dir_is_not_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 102, in test_dir_is_not_file\nAssertionError: \"Error: name 'is_file' is not defined\" is not false\n",
          "test_file_is_not_dir": "Traceback (most recent call last):\n  File \"__test__.py\", line 86, in test_file_is_not_dir\nAssertionError: \"Error: name 'is_dir' is not defined\" is not false\n",
          "test_has_no_special_char": "Traceback (most recent call last):\n  File \"__test__.py\", line 120, in test_has_no_special_char\nAssertionError: \"Error: name 'has_special_chars' is not defined\" is not false\n",
          "test_invalid_predicates": "Traceback (most recent call last):\n  File \"__test__.py\", line 150, in test_invalid_predicates\nAssertionError: ValueError not raised\n",
          "test_multiple_predicates": "Traceback (most recent call last):\n  File \"__test__.py\", line 137, in test_multiple_predicates\nAssertionError: \"Error: name 'is_dir' is not defined\" is not false\n"
        }
      }
    ],
    "BigCodeBench/990": [
      {
        "task_id": "BigCodeBench/990",
        "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n\n    pass\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    try:\n        # Decode the hexadecimal string to bytes\n        byte_data = bytes.fromhex(hex_string)\n        \n        # Convert bytes to UTF-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Prepare the dictionary with various encodings\n        result = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        # Handle ASCII encoding\n        try:\n            result['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n        except UnicodeEncodeError:\n            result['ASCII'] = 'Not representable in ASCII'\n        \n        return result\n    except ValueError:\n        raise ValueError(\"Invalid hexadecimal input\")\n    except UnicodeDecodeError:\n        raise ValueError(\"Input cannot be decoded into a valid UTF-8 string\")",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/998": [
      {
        "task_id": "BigCodeBench/998",
        "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n\n    pass\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate MD5 checksum\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_checksum = md5_hash.hexdigest()\n        \n        # Validate checksum\n        if calculated_checksum != EXPECTED_MD5_CHECKSUM:\n            print(f\"Checksum mismatch. Expected: {EXPECTED_MD5_CHECKSUM}, Got: {calculated_checksum}\")\n            os.remove(TARGET_TAR_FILE)\n            return False\n        \n        # Extract the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        \n        return True\n    \n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False",
        "status": "fail",
        "details": {
          "test_valid_file": "Traceback (most recent call last):\n  File \"__test__.py\", line 70, in test_valid_file\nAssertionError: True is not false\n"
        }
      }
    ],
    "BigCodeBench/999": [
      {
        "task_id": "BigCodeBench/999",
        "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n\n    pass\nimport urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a Counter to store the value counts\n        value_counts = collections.Counter()\n        \n        # Open and process the CSV file\n        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value_counts[row[column_name]] += 1\n        \n        # Return the counts as a dictionary\n        return dict(value_counts)\n    \n    except urllib.error.URLError as e:\n        raise Exception(f\"Failed to download the file: {e}\")\n    \n    finally:\n        # Ensure the CSV file is deleted after processing or if an error occurs\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1003": [
      {
        "task_id": "BigCodeBench/1003",
        "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n\n    pass\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n    except Exception as e:\n        raise ValueError(f\"Failed to fetch the XML file: {e}\")\n\n    try:\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the XML structure conforms to the expected format\n    if root.tag != 'root' or not all(child.tag == 'item' for child in root):\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Extract data from XML and create DataFrame\n    data = []\n    for item in root.findall('item'):\n        row = {child.tag: child.text for child in item}\n        data.append(row)\n\n    if not data:\n        raise ValueError(\"No valid items found in the XML\")\n\n    return pd.DataFrame(data)",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1004": [
      {
        "task_id": "BigCodeBench/1004",
        "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n\n    pass\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the specified URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            text = response.read().decode('utf-8')\n    except Exception as e:\n        print(f\"Error downloading or decoding the file: {e}\")\n        return None, None\n\n    # Process the text to count word frequencies\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    word_counts = Counter(words)\n\n    # Get the ten most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Prepare data for plotting\n    words, frequencies = zip(*most_common_words)\n\n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, frequencies)\n    ax.set_title('Ten Most Frequently Occurring Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return word_counts, ax",
        "status": "fail",
        "details": {
          "test_large_input": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 93, in test_large_input\nAssertionError: 0 != 10000\n",
          "test_non_text_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 71, in test_non_text_file\nAssertionError: Exception not raised\n",
          "test_word_frequencies": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 55, in test_word_frequencies\nAssertionError: 0 != 3\n",
          "test_empty_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 62, in test_empty_file\n  File \"__test__.py\", line 29, in task_func\nValueError: not enough values to unpack (expected 2, got 0)\n"
        }
      }
    ],
    "BigCodeBench/1006": [
      {
        "task_id": "BigCodeBench/1006",
        "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n\n    pass\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file from the URL\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n\n        # Check content type\n        content_type = response.headers.get('Content-Type', '').lower()\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Save the downloaded content to a temporary file\n        temp_zip_path = os.path.join(download_path, 'temp_download.zip')\n        with open(temp_zip_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        # Extract the ZIP file\n        try:\n            with ZipFile(temp_zip_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            os.remove(temp_zip_path)\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Clean up the temporary ZIP file\n        os.remove(temp_zip_path)\n\n        return download_path\n\n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"",
        "status": "fail",
        "details": {
          "test_valid_zip_url": "Traceback (most recent call last):\n  File \"__test__.py\", line 56, in test_valid_zip_url\nAssertionError: False is not true\n"
        }
      }
    ],
    "BigCodeBench/1008": [
      {
        "task_id": "BigCodeBench/1008",
        "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n\n    pass\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        # Make an HTTP request to the specified URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the table with the specified ID\n        table = soup.find('table', id=table_id)\n\n        # Raise a ValueError if the table is not found\n        if table is None:\n            raise ValueError(\"Table with the specified ID not found.\")\n\n        # Extract the rows of the table\n        rows = table.find_all('tr')\n\n        # If the table has no rows, return an empty DataFrame\n        if not rows:\n            return pd.DataFrame()\n\n        # Extract headers from the first row\n        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n\n        # Extract data from the remaining rows\n        data = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if cells:\n                data.append([cell.get_text(strip=True) for cell in cells])\n\n        # Create a DataFrame\n        if data:\n            df = pd.DataFrame(data, columns=headers)\n        else:\n            df = pd.DataFrame(columns=headers)\n\n        return df\n\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {str(e)}\")",
        "status": "fail",
        "details": {
          "test_empty_table": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 124, in test_empty_table\n  File \"__test__.py\", line 19, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n",
          "test_network_error": "Traceback (most recent call last):\n  File \"__test__.py\", line 15, in task_func\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1114, in __call__\n    return self._mock_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1118, in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1173, in _execute_mock_call\n    raise effect\nrequests.exceptions.ConnectionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 99, in test_network_error\n  File \"__test__.py\", line 54, in task_func\nrequests.exceptions.HTTPError: HTTP request failed: \n",
          "test_successful_scrape": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 79, in test_successful_scrape\n  File \"__test__.py\", line 19, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n",
          "test_table_not_found": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 93, in test_table_not_found\n  File \"__test__.py\", line 19, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 445, in __init__\n    for encoding in self.detector.encodings:\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 301, in encodings\n    self.declared_encoding = self.find_declared_encoding(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 378, in find_declared_encoding\n    declared_encoding_match = xml_re.search(markup, endpos=xml_endpos)\nTypeError: expected string or bytes-like object\n"
        }
      }
    ],
    "BigCodeBench/1013": [
      {
        "task_id": "BigCodeBench/1013",
        "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n\n    pass\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        # Send a GET request to the webpage\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Parse the webpage content using BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find all hyperlinks in the webpage and convert to absolute URLs\n        links = set()\n        for a_tag in soup.find_all('a', href=True):\n            href = a_tag['href']\n            absolute_url = urljoin(base_url, href)\n            links.add(absolute_url)\n\n        # Write the absolute URLs to a CSV file\n        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow([\"URL\"])\n            for link in links:\n                writer.writerow([link])\n\n        # Return the number of unique absolute links\n        return len(links)\n\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return 0",
        "status": "fail",
        "details": {
          "test_duplicate_links": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 95, in test_duplicate_links\nAssertionError: 0 != 1\n",
          "test_external_links": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 105, in test_external_links\nAssertionError: 0 != 1\n",
          "test_multiple_links": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 85, in test_multiple_links\nAssertionError: 0 != 2\n",
          "test_single_link": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 75, in test_single_link\nAssertionError: 0 != 1\n"
        }
      }
    ],
    "BigCodeBench/1012": [
      {
        "task_id": "BigCodeBench/1012",
        "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n\n    pass\nimport requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    try:\n        # Ensure the download and extraction directories exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        # Save the downloaded file\n        download_path = DOWNLOAD_DIR / filename\n        with open(download_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Extract the zip file\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get the list of extracted filenames\n        extracted_files = [str(file.relative_to(ZIP_DIR)) for file in ZIP_DIR.rglob('*') if file.is_file()]\n\n        return \"Success\", extracted_files\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Network-related exception - {str(e)}\", []\n\n    except (zipfile.BadZipFile, zipfile.LargeZipFile, IOError, OSError) as e:\n        return f\"Error: File-related exception - {str(e)}\", []\n\n    except Exception as e:\n        return f\"Error: Unexpected exception - {str(e)}\", []",
        "status": "fail",
        "details": {
          "test_invalid_url": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 67, in test_invalid_url\nAssertionError: 'Download failed' not found in 'Error: File-related exception - File is not a zip file'\n",
          "test_non_200_http_response": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 74, in test_non_200_http_response\nAssertionError: 'Download failed' not found in 'Error: File-related exception - File is not a zip file'\n",
          "test_successful_download_and_extraction": "Traceback (most recent call last):\n  File \"__test__.py\", line 60, in test_successful_download_and_extraction\nAssertionError: 'Download and extraction successful' not found in 'Success'\n"
        }
      }
    ],
    "BigCodeBench/1019": [
      {
        "task_id": "BigCodeBench/1019",
        "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n\n    pass\nfrom PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    def convert_encoding(text, from_enc, to_enc):\n        try:\n            return text.encode(from_enc).decode(to_enc)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(f\"Encoding conversion error: {e}\")\n\n    def extract_and_convert(extractor, converter):\n        try:\n            text = extractor()\n            if text:\n                return converter(text)\n        except ValueError:\n            raise\n        except Exception as e:\n            print(f\"Extraction or conversion failed: {e}\")\n        return None\n\n    def ocr_extractor():\n        with Image.open(filename) as img:\n            return pytesseract.image_to_string(img).strip()\n\n    def comment_extractor():\n        with Image.open(filename) as img:\n            return img.info.get('comment', '').strip()\n\n    converter = lambda text: convert_encoding(text, from_encoding, to_encoding)\n\n    try:\n        result = extract_and_convert(ocr_extractor, converter)\n        if result is not None:\n            return result\n\n        result = extract_and_convert(comment_extractor, converter)\n        if result is not None:\n            return result\n\n    except ValueError as e:\n        raise ValueError(f\"Encoding error: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n    return \"\"",
        "status": "fail",
        "details": {
          "test_ocr_fails_comment_extraction_succeeds": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 83, in test_ocr_fails_comment_extraction_succeeds\nAssertionError: '' != 'Mocked Comment in cp1251'\n+ Mocked Comment in cp1251\n"
        }
      }
    ],
    "BigCodeBench/1015": [
      {
        "task_id": "BigCodeBench/1015",
        "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n\n    pass\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content\n        if webpage_url.startswith(('http://', 'https://')):\n            response = requests.get(webpage_url)\n            response.raise_for_status()\n            content = response.content\n        else:\n            with open(webpage_url, 'rb') as file:\n                content = file.read()\n        \n        # Parse the HTML content\n        tree = html.fromstring(content)\n        tables = pd.read_html(html.tostring(tree))\n        \n        if not tables:\n            print(\"No tables found in the HTML content.\")\n            return 0\n        \n        # Use the first table found\n        df = tables[0]\n        \n        if df.empty:\n            print(\"The table is empty.\")\n            return 0\n        \n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        \n        # Replace the existing table with new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Get the number of rows in the new table\n        row_count = len(df)\n        \n        # Close the database connection\n        conn.close()\n        \n        return row_count\n\n    except requests.RequestException as e:\n        print(f\"Network error: {e}\")\n        raise\n    except sqlite3.DatabaseError as e:\n        print(f\"Database error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        raise",
        "status": "fail",
        "details": {
          "test_empty_table": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 109, in test_empty_table\n  File \"__test__.py\", line 25, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/html.py\", line 1212, in read_html\n    return _parse(\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/html.py\", line 977, in _parse\n    parser = _parser_dispatch(flav)\n  File \"/usr/local/lib/python3.10/site-packages/pandas/io/html.py\", line 930, in _parser_dispatch\n    bs4 = import_optional_dependency(\"bs4\")  # noqa:F841\n  File \"/usr/local/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 171, in import_optional_dependency\n    raise ImportError(msg)\nImportError: Pandas requires version '4.9.3' or newer of 'bs4' (version '4.8.2' currently installed).\n"
        }
      }
    ],
    "BigCodeBench/1020": [
      {
        "task_id": "BigCodeBench/1020",
        "solution": "import json\nimport requests\nimport chardet\n# Constants\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n\n    pass\nimport json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Initiate an HTTP GET request with a 5-second timeout\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Retrieve the response content in raw bytes\n        content = response.content\n\n        # Handle empty content case\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding is None:\n            detection = chardet.detect(content)\n            from_encoding = detection['encoding']\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode using detected or provided encoding, then re-encode to target encoding\n        decoded_content = content.decode(from_encoding)\n        encoded_content = decoded_content.encode(to_encoding)\n        \n        # JSON parse the re-encoded content\n        json_data = json.loads(encoded_content.decode(to_encoding))\n        \n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        print(f\"JSON parsing failed: {e}\")\n        return {}\n    except ValueError as e:\n        print(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return {}",
        "status": "fail",
        "details": {
          "test_get_data_with_invalid_json": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 92, in test_get_data_with_invalid_json\nAssertionError: JSONDecodeError not raised\n"
        }
      }
    ],
    "BigCodeBench/1022": [
      {
        "task_id": "BigCodeBench/1022",
        "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n\n    pass\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file '{csv_file_path}' was not found.\")\n\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Check if the CSV file is empty\n        if df.empty:\n            return pd.DataFrame()\n\n        # Check if the specified column exists\n        if column_name not in df.columns:\n            raise ValueError(f\"The specified column '{column_name}' is not present in the CSV file.\")\n\n        # Convert date values to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n        # Drop rows with invalid date values\n        df = df.dropna(subset=[column_name])\n\n        # Filter rows based on the current date\n        current_date = datetime.now().date()\n        df = df[df[column_name].dt.date <= current_date]\n\n        # Sort the resulting data by the date column\n        df = df.sort_values(by=column_name)\n\n        return df\n\n    except EmptyDataError:\n        # Handle the case of an empty CSV file\n        return pd.DataFrame()",
        "status": "fail",
        "details": {
          "test_no_future_dates": "Traceback (most recent call last):\n  File \"__test__.py\", line 108, in test_no_future_dates\nAssertionError: False is not true\n",
          "test_valid_input": "Traceback (most recent call last):\n  File \"__test__.py\", line 73, in test_valid_input\nAssertionError: True is not false\n"
        }
      }
    ],
    "BigCodeBench/1039": [
      {
        "task_id": "BigCodeBench/1039",
        "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n\n    pass\nimport ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    secure_socket = None\n    try:\n        # Set up SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        \n        # Wrap the client socket with SSL/TLS\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the file exists\n        if not os.path.isfile(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n        \n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            while True:\n                data = file.read(buffer_size)\n                if not data:\n                    break\n                sha256_hash.update(data)\n        \n        # Get the hexadecimal representation of the hash\n        hash_result = sha256_hash.hexdigest()\n        \n        # Send the hash back to the client\n        secure_socket.sendall(hash_result.encode('utf-8'))\n        \n        return hash_result\n    \n    except Exception as e:\n        error_message = f\"Error: {str(e)}\"\n        if secure_socket:\n            try:\n                secure_socket.sendall(error_message.encode('utf-8'))\n            except:\n                pass  # Ignore errors when sending error message\n        return error_message\n    \n    finally:\n        if secure_socket:\n            try:\n                secure_socket.shutdown(ssl.SOCK_STREAM)\n                secure_socket.close()\n            except:\n                pass  # Ignore errors when closing the socket",
        "status": "fail",
        "details": {
          "test_file_found": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 92, in test_file_found\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: open('path/to/requested_file.txt', 'rb')\nActual: not called.\n",
          "test_file_not_found": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 125, in test_file_not_found\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: send(b'File not found')\nActual: not called.\n",
          "test_task_func_empty_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 173, in test_task_func_empty_file\nAssertionError: 'File not found' != 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n- File not found\n+ e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n\n",
          "test_task_func_large_file": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 200, in test_task_func_large_file\nAssertionError: 'File not found' != 'cdc76e5c9914fb9281a1c7e284d73e67f1809a48a497200e046d39ccc7112cd0'\n- File not found\n+ cdc76e5c9914fb9281a1c7e284d73e67f1809a48a497200e046d39ccc7112cd0\n\n"
        }
      }
    ],
    "BigCodeBench/1042": [
      {
        "task_id": "BigCodeBench/1042",
        "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n\n    pass\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    try:\n        # Receive message from client socket\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        \n        # Get email details from user\n        sender_email = input(\"Enter sender's email: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        password = getpass.getpass(\"Enter sender's email password: \")\n\n        # Create email message\n        email = EmailMessage()\n        email.set_content(message)\n        email['Subject'] = 'Message from Client Socket'\n        email['From'] = sender_email\n        email['To'] = recipient_email\n\n        # Send email via SMTP server\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()\n            server.login(sender_email, password)\n            server.send_message(email)\n        \n        print(\"Email sent successfully.\")\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    \n    finally:\n        client_socket.close()\n    \n    return None",
        "status": "fail",
        "details": {
          "test_smtp_server_connection_error": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 115, in test_smtp_server_connection_error\nAssertionError: SMTPConnectError not raised\n",
          "test_successful_email_dispatch": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 153, in test_successful_email_dispatch\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: SMTP('smtp.gmail.com', 587)\nActual: not called.\n",
          "test_successful_email_send": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 74, in test_successful_email_send\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 920, in assert_called_with\n    raise AssertionError(error_message)\nAssertionError: expected call not found.\nExpected: SMTP('smtp.gmail.com', 587)\nActual: not called.\n"
        }
      }
    ],
    "BigCodeBench/1053": [
      {
        "task_id": "BigCodeBench/1053",
        "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n\n    pass\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path, header=None)\n        \n        # Check if the CSV file has a header\n        if df.iloc[0, 0].lower() == 'text':\n            df.columns = ['Text']\n            df = df.iloc[1:]\n        else:\n            df.columns = ['Text']\n        \n        # Combine all text data into a single string\n        text_data = ' '.join(df['Text'].astype(str))\n        \n        # Initialize and fit CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_count_matrix = vectorizer.fit_transform([text_data])\n        \n        # Get word frequencies\n        word_freq = pd.DataFrame(word_count_matrix.toarray().T, \n                                 index=vectorizer.get_feature_names_out(), \n                                 columns=['frequency'])\n        \n        # Sort and get top 10 words\n        top_10_words = word_freq.sort_values('frequency', ascending=False).head(10)\n        \n        # Create the histogram\n        fig, ax = plt.subplots(figsize=(12, 6))\n        top_10_words['frequency'].plot(kind='bar', ax=ax)\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path)\n            plt.close(fig)\n            return None\n        else:\n            plt.show()\n            return ax\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path '{file_path}' was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/417": [
      {
        "task_id": "BigCodeBench/417",
        "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n\n    pass\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training set (75%) and test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential([\n        Dense(10, input_dim=2, activation='sigmoid'),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy')\n\n    # Fit the model to the training data, also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the model's training and validation loss over epochs\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    return model, ax",
        "status": "fail",
        "details": {
          "test_model_weights": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_model_weights\nAssertionError: 4 != 2\n"
        }
      }
    ],
    "BigCodeBench/1057": [
      {
        "task_id": "BigCodeBench/1057",
        "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n\n    pass\nimport pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Predefined lists\n    default_animals = ['dog', 'cat', 'rabbit']\n    default_foods = ['apple', 'banana', 'carrot']\n\n    # Use default lists if input is None or empty\n    if not animals:\n        animals = default_animals\n    if not foods:\n        foods = default_foods\n\n    # Return empty DataFrame if both lists are empty\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    # Generate all combinations\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle combinations\n    np.random.shuffle(combinations)\n\n    # Create DataFrame\n    df = pd.DataFrame(index=animals, columns=foods)\n\n    # Fill DataFrame with shuffled combinations\n    for animal, food in combinations:\n        df.at[animal, food] = f'{animal}:{food}'\n\n    # Fill any remaining empty cells\n    df.fillna('', inplace=True)\n\n    return df",
        "status": "fail",
        "details": {
          "test_default_input": "Traceback (most recent call last):\n  File \"__test__.py\", line 52, in test_default_input\nAssertionError: Tuples differ: (3, 3) != (10, 7)\n\nFirst differing element 0:\n3\n10\n\n- (3, 3)\n+ (10, 7) : The shape of the DataFrame with default inputs is not as expected.\n",
          "test_empty_input": "Traceback (most recent call last):\n  File \"__test__.py\", line 78, in test_empty_input\nAssertionError: Tuples differ: (3, 3) != (0, 0)\n\nFirst differing element 0:\n3\n0\n\n- (3, 3)\n+ (0, 0) : The shape of the DataFrame with empty inputs is not as expected.\n",
          "test_partial_default": "Traceback (most recent call last):\n  File \"__test__.py\", line 109, in test_partial_default\nAssertionError: Tuples differ: (3, 3) != (3, 7)\n\nFirst differing element 1:\n3\n7\n\n- (3, 3)\n?     ^\n\n+ (3, 7)\n?     ^\n : The shape of the DataFrame with partial default inputs is not as expected.\n"
        }
      }
    ],
    "BigCodeBench/1077": [
      {
        "task_id": "BigCodeBench/1077",
        "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n\n    pass\nfrom datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Create a timezone object\n    tz = pytz.timezone(timezone)\n\n    # Convert timestamps to datetime objects in the specified timezone\n    converted_times = []\n    for ts in time_strings:\n        dt = datetime.fromisoformat(ts)\n        dt = dt.replace(tzinfo=pytz.UTC).astimezone(tz)\n        converted_times.append(dt)\n\n    # Calculate time differences in seconds\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = abs((converted_times[i] - converted_times[i-1]).total_seconds())\n        time_diffs.append(diff)\n\n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n\n    # Calculate and return the average time difference\n    return np.mean(time_diffs)",
        "status": "fail",
        "details": {
          "test_different_timezones": "Traceback (most recent call last):\n  File \"__test__.py\", line 55, in test_different_timezones\n  File \"__test__.py\", line 21, in task_func\nValueError: Invalid isoformat string: '01/04/21 12:00:00.000'\n",
          "test_example_case": "Traceback (most recent call last):\n  File \"__test__.py\", line 47, in test_example_case\n  File \"__test__.py\", line 21, in task_func\nValueError: Invalid isoformat string: '30/03/09 16:31:32.123'\n",
          "test_out_of_order_strings": "Traceback (most recent call last):\n  File \"__test__.py\", line 80, in test_out_of_order_strings\n  File \"__test__.py\", line 21, in task_func\nValueError: Invalid isoformat string: '01/04/21 12:02:02.000'\n",
          "test_span_across_days": "Traceback (most recent call last):\n  File \"__test__.py\", line 72, in test_span_across_days\n  File \"__test__.py\", line 21, in task_func\nValueError: Invalid isoformat string: '31/03/21 23:59:00.000'\n",
          "test_varying_differences": "Traceback (most recent call last):\n  File \"__test__.py\", line 64, in test_varying_differences\n  File \"__test__.py\", line 21, in task_func\nValueError: Invalid isoformat string: '01/04/21 12:00:00.000'\n"
        }
      }
    ],
    "BigCodeBench/1124": [
      {
        "task_id": "BigCodeBench/1124",
        "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n\n    pass\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string using regex\n    url_pattern = re.compile(r'https?://\\S+')\n    match = url_pattern.search(myString)\n    \n    if not match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = match.group()\n    \n    try:\n        # Fetch the content of the URL\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the webpage content to find the title\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if title_tag and title_tag.string:\n        return title_tag.string.strip()\n    else:\n        return \"No title tag found in the webpage.\"",
        "status": "fail",
        "details": {
          "test_multiple_urls_in_string": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 66, in test_multiple_urls_in_string\n  File \"__test__.py\", line 30, in task_func\nAttributeError: 'MockResponse' object has no attribute 'content'\n",
          "test_url_with_no_title_tag": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 72, in test_url_with_no_title_tag\n  File \"__test__.py\", line 30, in task_func\n  File \"/usr/local/lib/python3.10/site-packages/bs4/__init__.py\", line 319, in __init__\n    for (self.markup, self.original_encoding, self.declared_html_encoding,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/builder/_htmlparser.py\", line 325, in prepare_markup\n    dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 430, in __init__\n    self.detector = EncodingDetector(\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 264, in __init__\n    self.markup, self.sniffed_encoding = self.strip_byte_order_mark(markup)\n  File \"/usr/local/lib/python3.10/site-packages/bs4/dammit.py\", line 329, in strip_byte_order_mark\n    if (len(data) >= 4) and (data[:2] == b'\\xfe\\xff') \\\nTypeError: object of type 'Mock' has no len()\n",
          "test_valid_url_with_title": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 52, in test_valid_url_with_title\n  File \"__test__.py\", line 30, in task_func\nAttributeError: 'MockResponse' object has no attribute 'content'\n"
        }
      }
    ],
    "BigCodeBench/1085": [
      {
        "task_id": "BigCodeBench/1085",
        "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n\n    pass\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text and remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split into words\n    words = text.split()\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10_words)\n    \n    # Create bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts)\n    \n    # Customize the plot\n    ax.set_title('Top 10 Most Common Words', fontsize=16)\n    ax.set_xlabel('Words', fontsize=12)\n    ax.set_ylabel('Frequency', fontsize=12)\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout and add count labels\n    plt.tight_layout()\n    for i, count in enumerate(counts):\n        ax.text(i, count, str(count), ha='center', va='bottom')\n    \n    return top_10_words, ax",
        "status": "fail",
        "details": {
          "test_empty_text": "Traceback (most recent call last):\n  File \"__test__.py\", line 51, in test_empty_text\n  File \"__test__.py\", line 24, in task_func\nValueError: not enough values to unpack (expected 2, got 0)\n"
        }
      }
    ],
    "BigCodeBench/1129": [
      {
        "task_id": "BigCodeBench/1129",
        "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n\n    pass\nimport json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON data provided: {e}\")\n\n    # Check if the key exists in the JSON data\n    if unknown_key not in data:\n        raise KeyError(f\"Key '{unknown_key}' not found in the provided JSON data.\")\n    \n    # Get the URL associated with the specified key\n    url = data[unknown_key]\n\n    # Download the file from the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Failed to download the file from URL: {e}\")\n\n    # Generate the timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the directory to save the file\n    if save_dir is None:\n        save_dir = os.getcwd()\n    else:\n        os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\n    # Create the full path for the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)",
        "status": "fail",
        "details": {
          "test_download_to_default_directory": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 98, in test_download_to_default_directory\n  File \"__test__.py\", line 29, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n",
          "test_download_with_direct_key": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 73, in test_download_with_direct_key\n  File \"__test__.py\", line 29, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n",
          "test_download_with_specified_directory": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 89, in test_download_with_specified_directory\n  File \"__test__.py\", line 29, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n",
          "test_multiple_downloads": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/unittest/mock.py\", line 1379, in patched\n    return func(*newargs, **newkeywargs)\n  File \"__test__.py\", line 109, in test_multiple_downloads\n  File \"__test__.py\", line 29, in task_func\nAttributeError: 'MockResponse' object has no attribute 'raise_for_status'\n"
        }
      }
    ],
    "BigCodeBench/1137": [
      {
        "task_id": "BigCodeBench/1137",
        "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n\n    pass\nimport bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    def extract_phone_numbers(text: str) -> list:\n        phone_pattern = re.compile(r'\\+?1?[-.\\s]?\\(?[2-9]\\d{2}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}')\n        return phone_pattern.findall(text)\n\n    try:\n        if url.startswith(('http://', 'https://')):\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            content = response.text\n        else:\n            with open(url, 'r', encoding='utf-8') as file:\n                content = file.read()\n\n        soup = bs4.BeautifulSoup(content, 'html.parser')\n        text = soup.get_text()\n        phone_numbers = extract_phone_numbers(text)\n\n        # Remove duplicates while preserving order\n        phone_numbers = list(dict.fromkeys(phone_numbers))\n\n        with open(output_path, 'w', encoding='utf-8') as json_file:\n            json.dump(phone_numbers, json_file, indent=2)\n\n        return phone_numbers\n\n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n    except IOError as e:\n        print(f\"Error reading file: {e}\")\n    except json.JSONDecodeError as e:\n        print(f\"Error writing JSON: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n    return []",
        "status": "fail",
        "details": {
          "test_phone_numbers_page_1": "Traceback (most recent call last):\n  File \"__test__.py\", line 75, in test_phone_numbers_page_1\n  File \"__test__.py\", line 101, in run_test_case\nAssertionError: Lists differ: [] != ['+1234567890']\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n'+1234567890'\n\n- []\n+ ['+1234567890']\n",
          "test_phone_numbers_page_2": "Traceback (most recent call last):\n  File \"__test__.py\", line 80, in test_phone_numbers_page_2\n  File \"__test__.py\", line 101, in run_test_case\nAssertionError: Lists differ: [] != ['+9876543210', '+1122334455']\n\nSecond list contains 2 additional elements.\nFirst extra element 0:\n'+9876543210'\n\n- []\n+ ['+9876543210', '+1122334455']\n",
          "test_phone_numbers_page_4": "Traceback (most recent call last):\n  File \"__test__.py\", line 90, in test_phone_numbers_page_4\n  File \"__test__.py\", line 101, in run_test_case\nAssertionError: Lists differ: [] != ['+919876543210']\n\nSecond list contains 1 additional elements.\nFirst extra element 0:\n'+919876543210'\n\n- []\n+ ['+919876543210']\n",
          "test_phone_numbers_page_5": "Traceback (most recent call last):\n  File \"__test__.py\", line 95, in test_phone_numbers_page_5\n  File \"__test__.py\", line 101, in run_test_case\nAssertionError: Lists differ: [] != ['+14151234567', '+13171234567']\n\nSecond list contains 2 additional elements.\nFirst extra element 0:\n'+14151234567'\n\n- []\n+ ['+14151234567', '+13171234567']\n",
          "test_phone_numbers_page_3": "Traceback (most recent call last):\n  File \"__test__.py\", line 85, in test_phone_numbers_page_3\n  File \"__test__.py\", line 102, in run_test_case\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpgmut4o7q/output_test_page_3.html'\n"
        }
      }
    ],
    "BigCodeBench/587": [
      {
        "task_id": "BigCodeBench/587",
        "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n\n    pass\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n\n    # Generate a random 256-bit AES key\n    aes_key = os.urandom(32)\n\n    # Read the file to be encrypted\n    with open(file_path, 'rb') as f:\n        plaintext = f.read()\n\n    # Pad the plaintext\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(plaintext) + padder.finalize()\n\n    # Generate a random IV\n    iv = os.urandom(16)\n\n    # Create AES cipher object\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Encrypt the padded plaintext\n    ciphertext = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Save the encrypted file (IV + ciphertext)\n    encrypted_file_path = file_path + '.encrypted'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(iv + ciphertext)\n\n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Save the encrypted AES key\n    encrypted_key_path = file_path + '.key'\n    with open(encrypted_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    return public_key, encrypted_file_path, encrypted_key_path",
        "status": "pass",
        "details": {}
      }
    ],
    "BigCodeBench/1040": [
      {
        "task_id": "BigCodeBench/1040",
        "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n\n    pass\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a non-blocking TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setblocking(False)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # Initialize socket lists and message queues\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    print(f\"Server started on {server_address}:{server_port}\")\n    print(f\"Running for {run_duration} seconds...\")\n\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n\n        for s in readable:\n            if s is server_socket:\n                # Handle new connection\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n                print(f\"New connection from {client_address}\")\n            else:\n                # Handle data from client\n                try:\n                    data = s.recv(buffer_size)\n                    if data:\n                        print(f\"Received {data.decode()} from {s.getpeername()}\")\n                        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                        response = f\"{data.decode()} {current_time}\".encode()\n                        message_queues[s].put(response)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        # Client closed connection\n                        print(f\"Closing connection from {s.getpeername()}\")\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n                except ConnectionResetError:\n                    print(f\"Connection reset by {s.getpeername()}\")\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                print(f\"Sending {next_msg.decode()} to {s.getpeername()}\")\n                s.send(next_msg)\n\n        for s in exceptional:\n            print(f\"Handling exceptional condition for {s.getpeername()}\")\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Clean up\n    server_socket.close()\n    total_connections = len(message_queues)\n    return f\"Server ran for {run_duration} seconds. Handled {total_connections} connection(s).\"",
        "status": "fail",
        "details": {
          "test_queue_empty_condition": "Traceback (most recent call last):\n  File \"__test__.py\", line 122, in test_queue_empty_condition\nConnectionRefusedError: [Errno 111] Connection refused\n"
        }
      }
    ],
    "BigCodeBench/187": [
      {
        "task_id": "BigCodeBench/187",
        "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n\n    pass\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate input dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' or 'Lat' keys are missing in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"The values of 'Lon' and 'Lat' must be tuples.\")\n\n    # Extract coordinate ranges\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    # Generate random coordinates\n    lons = np.random.uniform(lon_min, lon_max, len(cities))\n    lats = np.random.uniform(lat_min, lat_max, len(cities))\n\n    # Create Point objects for each city\n    coordinates = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n\n    # Create and return the GeoDataFrame\n    return gpd.GeoDataFrame({'City': cities, 'Coordinates': coordinates}, geometry='Coordinates')",
        "status": "timeout",
        "details": {}
      }
    ],
    "BigCodeBench/583": [
      {
        "task_id": "BigCodeBench/583",
        "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n\n    pass\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n    \n    # Serialize private key\n    private_key_pem = private_key.save_pkcs1()\n    \n    # Generate random encryption password and nonce for AES\n    encryption_password = get_random_bytes(32)  # 32 bytes for AES-256\n    nonce = get_random_bytes(12)  # 12 bytes nonce for AES-GCM\n    \n    # Encrypt private key using AES-GCM\n    cipher = AES.new(encryption_password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, tag = cipher.encrypt_and_digest(private_key_pem)\n    \n    # Generate filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # Save encrypted private key to file\n    with open(filename, 'wb') as file:\n        file.write(nonce + tag + ciphertext)\n    \n    return public_key, filename, encryption_password, nonce",
        "status": "fail",
        "details": {
          "test_key_size": "Traceback (most recent call last):\n  File \"__test__.py\", line 63, in test_key_size\nAssertionError: 2048 != 512\n",
          "test_encryption_decryption": "Traceback (most recent call last):\n  File \"__test__.py\", line 74, in test_encryption_decryption\n  File \"/usr/local/lib/python3.10/codecs.py\", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode bytes in position 0-1: invalid continuation byte\n",
          "test_file_content": "Traceback (most recent call last):\n  File \"__test__.py\", line 58, in test_file_content\n  File \"/usr/local/lib/python3.10/codecs.py\", line 322, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 3: invalid start byte\n"
        }
      }
    ]
  }
}