{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\nfrom typing import List\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test') -> List[str]:\n    try:\n        # Connect to the FTP server\n        with ftplib.FTP(ftp_server) as ftp:\n            try:\n                # Login to the FTP server\n                ftp.login(user=ftp_user, passwd=ftp_password)\n            except ftplib.error_perm as e:\n                raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n            try:\n                # Change to the specified directory\n                ftp.cwd(ftp_dir)\n            except ftplib.error_perm as e:\n                raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n            # List files in the directory\n            filenames = ftp.nlst()\n\n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    # Download each file using wget in a subprocess\n    for filename in filenames:\n        file_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        try:\n            subprocess.run(['wget', '-q', file_url], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download file {filename}: {str(e)}\")\n\n    return filenames"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Ensure the output directory exists\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    # Read commands from the CSV file\n    with open(commands_file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader):\n            if not row:\n                continue\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n            output_files.append(output_file_path)\n\n            try:\n                # Execute the command and capture the output\n                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                output_content = result.stdout\n            except subprocess.CalledProcessError as e:\n                # If the command fails, capture the error message and exit code\n                output_content = f\"Command failed with exit code {e.returncode}\\nError message:\\n{e.stderr}\"\n\n            # Write the output or error message to the output file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(output_content)\n\n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    try:\n        # Check if the process is running\n        for proc in psutil.process_iter(['name']):\n            if proc.info['name'] == process_name:\n                # Process found, terminate it\n                proc.terminate()\n                proc.wait(timeout=5)  # Wait up to 5 seconds for the process to terminate\n                time.sleep(1)  # Short delay to ensure process has fully terminated\n                \n                # Restart the process\n                subprocess.Popen(process_name)\n                return f\"Process found. Restarting {process_name}.\"\n        \n        # Process not found, start it\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    \n    except psutil.NoSuchProcess:\n        # Process terminated between finding and attempting to terminate it\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    except psutil.TimeoutExpired:\n        # Process didn't terminate in time, force kill it\n        for proc in psutil.process_iter(['name']):\n            if proc.info['name'] == process_name:\n                proc.kill()\n        subprocess.Popen(process_name)\n        return f\"Process found but unresponsive. Forcefully restarting {process_name}.\"\n    except subprocess.SubprocessError as e:\n        return f\"Error starting process {process_name}: {str(e)}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {str(e)}\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n\n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n\n    # If there are no files, return None\n    if not files:\n        return None\n\n    # Define the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n\n    # Create the zip file and add all files\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n\n    # Return the path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    cleaned_text = re.sub(r'http[s]?://\\S+', '', text)\n    \n    # Remove any remaining whitespace and check if there are words left\n    cleaned_text = cleaned_text.strip()\n    if not cleaned_text:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Train Random Forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n    \n    # Get feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n    \n    # Create and sort DataFrame of feature importances\n    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n    \n    # Create bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='importance', y='feature', data=feature_importance_df, palette='viridis')\n    \n    # Set labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return clf, ax"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\nusers = {\n    'testuser': User(1, 'testuser', 'password123')\n}\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = users.get(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n            return 'Invalid username or password'\n        return render_template('login.html', form=form)\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! This is a protected page.'\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('home'))\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Extract the specified column\n    col_data = data[column].values.reshape(-1, 1)\n    \n    # Standardize the column\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(col_data)\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n    \n    # Create a mask to remove outliers\n    mask = np.ones(len(data), dtype=bool)\n    mask[outliers] = False\n    \n    # Remove outliers\n    data_without_outliers = data[mask].copy()\n    \n    # Visualize the data before and after outlier removal\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Data with Outliers\n    ax1.scatter(range(len(data)), data[column], alpha=0.7)\n    ax1.set_title('Data with Outliers')\n    ax1.set_xlabel('Index')\n    ax1.set_ylabel(column)\n    \n    # Data without Outliers\n    ax2.scatter(range(len(data_without_outliers)), data_without_outliers[column], alpha=0.7)\n    ax2.set_title('Data without Outliers')\n    ax2.set_xlabel('Index')\n    ax2.set_ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_without_outliers, outliers.tolist()"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pd.DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1.\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    centroids = kmeans.cluster_centers_\n\n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot data points\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis')\n    \n    # Plot centroids\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n    \n    # Customize plot\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    ax.set_title(f'K-means Clustering (k={n_clusters})')\n    ax.legend()\n\n    # Add colorbar\n    plt.colorbar(scatter, label='Cluster')\n\n    return cluster_labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Create DataFrame with transformed data\n    columns = [f\"PC{i+1}\" for i in range(n_components)]\n    df_transformed = pd.DataFrame(transformed_data, columns=columns)\n\n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(df_transformed['PC1'], df_transformed['PC2'])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_title('PCA Scatter Plot (First 2 Components)')\n        \n        if n_components > 2:\n            plt.figtext(0.5, 0.01, f\"Note: Only showing first 2 of {n_components} components\", \n                        ha=\"center\", fontsize=9, style='italic')\n    else:\n        ax.text(0.5, 0.5, \"Scatter plot requires at least 2 components\", \n                ha='center', va='center', fontsize=12)\n        ax.axis('off')\n\n    return df_transformed, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate a pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species', height=2.5)\n\n    # Set the plot title\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16, y=1.02)\n\n    # Customize axis labels\n    for i, feature in enumerate(iris.feature_names):\n        for ax in pair_plot.axes[i]:\n            ax.set_xlabel(feature)\n            ax.set_ylabel(feature)\n\n    # Adjust layout to prevent clipping of title\n    plt.tight_layout()\n\n    return pair_plot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=29)\n        dates = pd.date_range(start=start_date, end=end_date)\n\n        # Generate random time series data\n        data = [random.uniform(0, 100) for _ in range(len(dates))]\n        time_series = pd.Series(data, index=dates)\n\n        # Create the plot\n        plt.figure(figsize=(10, 6))\n        ax = time_series.plot(kind='line', marker='o', linestyle='-', color='b')\n\n        # Styling the plot\n        plt.xlabel('Date', fontname='Arial', fontsize=12)\n        plt.ylabel('Value', fontname='Arial', fontsize=12)\n        plt.title('Random Time Series Data', fontname='Arial', fontsize=14)\n        plt.xticks(rotation=45, ha='right', fontname='Arial', fontsize=10)\n        plt.yticks(fontname='Arial', fontsize=10)\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.tight_layout()\n\n        # Return the Axes object\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the data or plot: {e}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Set random seed for reproducibility\n        np.random.seed(seed)\n\n        # Load the dataset\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n        \n        columns = [\n            'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n            'TAX', 'PTRATIO', 'B', 'LSTAT'\n        ]\n        \n        # Create DataFrame\n        df = pd.DataFrame(data, columns=columns)\n        df['MEDV'] = target\n        \n        # Compute correlation matrix\n        corr_matrix = df.corr()\n        \n        # Create figure and axes\n        fig, ax = plt.subplots(figsize=(12, 10))\n        \n        # Plot heatmap\n        sns.heatmap(corr_matrix, ax=ax, annot=True, fmt=\".2f\", cmap='coolwarm',\n                    cbar=True, square=True, linewidths=.5, annot_kws={\"size\": 8})\n        \n        plt.title(\"Correlation Heatmap of Boston Housing Dataset\")\n        plt.tight_layout()\n        \n        # Save the plot if save_path is provided\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating or saving the plot: {e}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n\n    required_columns = {'date', 'value'}\n    if not required_columns.issubset(df.columns):\n        raise ValueError(f\"Input 'df' must contain the columns: {required_columns}\")\n\n    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n        raise ValueError(\"The 'date' column must be of datetime type.\")\n\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must be numeric.\")\n\n    # Validate frequency\n    valid_freqs = {'B', 'C', 'D', 'W', 'M', 'Q', 'Y', 'H', 'T', 'S', 'L', 'U'}\n    if freq not in valid_freqs:\n        raise ValueError(f\"Invalid frequency string '{freq}'. Valid options are {valid_freqs}.\")\n\n    # Validate decomposition model\n    if decomposition_model not in {'additive', 'multiplicative'}:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n\n    # Set the date column as the index\n    df = df.set_index('date')\n\n    # Perform the decomposition\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.Timedelta(freq).days)\n\n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n    \n    decomposition_result.observed.plot(ax=axes[0])\n    axes[0].set_ylabel('Observed')\n    axes[0].set_title('Time Series Decomposition')\n    \n    decomposition_result.trend.plot(ax=axes[1])\n    axes[1].set_ylabel('Trend')\n    \n    decomposition_result.seasonal.plot(ax=axes[2])\n    axes[2].set_ylabel('Seasonal')\n    \n    decomposition_result.resid.plot(ax=axes[3])\n    axes[3].set_ylabel('Residual')\n    \n    plt.tight_layout()\n\n    return (decomposition_result, axes)"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Input validation\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' cannot be later than 'end_date'.\")\n\n    # Set seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the number of days in the range (inclusive)\n    num_days = (end_date - start_date).days + 1\n\n    # Generate random dates\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    # Create and return a pandas Series\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The input 'my_list' must be a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append('12')\n    \n    # Calculate the sum of numeric elements in the list\n    total_files = sum(int(x) for x in my_list if str(x).isdigit())\n    \n    # Find all CSV files in the specified directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    csv_files = glob.glob(file_pattern)\n    \n    # Raise FileNotFoundError if no files are found\n    if not csv_files:\n        raise FileNotFoundError(f\"No {file_ext} files found in the directory: {file_dir}\")\n    \n    # Select the number of files to concatenate\n    files_to_concat = csv_files[:total_files]\n    \n    # Read and concatenate the selected CSV files\n    dataframes = [pd.read_csv(file) for file in files_to_concat]\n    concatenated_df = pd.concat(dataframes, ignore_index=True)\n    \n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Input validation\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in 'my_list' must be numeric (int or float).\")\n\n    # Enhance my_list\n    my_list.append(12)\n\n    # Calculate sum and determine size of random numbers list\n    total_sum = sum(my_list)\n    num_count = min(total_sum, size)\n\n    # Set random seed\n    random_seed(seed)\n\n    # Generate random numbers and measure time\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(num_count)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black', align='left')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Generated Random Numbers')\n    ax.set_xlim(0, 101)\n    ax.set_xticks(range(0, 101, 10))\n\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        # Send GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find the first table in the HTML\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page.\")\n        \n        # Extract table headers\n        headers = [th.get_text(strip=True) for th in table.find_all('th')]\n        \n        # Extract table rows\n        rows = []\n        for tr in table.find_all('tr'):\n            row = [td.get_text(strip=True) for td in tr.find_all('td')]\n            if row:  # Ignore empty rows\n                rows.append(row)\n        \n        if not rows:\n            raise ValueError(\"No data found in the table.\")\n        \n        # Create DataFrame\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n        return df\n\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the URL.\")\n    except requests.HTTPError as http_err:\n        raise requests.HTTPError(f\"HTTP request failed: {http_err}\")\n    except Exception as e:\n        raise ValueError(f\"Failed to parse the page content: {str(e)}\")"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n\n    # Select only the numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number])\n\n    # Raise an exception if there are no numeric columns\n    if numeric_cols.empty:\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    # Create a list to store the Axes objects\n    axes_list = []\n\n    # Plot histograms for each numeric column\n    for col in numeric_cols.columns:\n        fig, ax = plt.subplots()\n        ax.hist(df[col].dropna(), bins=30, edgecolor='black')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n\n    return axes_list"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    def scan_ip(ip, port, results):\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                sock.settimeout(1)\n                result = sock.connect_ex((str(ip), port))\n                results[str(ip)] = (result == 0)\n        except:\n            results[str(ip)] = False\n\n    network = IPv4Network(ip_range)\n    results = {}\n    threads = []\n\n    for ip in network:\n        thread = Thread(target=scan_ip, args=(ip, port, results))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Define regex pattern to extract log information\n    log_pattern = re.compile(r'^(INFO|DEBUG|WARNING|ERROR|CRITICAL): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)$')\n    \n    log_entries = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line.strip())\n            if match:\n                log_type, timestamp_str, message = match.groups()\n                try:\n                    # Validate timestamp\n                    datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                    log_entries.append((log_type, timestamp_str, message))\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp: {timestamp_str} in log entry: {line.strip()}\")\n\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Create DataFrame from log entries\n    df = pd.DataFrame(log_entries, columns=['Type', 'Timestamp', 'Message'])\n\n    # Save DataFrame to CSV\n    csv_file_path = 'structured_logs.csv'\n    df.to_csv(csv_file_path, index=False)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Tokenize the text to extract words\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    if not word_lengths:\n        # If there are no words, return an empty plot\n        ax.set_title(\"Distribution of Word Lengths\")\n        ax.set_xlabel(\"Word Length\")\n        ax.set_ylabel(\"Frequency\")\n        ax.text(0.5, 0.5, \"No words found\", ha='center', va='center', transform=ax.transAxes)\n        return ax\n    \n    # Calculate the range for bins\n    max_length = max(word_lengths)\n    bins = range(1, max_length + 2)  # +2 to include the max length in its own bin\n    \n    # Create the histogram\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, align='left', edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel(\"Word Length\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title(\"Distribution of Word Lengths\")\n    \n    # Set x-axis ticks to show only integer values\n    ax.set_xticks(range(1, max_length + 1))\n    \n    # Add grid for better readability\n    ax.grid(axis='y', linestyle='--', alpha=0.7)\n    \n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n    \n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nimport pandas as pd\nfrom collections import defaultdict\ndef task_func(df):\n    # Check if DataFrame is empty or does not contain the necessary columns\n    if df.empty or not {'Title', 'Content'}.issubset(df.columns):\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Define case-insensitive keywords\n    keywords = [\"like\", \"what\"]\n    pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n\n    # Filter the DataFrame to extract articles whose titles contain the specific keywords\n    filtered_df = df[df['Title'].apply(lambda title: bool(pattern.search(title)))]\n\n    # Initialize a dictionary to store word frequencies\n    word_freq = defaultdict(int)\n\n    # Define a regex pattern to remove punctuation\n    punc_pattern = re.compile(f\"[{re.escape(punctuation)}]\")\n\n    # Download the 'punkt' tokenizer if not already available\n    nltk.download('punkt', quiet=True)\n\n    # Tokenize and count word frequencies for the filtered articles\n    for content in filtered_df['Content']:\n        # Remove punctuation from the content\n        content_wo_punctuation = punc_pattern.sub('', content)\n        # Tokenize the content into words\n        words = nltk.word_tokenize(content_wo_punctuation)\n        # Update the word frequency dictionary\n        for word in words:\n            # Convert word to lowercase to ensure case-insensitivity\n            word = word.lower()\n            if word:  # Ensure the word is not empty\n                word_freq[word] += 1\n\n    # Convert the defaultdict to a regular dictionary for the final output\n    return dict(word_freq)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\ndef task_func(dataframe, text_column):\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    \n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the preprocessed text\n    text_matrix = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the resulting matrix to a DataFrame\n    word_count_df = pd.DataFrame(text_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return word_count_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate input dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' or 'Lat' keys are missing in the dictionary.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"The values of 'Lon' and 'Lat' must be tuples.\")\n\n    # Extract coordinate ranges\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n\n    # Generate random coordinates\n    lons = np.random.uniform(lon_min, lon_max, len(cities))\n    lats = np.random.uniform(lat_min, lat_max, len(cities))\n\n    # Create Point objects for each city\n    coordinates = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n\n    # Create and return the GeoDataFrame\n    return gpd.GeoDataFrame({'City': cities, 'Coordinates': coordinates}, geometry='Coordinates')"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate input parameters\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n\n    # Set random seed for reproducibility\n    set_seed(seed)\n\n    # Initialize list to store weather data\n    weather_data = []\n\n    # Generate weather report for each city\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone not provided for city: {city}\")\n\n        # Convert UTC time to local time\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n\n        # Format local time as required\n        formatted_time = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n\n        # Randomly select weather condition\n        weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Add data to list\n        weather_data.append({\n            'City': city,\n            'Local Time': formatted_time,\n            'Weather Condition': weather\n        })\n\n    # Create and return DataFrame\n    return pd.DataFrame(weather_data)"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    # Input validation\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate the random walk\n    walk = np.cumsum(steps)\n\n    # Calculate descriptive statistics\n    stats = {\n        \"count\": elements,\n        \"mean\": np.mean(walk),\n        \"standard deviation\": np.std(walk),\n        \"minimum\": np.min(walk),\n        \"5th percentile\": np.percentile(walk, 5),\n        \"25th percentile\": np.percentile(walk, 25),\n        \"median\": np.median(walk),\n        \"75th percentile\": np.percentile(walk, 75),\n        \"95th percentile\": np.percentile(walk, 95),\n        \"maximum\": np.max(walk)\n    }\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(range(elements), walk)\n    ax.set_title(\"Random Walk\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Position\")\n    ax.grid(True)\n\n    # Show the plot (can be commented out if not needed)\n    plt.show()\n\n    return stats, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers, stream=True)\n    response.raise_for_status()\n\n    # Get the filename from the URL or Content-Disposition header\n    filename = os.path.basename(url)\n    if 'Content-Disposition' in response.headers:\n        content_disposition = response.headers['Content-Disposition']\n        if 'filename=' in content_disposition:\n            filename = content_disposition.split('filename=')[1].strip('\"')\n\n    # Save the zip file\n    zip_path = os.path.join(destination_directory, filename)\n    with open(zip_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    # Extract the contents\n    extracted_files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        for file in zip_ref.namelist():\n            zip_ref.extract(file, destination_directory)\n            extracted_files.append(file)\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set random seeds for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate random RGB image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis labels\n\n    # Show the plot\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n\n    # Convert list L to MxN matrix\n    if len(L) != M * N:\n        raise ValueError(\"The length of list L is not equal to M * N\")\n    matrix = np.array(L).reshape(M, N)\n\n    # Read audio data from the file\n    data, sample_rate = sf.read(audio_file)\n\n    # Ensure data is mono\n    if data.ndim > 1:\n        data = data.mean(axis=1)\n\n    # Calculate SPL\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Normalize the matrix\n    normalized_matrix = matrix / np.linalg.norm(matrix)\n\n    # Adjust the amplitude of the normalized matrix using SPL\n    adjusted_matrix = normalized_matrix * 10**(spl / 20)\n\n    # Generate the spectrogram\n    S = librosa.stft(adjusted_matrix.ravel())\n    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n\n    # Create the spectrogram plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img = librosa.display.specshow(S_db, x_axis='time', y_axis='hz', ax=ax, sr=sample_rate)\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    ax.set_title('Spectrogram')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency (Hz)')\n\n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([value for tup in original for value in tup if isinstance(value, (int, float))])\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='skyblue', edgecolor='black')\n    \n    # Calculate and plot PDF\n    kde = stats.gaussian_kde(numeric_values)\n    x_range = np.linspace(numeric_values.min(), numeric_values.max(), 100)\n    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='PDF')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with Overlaid PDF')\n    ax.legend()\n    \n    return numeric_values, statistics, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using Min-Max scaling\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot original data\n    ax.plot(original_array, marker='o', linestyle='-', label='Original Data')\n    \n    # Plot normalized data\n    ax.plot(normalized_array, marker='s', linestyle='--', label='Normalized Data')\n    \n    # Customize the plot\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Original vs Normalized Data')\n    ax.legend()\n    ax.grid(True, linestyle=':', alpha=0.7)\n    \n    # Add value annotations\n    for i, (orig, norm) in enumerate(zip(original_array, normalized_array)):\n        ax.annotate(f'{orig:.2f}', (i, orig), textcoords=\"offset points\", xytext=(0,10), ha='center')\n        ax.annotate(f'{norm:.2f}', (i, norm), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    time = np.linspace(0, 1, sample_rate, endpoint=False)\n    signal = np.zeros(sample_rate)\n    \n    for key, value in data.items():\n        if isinstance(value, (int, float)):\n            try:\n                freq = float(key) if key != 'a' else 1.0\n                signal += value * np.sin(2 * np.pi * freq * time)\n            except ValueError:\n                pass  # Skip non-numeric keys\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    frequencies = fftpack.fftfreq(sample_rate, 1/sample_rate)\n\n    # 4. Plot the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(frequencies[:sample_rate//2], np.abs(fft_result)[:sample_rate//2])\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('FFT of the Signal')\n    plt.tight_layout()\n\n    return fft_result, ax"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\ndef task_func():\n    class CustomHTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n        def _send_json_response(self, status_code, response_data):\n            self.send_response(status_code)\n            self.send_header('Content-Type', 'application/json')\n            response_json = json.dumps(response_data)\n            self.send_header('Content-Length', len(response_json))\n            self.end_headers()\n            self.wfile.write(response_json.encode('utf-8'))\n\n        def do_POST(self):\n            content_type = self.headers.get('Content-Type')\n            \n            if content_type != 'application/json':\n                self._send_json_response(400, {\n                    'status': 'error',\n                    'message': 'Content-Type header is not application/json'\n                })\n                return\n\n            content_length = int(self.headers.get('Content-Length', 0))\n            \n            try:\n                post_data = self.rfile.read(content_length)\n                json_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self._send_json_response(400, {\n                    'status': 'error',\n                    'message': 'Invalid JSON'\n                })\n                return\n\n            if 'data' not in json_data:\n                self._send_json_response(400, {\n                    'status': 'error',\n                    'message': 'No data key in request'\n                })\n                return\n\n            self._send_json_response(200, SUCCESS_RESPONSE)\n\n    return CustomHTTPRequestHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers.get('Content-Length', 0))\n            \n            try:\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data.decode('utf-8'))\n\n                if not all(key in email_data for key in ['subject', 'message', 'to']):\n                    raise ValueError(\"Missing required fields: 'subject', 'message', or 'to'\")\n\n                subject = email_data['subject']\n                message = email_data['message']\n                to_address = email_data['to']\n\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_address\n\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n\n                response = json.dumps({'status': 'success', 'message': 'Email sent successfully'})\n                self.send_response(200)\n                self.send_header('Content-type', 'application/json')\n                self.send_header('Content-length', len(response))\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Bad Request: Invalid JSON\")\n            except ValueError as ve:\n                self.send_error(400, f\"Bad Request: {str(ve)}\")\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication Failed\")\n            except Exception as e:\n                self.send_error(500, f\"Internal Server Error: {str(e)}\")\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counts = Counter()\n    total_word_count = 0\n\n    # Traverse the directory and process .txt files\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    words = content.split()\n                    word_count = len(words)\n                    word_counts[file_path] = word_count\n                    total_word_count += word_count\n\n    # Export word counts to JSON file\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(dict(word_counts), f, ensure_ascii=False, indent=4)\n\n    return total_word_count"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ndef task_func(df, plot=False):\n    # Validate input DataFrame\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    if 'Value' not in df.columns:\n        raise ValueError(\"The input DataFrame does not contain the 'Value' column.\")\n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"The 'Value' column contains invalid data. All entries must be lists.\")\n\n    # Split lists in 'Value' column into separate columns\n    expanded_df = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Calculate Pearson correlation coefficient\n    corr_matrix = expanded_df.corr(method='pearson')\n    \n    if plot:\n        # Create heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n        ax.set_title(\"Correlation Heatmap\")\n        plt.tight_layout()\n        return corr_matrix, ax\n    \n    return corr_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n    \n    # Create DataFrame from the data dictionary\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # Calculate Average Grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate average grade for each subject\n    subject_averages = df[all_fields].mean()\n    \n    # Add 'Average' row to the DataFrame\n    df.loc['Average'] = subject_averages.tolist() + [df['Average Grade'].mean()]\n    \n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Ensure directory exists\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    \n    # Generate data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{_ + 1}\"\n        age = random.randint(18, 80)\n        height = round(random.uniform(150, 200), 2)  # cm\n        weight = round(random.uniform(45, 120), 2)  # kg\n        data.append([name, age, height, weight])\n    \n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 2)\n    avg_height = round(mean(row[2] for row in data), 2)\n    avg_weight = round(mean(row[3] for row in data), 2)\n    \n    # Write to CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n        writer.writerow(['Average', avg_age, avg_height, avg_weight])\n    \n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The provided path '{directory}' is not a valid directory.\")\n\n    moved_files = {}\n    pattern = re.compile(r'^([^[\\]]+)')\n\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            match = pattern.search(filename)\n            if match:\n                subdirectory_name = match.group(1).strip()\n                if subdirectory_name:\n                    subdirectory_path = os.path.join(directory, subdirectory_name)\n                    os.makedirs(subdirectory_path, exist_ok=True)\n                    \n                    new_file_path = os.path.join(subdirectory_path, filename)\n                    shutil.move(file_path, new_file_path)\n                    \n                    if subdirectory_name not in moved_files:\n                        moved_files[subdirectory_name] = []\n                    moved_files[subdirectory_name].append(filename)\n\n    return directory, moved_files"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    def run_subprocess(file, results, index):\n        try:\n            process = subprocess.Popen(['python', file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            process.wait()\n            results[index] = process.returncode\n        except Exception as e:\n            print(f\"Error running {file}: {str(e)}\")\n            results[index] = -1\n\n    threads = []\n    results = [None] * len(file_list)\n\n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(file, results, i))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    results = []\n    \n    # Find all .bat files in the given directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Run the .bat file and capture the exit code\n            process = subprocess.run([bat_file], shell=True, check=False, capture_output=True)\n            exit_code = process.returncode\n        except Exception:\n            # If the file couldn't be executed, set exit code to None\n            exit_code = None\n        \n        results.append((file_name, exit_code))\n    \n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input DataFrame must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' does not exist in the DataFrame\")\n\n    # Create figure and subplots\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n\n    # Histogram with KDE for numerical data\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(data=df, x=col, kde=True, ax=ax1)\n    else:\n        sns.histplot(data=df, x=col, ax=ax1)\n    ax1.set_title(f'Histogram of {col}')\n    ax1.set_xlabel(col)\n    ax1.set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(data=df, x=col, ax=ax2)\n    ax2.set_title(f'Box Plot of {col}')\n    ax2.set_xlabel(col)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script '{script_path}' does not exist.\")\n\n    # Construct the command to run the script\n    command = [sys.executable, script_path] + list(args)\n\n    try:\n        # Start the subprocess\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        if wait:\n            # Wait for the process to complete\n            stdout, stderr = process.communicate()\n            return_code = process.returncode\n\n            if return_code != 0:\n                raise subprocess.CalledProcessError(return_code, command, output=stdout, stderr=stderr)\n\n            return return_code\n        else:\n            # Do not wait for the process to complete\n            return None\n\n    except subprocess.CalledProcessError:\n        # Re-raise the CalledProcessError\n        raise\n    except Exception as e:\n        # For any other exception, raise a CalledProcessError\n        raise subprocess.CalledProcessError(-1, command, str(e))"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n\n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\")\n\n    # Calculate mean and standard deviation for each column\n    stats = {}\n    for column in df.columns:\n        if pd.api.types.is_numeric_dtype(df[column]):\n            stats[column] = {\n                'mean': df[column].mean(),\n                'std': df[column].std()\n            }\n\n    # Prepare data for the bar chart\n    columns = list(stats.keys())\n    means = [stats[col]['mean'] for col in columns]\n    stds = [stats[col]['std'] for col in columns]\n\n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    x = np.arange(len(columns))\n    width = 0.35\n\n    ax.bar(x - width/2, means, width, label='Mean', color='skyblue')\n    ax.bar(x + width/2, stds, width, label='Standard Deviation', color='lightgreen')\n\n    # Customize the chart\n    ax.set_xlabel('Columns', fontsize=12)\n    ax.set_ylabel('Values', fontsize=12)\n    ax.set_title('Mean and Standard Deviation', fontsize=14)\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns, rotation=45, ha='right')\n    ax.legend()\n\n    # Adjust layout and display\n    fig.tight_layout()\n\n    return stats, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Check if all elements in activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n    \n    # Count activities for each day of the week\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n    \n    # Prepare data for plotting\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_counts[day] for day in days]\n    \n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(days, counts, color='skyblue', edgecolor='navy')\n    \n    # Customize the chart\n    ax.set_xlabel('Day of the Week', fontsize=12)\n    ax.set_ylabel('Number of Activities', fontsize=12)\n    ax.set_title('Weekly Activity', fontsize=14, fontweight='bold')\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    \n    # Add value labels on top of each bar\n    for i, count in enumerate(counts):\n        ax.text(i, count, str(count), ha='center', va='bottom')\n    \n    # Adjust layout and improve appearance\n    plt.tight_layout()\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    \n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Get list of files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n\n    # Check if there are files in the source directory\n    if not files:\n        raise ValueError(\"No files found in the source directory.\")\n\n    # Choose a random file\n    random_file = random.choice(files)\n\n    # Construct full paths\n    src_path = os.path.join(src_dir, random_file)\n    dest_path = os.path.join(dest_dir, random_file)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the moved file\n    return random_file"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Counter for processed files\n    processed_files = 0\n    \n    # Compile regex pattern for double quotes\n    quote_pattern = re.compile(r'\"')\n    \n    for file_path in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(filename=file_path)\n        \n        # Process each sheet in the workbook\n        for sheet in workbook.worksheets:\n            for row in sheet.iter_rows():\n                for cell in row:\n                    if isinstance(cell.value, str):\n                        # Replace double quotes with escaped double quotes\n                        cell.value = quote_pattern.sub(r'\\\\\"', cell.value)\n        \n        # Save the modified workbook\n        workbook.save(file_path)\n        processed_files += 1\n    \n    return processed_files"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    # Validate input parameters\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Generate time values\n    t = np.linspace(0, 1, sample_size)\n\n    # Calculate sine and cosine waves\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave', color='blue')\n    ax.plot(t, cosine_wave, label='Cosine Wave', color='red')\n\n    # Set title and labels\n    ax.set_title(f'Sine and Cosine Waves (Frequency: {frequency} Hz)')\n    ax.set_xlabel('Time (s)')\n    ax.set_ylabel('Amplitude')\n\n    # Add legend and grid\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    # Set y-axis limits for better visualization\n    ax.set_ylim(-1.1, 1.1)\n\n    # Return the figure and axes objects\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables or use defaults\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    mail_port = int(os.environ.get('MAIL_PORT', 25))\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', '').lower() in ('true', '1', 'yes', 'on')\n    mail_username = os.environ.get('MAIL_USERNAME')\n    mail_password = os.environ.get('MAIL_PASSWORD')\n\n    # Configure Flask app with mail settings\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Return tuple with Flask-Mail instance and mail configurations\n    return mail, app.config"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The Excel file does not exist at the specified path: {full_path}\")\n    \n    # Read the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise FileNotFoundError(f\"Error reading the Excel file: {e}\")\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not found in the Excel file.\")\n    \n    # Calculate mean, median, and standard deviation\n    mean_value = df[column_name].mean()\n    median_value = df[column_name].median()\n    std_dev_value = df[column_name].std()\n    \n    # Return the results in a dictionary\n    return {\n        \"mean\": mean_value,\n        \"median\": median_value,\n        \"standard_deviation\": std_dev_value\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training set (75%) and test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential([\n        Dense(10, input_dim=2, activation='sigmoid'),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer with a specified learning rate\n    model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy')\n\n    # Fit the model to the training data, also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the model's training and validation loss over epochs\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    return model, ax"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(10, activation='sigmoid', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer specifying a learning rate\n    model.compile(loss='binary_crossentropy', \n                  optimizer=keras.optimizers.SGD(learning_rate=0.01), \n                  metrics=['accuracy'])\n\n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_pred = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.set_title('ROC curve')\n    ax.legend(loc=\"lower right\")\n\n    return model, ax"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer.\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Failed to read the image from the path: {image_path}\")\n\n    # Convert the image from BGR to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixels = image_rgb.reshape((-1, 3))\n\n    # Convert to float type for K-means\n    pixels_float = np.float32(pixels)\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels_float)\n\n    # Get the labels and cluster centers\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n\n    # Convert back to 8-bit values\n    centers = np.uint8(centers)\n\n    # Map each pixel to its corresponding cluster center\n    segmented_image = centers[labels.flatten()]\n\n    # Reshape the segmented image back to the original shape\n    segmented_image = segmented_image.reshape(image_rgb.shape)\n\n    # Save each region as a separate image\n    for i in range(n_clusters):\n        mask = labels == i\n        region = np.zeros_like(image_rgb)\n        region[mask.reshape(image_rgb.shape[:2])] = image_rgb[mask.reshape(image_rgb.shape[:2])]\n        cv2.imwrite(f'region_{i}.png', cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n\n    return image_rgb, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Validate input dimensions\n    if P.ndim != 2 or T.ndim != 3:\n        raise ValueError(\"P must be a 2D matrix and T must be a 3D tensor.\")\n    if P.shape[1] != T.shape[2]:\n        raise ValueError(\"The number of columns in P must match the last dimension of T.\")\n\n    # Calculate the product of P and T\n    result = np.dot(T.reshape(-1, T.shape[2]), P.T).reshape(T.shape[0], T.shape[1], P.shape[0])\n\n    # Flatten the result\n    flattened_result = result.reshape(-1, P.shape[0])\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_result)\n\n    # Visualize the clustering result\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis')\n    legend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n    ax.add_artist(legend1)\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Input validation\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input points must be a numpy array\")\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Points must be a 2D array with shape (n, 2)\")\n    \n    if points.size == 0:\n        raise ValueError(\"Points array must not be empty\")\n\n    # Set random seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Apply jittering to the points\n    jitter = rng.normal(scale=1e-6, size=points.shape)\n    jittered_points = points + jitter\n    \n    # Compute the Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=2)\n    \n    # Plot the original points\n    ax.plot(points[:, 0], points[:, 1], 'ko', markersize=5)\n    \n    # Set plot limits and labels\n    ax.set_xlim(points[:, 0].min() - 0.1, points[:, 0].max() + 0.1)\n    ax.set_ylim(points[:, 1].min() - 0.1, points[:, 1].max() + 0.1)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Voronoi Diagram')\n    \n    # Return the Voronoi object and the plot axes\n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if the source directory exists\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    \n    # Check if the destination directory exists\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n    \n    # Ensure the extension starts with a dot\n    if not ext.startswith('.'):\n        ext = '.' + ext\n    \n    # Create the search pattern for the specified extension\n    search_pattern = os.path.join(src_dir, f\"*{ext}\")\n    \n    # Find all files in the source directory that match the specified extension\n    files_to_move = glob.glob(search_pattern)\n    \n    # List to store the full paths of files that were successfully moved\n    moved_files = []\n    \n    for file_path in files_to_move:\n        # Extract the file name from the full path\n        file_name = os.path.basename(file_path)\n        \n        # Create the destination path for the file\n        dest_path = os.path.join(dest_dir, file_name)\n        \n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_path):\n            # Move the file to the destination directory\n            shutil.move(file_path, dest_path)\n            # Add the destination path to the list of moved files\n            moved_files.append(dest_path)\n    \n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    def double_numeric(value):\n        if isinstance(value, (int, float)):\n            return float(value * 2)\n        elif isinstance(value, str):\n            match = re.match(r'^-?\\d+(\\.\\d+)?$', value)\n            if match:\n                return float(float(value) * 2)\n        elif isinstance(value, list):\n            return [double_numeric(item) for item in value]\n        return value\n\n    try:\n        data_dict = json.loads(json_str)\n        if not isinstance(data_dict, dict):\n            return pd.DataFrame()\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    normalized_dict = {k: double_numeric(v) for k, v in data_dict.items()}\n    \n    try:\n        df = pd.DataFrame([normalized_dict])\n        return df\n    except ValueError:\n        return pd.DataFrame()"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path '{script_path}' does not exist.\")\n\n    accumulated_cpu_usage = 0.0\n    accumulated_memory_usage = 0\n\n    try:\n        process = subprocess.Popen(['bash', script_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        start_time = time.time()\n\n        while process.poll() is None:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                process.wait()\n                raise TimeoutError(f\"Script execution exceeded the {timeout} second timeout.\")\n\n            try:\n                proc = psutil.Process(process.pid)\n                cpu_percent = proc.cpu_percent(interval=1)\n                memory_info = proc.memory_info()\n\n                accumulated_cpu_usage += cpu_percent\n                accumulated_memory_usage += memory_info.rss\n\n            except (psutil.NoSuchProcess, psutil.ZombieProcess):\n                break\n\n    except Exception as e:\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n        raise e\n\n    finally:\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n\n    return {\n        'CPU Usage': accumulated_cpu_usage,\n        'Memory Usage': accumulated_memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate categories\n    if N >= len(CATEGORIES):\n        # Ensure each category appears at least once\n        categories = np.array(CATEGORIES + list(np.random.choice(CATEGORIES, N - len(CATEGORIES))))\n        np.random.shuffle(categories)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=pd.Categorical(df['category']).codes, cmap='viridis')\n    \n    # Add legend\n    legend = ax.legend(*scatter.legend_elements(), title=\"Category\")\n    ax.add_artist(legend)\n    \n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter plot of x vs y, colored by category')\n    \n    return df, ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generate and plot a time series with specified parameters.\n\n    Parameters:\n    start_time (str): Start time in 'YYYY-MM-DD HH:MM:SS' format.\n    end_time (str): End time in 'YYYY-MM-DD HH:MM:SS' format.\n    step (str): Time step (e.g., '1H' for hourly, '1D' for daily).\n    trend (float): Linear trend to add per time step.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    matplotlib.pyplot.Axes: The Axes object of the generated plot.\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Create date range\n    date_range = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate random values from normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(date_range))\n\n    # Add linear trend\n    trend_component = np.arange(len(date_range)) * trend\n    values += trend_component\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': date_range, 'Value': values})\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n\n    # Customize plot\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n\n    # Adjust layout to prevent cutoff\n    plt.tight_layout()\n\n    return ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Input validation\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    end_date = datetime.now()\n\n    # Initialize list to store sales data\n    sales_data = []\n\n    # Generate sales data for each day and product\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({\n                'Product': product,\n                'Date': current_date,\n                'Sales': sales\n            })\n        current_date += timedelta(days=1)\n\n    # Create DataFrame from sales data\n    df = pd.DataFrame(sales_data)\n\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is a valid type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n    \n    # Convert JSON data to a DataFrame\n    if isinstance(data, list) and not data:\n        # Handle empty JSON array\n        df = pd.DataFrame()\n    else:\n        df = pd.DataFrame(data)\n    \n    # Write DataFrame to Excel file\n    try:\n        with pd.ExcelWriter(filename, engine='xlwt') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(f\"Error writing Excel file: {e}\")\n    \n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random data\n    data = []\n    for date in date_range:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append({\n                'Date': date.strftime('%Y-%m-%d'),\n                'Activity': activity,\n                'Duration': duration\n            })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Customize plot\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices\n    stock_data = {stock: np.random.random(days_in_past) for stock in stock_names}\n    \n    # Create DataFrame\n    df = pd.DataFrame(stock_data, index=date_range)\n    \n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the contents of both files\n        with open(file_path1, 'r', newline='', encoding='utf-8') as file1:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            lines1 = list(reader1)\n\n        with open(file_path2, 'r', newline='', encoding='utf-8') as file2:\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            lines2 = list(reader2)\n\n        # Check if either file is empty\n        if not lines1:\n            raise ValueError(f\"File {file_path1} is empty.\")\n        if not lines2:\n            raise ValueError(f\"File {file_path2} is empty.\")\n\n        # Convert lines to strings for comparison\n        lines1_str = [delimiter.join(line) for line in lines1]\n        lines2_str = [delimiter.join(line) for line in lines2]\n\n        # Generate the diff\n        diff = list(ndiff(lines1_str, lines2_str))\n\n        # Prepare the difference report\n        differences = []\n        for line_num, line in enumerate(diff, start=1):\n            status = line[0]\n            content = line[2:]\n            if status in ('-', '+', ' '):\n                differences.append({\n                    'Line Number': line_num,\n                    'Status': status,\n                    'Content': content.strip()\n                })\n\n        # Create and return the DataFrame\n        return pd.DataFrame(differences)\n\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\")\n    except ValueError as e:\n        raise e\n    except Exception as e:\n        raise Exception(f\"An error occurred: {str(e)}\")"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Handle empty data list\n    if not data:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.pie([1], labels=['No Data'])\n        ax.set_title('Empty Dataset')\n        return stats, ax\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in the dataset.\")\n\n    # Calculate statistics\n    col_data = df[column]\n    stats = {\n        'sum': col_data.sum(),\n        'mean': col_data.mean(),\n        'min': col_data.min(),\n        'max': col_data.max()\n    }\n\n    # Create pie chart\n    fig, ax = plt.subplots()\n    if 'Age' in df.columns:\n        age_counts = df['Age'].value_counts().sort_index()\n        ax.pie(age_counts.values, labels=age_counts.index, autopct='%1.1f%%', startangle=90)\n        ax.set_title('Age Distribution')\n    else:\n        unique_values = col_data.value_counts().sort_index()\n        ax.pie(unique_values.values, labels=unique_values.index, autopct='%1.1f%%', startangle=90)\n        ax.set_title(f'{column} Distribution')\n\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n\n    return stats, ax\ndata = [\n    {'Name': 'Alice', 'Age': 30, 'Salary': 50000},\n    {'Name': 'Bob', 'Age': 35, 'Salary': 60000},\n    {'Name': 'Charlie', 'Age': 30, 'Salary': 55000},\n    {'Name': 'David', 'Age': 40, 'Salary': 70000},\n]"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if the data list is empty\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n\n    # Convert the data list to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate non-negative values for steps, calories burned, and distance walked\n    numeric_columns = ['steps', 'calories_burned', 'distance_walked']\n    for col in numeric_columns:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in '{col}' column.\")\n\n    # Calculate statistics for the specified column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create the line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df[column], marker='o', linestyle='-', linewidth=2, markersize=6)\n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel(column.capitalize(), fontsize=12)\n    ax.set_title(f'Line Chart of {column.capitalize()}', fontsize=14)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Add grid for better readability\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    # Adjust layout to prevent cutoff of labels\n    plt.tight_layout()\n\n    return stats, ax"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Load JSON data from the file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Dictionary to store numeric values for each key\n    values_dict = defaultdict(list)\n    \n    # Collect numeric values for each key\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                values_dict[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = {}\n    for key, values in values_dict.items():\n        if values:\n            results[key] = {\n                'mean': np.mean(values),\n                'median': np.median(values)\n            }\n    \n    # Convert results to a Pandas DataFrame\n    df = pd.DataFrame.from_dict(results, orient='index')\n    \n    # Sort the DataFrame by index (variable names)\n    df = df.sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension.\")\n\n    # Read the CSV file and identify duplicate rows\n    with open(file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        header = next(reader)  # Read the header\n        rows = [tuple(row) for row in reader]  # Convert rows to tuples for hashability\n\n    # Count the occurrences of each row\n    row_counts = Counter(rows)\n\n    # Filter out duplicates (rows with count > 1)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to a pandas DataFrame\n    df_data = []\n    for row, count in duplicates.items():\n        df_data.extend([row] * count)  # Add each duplicate row 'count' times\n    df = pd.DataFrame(df_data, columns=header)\n\n    # Plot the duplicate rows using matplotlib\n    fig, ax = plt.subplots(figsize=(12, 6))\n    bars = ax.bar(range(len(duplicates)), list(duplicates.values()))\n    ax.set_xticks(range(len(duplicates)))\n    ax.set_xticklabels([str(row) for row in duplicates.keys()], rotation=90, ha='right')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_title('Count of Duplicate Rows')\n\n    # Add value labels on top of each bar\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height}', ha='center', va='bottom')\n\n    plt.tight_layout()\n\n    return duplicates, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if required columns exist\n    if 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns.\")\n    \n    # Round down ages to nearest integer\n    df['age'] = np.floor(df['age']).astype(int)\n    \n    # Check for negative ages\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative.\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    # If no duplicates, return empty Counter and None\n    if duplicates.empty:\n        return Counter(), None\n    \n    # Calculate age distribution for duplicates\n    age_distribution = Counter(duplicates['age'])\n    \n    # Create histogram plot\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5)\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.histplot(data=duplicates, x='age', bins=bins, ax=ax)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution for Duplicate Names')\n    \n    return age_distribution, ax"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = df['value'].value_counts()\n    duplicates = value_counts[value_counts > 1]\n    duplicate_counter = Counter(duplicates.to_dict())\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    values = df['value']\n    ax.hist(values, bins=bins, color='green', alpha=0.6, density=True)\n\n    # Fit and overlay normal distribution curve\n    mu, std = norm.fit(values)\n    x = np.linspace(values.min(), values.max(), 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Customize plot\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return duplicate_counter, ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    # Determine the number of columns based on the length of list 'b'\n    num_columns = len(b)\n    \n    # Generate random values for the DataFrame\n    data = np.random.rand(len(a), num_columns)\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_columns])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', stacked=True, figsize=(10, 6))\n    \n    # Customize the plot\n    plt.title('Bar Chart of Random Data')\n    plt.xlabel('Row Indices')\n    plt.ylabel('Values')\n    plt.legend(title='Columns', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\na = ['Row1', 'Row2', 'Row3', 'Row4']\nb = [1, 2, 3]"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): DataFrame with columns 'month' and 'value'.\n\n    Returns:\n    matplotlib.axes.Axes: Axes object representing the plot.\n    \"\"\"\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n    if not {'month', 'value'}.issubset(data.columns):\n        raise ValueError(\"DataFrame must contain 'month' and 'value' columns\")\n\n    # Ensure 'month' is datetime type and sort the data\n    data['month'] = pd.to_datetime(data['month'])\n    data = data.sort_values('month')\n\n    # Extract the year from the first month entry\n    year = data['month'].iloc[0].year\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(data['month'], data['value'], width=20, align='center')\n\n    # Set title and labels\n    ax.set_title(f'Monthly Data for {year}', fontsize=16)\n    ax.set_xlabel('Month', fontsize=12)\n    ax.set_ylabel('Value', fontsize=12)\n\n    # Format x-axis\n    ax.xaxis.set_major_locator(plt.MonthLocator())\n    ax.xaxis.set_major_formatter(plt.DateFormatter('%b'))\n\n    # Rotate and align the tick labels so they look better\n    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n\n    # Use a tight layout to prevent label clipping\n    plt.tight_layout()\n\n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws a histogram to visualize the frequency distribution of numeric values \n    provided in a string format.\n\n    Parameters:\n    data (str): A string of numeric values separated by spaces.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): The Axes object of the created histogram.\n    \"\"\"\n    # Convert the input string to a list of floats\n    numeric_values = [float(value) for value in data.split()]\n    \n    # Create a Pandas Series from the numeric values\n    data_series = pd.Series(numeric_values)\n    \n    # Calculate the bins\n    bins = np.arange(data_series.min(), data_series.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_series, bins=bins, edgecolor='black')\n    \n    # Set title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2*np.pi, array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Generate the original sine wave\n    true_params = [1.0, 1.0, 0.0, 0.0]  # [amplitude, frequency, phase, offset]\n    y_true = sine_func(x, *true_params)\n    \n    # Add noise to the sine wave\n    noise = noise_level * np.random.normal(size=array_length)\n    y_noisy = y_true + noise\n    \n    # Fit the curve\n    initial_guess = [1.0, 1.0, 0.0, 0.0]\n    fitted_params, _ = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n    \n    # Generate the fitted curve\n    y_fitted = sine_func(x, *fitted_params)\n    \n    # Plot the noisy sine wave and the fitted curve\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(x, y_noisy, 'b.', label='Noisy Data', alpha=0.5)\n    ax.plot(x, y_fitted, 'r-', label='Fitted Curve', linewidth=2)\n    ax.plot(x, y_true, 'g--', label='True Sine Wave', linewidth=1)\n    ax.legend()\n    ax.set_title(f\"Noisy Sine Wave and Fitted Curve (Noise Level: {noise_level})\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        # Read the CSV file\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = ' '.join([' '.join(row) for row in reader])\n\n        # Normalize text to ASCII\n        normalized_text = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('ascii')\n\n        # Tokenize and count words\n        words = normalized_text.lower().split()\n        word_counts = Counter(words)\n\n        # Get the 10 most common words\n        common_words = word_counts.most_common(10)\n\n        # Create bar plot\n        words, counts = zip(*common_words)\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.bar(words, counts)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('10 Most Common Words')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n\n        return ax, common_words\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The CSV file '{csv_file}' cannot be found at the specified path.\")\n    except IOError as e:\n        raise IOError(f\"An error occurred while reading the file: {str(e)}\")"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot histogram\n    ax.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n    \n    # Calculate mean and standard deviation\n    mu, std = np.mean(data), np.std(data)\n    \n    # Generate x values for PDF\n    x = np.linspace(mu - 4*std, mu + 4*std, 100)\n    \n    # Calculate PDF\n    pdf = stats.norm.pdf(x, mu, std)\n    \n    # Plot PDF\n    ax.plot(x, pdf, 'r-', lw=2, label='Normal PDF')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    \n    # Add legend\n    ax.legend()\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n    \n    # Serialize private key\n    private_key_pem = private_key.save_pkcs1()\n    \n    # Generate random encryption password and nonce for AES\n    encryption_password = get_random_bytes(32)  # 32 bytes for AES-256\n    nonce = get_random_bytes(12)  # 12 bytes nonce for AES-GCM\n    \n    # Encrypt private key using AES-GCM\n    cipher = AES.new(encryption_password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, tag = cipher.encrypt_and_digest(private_key_pem)\n    \n    # Generate filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # Save encrypted private key to file\n    with open(filename, 'wb') as file:\n        file.write(nonce + tag + ciphertext)\n    \n    return public_key, filename, encryption_password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n\n    # Generate a random 256-bit AES key\n    aes_key = os.urandom(32)\n\n    # Read the file to be encrypted\n    with open(file_path, 'rb') as f:\n        plaintext = f.read()\n\n    # Pad the plaintext\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(plaintext) + padder.finalize()\n\n    # Generate a random IV\n    iv = os.urandom(16)\n\n    # Create AES cipher object\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Encrypt the padded plaintext\n    ciphertext = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Save the encrypted file (IV + ciphertext)\n    encrypted_file_path = file_path + '.encrypted'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(iv + ciphertext)\n\n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Save the encrypted AES key\n    encrypted_key_path = file_path + '.key'\n    with open(encrypted_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    return public_key, encrypted_file_path, encrypted_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nfrom urllib.error import URLError\ndef task_func(url):\n    # Validate URL\n    if not url or not isinstance(url, str):\n        raise ValueError(\"The provided URL is invalid or empty.\")\n\n    try:\n        # Fetch HTML content\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n\n        # Parse HTML\n        doc = pq(html_content)\n\n        # Extract anchor tags\n        anchors = doc('a')\n\n        # Prepare data\n        data = []\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n        for anchor in anchors.items():\n            data.append({\n                'text': anchor.text(),\n                'href': anchor.attr('href'),\n                'fetch_time': fetch_time\n            })\n\n        # Create DataFrame\n        return pd.DataFrame(data)\n\n    except URLError as e:\n        raise URLError(f\"There is an issue with network connectivity or the server: {str(e)}\")"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate filename with current timestamp\n    current_time = datetime.now()\n    filename = f\"sensor_data_{current_time.strftime('%Y%m%d_%H%M%S')}.csv\"\n    filepath = os.path.join(output_dir, filename)\n\n    # Generate data\n    data = []\n    for minute in range(hours * 60):\n        timestamp = current_time + timedelta(minutes=minute)\n        temperature = randint(-10, 40)  # Temperature range: -10\u00b0C to 40\u00b0C\n        humidity = randint(0, 100)      # Humidity range: 0% to 100%\n        pressure = randint(950, 1050)   # Pressure range: 950 hPa to 1050 hPa\n        data.append([timestamp.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n\n    # Write data to CSV file\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writerows(data)\n\n    print(f\"Sensor data for {hours} hours has been saved to {filepath}\")"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate traffic data\n    data = []\n    start_time = datetime.now()\n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour)\n        row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n        row.extend([randint(0, 100) for _ in VEHICLE_TYPES])\n        data.append(row)\n\n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n\n    # Read CSV and plot data\n    df = pd.read_csv(csv_file_path)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return csv_file_path, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(BACKUP_DIR, exist_ok=True)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = start_time + timedelta(hours=i)\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append((time.strftime('%Y-%m-%d %H:%M:%S'), condition))\n\n    # Create CSV file with timestamp in filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_filename = f'weather_data_{timestamp}.csv'\n    csv_file_path = os.path.join(output_dir, csv_filename)\n\n    # Write data to CSV file\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file\n    backup_file_path = os.path.join(BACKUP_DIR, csv_filename)\n    shutil.copy(csv_file_path, backup_file_path)\n\n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Ensure the input lists have the same length as TEAMS\n    assert len(goals) == len(penalties) == len(TEAMS), \"Input lists must have the same length as TEAMS\"\n\n    # Create the DataFrame\n    df = pd.DataFrame({\n        'Team': TEAMS,\n        'Goals': goals,\n        'Penalties': penalties,\n        'Penalty Cost': [p * PENALTY_COST for p in penalties]\n    })\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Plot goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=ax1)\n    ax1.set_title('Goals Scored by Each Team')\n    ax1.set_ylabel('Number of Goals')\n\n    # Plot penalty costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=ax2)\n    ax2.set_title('Penalty Costs for Each Team')\n    ax2.set_ylabel('Penalty Cost ($)')\n\n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n    return df, [ax1, ax2]\ngoals = [randint(0, 5) for _ in TEAMS]\npenalties = [randint(0, 3) for _ in TEAMS]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Generate random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create the bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    non_zero_counts.plot(kind='bar', ax=ax, color='skyblue')\n    \n    # Customize the plot\n    ax.set_title('Non-Zero Value Counts per Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-Zero Counts')\n    ax.set_ylim(0, rows)  # Set y-axis limit to the number of rows\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(non_zero_counts):\n        ax.text(i, v, str(v), ha='center', va='bottom')\n    \n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate random grades for each student in each course\n    np.random.seed(42)  # For reproducibility\n    data = {course: np.random.randint(0, 101, num_students) for course in courses}\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate average grade for each course\n    avg_grades = df.mean()\n    \n    # Calculate number of students with passing grade (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot data\n    x = np.arange(len(courses))\n    width = 0.35\n    \n    ax.bar(x - width/2, avg_grades, width, label='Average Grade', color='skyblue')\n    ax.bar(x + width/2, passing_counts, width, label='Passing Students', color='lightgreen')\n    \n    # Customize plot\n    ax.set_ylabel('Values')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xticks(x)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(avg_grades):\n        ax.text(i - width/2, v, f'{v:.1f}', ha='center', va='bottom')\n    for i, v in enumerate(passing_counts):\n        ax.text(i + width/2, v, str(v), ha='center', va='bottom')\n    \n    plt.tight_layout()\n    \n    return df, ax\nnum_students = 30"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Find indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n    \n    if len(indices) == 0:\n        raise ValueError(f\"No matching values found for target_value {target_value}\")\n    \n    # Extract corresponding values from the second column\n    values = array[indices, 1]\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Perform curve fitting\n    initial_guess = [max(values), 0.1, min(values)]\n    popt, _ = optimize.curve_fit(exp_decay, indices, values, p0=initial_guess)\n    \n    # Create plot\n    fig, ax = plt.subplots()\n    ax.scatter(indices, values, label='Data')\n    \n    # Generate smooth curve for plotting\n    x_smooth = np.linspace(min(indices), max(indices), 100)\n    y_smooth = exp_decay(x_smooth, *popt)\n    ax.plot(x_smooth, y_smooth, 'r-', label='Fitted Curve')\n    \n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Exponential Decay Fit (Target Value: {target_value})')\n    ax.legend()\n    \n    return popt, ax"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    def preprocess_text(text):\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n\n    # Preprocess texts\n    processed_texts = [preprocess_text(text) for text in texts]\n\n    # Vectorize using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=1000)\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=42, init='nndsvda')\n    nmf.fit(tfidf_matrix)\n\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract topics\n    topics = []\n    for topic in nmf.components_:\n        top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n        top_words = [feature_names[i] for i in top_words_idx]\n        topics.append(top_words)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Download NLTK stopwords if not already downloaded\n    nltk.download('stopwords', quiet=True)\n    \n    # Use NLTK's English stopwords if none are provided\n    if stopwords is None:\n        stopwords = set(stopwords.words('english'))\n    \n    def preprocess_text(text):\n        # Remove non-alphanumeric characters (except space)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase the text\n        text = text.lower()\n        # Split into words and remove stopwords\n        words = [word for word in text.split() if word not in stopwords]\n        return words\n    \n    # Preprocess all texts\n    processed_texts = [preprocess_text(text) for text in texts]\n    \n    # Train Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Create the directory if it doesn't exist\n    os.makedirs(path, exist_ok=True)\n\n    # Create the processed subdirectory\n    processed_dir = os.path.join(path, 'processed')\n    os.makedirs(processed_dir, exist_ok=True)\n\n    # Get a list of JSON files in the directory, sorted alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n\n    # List to store individual DataFrames\n    dataframes = []\n\n    for json_file in json_files:\n        file_path = os.path.join(path, json_file)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # Convert JSON to DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add Source column\n        df['Source'] = json_file\n        \n        # Append to list of DataFrames\n        dataframes.append(df)\n        \n        # Move processed file to the processed subdirectory\n        shutil.move(file_path, os.path.join(processed_dir, json_file))\n\n    # Combine all DataFrames\n    if dataframes:\n        combined_df = pd.concat(dataframes, ignore_index=True)\n    else:\n        combined_df = pd.DataFrame()\n\n    return combined_df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    # Generate random sensor data\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    temperature = round(random.uniform(-10.0, 40.0), 2)\n    humidity = round(random.uniform(0.0, 100.0), 2)\n\n    # Write data to CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        if file.tell() == 0:  # Write header if file is empty\n            writer.writerow([\"Timestamp\", \"Temperature(C)\", \"Humidity(%)\"])\n        writer.writerow([timestamp, temperature, humidity])\n\n    # Store the file path before deletion\n    return_path = file_path\n\n    # Delete the file after use\n    os.remove(file_path)\n\n    # Return the path to the CSV file before deletion\n    return return_path"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a request to the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            web_content = response.read()\n    except urllib.error.URLError as e:\n        print(f\"Error accessing the URL: {e}\")\n        return None\n\n    # Parse the webpage content\n    soup = BeautifulSoup(web_content, 'html.parser')\n\n    # Extract data from the page\n    # This example extracts all text from paragraph tags\n    # Modify this section based on the structure of the target website\n    data = []\n    paragraphs = soup.find_all('p')\n    for p in paragraphs:\n        data.append([p.text.strip()])\n\n    # If no data was found, return None\n    if not data:\n        print(\"No data found on the page.\")\n        return None\n\n    # Write data to a CSV file\n    try:\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csv_file:\n            writer = csv.writer(csv_file)\n            writer.writerow(['Content'])  # Header\n            writer.writerows(data)\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return None\n\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If data is not a DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"If data is empty.\")\n    \n    # Check if target_column is in data\n    if target_column not in data.columns:\n        raise ValueError(\"If target_column ist not a column of data.\")\n    \n    # Check if all values in data are numeric\n    if not data.select_dtypes(include=[np.number]).columns.tolist() == data.columns.tolist():\n        raise ValueError(\"If data contains values that are not numeric.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"If random_state is not an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"If test_size is not between 0 and 1.\")\n    \n    # Split the data into features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    # Train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Calculate and return the model score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    # Set random seed for reproducibility\n    np.random.seed(rng_seed)\n\n    # Generate 100 random records\n    num_records = 100\n\n    # Generate IDs\n    ids = np.arange(1, num_records + 1)\n\n    # Combine and select names\n    all_names = latin_names + other_names\n    names = np.random.choice(all_names, num_records)\n\n    # Generate random dates of birth\n    start_date = datetime.date(start_year, 1, 1)\n    end_date = datetime.date(end_year, 12, 31)\n    days_range = (end_date - start_date).days\n    random_days = np.random.randint(0, days_range + 1, num_records)\n    dates_of_birth = [start_date + datetime.timedelta(days=int(day)) for day in random_days]\n\n    # Generate emails\n    def create_email(name, dob):\n        # Correct encoding for Latin characters\n        name = codecs.decode(name.encode('latin1'), 'utf-8', 'ignore')\n        # Remove any remaining non-ASCII characters\n        name = re.sub(r'[^\\x00-\\x7F]+', '', name)\n        # Create email\n        return f\"{name.lower()}{dob.year}@{email_domain}\"\n\n    emails = [create_email(name, dob) for name, dob in zip(names, dates_of_birth)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n\n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"Input JSON must be a list of dictionaries\")\n\n    # Collect all values for each key\n    values_by_key = defaultdict(list)\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                values_by_key[key].append(value)\n\n    # Calculate mean and median for each key\n    result = {}\n    for key, values in values_by_key.items():\n        result[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, stats in result.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n\n    return result"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    for file_path in kwargs.values():\n        source_file = Path(file_path)\n        if source_file.is_file() and source_file.stat().st_size > 0:\n            try:\n                dest_file = target_path / source_file.name\n                shutil.copy2(source_file, dest_file)\n                copied_files.append(str(dest_file.resolve()))\n            except Exception as e:\n                print(f\"Error copying {source_file}: {e}\")\n\n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Ensure the directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # List to hold the names of new files\n    new_files = []\n\n    # Iterate over all CSV files in the specified directory\n    for file_path in Path(directory).glob('*.csv'):\n        file_name = file_path.name\n\n        # Check if the file name matches the pattern\n        match = regex.match(file_name)\n        if match:\n            # Extract the base name from the regex match\n            base_name = match.group(1)\n\n            # Create the new file name\n            new_file_name = f\"{base_name}-processed.csv\"\n            new_file_path = file_path.with_name(new_file_name)\n\n            # Copy the content from the old file to the new file\n            try:\n                with open(file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n                     open(new_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n                    reader = csv.reader(infile)\n                    writer = csv.writer(outfile)\n\n                    # Write the content to the new file\n                    for row in reader:\n                        writer.writerow(row)\n\n                # Add the new file name to the list\n                new_files.append(new_file_name)\n            except IOError as e:\n                print(f\"Error processing file {file_name}: {e}\")\n                continue\n\n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzips all zip files in a directory whose name matches a certain pattern by splitting\n    the filename the last time \"-\" occurs and using the prefix part of the filename as\n    the directory to extract.\n\n    Args:\n    directory (str): The directory containing the zip files.\n    pattern (str): The regex pattern to match the zip file names.\n\n    Returns:\n    list: A list of directories where the files were extracted.\n    \"\"\"\n    extracted_dirs = []\n    regex = re.compile(pattern)\n\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            match = regex.match(filename)\n            if match:\n                full_path = os.path.join(root, filename)\n                prefix = match.group(1)\n                extract_dir = os.path.join(root, prefix)\n\n                os.makedirs(extract_dir, exist_ok=True)\n\n                with zipfile.ZipFile(full_path, 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n\n                if extract_dir not in extracted_dirs:\n                    extracted_dirs.append(extract_dir)\n\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport tarfile\nfrom datetime import datetime\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archive all files matching the given pattern and delete the originals.\n\n    Args:\n        pattern (str): The file pattern to match.\n\n    Returns:\n        str: The archive file path.\n\n    Raises:\n        FileNotFoundError: If no files match the given pattern.\n    \"\"\"\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files that match the pattern\n    matching_files = glob.glob(pattern)\n\n    if not matching_files:\n        raise FileNotFoundError(f\"No files found matching pattern: {pattern}\")\n\n    # Create a timestamped archive file name\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    archive_file = os.path.join(ARCHIVE_DIR, f\"archive_{timestamp}.tar.gz\")\n\n    # Create the archive\n    with tarfile.open(archive_file, \"w:gz\") as tar:\n        for file in matching_files:\n            tar.add(file, arcname=os.path.basename(file))\n\n    # Delete the original files\n    for file in matching_files:\n        os.remove(file)\n\n    return archive_file"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    count = Counter()\n\n    if os.path.exists(csv_file_path):\n        with open(csv_file_path, 'r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                count['goals'] += int(row['goals'])\n                count['penalties'] += int(row['penalties'])\n\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    with open(csv_file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['New Entry', str(goals), str(penalties)])\n\n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from the source directory to the target directory based on a specified pattern.\n\n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        file_pattern (str, optional): The regex pattern to match file names. \n                                      Defaults to r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'.\n\n    Returns:\n        int: The number of files that were successfully moved from the source directory \n             to the target directory.\n    \"\"\"\n    # Ensure the source directory exists\n    if not os.path.exists(source_dir):\n        raise ValueError(f\"Source directory '{source_dir}' does not exist.\")\n\n    # Ensure the target directory exists, create it if it doesn't\n    os.makedirs(target_dir, exist_ok=True)\n\n    moved_files_count = 0\n    pattern = re.compile(file_pattern)\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        if pattern.match(filename):\n            source_file = os.path.join(source_dir, filename)\n            target_file = os.path.join(target_dir, filename)\n\n            try:\n                # Move the file to the target directory\n                shutil.move(source_file, target_file)\n                moved_files_count += 1\n            except (IOError, OSError) as e:\n                print(f\"Error moving file {filename}: {str(e)}\")\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    def clean_text(text):\n        return ALPHANUMERIC.sub(' ', text).lower()\n\n    def cosine_similarity(text1, text2):\n        vec1 = Counter(text1.split())\n        vec2 = Counter(text2.split())\n        \n        words = set(vec1.keys()) | set(vec2.keys())\n        \n        vec1_array = np.array([vec1.get(word, 0) for word in words])\n        vec2_array = np.array([vec2.get(word, 0) for word in words])\n        \n        dot_product = np.dot(vec1_array, vec2_array)\n        norm1 = np.linalg.norm(vec1_array)\n        norm2 = np.linalg.norm(vec2_array)\n        \n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n        return dot_product / (norm1 * norm2)\n\n    cleaned_text1 = clean_text(text1)\n    cleaned_text2 = clean_text(text2)\n\n    cosine_sim = cosine_similarity(cleaned_text1, cleaned_text2)\n    levenshtein_ratio = ratio(cleaned_text1, cleaned_text2)\n\n    return (cosine_sim, levenshtein_ratio)\ntext1 = \"Hello, this is a sample text!\"\ntext2 = \"Hello there, this is another example.\""}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Check if numbers is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n\n    # Check if any number is negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"Input numbers cannot be negative\")\n\n    # Handle empty list case\n    if not numbers:\n        return [], []\n\n    # Generate all permutations\n    all_permutations = list(permutations(numbers))\n\n    # Calculate sum of factorials for each permutation\n    factorial_sums = [sum(math.factorial(num) for num in perm) for perm in all_permutations]\n\n    # Convert permutations to list of lists\n    permutations_list = [list(perm) for perm in all_permutations]\n\n    return factorial_sums, permutations_list"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure the destination directory exists\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Iterate over each file extension\n    for ext in EXTENSIONS:\n        # Create a glob pattern for the current extension\n        pattern = os.path.join(SOURCE_DIR, f'*{ext}')\n        \n        # Find all files matching the pattern\n        for file_path in glob.glob(pattern):\n            file_name = os.path.basename(file_path)\n            dest_path = os.path.join(DEST_DIR, file_name)\n            \n            try:\n                # Attempt to move the file\n                shutil.move(file_path, dest_path)\n                transferred_files.append(file_name)\n                print(f\"Transferred: {file_name}\")\n                \n                # Small delay to prevent potential issues with rapid file operations\n                time.sleep(0.1)\n                \n            except Exception as e:\n                warnings.warn(f\"Could not transfer {file_name}: {str(e)}\")\n\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items, counts, weights = zip(*data)\n    \n    # Convert counts and weights to numpy arrays\n    counts_array = np.array(counts)\n    weights_array = np.array(weights)\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts_array)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights_array.reshape(-1, 1)).flatten()\n    \n    # Create and return the DataFrame\n    return pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Determine the maximum length of tuples in the data list\n    max_length = max(len(t) for t in data_list)\n\n    # Initialize lists to hold numeric values for each position\n    numeric_values = [[] for _ in range(max_length)]\n\n    # Collect numeric values for each position\n    for tup in data_list:\n        for i, value in enumerate(tup):\n            if isinstance(value, (int, float)):\n                numeric_values[i].append(value)\n\n    # Calculate the mean value for each position\n    mean_values = [np.mean(values) if values else np.nan for values in numeric_values]\n\n    # Create a DataFrame with the results\n    index = [f'Position {i}' for i in range(max_length)]\n    df = pd.DataFrame({'Mean Value': mean_values}, index=index)\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n\n    # Check if the specified columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns '{col1}' and/or '{col2}' are not in the DataFrame.\")\n\n    # Check if the columns contain categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not isinstance(data[col1].dtype, pd.CategoricalDtype):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not isinstance(data[col2].dtype, pd.CategoricalDtype):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data.\")\n\n    # Check if the columns have multiple categories\n    if data[col1].nunique() < 2:\n        raise ValueError(f\"Column '{col1}' does not have multiple categories.\")\n    if data[col2].nunique() < 2:\n        raise ValueError(f\"Column '{col2}' does not have multiple categories.\")\n\n    # Create the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Check if each cell in the contingency table has at least 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating chi-square test assumptions.\")\n\n    # Perform the chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n\n    return p_value"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    results = np.random.choice(NUMBERS, size=rolls)\n    \n    # Calculate frequency of each outcome\n    frequency = np.bincount(results, minlength=7)[1:]\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(0.5, 7.5, 1), align='mid', rwidth=0.8)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    ax.set_xlim(0.5, 6.5)\n    \n    return frequency, ax"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Full path to the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a new ZIP file\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source directory\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                if file.endswith('_processed'):\n                    file_path = os.path.join(root, file)\n                    # Add file to the ZIP archive with its relative path\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the DataFrame contains 'Date' and 'Close' columns\n    if 'Date' not in df.columns or 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Close' columns\")\n\n    # Convert 'Date' to datetime and sort the DataFrame\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df.sort_values('Date')\n\n    # Convert dates to timestamps (seconds since epoch)\n    df['Timestamp'] = df['Date'].astype(np.int64) // 10**9\n\n    # Prepare data for linear regression\n    X = df[['Timestamp']]\n    y = df['Close']\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps for the next 7 days\n    time_step = 24 * 60 * 60  # 1 day in seconds\n    last_timestamp = df['Timestamp'].iloc[-1]\n    future_timestamps = np.array([last_timestamp + i * time_step for i in range(1, 8)])\n\n    # Predict closing prices for the next 7 days\n    future_prices = model.predict(future_timestamps.reshape(-1, 1))\n\n    # Convert future timestamps to dates for plotting\n    future_dates = pd.to_datetime(future_timestamps, unit='s')\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Date'], df['Close'], label='Historical Prices')\n    ax.plot(future_dates, future_prices, 'r--', label='Predicted Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Price Prediction')\n    ax.legend()\n    ax.grid(True)\n\n    # Format x-axis dates\n    plt.gcf().autofmt_xdate()\n\n    return future_prices.tolist(), ax"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    # Calculate Z-scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-score threshold\n    outliers = df[np.abs(z_scores) > z_threshold].copy()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot all data points\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Closing Prices')\n    \n    # Highlight outliers\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', marker='x', s=100, label='Outliers')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    # Add legend\n    ax.legend()\n    \n    # Add grid for better readability\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    return outliers[['closing_price']], ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Check if 'Close' column exists in the DataFrame\n    if 'Close' not in df.columns:\n        raise ValueError(\"The DataFrame must contain a 'Close' column with closing prices.\")\n\n    # Create a figure with two subplots\n    fig, (ax_box, ax_hist) = plt.subplots(nrows=2, ncols=1, figsize=(10, 8), gridspec_kw={'height_ratios': [1, 2]})\n\n    # Box plot\n    sns.boxplot(x=df['Close'], ax=ax_box)\n    ax_box.set_title('Box Plot of Closing Prices')\n    ax_box.set_xlabel('')  # Remove x-label for cleaner look\n\n    # Histogram\n    sns.histplot(df['Close'], kde=True, ax=ax_hist)\n    ax_hist.set_title('Histogram of Closing Prices')\n    ax_hist.set_xlabel('Closing Price')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return (ax_box, ax_hist)"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has a 'Close' column\n    if 'Close' not in df.columns:\n        raise ValueError(\"The DataFrame must contain a 'Close' column.\")\n\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(1,1,1))  # You may need to adjust these parameters\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecasted_prices = forecast.tolist()\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    df['Close'].plot(ax=ax, label='Historical Closing Prices', color='blue')\n    \n    # Plot forecasted data\n    forecast_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7)\n    ax.plot(forecast_dates, forecasted_prices, color='red', label='Forecasted Closing Prices')\n    \n    # Customize the plot\n    ax.set_title('Share Closing Prices Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return forecasted_prices, ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations\n    all_combinations = [''.join(comb) for comb in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    result = {comb: 0 for comb in all_combinations}\n    \n    # Count the occurrences of two-letter combinations in the word\n    word_combinations = Counter(word[i:i+2].lower() for i in range(len(word) - 1))\n    \n    # Update the result dictionary with the counts from the word\n    for combo, count in word_combinations.items():\n        if combo in result:\n            result[combo] = count\n    \n    return result\nword = \"exampleword\""}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create DataFrame\n    data = []\n    for category in categories:\n        sales = np.random.randint(1000, 5000, size=periods)\n        for date, sale in zip(date_range, sales):\n            data.append({'Date': date, 'Category': category, 'Sales': sale})\n    \n    df = pd.DataFrame(data)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create or use provided sales data\n    if sales_data is None:\n        sales_data = np.random.randint(100, 200, size=periods)\n    else:\n        sales_data = np.array(sales_data)\n    \n    # Prepare data for regression\n    X = np.arange(periods).reshape(-1, 1)\n    y = sales_data\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Forecast future sales\n    future_X = np.arange(periods, 2*periods).reshape(-1, 1)\n    forecasted_sales = model.predict(future_X)\n    \n    return forecasted_sales"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Ensure there are enough tasks to assign\n    if n_tasks > len(sanitized_tasks):\n        raise ValueError(\"n_tasks exceeds the number of available tasks\")\n\n    # Randomly select tasks\n    selected_tasks = random.sample(sanitized_tasks, n_tasks)\n\n    # Get the current date\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Create the assignments\n    assignments = []\n    for task in selected_tasks:\n        assigned_employee = random.choice(employees)\n        assignments.append({\n            \"Task Name\": task,\n            \"Assigned To\": assigned_employee,\n            \"Due Date\": current_date\n        })\n\n    # Create and return the DataFrame\n    return pd.DataFrame(assignments)"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text.strip():\n        raise ValueError(\"The input text is empty.\")\n\n    # Convert text to lowercase for case-insensitive operations\n    text = text.lower()\n\n    # Replace spaces with underscores in given words and convert to lowercase\n    modified_words = [word.lower().replace(' ', '_') for word in mystrings]\n\n    # Find all words in the text\n    words_in_text = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_counts = Counter(words_in_text)\n\n    # Get frequencies for the modified words in the order they appear\n    frequencies = [word_counts[word] for word in modified_words]\n\n    # Create the frequency plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(modified_words, frequencies)\n\n    # Set labels and title\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n\n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n\n    # Valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Ensure the target directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Create the full path for the zip file\n    zip_path = Path(target_directory) / f\"{zip_name}.zip\"\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for ext in valid_extensions:\n            # Find all files with the current extension in the source directory\n            for file_path in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add the file to the zip archive\n                zipf.write(file_path, os.path.basename(file_path))\n\n    # Return the full path to the created zip file\n    return str(zip_path)"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n\n    os.makedirs(target_directory, exist_ok=True)\n\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    converted_count = 0\n\n    for file_path in Path(source_directory).rglob('*'):\n        if file_path.suffix.lower() in extensions:\n            try:\n                if file_path.suffix.lower() == '.txt':\n                    with open(file_path, 'r', encoding='utf-8') as file:\n                        content = file.read()\n                elif file_path.suffix.lower() == '.docx':\n                    doc = docx.Document(file_path)\n                    content = \"\\n\".join(para.text for para in doc.paragraphs)\n                elif file_path.suffix.lower() == '.xlsx':\n                    df = pd.read_excel(file_path, header=None)\n                    content = df.to_csv(header=False, index=False)\n                else:  # .csv\n                    df = pd.read_csv(file_path, header=None)\n                    content = df.to_csv(header=False, index=False)\n\n                target_file_path = Path(target_directory) / f\"{file_path.stem}.csv\"\n                pd.DataFrame({'Text': [content]}).to_csv(target_file_path, index=False)\n                converted_count += 1\n\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n\n    return converted_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Check for non-numeric data types\n    if not np.issubdtype(df.dtypes, np.number).all():\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    # Check for NaN values\n    if df.isna().any().any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n\n    # Compute the cumulative sum for each column\n    cumsum_df = df.cumsum()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Fit and transform the cumulative sum DataFrame\n    normalized_array = scaler.fit_transform(cumsum_df)\n\n    # Convert the normalized array back to a DataFrame with original column names\n    normalized_df = pd.DataFrame(normalized_array, columns=df.columns, index=df.index)\n\n    return normalized_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    dir_path = Path(directory_path)\n    \n    if not dir_path.is_dir():\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    file_info = []\n    \n    for entry in dir_path.iterdir():\n        if entry.is_file():\n            stats = entry.stat()\n            \n            # Get file size\n            size = stats.st_size\n            \n            # Get creation time (or fallback to metadata change time)\n            try:\n                creation_time = datetime.fromtimestamp(stats.st_ctime, tz=timezone.utc).isoformat()\n            except AttributeError:\n                creation_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc).isoformat()\n            \n            # Get modification time\n            modification_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc).isoformat()\n            \n            file_info.append((entry.name, size, creation_time, modification_time))\n    \n    return file_info"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        # Parse JSON data\n        data = json.loads(json_data)\n        \n        # Check if data is a non-empty list\n        if not isinstance(data, list) or len(data) == 0:\n            raise ValueError(\"The JSON data is empty or not a list.\")\n        \n        countries = []\n        populations = []\n        \n        for item in data:\n            if not isinstance(item, dict):\n                raise ValueError(\"Invalid item format in JSON data.\")\n            \n            country = item.get(\"Country\")\n            population = item.get(\"Population\")\n            \n            # Validate country name\n            if not isinstance(country, str):\n                raise ValueError(\"Non-string country name found.\")\n            \n            # Validate population\n            if not isinstance(population, (int, float)):\n                raise ValueError(\"Non-numeric population found.\")\n            if population < 0:\n                raise ValueError(\"Negative population found.\")\n            \n            # Round down float populations\n            population = math.floor(population)\n            \n            countries.append(country)\n            populations.append(population)\n        \n        # Create DataFrame\n        df = pd.DataFrame({\"Country\": countries, \"Population\": populations})\n        \n        # Ensure output directory exists\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Generate full file path\n        file_path = os.path.join(output_dir, file_name)\n        \n        # Write DataFrame to CSV\n        df.to_csv(file_path, index=False)\n        \n        return file_path, df\n    \n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    except IOError as e:\n        raise IOError(f\"The file cannot be written to the specified directory: {str(e)}\")\n    except Exception as e:\n        raise ValueError(str(e))"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Validate the directory path\n    if not Path(dir_path).is_dir():\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Deduplicate predicates\n    predicates = list(set(predicates))\n    \n    # Validate predicates\n    if not predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n    \n    results = {}\n    \n    for item in os.listdir(dir_path):\n        results[item] = {}\n        for predicate in predicates:\n            try:\n                # Evaluate the predicate against the item name\n                result = eval(predicate, {\"re\": re}, {\"item_name\": item})\n                results[item][predicate] = bool(result)\n            except Exception as e:\n                # If there's an error evaluating the predicate, store the error message\n                results[item][predicate] = f\"Error: {str(e)}\"\n    \n    return results"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    try:\n        # Decode the hexadecimal string to bytes\n        byte_data = bytes.fromhex(hex_string)\n        \n        # Convert bytes to UTF-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Prepare the dictionary with various encodings\n        result = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        # Handle ASCII encoding\n        try:\n            result['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n        except UnicodeEncodeError:\n            result['ASCII'] = 'Not representable in ASCII'\n        \n        return result\n    except ValueError:\n        raise ValueError(\"Invalid hexadecimal input\")\n    except UnicodeDecodeError:\n        raise ValueError(\"Input cannot be decoded into a valid UTF-8 string\")"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate MD5 checksum\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_checksum = md5_hash.hexdigest()\n        \n        # Validate checksum\n        if calculated_checksum != EXPECTED_MD5_CHECKSUM:\n            print(f\"Checksum mismatch. Expected: {EXPECTED_MD5_CHECKSUM}, Got: {calculated_checksum}\")\n            os.remove(TARGET_TAR_FILE)\n            return False\n        \n        # Extract the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        \n        return True\n    \n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a Counter to store the value counts\n        value_counts = collections.Counter()\n        \n        # Open and process the CSV file\n        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value_counts[row[column_name]] += 1\n        \n        # Return the counts as a dictionary\n        return dict(value_counts)\n    \n    except urllib.error.URLError as e:\n        raise Exception(f\"Failed to download the file: {e}\")\n    \n    finally:\n        # Ensure the CSV file is deleted after processing or if an error occurs\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n    except Exception as e:\n        raise ValueError(f\"Failed to fetch the XML file: {e}\")\n\n    try:\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the XML structure conforms to the expected format\n    if root.tag != 'root' or not all(child.tag == 'item' for child in root):\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Extract data from XML and create DataFrame\n    data = []\n    for item in root.findall('item'):\n        row = {child.tag: child.text for child in item}\n        data.append(row)\n\n    if not data:\n        raise ValueError(\"No valid items found in the XML\")\n\n    return pd.DataFrame(data)"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the specified URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            text = response.read().decode('utf-8')\n    except Exception as e:\n        print(f\"Error downloading or decoding the file: {e}\")\n        return None, None\n\n    # Process the text to count word frequencies\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    word_counts = Counter(words)\n\n    # Get the ten most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Prepare data for plotting\n    words, frequencies = zip(*most_common_words)\n\n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, frequencies)\n    ax.set_title('Ten Most Frequently Occurring Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file from the URL\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n\n        # Check content type\n        content_type = response.headers.get('Content-Type', '').lower()\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Save the downloaded content to a temporary file\n        temp_zip_path = os.path.join(download_path, 'temp_download.zip')\n        with open(temp_zip_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        # Extract the ZIP file\n        try:\n            with ZipFile(temp_zip_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            os.remove(temp_zip_path)\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Clean up the temporary ZIP file\n        os.remove(temp_zip_path)\n\n        return download_path\n\n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        # Make an HTTP request to the specified URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the table with the specified ID\n        table = soup.find('table', id=table_id)\n\n        # Raise a ValueError if the table is not found\n        if table is None:\n            raise ValueError(\"Table with the specified ID not found.\")\n\n        # Extract the rows of the table\n        rows = table.find_all('tr')\n\n        # If the table has no rows, return an empty DataFrame\n        if not rows:\n            return pd.DataFrame()\n\n        # Extract headers from the first row\n        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n\n        # Extract data from the remaining rows\n        data = []\n        for row in rows[1:]:\n            cells = row.find_all('td')\n            if cells:\n                data.append([cell.get_text(strip=True) for cell in cells])\n\n        # Create a DataFrame\n        if data:\n            df = pd.DataFrame(data, columns=headers)\n        else:\n            df = pd.DataFrame(columns=headers)\n\n        return df\n\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {str(e)}\")"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    try:\n        # Ensure the download and extraction directories exist\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n        ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        # Save the downloaded file\n        download_path = DOWNLOAD_DIR / filename\n        with open(download_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Extract the zip file\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get the list of extracted filenames\n        extracted_files = [str(file.relative_to(ZIP_DIR)) for file in ZIP_DIR.rglob('*') if file.is_file()]\n\n        return \"Success\", extracted_files\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Network-related exception - {str(e)}\", []\n\n    except (zipfile.BadZipFile, zipfile.LargeZipFile, IOError, OSError) as e:\n        return f\"Error: File-related exception - {str(e)}\", []\n\n    except Exception as e:\n        return f\"Error: Unexpected exception - {str(e)}\", []"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        # Send a GET request to the webpage\n        response = requests.get(url)\n        response.raise_for_status()\n\n        # Parse the webpage content using BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find all hyperlinks in the webpage and convert to absolute URLs\n        links = set()\n        for a_tag in soup.find_all('a', href=True):\n            href = a_tag['href']\n            absolute_url = urljoin(base_url, href)\n            links.add(absolute_url)\n\n        # Write the absolute URLs to a CSV file\n        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow([\"URL\"])\n            for link in links:\n                writer.writerow([link])\n\n        # Return the number of unique absolute links\n        return len(links)\n\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return 0"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content\n        if webpage_url.startswith(('http://', 'https://')):\n            response = requests.get(webpage_url)\n            response.raise_for_status()\n            content = response.content\n        else:\n            with open(webpage_url, 'rb') as file:\n                content = file.read()\n        \n        # Parse the HTML content\n        tree = html.fromstring(content)\n        tables = pd.read_html(html.tostring(tree))\n        \n        if not tables:\n            print(\"No tables found in the HTML content.\")\n            return 0\n        \n        # Use the first table found\n        df = tables[0]\n        \n        if df.empty:\n            print(\"The table is empty.\")\n            return 0\n        \n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        \n        # Replace the existing table with new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Get the number of rows in the new table\n        row_count = len(df)\n        \n        # Close the database connection\n        conn.close()\n        \n        return row_count\n\n    except requests.RequestException as e:\n        print(f\"Network error: {e}\")\n        raise\n    except sqlite3.DatabaseError as e:\n        print(f\"Database error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        raise"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    def convert_encoding(text, from_enc, to_enc):\n        try:\n            return text.encode(from_enc).decode(to_enc)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(f\"Encoding conversion error: {e}\")\n\n    def extract_and_convert(extractor, converter):\n        try:\n            text = extractor()\n            if text:\n                return converter(text)\n        except ValueError:\n            raise\n        except Exception as e:\n            print(f\"Extraction or conversion failed: {e}\")\n        return None\n\n    def ocr_extractor():\n        with Image.open(filename) as img:\n            return pytesseract.image_to_string(img).strip()\n\n    def comment_extractor():\n        with Image.open(filename) as img:\n            return img.info.get('comment', '').strip()\n\n    converter = lambda text: convert_encoding(text, from_encoding, to_encoding)\n\n    try:\n        result = extract_and_convert(ocr_extractor, converter)\n        if result is not None:\n            return result\n\n        result = extract_and_convert(comment_extractor, converter)\n        if result is not None:\n            return result\n\n    except ValueError as e:\n        raise ValueError(f\"Encoding error: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n    return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Initiate an HTTP GET request with a 5-second timeout\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Retrieve the response content in raw bytes\n        content = response.content\n\n        # Handle empty content case\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding is None:\n            detection = chardet.detect(content)\n            from_encoding = detection['encoding']\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode using detected or provided encoding, then re-encode to target encoding\n        decoded_content = content.decode(from_encoding)\n        encoded_content = decoded_content.encode(to_encoding)\n        \n        # JSON parse the re-encoded content\n        json_data = json.loads(encoded_content.decode(to_encoding))\n        \n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        print(f\"JSON parsing failed: {e}\")\n        return {}\n    except ValueError as e:\n        print(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return {}"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file '{csv_file_path}' was not found.\")\n\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Check if the CSV file is empty\n        if df.empty:\n            return pd.DataFrame()\n\n        # Check if the specified column exists\n        if column_name not in df.columns:\n            raise ValueError(f\"The specified column '{column_name}' is not present in the CSV file.\")\n\n        # Convert date values to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n        # Drop rows with invalid date values\n        df = df.dropna(subset=[column_name])\n\n        # Filter rows based on the current date\n        current_date = datetime.now().date()\n        df = df[df[column_name].dt.date <= current_date]\n\n        # Sort the resulting data by the date column\n        df = df.sort_values(by=column_name)\n\n        return df\n\n    except EmptyDataError:\n        # Handle the case of an empty CSV file\n        return pd.DataFrame()"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    secure_socket = None\n    try:\n        # Set up SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        \n        # Wrap the client socket with SSL/TLS\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the file exists\n        if not os.path.isfile(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n        \n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            while True:\n                data = file.read(buffer_size)\n                if not data:\n                    break\n                sha256_hash.update(data)\n        \n        # Get the hexadecimal representation of the hash\n        hash_result = sha256_hash.hexdigest()\n        \n        # Send the hash back to the client\n        secure_socket.sendall(hash_result.encode('utf-8'))\n        \n        return hash_result\n    \n    except Exception as e:\n        error_message = f\"Error: {str(e)}\"\n        if secure_socket:\n            try:\n                secure_socket.sendall(error_message.encode('utf-8'))\n            except:\n                pass  # Ignore errors when sending error message\n        return error_message\n    \n    finally:\n        if secure_socket:\n            try:\n                secure_socket.shutdown(ssl.SOCK_STREAM)\n                secure_socket.close()\n            except:\n                pass  # Ignore errors when closing the socket"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a non-blocking TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setblocking(False)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # Initialize socket lists and message queues\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    print(f\"Server started on {server_address}:{server_port}\")\n    print(f\"Running for {run_duration} seconds...\")\n\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n\n        for s in readable:\n            if s is server_socket:\n                # Handle new connection\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n                print(f\"New connection from {client_address}\")\n            else:\n                # Handle data from client\n                try:\n                    data = s.recv(buffer_size)\n                    if data:\n                        print(f\"Received {data.decode()} from {s.getpeername()}\")\n                        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                        response = f\"{data.decode()} {current_time}\".encode()\n                        message_queues[s].put(response)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        # Client closed connection\n                        print(f\"Closing connection from {s.getpeername()}\")\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n                except ConnectionResetError:\n                    print(f\"Connection reset by {s.getpeername()}\")\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                print(f\"Sending {next_msg.decode()} to {s.getpeername()}\")\n                s.send(next_msg)\n\n        for s in exceptional:\n            print(f\"Handling exceptional condition for {s.getpeername()}\")\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Clean up\n    server_socket.close()\n    total_connections = len(message_queues)\n    return f\"Server ran for {run_duration} seconds. Handled {total_connections} connection(s).\""}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    try:\n        # Receive message from client socket\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        \n        # Get email details from user\n        sender_email = input(\"Enter sender's email: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        password = getpass.getpass(\"Enter sender's email password: \")\n\n        # Create email message\n        email = EmailMessage()\n        email.set_content(message)\n        email['Subject'] = 'Message from Client Socket'\n        email['From'] = sender_email\n        email['To'] = recipient_email\n\n        # Send email via SMTP server\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()\n            server.login(sender_email, password)\n            server.send_message(email)\n        \n        print(\"Email sent successfully.\")\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    \n    finally:\n        client_socket.close()\n    \n    return None"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path, header=None)\n        \n        # Check if the CSV file has a header\n        if df.iloc[0, 0].lower() == 'text':\n            df.columns = ['Text']\n            df = df.iloc[1:]\n        else:\n            df.columns = ['Text']\n        \n        # Combine all text data into a single string\n        text_data = ' '.join(df['Text'].astype(str))\n        \n        # Initialize and fit CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_count_matrix = vectorizer.fit_transform([text_data])\n        \n        # Get word frequencies\n        word_freq = pd.DataFrame(word_count_matrix.toarray().T, \n                                 index=vectorizer.get_feature_names_out(), \n                                 columns=['frequency'])\n        \n        # Sort and get top 10 words\n        top_10_words = word_freq.sort_values('frequency', ascending=False).head(10)\n        \n        # Create the histogram\n        fig, ax = plt.subplots(figsize=(12, 6))\n        top_10_words['frequency'].plot(kind='bar', ax=ax)\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path)\n            plt.close(fig)\n            return None\n        else:\n            plt.show()\n            return ax\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path '{file_path}' was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Predefined lists\n    default_animals = ['dog', 'cat', 'rabbit']\n    default_foods = ['apple', 'banana', 'carrot']\n\n    # Use default lists if input is None or empty\n    if not animals:\n        animals = default_animals\n    if not foods:\n        foods = default_foods\n\n    # Return empty DataFrame if both lists are empty\n    if not animals or not foods:\n        return pd.DataFrame()\n\n    # Generate all combinations\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle combinations\n    np.random.shuffle(combinations)\n\n    # Create DataFrame\n    df = pd.DataFrame(index=animals, columns=foods)\n\n    # Fill DataFrame with shuffled combinations\n    for animal, food in combinations:\n        df.at[animal, food] = f'{animal}:{food}'\n\n    # Fill any remaining empty cells\n    df.fillna('', inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Create a timezone object\n    tz = pytz.timezone(timezone)\n\n    # Convert timestamps to datetime objects in the specified timezone\n    converted_times = []\n    for ts in time_strings:\n        dt = datetime.fromisoformat(ts)\n        dt = dt.replace(tzinfo=pytz.UTC).astimezone(tz)\n        converted_times.append(dt)\n\n    # Calculate time differences in seconds\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = abs((converted_times[i] - converted_times[i-1]).total_seconds())\n        time_diffs.append(diff)\n\n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n\n    # Calculate and return the average time difference\n    return np.mean(time_diffs)"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text and remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split into words\n    words = text.split()\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10_words)\n    \n    # Create bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts)\n    \n    # Customize the plot\n    ax.set_title('Top 10 Most Common Words', fontsize=16)\n    ax.set_xlabel('Words', fontsize=12)\n    ax.set_ylabel('Frequency', fontsize=12)\n    ax.tick_params(axis='both', which='major', labelsize=10)\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout and add count labels\n    plt.tight_layout()\n    for i, count in enumerate(counts):\n        ax.text(i, count, str(count), ha='center', va='bottom')\n    \n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Extract URL from the string using regex\n    url_pattern = re.compile(r'https?://\\S+')\n    match = url_pattern.search(myString)\n    \n    if not match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = match.group()\n    \n    try:\n        # Fetch the content of the URL\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the webpage content to find the title\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if title_tag and title_tag.string:\n        return title_tag.string.strip()\n    else:\n        return \"No title tag found in the webpage.\""}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON data provided: {e}\")\n\n    # Check if the key exists in the JSON data\n    if unknown_key not in data:\n        raise KeyError(f\"Key '{unknown_key}' not found in the provided JSON data.\")\n    \n    # Get the URL associated with the specified key\n    url = data[unknown_key]\n\n    # Download the file from the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Failed to download the file from URL: {e}\")\n\n    # Generate the timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the directory to save the file\n    if save_dir is None:\n        save_dir = os.getcwd()\n    else:\n        os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\n    # Create the full path for the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    def extract_phone_numbers(text: str) -> list:\n        phone_pattern = re.compile(r'\\+?1?[-.\\s]?\\(?[2-9]\\d{2}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}')\n        return phone_pattern.findall(text)\n\n    try:\n        if url.startswith(('http://', 'https://')):\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            content = response.text\n        else:\n            with open(url, 'r', encoding='utf-8') as file:\n                content = file.read()\n\n        soup = bs4.BeautifulSoup(content, 'html.parser')\n        text = soup.get_text()\n        phone_numbers = extract_phone_numbers(text)\n\n        # Remove duplicates while preserving order\n        phone_numbers = list(dict.fromkeys(phone_numbers))\n\n        with open(output_path, 'w', encoding='utf-8') as json_file:\n            json.dump(phone_numbers, json_file, indent=2)\n\n        return phone_numbers\n\n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n    except IOError as e:\n        print(f\"Error reading file: {e}\")\n    except json.JSONDecodeError as e:\n        print(f\"Error writing JSON: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n    return []"}
