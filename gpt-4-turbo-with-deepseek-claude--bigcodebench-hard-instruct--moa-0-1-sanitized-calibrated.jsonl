{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        ftp.close()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.close()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Get list of files in the directory\n    try:\n        file_list = ftp.nlst()\n    except Exception as e:\n        ftp.close()\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.close()\n\n    # Construct the FTP URL\n    ftp_url = f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/'\n\n    # Download files using wget\n    downloaded_files = []\n    for file in file_list:\n        file_url = ftp_url + file\n        try:\n            subprocess.run(['wget', '-q', file_url], check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError:\n            print(f\"Failed to download {file}\")\n\n    return downloaded_files"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the CSV file containing the commands exists\n    if not os.path.isfile(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    # Read commands from the CSV file\n    with open(commands_file_path, 'r') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        for index, row in enumerate(csv_reader, start=1):\n            if row:  # Skip empty rows\n                command = row[0]\n                output_file = os.path.join(output_dir_path, f\"command_{index}_output.txt\")\n                output_files.append(output_file)\n\n                try:\n                    # Execute the command and capture output\n                    result = subprocess.run(command, shell=True, capture_output=True, text=True, check=True)\n                    output = result.stdout\n                except subprocess.CalledProcessError as e:\n                    # If the command fails, capture the error message and exit code\n                    output = f\"Command failed with exit code {e.returncode}\\n\"\n                    output += f\"Error message: {e.stderr}\"\n\n                # Write output to file\n                with open(output_file, 'w') as f:\n                    f.write(output)\n\n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    running = False\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            running = True\n            pid = proc.pid\n            break\n    \n    if not running:\n        # Process is not running, start it\n        try:\n            subprocess.Popen([process_name])\n            return f\"Process not found. Starting {process_name}.\"\n        except FileNotFoundError:\n            return f\"Error: {process_name} not found. Unable to start the process.\"\n    else:\n        # Process is running, terminate and restart it\n        try:\n            process = psutil.Process(pid)\n            process.terminate()\n            \n            # Wait for the process to terminate\n            while psutil.pid_exists(pid):\n                time.sleep(0.1)\n            \n            # Start the process again\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n        except psutil.NoSuchProcess:\n            # In case the process disappeared between finding it and trying to terminate it\n            subprocess.Popen([process_name])\n            return f\"Process disappeared. Starting {process_name}.\"\n        except FileNotFoundError:\n            return f\"Error: {process_name} not found. Unable to restart the process.\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n\n    # Get all files in the directory (not including subdirectories)\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]\n\n    # If there are no files, return None\n    if not files:\n        return None\n\n    # Create the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n\n    # Create a zip file and add all files to it\n    with zipfile.ZipFile(zip_path, 'w') as zip_file:\n        for file in files:\n            zip_file.write(file, os.path.basename(file))\n\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text using a regular expression\n    text_without_urls = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove extra whitespace and check if there are any words left\n    cleaned_text = ' '.join(text_without_urls.split())\n    if not cleaned_text.strip():\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud from the cleaned text\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n    # Return the WordCloud object\n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Initialize and train the RandomForestClassifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n\n    # Retrieve and sort feature importances\n    feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns).sort_values(ascending=False)\n\n    # Plotting the feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x=feature_importances, y=feature_importances.index)\n\n    # Setting the labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    plt.tight_layout()  # Adjusts the plot to ensure everything fits without overlap\n\n    # Return the classifier and the Axes object\n    return rf_classifier, ax"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password_hash = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password_hash, password)\n\n    users = {\n        '1': User('1', 'user1', 'password1'),\n        '2': User('2', 'user2', 'password2'),\n        '3': User('3', 'user3', 'password3')\n    }\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return users.get(user_id)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        if current_user.is_authenticated:\n            return redirect(url_for('protected'))\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = users.get('1')  # Example: Only user1 is able to login\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! This is a protected page.'\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    # Ensure the data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    \n    # Check if column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"The specified column '{column}' does not exist in the DataFrame.\")\n    \n    # Extract the specified column data\n    column_data = data[column].values.reshape(-1, 1)\n    \n    # Standardize the column data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data)\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n    \n    # Identify outlier indices\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers from the data\n    data_without_outliers = data.drop(index=data.index[outlier_indices])\n    \n    # Visualization\n    plt.figure(figsize=(12, 6))\n    \n    # Plot original data with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='blue', alpha=0.6, label='Original Data')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    \n    # Plot data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', alpha=0.6, label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return the original data, data without outliers, and indices of the outliers\n    return (data, data_without_outliers, outlier_indices)"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"The 'data' parameter must be a pd.DataFrame.\")\n    \n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"The 'n_clusters' parameter must be an integer greater than 1.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(data)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot data points colored by cluster\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis', alpha=0.6)\n    \n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=200, linewidths=3, color='red', label='Centroids')\n    \n    # Set plot labels and title\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    ax.set_title(f'K-means Clustering (k={n_clusters})')\n    \n    # Add legend\n    ax.legend()\n    \n    # Add colorbar\n    plt.colorbar(scatter, label='Cluster')\n    \n    return labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    # Validate n_components argument\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA on the data\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert the transformed data into a DataFrame\n    column_names = [f'PC{i+1}' for i in range(n_components)]\n    df_transformed = pd.DataFrame(transformed_data, columns=column_names)\n    \n    # Generate a scatter plot of the transformed data\n    fig, ax = plt.subplots(figsize=(8, 6))\n    if n_components >= 2:\n        ax.scatter(df_transformed['PC1'], df_transformed['PC2'], alpha=0.7)\n        ax.set_xlabel('First Principal Component')\n        ax.set_ylabel('Second Principal Component')\n        ax.set_title('Scatter Plot of PCA Results')\n    elif n_components == 1:\n        ax.scatter(range(len(df_transformed)), df_transformed['PC1'], alpha=0.7)\n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel('First Principal Component')\n        ax.set_title('Scatter Plot of PCA Result (1 Component)')\n\n    plt.tight_layout()\n    plt.show()\n    \n    return df_transformed, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Set the global font to Arial for better readability and visual appeal\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Create the pair plot\n    g = sns.pairplot(iris_df, hue='species', height=2.5)\n\n    # Set the plot title and format the labels for better readability\n    g.fig.suptitle('Iris Dataset Pair Plot', fontsize=16, y=1.02)\n\n    # Format the labels on each axis\n    for ax in g.axes.flatten():\n        # Format x-axis labels\n        if ax.get_xlabel():\n            ax.set_xlabel(ax.get_xlabel().replace(' (cm)', '').title() + ' (cm)')\n        # Format y-axis labels\n        if ax.get_ylabel():\n            ax.set_ylabel(ax.get_ylabel().replace(' (cm)', '').title() + ' (cm)')\n\n    # Adjust layout to prevent overlap and ensure the title is well positioned\n    plt.tight_layout()\n    g.fig.subplots_adjust(top=0.95)  # Adjust the top margin to accommodate the title\n\n    return g.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.now().date()\n        start_date = end_date - timedelta(days=29)\n        dates = [start_date + timedelta(days=i) for i in range(30)]\n\n        # Generate random values\n        values = [random.uniform(0, 100) for _ in range(30)]\n\n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Create the plot\n        plt.figure(figsize=(12, 6))\n        ax = plt.gca()\n\n        # Plot the data\n        ax.plot(df['Date'], df['Value'], marker='o')\n\n        # Customize the plot\n        ax.set_xlabel('Date', fontname='Arial', fontsize=12)\n        ax.set_ylabel('Value', fontname='Arial', fontsize=12)\n        ax.set_title('Random Time Series Data', fontname='Arial', fontsize=14, fontweight='bold')\n\n        # Rotate x-axis labels for better readability\n        plt.xticks(rotation=45, ha='right', fontname='Arial')\n        plt.yticks(fontname='Arial')\n\n        # Adjust layout to prevent cutting off labels\n        plt.tight_layout()\n\n        # Set font for legend and tick labels\n        for label in ax.get_xticklabels() + ax.get_yticklabels():\n            label.set_fontname('Arial')\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"Error generating plot: {str(e)}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    \"\"\"\n    Draw the correlation heatmap of the Boston Housing dataset using Seaborn.\n\n    Args:\n    data_url (str): URL of the Boston Housing dataset.\n    seed (int): Random seed for reproducibility.\n    save_path (str, optional): Path to save the plot. If None, the plot is not saved.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n\n    Raises:\n    ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    try:\n        # Set random seed for reproducibility\n        np.random.seed(seed)\n\n        # Load the Boston Housing dataset\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n\n        # Create a DataFrame with feature names\n        feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n                         'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        df = pd.DataFrame(data, columns=feature_names)\n        df['MEDV'] = target\n\n        # Calculate the correlation matrix\n        corr_matrix = df.corr()\n\n        # Create a new figure and axis\n        fig, ax = plt.subplots(figsize=(12, 10))\n\n        # Draw the heatmap\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n\n        # Set the title\n        ax.set_title('Correlation Heatmap of Boston Housing Dataset')\n\n        # Save the plot if a save path is provided\n        if save_path:\n            plt.savefig(save_path)\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {str(e)}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Ensure the DataFrame contains the required columns\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Check if 'value' column is numeric\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must be numeric.\")\n    \n    # Validate freq parameter\n    try:\n        pd.date_range(end='2020-01-01', periods=1, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' must be a valid frequency string.\")\n    \n    # Validate decomposition_model parameter\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n    \n    # Perform the seasonal decomposition\n    result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.infer_freq(df.index) or 1)\n    \n    # Create plot\n    fig, axes = plt.subplots(4, 1, figsize=(10, 8))\n    result.observed.plot(ax=axes[0], title='Observed')\n    result.trend.plot(ax=axes[1], title='Trend')\n    result.seasonal.plot(ax=axes[2], title='Seasonal')\n    result.resid.plot(ax=axes[3], title='Residual')\n    plt.tight_layout()\n\n    return result, axes"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate the input dates to ensure they are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n    # Ensure start_date is not after end_date\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must not be later than 'end_date'.\")\n\n    # Set the random seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the inclusive number of days between start_date and end_date\n    date_range = (end_date - start_date).days + 1\n\n    # Generate random dates within the specified inclusive range\n    random_dates = [start_date + timedelta(days=randint(0, date_range - 1)) for _ in range(date_range)]\n\n    # Return a pandas Series containing the random dates\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The provided 'my_list' is not a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append(12)\n    \n    # Calculate the sum of the elements in the list to determine the number of files to concatenate\n    num_files_to_concat = sum(my_list)\n    \n    # Construct the file path pattern for glob to find matching files\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(file_pattern)\n    \n    # Check if there are any files in the directory\n    if not files:\n        raise FileNotFoundError(f\"No files found in the specified directory: {file_dir}\")\n    \n    # Sort files to ensure the order is consistent\n    files.sort()\n    \n    # Ensure we do not try to concatenate more files than available\n    files_to_read = files[:num_files_to_concat]\n    \n    # Read and concatenate the selected CSV files into a single DataFrame\n    concatenated_df = pd.concat([pd.read_csv(file) for file in files_to_read], ignore_index=True)\n    \n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Validate that my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input 'my_list' must be a list\")\n\n    # Validate that my_list has only numeric elements\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in 'my_list' must be numeric (int or float)\")\n\n    # Append 12 to my_list\n    my_list.append(12)\n\n    # Set the random seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the sum of my_list\n    list_sum = sum(my_list)\n\n    # Determine the size of the random numbers list based on the sum of my_list and the specified size limit\n    random_list_size = min(list_sum, size)\n\n    # Start timing the random number generation\n    start_time = time.time()\n\n    # Generate the list of random integers\n    random_list = [randint(1, 100) for _ in range(random_list_size)]\n\n    # Stop timing the generation process\n    end_time = time.time()\n\n    # Calculate the time taken to generate the random numbers\n    time_taken = end_time - start_time\n\n    # Create a histogram to visualize the distribution of the random numbers\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 102), align='left', rwidth=0.8)\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Random Numbers')\n    ax.set_xlim(0, 101)\n\n    # Return the time taken and the matplotlib Axes object for further manipulation or display\n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # This will raise an HTTPError for bad responses\n\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the URL\")\n\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP request failed: {e}\")\n\n    try:\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the first table in the HTML\n        table = soup.find('table')\n        if table is None:\n            raise ValueError(\"No table found on the page\")\n\n        # Extract headers (if present)\n        headers = []\n        header_row = table.find('tr')\n        if header_row:\n            headers = [th.text.strip() for th in header_row.find_all('th')]\n            if not headers:  # Check if <th> is absent and <td> is used instead in the header row\n                headers = [td.text.strip() for td in header_row.find_all('td')]\n\n        # Extract table data\n        data = []\n        for row in table.find_all('tr'):\n            row_data = [td.text.strip() for td in row.find_all('td')]\n            if row_data:\n                data.append(row_data)\n\n        if not data:\n            raise ValueError(\"No data found in the table\")\n\n        # Create DataFrame\n        df = pd.DataFrame(data, columns=headers if headers else None)\n        return df\n\n    except Exception as e:\n        raise ValueError(f\"Error parsing table data: {e}\")"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n    \n    # Check if there are any numeric columns in the DataFrame\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n    if numeric_columns.empty:\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n    \n    # Create histograms for each numeric column and collect Axes objects\n    axes_list = []\n    for column in numeric_columns:\n        fig, ax = plt.subplots()\n        ax.hist(df[column].dropna(), bins='auto', edgecolor='black')\n        ax.set_title(column)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n        plt.close(fig)  # Close the figure to prevent display\n    \n    return axes_list"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    def check_port(ip, port, results):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        results[str(ip)] = result == 0\n        sock.close()\n\n    results = {}\n    threads = []\n\n    for ip in IPv4Network(ip_range):\n        thread = Thread(target=check_port, args=(ip, port, results))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Regular expression pattern to match log entries\n    pattern = r'(\\w+):\\s+\\[(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\\]\\s+-\\s+(.+)'\n    \n    log_data = []\n    \n    try:\n        with open(log_file, 'r') as file:\n            for line in file:\n                match = re.match(pattern, line.strip())\n                if match:\n                    log_type, timestamp, message = match.groups()\n                    \n                    # Validate timestamp\n                    try:\n                        datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    except ValueError:\n                        raise ValueError(f\"Invalid timestamp format in log entry: {line.strip()}\")\n                    \n                    log_data.append({\n                        'Type': log_type,\n                        'Timestamp': timestamp,\n                        'Message': message\n                    })\n    \n        if not log_data:\n            raise ValueError(\"No valid log entries found in the file.\")\n        \n        # Create a DataFrame from the log data\n        df = pd.DataFrame(log_data)\n        \n        # Generate output CSV file name\n        output_file = log_file.rsplit('.', 1)[0] + '_structured.csv'\n        \n        # Save DataFrame to CSV\n        df.to_csv(output_file, index=False)\n        \n        return output_file\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Log file not found: {log_file}\")\n    except PermissionError:\n        raise PermissionError(f\"Permission denied: Unable to read {log_file}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred while processing the log file: {str(e)}\")"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Clean the text and split into words using regex to match word boundaries\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Check if there are any words to plot\n    if word_lengths:\n        # Determine the range of word lengths\n        min_length = min(word_lengths)\n        max_length = max(word_lengths)\n        \n        # Create histogram of word lengths\n        n, bins, patches = ax.hist(word_lengths, bins=range(min_length, max_length + 2), \n                                   rwidth=rwidth, align='left', edgecolor='black')\n        \n        # Set labels and title\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths')\n        \n        # Set x-axis ticks to clearly show each word length\n        ax.set_xticks(range(min_length, max_length + 1))\n        \n        # Optionally, add value labels on top of each bar for better visualization\n        for i, v in enumerate(n):\n            ax.text(bins[i] + 0.5, v, str(int(v)), ha='center', va='bottom')\n        \n        # Adjust layout to prevent clipping of tick-labels\n        plt.tight_layout()\n    else:\n        # Display a message if no words are found in the text\n        ax.text(0.5, 0.5, 'No words found in the text', ha='center', va='center', fontsize=12)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\ndef task_func(df):\n    # Check if DataFrame is empty or missing required columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Ensure the 'punkt' tokenizer from NLTK is available\n    try:\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    # Filter articles based on keywords in title\n    keywords = ['like', 'what']\n    filtered_df = df[df['Title'].str.lower().str.contains('|'.join(keywords), regex=True)]\n\n    # Combine all content from filtered articles and remove punctuation\n    all_content = ' '.join(filtered_df['Content'].astype(str))\n    all_content = re.sub(f'[{punctuation}]', '', all_content.lower())\n\n    # Tokenize the content into words\n    words = nltk.word_tokenize(all_content)\n\n    # Count word frequencies\n    word_freq = Counter(words)\n\n    return dict(word_freq)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    def preprocess_text(text):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Remove stopwords\n        words = text.split()\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n\n    # Apply preprocessing to the specified text column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Initialize and fit the CountVectorizer\n    vectorizer = CountVectorizer()\n    word_count_matrix = vectorizer.fit_transform(dataframe[text_column])\n\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Convert the matrix to a DataFrame\n    word_count_df = pd.DataFrame(word_count_matrix.toarray(), columns=feature_names)\n\n    return word_count_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate dictionary for 'Lon' and 'Lat' keys and their tuple type\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Both 'Lon' and 'Lat' keys must be present in the dictionary.\")\n    if not (isinstance(dic['Lon'], tuple) and isinstance(dic['Lat'], tuple)):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples.\")\n    \n    # Generate random coordinates within the specified ranges\n    longitudes = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    latitudes = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n    \n    # Create Point objects for each city based on the generated coordinates\n    geometries = [Point(lon, lat) for lon, lat in zip(longitudes, latitudes)]\n    \n    # Create a GeoDataFrame with 'City' and 'Coordinates' columns\n    gdf = gpd.GeoDataFrame({\n        'City': cities,\n        'Coordinates': geometries\n    }, geometry='Coordinates')\n    \n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Input validation\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    \n    # Set random seed\n    set_seed(seed)\n    \n    # Create a list to store weather data\n    weather_data = []\n    \n    # Generate weather report for each city\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone not provided for city: {city}\")\n        \n        # Get the timezone for the city\n        tz = pytz.timezone(timezones[city])\n        \n        # Convert UTC time to local time\n        local_time = utc_datetime.replace(tzinfo=pytz.UTC).astimezone(tz)\n        \n        # Format local time as string\n        local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Generate random weather condition\n        weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Add data to the list\n        weather_data.append({\n            'City': city,\n            'Local Time': local_time_str,\n            'Weather Condition': weather\n        })\n    \n    # Create DataFrame from the weather data\n    df = pd.DataFrame(weather_data)\n    \n    return df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    # Input validation\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate cumulative sum for the walk\n    walk = np.cumsum(steps)\n\n    # Calculate descriptive statistics\n    stats = pd.Series(walk).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    stats_dict = {\n        'count': stats['count'],\n        'mean': stats['mean'],\n        'std': stats['std'],\n        'min': stats['min'],\n        '5%': stats['5%'],\n        '25%': stats['25%'],\n        'median': stats['50%'],\n        '75%': stats['75%'],\n        '95%': stats['95%'],\n        'max': stats['max']\n    }\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(range(elements + 1), np.insert(walk, 0, 0), color='blue')\n    ax.set_title(f'Random Walk of {elements} Steps')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.grid(True)\n\n    return stats_dict, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    # Define the path for the downloaded zip file\n    zip_file_path = os.path.join(destination_directory, 'downloaded.zip')\n\n    try:\n        # Download the zip file\n        with requests.get(url, headers=headers, stream=True) as response:\n            response.raise_for_status()  # Check that the request was successful\n            with open(zip_file_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n        # Extract the contents of the zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n\n        # Create a list of filenames of the extracted files\n        extracted_files = []\n        for root, _, files in os.walk(destination_directory):\n            for file in files:\n                if file != 'downloaded.zip':  # Exclude the zip file itself\n                    extracted_files.append(os.path.join(root, file))\n\n        return extracted_files\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading the zip file: {e}\")\n        return []\n    except zipfile.BadZipFile:\n        print(\"The downloaded file is not a valid zip file.\")\n        return []\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []\nurl = \"https://example.com/sample.zip\"\ndestination = \"/path/to/extract\""}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Args:\n        seed (int): Random seed for reproducibility.\n        image_size (tuple): Size of the image (height, width, channels).\n        range_low (int): Lower bound of pixel values.\n        range_high (int): Upper bound of pixel values.\n\n    Returns:\n        tuple: (ax, image)\n            ax (matplotlib.axes.Axes): Axes object of the plot.\n            image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate random image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Hide axis details\n    plt.title(\"Random RGB Image\")\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file '{audio_file}' does not exist.\")\n\n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the Sound Pressure Level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    if len(L) != M * N:\n        raise ValueError(\"The length of list L must be exactly M * N\")\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / np.max(np.abs(matrix)) * spl\n\n    # Generate the spectrogram\n    S = librosa.stft(normalized_matrix.flatten())\n    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n\n    # Create the figure and plot the spectrogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img = librosa.display.specshow(S_db, sr=samplerate, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([value for tup in original for value in tup if isinstance(value, (int, float))])\n\n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'standard_deviation': np.std(numeric_values),\n        'minimum': np.min(numeric_values),\n        'maximum': np.max(numeric_values)\n    }\n\n    # Create histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    \n    # Plot histogram\n    ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='skyblue', edgecolor='black')\n    \n    # Calculate and plot PDF\n    x = np.linspace(np.min(numeric_values), np.max(numeric_values), 100)\n    kde = stats.gaussian_kde(numeric_values)\n    ax.plot(x, kde(x), 'r-', lw=2)\n\n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with PDF')\n\n    # Add grid\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    return numeric_values, statistics, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])[0]\n\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the original array\n    ax.plot(original_array, label='Original', marker='o', color='blue')\n\n    # Plot the normalized array\n    ax.plot(normalized_array, label='Normalized', marker='s', color='green')\n\n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Comparison of Original and Normalized Arrays')\n\n    # Add legend\n    ax.legend()\n\n    # Add grid for better visibility\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    # Display the plot\n    plt.show()\n\n    # Return the original and normalized arrays, and the Axes object\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Add a new key \"a\" with value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    # Assume the keys are frequencies and values are amplitudes (if applicable)\n    time = np.arange(0, 1, 1/sample_rate)  # Time vector for 1 second\n    signal = np.zeros_like(time)\n    for frequency, amplitude in data.items():\n        if type(frequency) in [int, float]:  # Ensure that the frequency is numeric\n            signal += amplitude * np.sin(2 * np.pi * frequency * time)\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), 1/sample_rate)\n\n    # 4. Plot the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(freqs[:len(freqs)//2], np.abs(fft_result)[:len(fft_result)//2])  # Plot only the positive frequencies\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('FFT of the Signal')\n\n    # Return the FFT result and the plot axes\n    return fft_result, ax"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\ndef task_func():\n    class DataHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            content_type = self.headers.get('Content-Type')\n\n            if content_type != 'application/json':\n                self.send_error(400, \"Content-Type header is not application/json\")\n                return\n\n            try:\n                post_data = self.rfile.read(content_length)\n                json_data = json.loads(post_data.decode('utf-8'))\n\n                if 'data' not in json_data:\n                    self.send_error(400, \"No data key in request\")\n                    return\n\n                # Process the data here (not implemented in this example)\n\n                self.send_response(200)\n                self.send_header('Content-Type', 'application/json')\n                response = json.dumps(SUCCESS_RESPONSE)\n                self.send_header('Content-Length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON\")\n\n    return DataHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length).decode('utf-8')\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_error(400, \"Bad Request: Invalid JSON\")\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            if not all(key in email_data for key in required_keys):\n                self.send_error(400, \"Bad Request: Missing required keys\")\n                return\n\n            subject = email_data['subject']\n            message = email_data['message']\n            recipient = email_data['to']\n\n            msg = MIMEText(message)\n            msg['Subject'] = subject\n            msg['From'] = smtp_username\n            msg['To'] = recipient\n\n            try:\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n\n                response = \"Email sent successfully\"\n                self.send_response(200)\n                self.send_header('Content-type', 'text/plain')\n                self.send_header('Content-length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication Failed\")\n            except Exception as e:\n                self.send_error(500, f\"Internal Server Error: {str(e)}\")\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    # Initialize a Counter to store word counts for each file\n    word_counts = Counter()\n    \n    # Initialize total word count\n    total_words = 0\n    \n    # Iterate through files in the specified directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                \n                # Read the content of the file\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    \n                    # Split content into words and count them\n                    words = content.split()\n                    file_word_count = len(words)\n                    \n                    # Update the Counter and total word count\n                    word_counts[file_path] = file_word_count\n                    total_words += file_word_count\n    \n    # Export word counts to a JSON file\n    with open(os.path.join(directory, filename), 'w', encoding='utf-8') as json_file:\n        json.dump(dict(word_counts), json_file, indent=4)\n    \n    # Return the total number of words\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    # Validate the input DataFrame\n    if df.empty or not all(col in df.columns for col in COLUMNS):\n        raise ValueError(\"DataFrame is empty or missing required columns.\")\n\n    # Ensure that the 'Value' column contains only lists\n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"'Value' column must contain lists.\")\n\n    # Split lists in 'Value' column into separate columns\n    max_length = max(len(val) for val in df['Value'])\n    split_df = pd.DataFrame(df['Value'].tolist(), columns=[f'Value_{i+1}' for i in range(max_length)])\n    \n    # Calculate the Pearson correlation coefficient\n    correlation_matrix = split_df.corr(method='pearson')\n\n    # Visualize the correlation matrix if plot is True\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n        plt.title(\"Correlation Heatmap\")\n        plt.tight_layout()\n        plt.show()\n        return correlation_matrix, ax\n\n    return correlation_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with any additional fields provided\n    all_fields = FIELDS + additional_fields\n\n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n\n    # Create a DataFrame with the generated data\n    df = pd.DataFrame(data, index=STUDENTS)\n\n    # Calculate the average grade for each student and add it as a new column\n    df['Average Grade'] = df.mean(axis=1)\n\n    # Calculate the average grade for each subject and add it as a new row\n    subject_averages = df.mean()\n    subject_averages.name = 'Average'\n\n    # Append the subject averages as a new row in the DataFrame\n    df = df.append(subject_averages)\n\n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate simulated data\n    data = []\n    for i in range(PEOPLE_COUNT):\n        name = f\"Person_{i + 1}\"\n        age = random.randint(18, 80)\n        height = round(random.uniform(150, 200), 1)  # in cm\n        weight = round(random.uniform(45, 120), 1)  # in kg\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = mean([row[1] for row in data])\n    avg_height = mean([row[2] for row in data])\n    avg_weight = mean([row[3] for row in data])\n\n    # Write data to CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n        \n        # Append averages\n        writer.writerow(['Average', round(avg_age, 1), round(avg_height, 1), round(avg_weight, 1)])\n\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Dictionary to store subdirectories and the files moved to them\n    organized_files = {}\n\n    # Regular expression to find the first text not enclosed in square brackets\n    pattern = re.compile(r'(?<!\\[)[^\\[\\]]+(?!\\])')\n\n    # Process each file in the directory\n    for filename in os.listdir(directory):\n        # Construct the full file path\n        filepath = os.path.join(directory, filename)\n\n        # Skip if it's not a file\n        if not os.path.isfile(filepath):\n            continue\n\n        # Use regex to find matching text\n        match = pattern.search(filename)\n\n        if match:\n            subdirectory_name = match.group().strip()\n\n            # Create subdirectory if it doesn't exist\n            subdirectory_path = os.path.join(directory, subdirectory_name)\n            if not os.path.exists(subdirectory_path):\n                os.makedirs(subdirectory_path)\n\n            # Move file to the subdirectory\n            new_filepath = os.path.join(subdirectory_path, filename)\n            shutil.move(filepath, new_filepath)\n\n            # Track moved files\n            if subdirectory_name not in organized_files:\n                organized_files[subdirectory_name] = []\n            organized_files[subdirectory_name].append(filename)\n\n    # Generate a summary path with a timestamp to ensure uniqueness\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    organized_directory = f\"{directory}_organized_{timestamp}\"\n\n    # Return the path and the organization details\n    return (organized_directory, organized_files)"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport threading\ndef task_func(file_list):\n    exit_codes = [None] * len(file_list)  # Initialize list to store exit codes\n    \n    def run_file(index, file_path):\n        # Run the file as a subprocess and capture the exit code\n        process = subprocess.Popen(['python', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()  # Wait for the subprocess to finish\n        exit_codes[index] = process.returncode  # Store the exit code at the corresponding index\n\n    threads = []\n    for i, file_path in enumerate(file_list):\n        # Create a new thread for each file in the list\n        thread = threading.Thread(target=run_file, args=(i, file_path))\n        threads.append(thread)\n        thread.start()  # Start the thread\n\n    for thread in threads:\n        thread.join()  # Wait for all threads to complete\n\n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    # Ensure the directory path exists\n    if not os.path.isdir(directory_path):\n        print(f\"Error: '{directory_path}' is not a valid directory.\")\n        return []\n\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    results = []\n\n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Run the .bat file\n            if sys.platform == \"win32\":\n                # On Windows, use shell=True to run .bat files\n                process = subprocess.run(bat_file, shell=True, check=True)\n            else:\n                # On non-Windows systems, use 'cmd /c' to run .bat files\n                process = subprocess.run([\"cmd\", \"/c\", bat_file], check=True)\n            \n            exit_code = process.returncode\n        except subprocess.CalledProcessError as e:\n            # If the process fails, capture the non-zero exit code\n            exit_code = e.returncode\n        except Exception:\n            # If the file couldn't be executed for any other reason, set exit_code to None\n            exit_code = None\n\n        results.append((file_name, exit_code))\n\n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    # Validate input requirements\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' does not exist in the DataFrame\")\n\n    # Determine if the column data is numeric or categorical\n    is_numeric = pd.api.types.is_numeric_dtype(df[col])\n\n    # Prepare figure and subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n\n    # First subplot: Histogram with KDE for numeric data, Bar plot for categorical data\n    if is_numeric:\n        sns.histplot(data=df, x=col, kde=True, ax=ax1)\n        ax1.set_title(f'Histogram with KDE for {col}')\n    else:\n        sns.countplot(data=df, x=col, ax=ax1)\n        ax1.set_title(f'Bar Plot for {col}')\n    ax1.set_xlabel(col)\n    ax1.set_ylabel('Frequency')\n\n    # Second subplot: Box plot\n    sns.boxplot(data=df, x=col, ax=ax2)\n    ax2.set_title(f'Box Plot for {col}')\n\n    # Adjust layout for clear visualization\n    plt.tight_layout()\n\n    # Return the matplotlib figure object\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    # Prepare the command to execute the Python script with provided arguments\n    command = [sys.executable, script_path] + list(args)\n\n    try:\n        # If wait is True, use subprocess.run to wait for the process to complete\n        if wait:\n            result = subprocess.run(command, check=True, capture_output=True, text=True)\n            return result.returncode  # Return the return code of the subprocess\n        else:\n            # If wait is False, use subprocess.Popen to start the process and return None\n            subprocess.Popen(command, start_new_session=True)\n            return None\n    except subprocess.CalledProcessError as e:\n        # Handle the exception raised if the subprocess exits with a non-zero status\n        print(f\"Script output (stdout):\\n{e.stdout}\")\n        print(f\"Script output (stderr):\\n{e.stderr}\")\n        raise  # Re-raise the exception to indicate failure"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if the Excel file exists at the specified location\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    # Attempt to read the specified sheet from the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\")\n    \n    # Calculate the mean and standard deviation for each column in the DataFrame\n    stats = {column: {'mean': df[column].mean(), 'std': df[column].std()} for column in df.columns}\n    \n    # Create a bar chart with the calculated statistics\n    fig, ax = plt.subplots(figsize=(10, 6))\n    columns = np.arange(len(df.columns))\n    bar_width = 0.35\n\n    # Plotting means and standard deviations\n    means = [stats[col]['mean'] for col in df.columns]\n    stds = [stats[col]['std'] for col in df.columns]\n\n    ax.bar(columns - bar_width/2, means, width=bar_width, label='Mean', color='b', yerr=stds, capsize=5)\n    ax.bar(columns + bar_width/2, stds, width=bar_width, label='Standard Deviation', color='r')\n\n    # Setting the labels and title for the chart\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(columns)\n    ax.set_xticklabels(df.columns, rotation=45, ha='right')\n    ax.legend()\n\n    plt.tight_layout()\n\n    return stats, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n\n    # Count activities for each day of the week\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_name = activity.strftime('%A')\n        day_counts[day_name] += 1\n\n    # Define the order of days\n    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    # Create lists for x-axis labels and y-axis values\n    days = [day for day in days_order]\n    counts = [day_counts[day] for day in days_order]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n\n    # Customize the chart\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n\n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Check if the source directory exists\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n\n    # Check if the destination directory exists, create it if it doesn't\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n\n    # Get list of files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n\n    # Check if there are any files to move\n    if not files:\n        raise FileNotFoundError(f\"No files found in source directory '{src_dir}'.\")\n\n    # Select a random file\n    file_to_move = random.choice(files)\n\n    # Define source and destination paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    # Return the name of the moved file\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Ensure the directory path ends with a slash\n    if not directory_path.endswith('/'):\n        directory_path += '/'\n\n    # Get all .xlsx files in the specified directory\n    xlsx_files = glob.glob(directory_path + '*.xlsx')\n    processed_files = 0\n\n    for file_path in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file_path)\n\n        # Flag to check if any changes were made\n        changes_made = False\n\n        # Iterate through all sheets in the workbook\n        for sheet in workbook.sheetnames:\n            worksheet = workbook[sheet]\n\n            # Iterate through all cells in the worksheet\n            for row in worksheet.iter_rows():\n                for cell in row:\n                    if cell.data_type == 's':  # Check if cell contains a string\n                        # Use regex to find and replace unescaped double quotes\n                        new_value = re.sub(r'(?<!\\\\)\"', r'\\\\\"', cell.value)\n                        if new_value != cell.value:\n                            cell.value = new_value\n                            changes_made = True\n\n        # Save the workbook if changes were made\n        if changes_made:\n            workbook.save(file_path)\n            processed_files += 1\n\n    return processed_files"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Create a diagram of a sine wave and cosine wave with a given frequency.\n\n    Parameters:\n    frequency (float): The frequency of the waves.\n    sample_size (int, optional): The number of points to sample. Defaults to 10000.\n\n    Returns:\n    tuple: (matplotlib.figure.Figure, matplotlib.axes.Axes)\n\n    Raises:\n    ValueError: If the frequency is negative or if the sample size is non-positive.\n    \"\"\"\n    # Validate input parameters\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    # Generate time values\n    t = np.linspace(0, 2 * np.pi, sample_size)\n    \n    # Calculate sine and cosine waves\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    \n    # Set labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves (Frequency: {frequency} Hz)')\n    \n    # Add legend\n    ax.legend()\n    \n    # Add grid\n    ax.grid(True)\n    \n    # Set y-axis limits\n    ax.set_ylim(-1.1, 1.1)\n    \n    # Return the figure and axes object\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    # Create Flask app\n    app = Flask(app_name)\n\n    # Configure mail settings\n    mail_config = {\n        'MAIL_SERVER': os.getenv('MAIL_SERVER', 'localhost'),\n        'MAIL_PORT': int(os.getenv('MAIL_PORT', 25)),\n        'MAIL_USE_TLS': os.getenv('MAIL_USE_TLS', 'False').lower() in ('true', '1', 't'),\n        'MAIL_USERNAME': os.getenv('MAIL_USERNAME', None),\n        'MAIL_PASSWORD': os.getenv('MAIL_PASSWORD', None)\n    }\n\n    # Update app config with mail settings\n    app.config.update(mail_config)\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Return a tuple containing the Flask-Mail instance and the app's mail configurations\n    return mail, app.config"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full file path\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The Excel file '{full_path}' does not exist.\")\n    \n    # Load the Excel file\n    df = pd.read_excel(full_path)\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Extract the data from the specified column\n    data = df[column_name]\n    \n    # Calculate mean, median, and standard deviation\n    mean = np.mean(data)\n    median = np.median(data)\n    std_dev = np.std(data)\n    \n    # Create and return the result dictionary\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"standard_deviation\": std_dev\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y, learning_rate=0.01, epochs=50):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Create the Sequential model with one hidden layer\n    model = Sequential([\n        Dense(4, activation='sigmoid', input_dim=2),  # Hidden layer with 4 neurons\n        Dense(1, activation='sigmoid')  # Output layer\n    ])\n\n    # Compile the model using binary cross-entropy loss and SGD optimizer\n    model.compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy')\n\n    # Train the model on the training data, also using test set as validation data\n    history = model.fit(X_train, Y_train, epochs=epochs, verbose=0, validation_data=(X_test, Y_test))\n\n    # Plot the training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    # Return the trained model and the Axes object of the plot\n    return model, ax"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer\n    model = keras.Sequential([\n        keras.layers.Dense(units=64, activation='sigmoid', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(units=1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    # Fit the model to the training data\n    model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n\n    # Make predictions on the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n\n    # Calculate ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.set_title('ROC Curve')\n    ax.legend(loc=\"lower right\")\n\n    return model, ax"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Validate existence of the image file\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"Image file not found at {image_path}\")\n    \n    # Validate n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"Unable to read image at {image_path}\")\n\n    # Convert BGR to RGB as the function assumes image is in RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixels = image_rgb.reshape((-1, 3))\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n\n    # Get the labels and cluster centers\n    labels = kmeans.labels_\n    centers = np.uint8(kmeans.cluster_centers_)\n\n    # Create the segmented image\n    segmented = centers[labels].reshape(image_rgb.shape)\n\n    # Save each region as a separate image\n    for i in range(n_clusters):\n        mask = (labels == i).reshape(image_rgb.shape[:2])\n        region = np.zeros_like(image_rgb)\n        region[mask] = segmented[mask]\n        cv2.imwrite(f'region_{i + 1}.jpg', cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n\n    return (image_rgb, segmented)"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Calculate the product of matrix P and tensor T using np.tensordot\n    result = np.tensordot(P, T, axes=([1], [0]))\n\n    # Flatten the result\n    flattened_result = result.flatten()\n\n    # Reshape the flattened result for KMeans (each sample should be a row)\n    X = flattened_result.reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(X)\n\n    # Visualize the clustering\n    fig, ax = plt.subplots(figsize=(10, 6))\n    scatter = ax.scatter(X, np.zeros_like(X), c=cluster_result, cmap='viridis', alpha=0.6)\n    \n    # Add colorbar\n    colorbar = plt.colorbar(scatter)\n    colorbar.set_label('Cluster')\n\n    # Set title and labels\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Flattened Data')\n    ax.set_yticks([])  # Remove y-axis ticks as they are not meaningful in this context\n\n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Input validation\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input 'points' must be a numpy array\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input 'points' must be a 2D array with shape (n, 2)\")\n    if not isinstance(seed, int):\n        raise TypeError(\"Input 'seed' must be an integer\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Apply jittering to avoid colinear points and degenerate cases\n    jitter = np.random.normal(0, 1e-8, points.shape)\n    jittered_points = points + jitter\n\n    # Calculate Voronoi diagram\n    vor = Voronoi(jittered_points)\n\n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=2)\n    ax.plot(points[:, 0], points[:, 1], 'ko')\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_aspect('equal')\n\n    plt.show()\n\n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory not found: {src_dir}\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory not found: {dest_dir}\")\n\n    # Ensure the extension starts with a dot\n    if not ext.startswith('.'):\n        ext = '.' + ext\n\n    # Get all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f'*{ext}'))\n\n    moved_files = []\n\n    for file in files:\n        filename = os.path.basename(file)\n        dest_path = os.path.join(dest_dir, filename)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_path):\n            # Move the file\n            shutil.move(file, dest_path)\n            moved_files.append(dest_path)\n\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    # Check if the input string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    try:\n        # Load JSON string into a dictionary\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        print(\"Invalid JSON string\")\n        return pd.DataFrame()\n\n    # Function to double numerical values, including those in strings\n    def double_value(value):\n        if isinstance(value, (int, float)):\n            return float(value * 2)\n        elif isinstance(value, str):\n            # Try to extract and double numerical values from strings\n            numbers = re.findall(r'-?\\d+\\.?\\d*', value)\n            if numbers:\n                doubled_numbers = [str(float(num) * 2) for num in numbers]\n                for old, new in zip(numbers, doubled_numbers):\n                    value = value.replace(old, new)\n            return value\n        elif isinstance(value, list):\n            return [double_value(item) for item in value]\n        else:\n            return value\n\n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {key: double_value(value) for key, value in data_dict.items()}\n\n    # Create a DataFrame from the normalized dictionary\n    df = pd.DataFrame([normalized_dict])\n\n    # Ensure numerical values are stored as floats\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            try:\n                df[column] = df[column].astype(float)\n            except ValueError:\n                pass  # Keep as object if conversion to float fails\n\n    return df"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script not found: {script_path}\")\n\n    # Start the subprocess using bash\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    start_time = time.time()\n    cpu_usage = 0\n    memory_usage = 0\n\n    try:\n        while process.poll() is None:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                raise TimeoutError(f\"Script execution timed out after {timeout} seconds\")\n\n            try:\n                p = psutil.Process(process.pid)\n                if p.status() == psutil.STATUS_ZOMBIE:\n                    break\n\n                cpu_percent = p.cpu_percent(interval=0.1)\n                cpu_usage += cpu_percent * 0.1  # Accumulate CPU usage\n\n                memory_info = p.memory_info()\n                memory_usage += max(0, memory_info.rss - memory_usage)  # Accumulate positive memory increments\n\n            except psutil.NoSuchProcess:\n                break\n\n            time.sleep(0.1)\n\n    finally:\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Assign categories ensuring each category appears at least once if possible\n    if N >= len(CATEGORIES):\n        # Fill categories so each appears at least once\n        categories = CATEGORIES + list(np.random.choice(CATEGORIES, N - len(CATEGORIES)))\n        np.random.shuffle(categories)\n    else:\n        # Randomly sample categories without replacement\n        categories = list(np.random.choice(CATEGORIES, N, replace=False))\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in CATEGORIES:\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category, alpha=0.7)\n    \n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Scatter Plot of X vs Y, Colored by Category')\n    ax.legend(title='Category')\n    \n    plt.show()\n    \n    return (df, ax)"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Convert epoch times to datetime objects\n    start_time = datetime.fromtimestamp(start_time)\n    end_time = datetime.fromtimestamp(end_time)\n\n    # Generate a range of timestamps\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n\n    # Generate values from a normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps))\n\n    # Apply a linear trend to the values\n    trend_values = np.linspace(0, trend * (len(timestamps) - 1), len(timestamps))\n    values += trend_values\n\n    # Create a DataFrame to store the time and values\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n\n    # Plotting the time series\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.plot(df['Time'], df['Value'], label='Time Series with Trend')\n\n    # Setting labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    # Format x-axis to display dates and times\n    ax.xaxis.set_major_locator(plt.AutoLocator())\n    ax.xaxis.set_major_formatter(plt.DateFormatter('%Y-%m-%d %H:%M:%S'))\n    plt.xticks(rotation=45)\n    plt.legend()\n\n    # Adjust layout to ensure no labels are cut off\n    plt.tight_layout()\n\n    return ax\nstart_epoch = 1672531200\nend_epoch = 1672617600\nstep = 'H'\ntrend = 0.1"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Check if the input is a valid integer\n    if not isinstance(epoch_milliseconds, int):\n        raise ValueError(\"epoch_milliseconds must be an integer\")\n\n    # Convert epoch milliseconds to a datetime object\n    try:\n        start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    except OSError:\n        raise ValueError(\"Invalid epoch timestamp\")\n\n    # Ensure the start date is not in the future\n    if start_date > datetime.now():\n        raise ValueError(\"Start date cannot be in the future\")\n\n    # Generate a date range from the start date to the current date\n    end_date = datetime.now()\n    date_range = pd.date_range(start=start_date.date(), end=end_date.date(), freq='D')\n\n    # Generate sales data\n    data = []\n    for current_date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append({\n                'Product': product,\n                'Date': current_date,\n                'Sales': sales\n            })\n\n    # Create and return the DataFrame\n    df = pd.DataFrame(data)\n    df = df.sort_values(['Date', 'Product'])\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if the input is of the correct type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    try:\n        # Parse the JSON string\n        data = json.loads(json_str)\n    \n        # Check if the parsed data is a list (which JSON arrays should be)\n        if not isinstance(data, list):\n            raise ValueError(\"JSON must represent an array\")\n    \n        # Convert the JSON data to a pandas DataFrame\n        df = pd.DataFrame(data)\n    \n        # Create an Excel workbook and worksheet using xlwt\n        workbook = xlwt.Workbook()\n        worksheet = workbook.add_sheet(sheet_name)\n    \n        # Write the headers and data to the Excel worksheet if DataFrame is not empty\n        if not df.empty:\n            for col, header in enumerate(df.columns):\n                worksheet.write(0, col, header)\n            for row in range(len(df)):\n                for col, value in enumerate(df.iloc[row]):\n                    worksheet.write(row + 1, col, value)\n    \n        # Save the workbook to the specified filename and ensure .xls extension\n        if not filename.endswith('.xls'):\n            filename += '.xls'\n        abs_path = os.path.abspath(filename)\n        workbook.save(abs_path)\n    \n        return abs_path\n\n    except json.JSONDecodeError:\n        raise ValueError(\"json_str is not valid JSON\")\n    except Exception as e:\n        raise Exception(f\"Error writing to Excel file: {str(e)}\")"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Calculate the date range for the past specified number of days\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    dates = [start_date + timedelta(days=i) for i in range(days_in_past)]\n    \n    # Generate random data for each activity on each date\n    data = []\n    for date in dates:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append({'Date': date, 'Activity': activity, 'Duration': duration})\n    \n    # Create DataFrame from the generated data\n    df = pd.DataFrame(data)\n    \n    # Create a Seaborn line plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity')\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.legend(title='Activity', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    \n    # Return the plot axis and the DataFrame\n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Calculate the date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past-1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices\n    stock_prices = np.random.random((len(date_range), len(stock_names)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(stock_prices, columns=stock_names, index=date_range)\n    \n    # Round prices to 4 decimal places for better readability\n    df = df.round(4)\n    \n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the contents of both CSV files\n        with open(file_path1, 'r', newline='', encoding='utf-8') as file1, open(file_path2, 'r', newline='', encoding='utf-8') as file2:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            lines1 = [line for line in reader1]\n            lines2 = [line for line in reader2]\n\n        # Check if either file is empty\n        if not lines1:\n            raise ValueError(f\"File '{file_path1}' is empty.\")\n        if not lines2:\n            raise ValueError(f\"File '{file_path2}' is empty.\")\n\n        # Compare the files using ndiff\n        diff = list(ndiff(lines1, lines2))\n\n        # Process the differences to format them into the required output\n        differences = []\n        line_number = 0\n        for line in diff:\n            status = line[0]\n            content = ','.join(line[2:]) if isinstance(line[2:], tuple) else line[2:]\n            if status in ['-', '+', ' ']:\n                if status != ' ':\n                    line_number += 1\n                differences.append({\n                    'Line Number': line_number,\n                    'Status': status,\n                    'Content': content\n                })\n\n        # Create a DataFrame from the differences\n        df = pd.DataFrame(differences, columns=['Line Number', 'Status', 'Content'])\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files cannot be found.\")\n    except ValueError as ve:\n        raise ve\n    except Exception as e:\n        raise Exception(f\"An IO-related error occurred: {str(e)}\")"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data)\n    \n    # Initialize the dictionary to store statistics\n    stats = {\n        'sum': 0 if df.empty else df[column].sum(),\n        'mean': np.nan if df.empty else df[column].mean(),\n        'min': np.nan if df.empty else df[column].min(),\n        'max': np.nan if df.empty else df[column].max()\n    }\n    \n    # Initialize the plot\n    fig, ax = plt.subplots()\n    \n    if not df.empty and 'Age' in df.columns:\n        # Group data by 'Age' and count each age's occurrence\n        age_counts = df['Age'].value_counts()\n        \n        # Plot a pie chart\n        ax.pie(age_counts, labels=age_counts.index, autopct='%1.1f%%', startangle=90)\n        ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n        ax.set_title(f'Distribution of Ages for {column}')\n    else:\n        # Display a message if no data is available\n        ax.text(0.5, 0.5, 'No Age data available' if not df.empty else 'No data available', \n                horizontalalignment='center', verticalalignment='center', fontsize=12)\n        ax.axis('off')\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if data is empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Validate column existence\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' not found in data\")\n\n    # Validate non-negative values for numeric columns\n    for col in ['steps', 'calories_burned', 'distance_walked']:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in '{col}' column\")\n\n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['date'], df[column], marker='o', linestyle='-')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column.capitalize())\n    ax.set_title(f'Line Chart of {column.capitalize()}')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return stats, ax"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Read JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize defaultdict to store numeric values for each key\n    numeric_values = defaultdict(list)\n    \n    # Iterate through each dictionary in the list\n    for item in data:\n        for key, value in item.items():\n            # Check if value is numeric (int or float)\n            if isinstance(value, (int, float)):\n                numeric_values[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = []\n    for key, values in numeric_values.items():\n        mean = np.mean(values)\n        median = np.median(values)\n        results.append({'variable': key, 'mean': mean, 'median': median})\n    \n    # Create DataFrame from results\n    df = pd.DataFrame(results)\n    \n    # Set 'variable' as index and sort\n    df.set_index('variable', inplace=True)\n    df.sort_index(inplace=True)\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.lower().endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension\")\n    \n    # Read the CSV file and identify duplicate rows\n    try:\n        with open(file_path, 'r', newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            rows = [tuple(row) for row in reader]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found\")\n    except csv.Error as e:\n        raise ValueError(f\"Error reading CSV file: {e}\")\n\n    # Count duplicate rows\n    row_counts = Counter(rows)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to DataFrame\n    df = pd.DataFrame.from_dict(duplicates, orient='index', columns=['Count'])\n    df.index = [', '.join(map(str, idx)) for idx in df.index]\n    df = df.sort_values('Count', ascending=False)\n\n    # Plot the results\n    fig, ax = plt.subplots(figsize=(12, 6))\n    df.plot(kind='bar', ax=ax)\n    ax.set_title('Duplicate Rows in CSV File')\n    ax.set_xlabel('Row Content')\n    ax.set_ylabel('Count')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return duplicates, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure 'name' and 'age' columns are present\n    if 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns\")\n    \n    # Convert age to integer, rounding down, and check for negative values\n    df['age'] = df['age'].apply(np.floor).astype(int)\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated('name', keep=False)]\n    \n    if duplicates.empty:\n        return Counter(), None\n    \n    # Calculate age distribution for duplicates\n    age_distribution = Counter(duplicates['age'])\n    \n    # Plot histogram\n    plt.figure(figsize=(10, 6))\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)  # Adjust bins to center integer ages\n    \n    ax = sns.histplot(duplicates, x='age', bins=bins, kde=False)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution for Duplicate Names')\n    \n    # Adjust x-axis ticks to show integer values\n    ax.set_xticks(np.arange(min_age, max_age + 1))\n    \n    plt.tight_layout()\n    \n    return age_distribution, ax"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates = {k: v for k, v in value_counts.items() if v > 1}\n\n    # Create histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    n, bins, patches = ax.hist(df['value'], bins=bins, edgecolor='black', color='green', alpha=0.6)\n\n    # Fit normal distribution to the data\n    mu, std = norm.fit(df['value'])\n\n    # Generate points for the normal distribution curve\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n\n    # Scale the normal distribution curve to match the histogram height\n    p = p * (n.max() / p.max())\n\n    # Plot the normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot title and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n\n    return Counter(duplicates), ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    # Determine the number of rows from list 'a' and the number of columns from the length of list 'b'\n    num_rows = len(a)\n    num_cols = min(len(b), len(COLUMNS))  # Limit the number of columns to the size of COLUMNS\n\n    # Generate random data for the DataFrame\n    data = np.random.rand(num_rows, num_cols)\n    \n    # Create the DataFrame with 'a' as row indices and the appropriate number of columns from COLUMNS\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_cols])\n    \n    # Plotting the DataFrame\n    fig, ax = plt.subplots(figsize=(10, 6))  # Create a matplotlib figure and axis object\n    df.plot(kind='bar', ax=ax)  # Plot the DataFrame as a bar chart on the axis\n\n    # Customize the plot with labels and title\n    ax.set_title('Bar Chart of Random Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Values')\n    ax.legend(title='Columns')\n    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout\n\n    return ax  # Return the matplotlib Axes object\na = ['X', 'Y', 'Z']\nb = [1, 2, 3, 4]"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Ensure the 'month' column is in a proper datetime format\n    if data['month'].dtype == 'int64':\n        data['month'] = data['month'].apply(lambda x: datetime.strptime(str(x), '%m').strftime('%B'))\n    elif data['month'].dtype == 'object':\n        data['month'] = data['month'].apply(lambda x: datetime.strptime(x, '%B').strftime('%B'))\n    \n    # Sort the data by month\n    month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n                   'July', 'August', 'September', 'October', 'November', 'December']\n    data['month'] = pd.Categorical(data['month'], categories=month_order, ordered=True)\n    data = data.sort_values('month')\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(data['month'], data['value'])\n\n    # Set the title and labels\n    ax.set_title('Monthly Data for {}'.format(datetime.now().year))\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n    \n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Split the input data string and convert to numeric, handling non-numeric values\n    numeric_data = pd.to_numeric(data.split(), errors='coerce')\n    \n    # Remove any NaN values that arise from non-numeric entries\n    numeric_data = numeric_data.dropna()\n    \n    # Calculate the histogram bins\n    bins = np.arange(numeric_data.min(), numeric_data.max() + 2) - 0.5\n    \n    # Create the histogram plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.hist(numeric_data, bins=bins, edgecolor='black')\n    \n    # Set the x-axis and y-axis labels, and the title of the histogram\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Adjust the layout to ensure everything fits without overlap\n    plt.tight_layout()\n    \n    # Return the Axes object for further manipulation if necessary\n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate noisy sine wave\n    y_noisy = np.sin(x) + np.random.normal(0, noise_level, array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Initial guess for the parameters\n    initial_params = [1, 1, 0, 0]\n    \n    # Perform curve fitting\n    popt, _ = curve_fit(sine_func, x, y_noisy, p0=initial_params)\n    \n    # Generate fitted curve based on the optimized parameters\n    y_fit = sine_func(x, *popt)\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot noisy data\n    ax.scatter(x, y_noisy, color='blue', alpha=0.5, label='Noisy data')\n    \n    # Plot fitted curve\n    ax.plot(x, y_fit, color='red', label='Fitted curve')\n    \n    # Set labels and title\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy Sine Wave with Fitted Curve')\n    \n    # Add legend\n    ax.legend()\n    \n    # Set y-axis limits\n    ax.set_ylim(-1.5, 1.5)\n    \n    # Add grid\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        # Read the CSV file\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join([' '.join(row) for row in reader])\n\n        # Normalize text to ASCII\n        normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n\n        # Count words\n        words = normalized_text.lower().split()\n        word_counts = Counter(words)\n\n        # Get the 10 most common words\n        top_10 = word_counts.most_common(10)\n\n        # Create a bar plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        words, counts = zip(*top_10)\n        ax.bar(words, counts)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        ax.set_title('10 Most Common Words')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n\n        # Return the plot's Axes object and the list of top words\n        return ax, top_10\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{csv_file}' was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file '{csv_file}'.\")"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n\n    # Create a figure and an axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot the histogram of the data\n    n, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black', label='Histogram')\n    \n    # Calculate the PDF from theoretical normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, loc=0, scale=1)\n    \n    # Plot the PDF\n    ax.plot(x, p, 'r-', lw=2, label='Normal PDF')\n    \n    # Customize the plot\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title(f'Histogram and PDF of {size} Normally Distributed Random Numbers')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Return the figure object\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Convert private key to PEM format\n    privkey_pem = privkey.save_pkcs1().decode('utf-8')\n    \n    # Generate a random password and nonce for AES encryption\n    password = get_random_bytes(32)  # 256-bit key\n    nonce = get_random_bytes(16)  # 128-bit nonce\n\n    # Encrypt the private key\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, tag = cipher.encrypt_and_digest(privkey_pem.encode('utf-8'))\n\n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Combine nonce, ciphertext, and tag for storage\n    encrypted_data = nonce + ciphertext + tag\n\n    # Save the encrypted private key to the file\n    with open(filename, 'wb') as f:\n        f.write(encrypted_data)\n\n    return pubkey, filename, password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256-bit key\n\n    # Read the file content\n    with open(file_path, 'rb') as file:\n        file_content = file.read()\n\n    # Pad the file content\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(file_content) + padder.finalize()\n\n    # Generate a random IV\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Encrypt the file content\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save the encrypted file\n    encrypted_file_name = file_path + \".encrypted\"\n    with open(encrypted_file_name, 'wb') as encrypted_file:\n        encrypted_file.write(iv + encrypted_data)\n\n    # Save the encrypted AES key\n    encrypted_key_file_name = file_path + \".key\"\n    with open(encrypted_key_file_name, 'wb') as key_file:\n        key_file.write(encrypted_aes_key)\n\n    return pubkey, encrypted_file_name, encrypted_key_file_name"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nfrom urllib.error import URLError\ndef task_func(url):\n    # Validate URL: Check if URL is empty or invalid\n    if not url or not url.startswith(('http://', 'https://')):\n        raise ValueError(\"Invalid or empty URL provided\")\n\n    try:\n        # Fetch HTML content\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n        \n        # Parse HTML using PyQuery\n        doc = pq(html)\n        \n        # Extract text and href from anchor tags\n        anchors = doc('a')\n        texts = [anchor.text() for anchor in anchors.items()]\n        hrefs = [anchor.attr('href') for anchor in anchors.items()]\n        \n        # Get current timestamp for the 'fetch_time' column\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        \n        # Create DataFrame with extracted data and a timestamp\n        df = pd.DataFrame({\n            'text': texts,\n            'href': hrefs,\n            'fetch_time': [fetch_time] * len(texts)\n        })\n        \n        return df\n\n    except URLError as e:\n        # Handle errors related to network issues\n        raise URLError(f\"Network connectivity issue or server error: {str(e)}\")\n    except Exception as e:\n        # Handle other unexpected errors\n        raise Exception(f\"An unexpected error occurred: {str(e)}\")"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint, uniform\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate filename with current timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"sensor_data_{timestamp}.csv\"\n    filepath = os.path.join(output_dir, filename)\n\n    # Generate data\n    data = []\n    start_time = datetime.now()\n    for i in range(hours * 60):  # Generate data for every minute\n        current_time = start_time + timedelta(minutes=i)\n        temperature = round(uniform(20, 30), 1)  # Temperature between 20\u00b0C and 30\u00b0C\n        humidity = randint(30, 70)  # Humidity between 30% and 70%\n        pressure = round(uniform(990, 1030), 1)  # Pressure between 990 hPa and 1030 hPa\n        data.append([current_time.strftime(\"%Y-%m-%d %H:%M:%S\"), temperature, humidity, pressure])\n\n    # Write data to CSV file\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writerows(data)\n\n    print(f\"Sensor data for {hours} hours has been generated and saved to {filepath}\")"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate data\n    start_time = datetime.now().replace(minute=0, second=0, microsecond=0)\n    data = []\n    for i in range(hours):\n        current_time = start_time + timedelta(hours=i)\n        row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n        row.extend([randint(0, 100) for _ in VEHICLE_TYPES])\n        data.append(row)\n\n    # Save data to CSV\n    csv_filename = os.path.join(output_dir, f'traffic_data_{start_time.strftime(\"%Y%m%d_%H%M%S\")}.csv')\n    with open(csv_filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n\n    # Read CSV and create plot\n    df = pd.read_csv(csv_filename, parse_dates=['Time'])\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle_type in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Save plot\n    plot_filename = os.path.join(output_dir, f'traffic_plot_{start_time.strftime(\"%Y%m%d_%H%M%S\")}.png')\n    plt.savefig(plot_filename)\n\n    return csv_filename, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR, backup_dir=BACKUP_DIR):\n    # Ensure output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(backup_dir, exist_ok=True)\n\n    # Generate a timestamped filename for uniqueness\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"weather_data_{timestamp}.csv\"\n    file_path = os.path.join(output_dir, filename)\n    \n    # Generate weather data\n    current_time = datetime.now()\n    weather_data = []\n    for _ in range(hours):\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append([current_time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n        current_time += timedelta(hours=1)\n\n    # Write data to CSV file\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Path for the backup file\n    backup_file_path = os.path.join(backup_dir, filename)\n    shutil.copy(file_path, backup_file_path)\n\n    return file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Generate DataFrame with random data\n    data = []\n    for team in TEAMS:\n        for _ in range(goals):\n            goals_scored = randint(0, 5)\n            penalties_received = randint(0, 3)\n            fine = penalties_received * PENALTY_COST\n            data.append({\n                'Team': team,\n                'Goals': goals_scored,\n                'Penalties': penalties_received,\n                'Penalty Cost': fine\n            })\n\n    df = pd.DataFrame(data)\n\n    # Create visualizations\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Goals visualization\n    sns.barplot(x='Team', y='Goals', data=df, ax=ax1)\n    ax1.set_title('Goals Scored by Team')\n    ax1.set_xlabel('Team')\n    ax1.set_ylabel('Goals')\n\n    # Penalty costs visualization\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=ax2)\n    ax2.set_title('Total Penalty Costs by Team')\n    ax2.set_xlabel('Team')\n    ax2.set_ylabel('Penalty Cost (USD)')\n\n    plt.tight_layout()\n    \n    return df, [ax1, ax2]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Create a DataFrame with random integer values between 0 and 9\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.astype(bool).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    non_zero_counts.plot(kind='bar', ax=ax)\n    \n    # Customize the plot\n    ax.set_title('Non-Zero Value Counts by Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    ax.set_ylim(0, rows)  # Set y-axis limit to the number of rows\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(non_zero_counts):\n        ax.text(i, v + 1, str(v), ha='center', va='bottom')\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define the courses offered\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    \n    # Generate random student IDs\n    student_ids = ['Student_' + str(i) for i in sample(range(1000, 9999), num_students)]\n\n    # Create random grades for each student in each course\n    data = {course: np.random.randint(0, 101, num_students) for course in courses}\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate average grade and passing counts for each course\n    averages = df.mean()\n    passing_counts = (df >= 60).sum()\n\n    # Prepare DataFrame for visualization\n    visualization_data = pd.DataFrame({\n        'Average Grade': averages,\n        'Passing Count': passing_counts\n    })\n\n    # Plotting the data\n    ax = visualization_data.plot(kind='bar', color=['skyblue', 'lightgreen'], figsize=(10, 6))\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Counts / Grades')\n    ax.legend(['Average Grade', 'Passing Count'])\n\n    return df, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Find indices where the first column matches the target value\n    target_indices = np.where(array[:, 0] == target_value)[0]\n    \n    # If no matching indices found, return None\n    if len(target_indices) == 0:\n        return None\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Prepare data for fitting\n    x_data = np.arange(len(target_indices))\n    y_data = array[target_indices, 1]  # Assuming we want to fit the second column values\n    \n    # Perform the curve fitting\n    try:\n        popt, _ = optimize.curve_fit(exp_decay, x_data, y_data)\n    except RuntimeError:\n        return None  # Return None if curve fitting fails\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Data')\n    \n    # Generate points for the fitted curve\n    x_fit = np.linspace(0, len(target_indices) - 1, 100)\n    y_fit = exp_decay(x_fit, *popt)\n    \n    # Plot the fitted curve\n    ax.plot(x_fit, y_fit, 'r-', label='Fitted Curve')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Exponential Decay Fit for Target Value: {target_value}')\n    ax.legend()\n    \n    return (popt, ax)"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    def preprocess(text):\n        text = ALPHANUMERIC.sub(' ', text).lower()\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n\n    processed_texts = [preprocess(text) for text in texts]\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=5000)\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract topics\n    nmf_model = NMF(n_components=num_topics, init='nndsvda', random_state=42)\n    nmf_output = nmf_model.fit_transform(tfidf_matrix)\n\n    # Get feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n        top_words = [feature_names[i] for i in top_words_idx]\n        topics.append(top_words)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords as nltk_stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Ensure the NLTK stopwords are downloaded\n    nltk.download('stopwords', quiet=True)\n    \n    # Use English stopwords from NLTK if not provided\n    if stopwords is None:\n        stopwords = set(nltk_stopwords.words('english'))\n    \n    # Function to clean and tokenize text\n    def clean_and_tokenize(text):\n        # Remove non-alphanumeric characters except space, and convert to lowercase\n        cleaned_text = ALPHANUMERIC.sub(' ', text.lower())\n        # Tokenize the text\n        tokens = cleaned_text.split()\n        # Filter out stopwords\n        filtered_tokens = [token for token in tokens if token not in stopwords]\n        return filtered_tokens\n    \n    # Process all texts\n    processed_texts = [clean_and_tokenize(text) for text in texts]\n\n    # Train the Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Check and create the directory if it doesn't exist\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    # Create the 'processed' subdirectory if it doesn't exist\n    processed_dir = os.path.join(path, 'processed')\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    # Get a list of JSON files in the directory, sorted alphabetically\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()\n\n    # Initialize an empty list to store DataFrames\n    data_frames = []\n\n    # Process each JSON file\n    for file in json_files:\n        file_path = os.path.join(path, file)\n        \n        # Read JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n\n        # Convert to DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add 'Source' column\n        df['Source'] = file\n        \n        # Append to list of DataFrames\n        data_frames.append(df)\n        \n        # Move file to 'processed' subdirectory\n        shutil.move(file_path, os.path.join(processed_dir, file))\n\n    # Concatenate all DataFrames\n    if data_frames:\n        final_df = pd.concat(data_frames, ignore_index=True)\n    else:\n        final_df = pd.DataFrame()\n\n    return final_df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the file path\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    \n    # Generate random sensor data\n    data = [\n        {\n            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'temperature': round(random.uniform(20, 30), 2),\n            'humidity': round(random.uniform(30, 70), 2)\n        }\n        for _ in range(10)  # Generate 10 data points\n    ]\n    \n    # Write data to CSV file in append mode\n    with open(file_path, 'a', newline='') as csvfile:\n        fieldnames = ['timestamp', 'temperature', 'humidity']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header if file is empty\n        if os.stat(file_path).st_size == 0:\n            writer.writeheader()\n        \n        for row in data:\n            writer.writerow(row)\n    \n    # Store the file path before deletion\n    result = file_path\n    \n    # Delete the file\n    os.remove(file_path)\n    \n    # Return the path to the CSV file before deletion\n    return result"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    try:\n        # Fetch the HTML content from the URL\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n\n        # Parse the HTML content\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Extract data from the parsed HTML\n        # Example: extracting <a> tags with href attributes\n        data = []\n        for a in soup.find_all('a', href=True):\n            data.append([a['href'], a.text.strip()])\n\n        # Save the scraped data to a CSV file\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['URL', 'Text'])  # Header row\n            writer.writerows(data)\n\n        return CSV_FILE_PATH\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        return None"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validate that the input data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a DataFrame.\")\n    \n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"Data must not be empty.\")\n    \n    # Ensure the target column is part of the DataFrame\n    if target_column not in data.columns:\n        raise ValueError(f\"Target column '{target_column}' is not a column of the DataFrame.\")\n    \n    # Confirm that all values in the DataFrame are numeric\n    if not data.select_dtypes(include=[np.number]).columns.tolist() == list(data.columns):\n        raise ValueError(\"All columns in the DataFrame must contain numeric values.\")\n    \n    # Validate that random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be an integer.\")\n    \n    # Validate that test_size is a float in the range (0, 1)\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size must be between 0 and 1.\")\n    \n    # Split the data into training and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Calculate and return the model's score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Generate random data\n    num_records = 100\n    ids = np.arange(1, num_records + 1)\n    \n    # Combine and correct names\n    all_names = latin_names + other_names\n    all_names = [codecs.decode(name.encode('latin-1'), 'utf-8') for name in all_names]\n    names = np.random.choice(all_names, num_records)\n    \n    # Generate random dates\n    start_date = datetime.datetime(start_year, 1, 1)\n    end_date = datetime.datetime(end_year, 12, 31)\n    date_range = (end_date - start_date).days\n    random_days = np.random.randint(0, date_range, num_records)\n    dates_of_birth = [start_date + datetime.timedelta(days=day) for day in random_days]\n    \n    # Generate emails\n    def create_email(name, dob):\n        name_part = re.sub(r'[^\\w]', '', name.lower())\n        year_part = str(dob.year)\n        return f\"{name_part}{year_part}@{email_domain}\"\n    \n    emails = [create_email(name, dob) for name, dob in zip(names, dates_of_birth)]\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Validate the input data format\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"Input JSON must be a list of dictionaries\")\n\n    # Initialize defaultdict to store values for each key\n    values_dict = defaultdict(list)\n\n    # Collect values for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, (int, float)):  # Ensure the value is numeric\n                values_dict[key].append(value)\n\n    # Calculate mean and median for each key\n    result = {}\n    for key, values in values_dict.items():\n        result[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key, stats in result.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n\n    return result"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    \n    # Ensure the target directory exists\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    for file_path in kwargs.values():\n        file_path = Path(file_path)\n        \n        # Check if the file exists and is not empty\n        if file_path.exists() and file_path.stat().st_size > 0:\n            # Copy the file to the target directory\n            dest_path = target_path / file_path.name\n            shutil.copy2(file_path, dest_path)\n            copied_files.append(str(dest_path.resolve()))\n    \n    return {\"copied_files\": copied_files}"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    directory_path = Path(directory)\n    \n    # Ensure the directory exists\n    if not directory_path.is_dir():\n        raise ValueError(f\"The directory {directory} does not exist.\")\n    \n    # Compile the regex pattern\n    regex = re.compile(pattern)\n    \n    # Iterate over all files in the directory\n    for file_path in directory_path.glob('*.csv'):\n        match = regex.match(file_path.name)\n        if match:\n            # Extract the base name from the pattern\n            base_name = match.group(1)\n            # Create the new file name\n            new_file_name = f\"{base_name}_new.csv\"\n            new_file_path = directory_path / new_file_name\n            \n            # Copy the content to the new file\n            with open(file_path, 'r', newline='') as old_file, \\\n                 open(new_file_path, 'w', newline='') as new_file:\n                reader = csv.reader(old_file)\n                writer = csv.writer(new_file)\n                for row in reader:\n                    writer.writerow(row)\n            \n            # Append the new file name to the list\n            new_files.append(new_file_name)\n    \n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = set()\n\n    # Ensure the input directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The specified directory '{directory}' does not exist.\")\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Iterate through all files in the directory\n    for filename in os.listdir(directory):\n        match = regex.match(filename)\n        if match:\n            # Get the full path of the zip file\n            zip_path = os.path.join(directory, filename)\n            \n            # Extract the prefix (directory name) from the filename using the last occurrence of \"-\"\n            prefix = filename.rsplit('-', 1)[0]\n            \n            # Create the extraction directory\n            extract_dir = os.path.join(directory, prefix)\n            os.makedirs(extract_dir, exist_ok=True)\n            \n            # Unzip the file\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n            \n            # Add the extraction directory to the set\n            extracted_dirs.add(extract_dir)\n    \n    # Convert the set to a list and return it\n    return list(extracted_dirs)"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nfrom datetime import datetime\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    if not files:\n        print(f\"No files found matching the pattern: {pattern}\")\n        return None\n\n    # Create a unique archive filename using the current timestamp for uniqueness\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    archive_filename = f\"archive_{timestamp}.tar.gz\"\n    archive_path = os.path.join(ARCHIVE_DIR, archive_filename)\n\n    # Create the archive using tar command\n    try:\n        subprocess.run(['tar', '-czf', archive_path] + files, check=True)\n        # Delete original files\n        for file in files:\n            os.remove(file)\n\n        return archive_path\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating archive: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    count = Counter()\n\n    # Read existing data from CSV file\n    try:\n        with open(csv_file_path, 'r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                count['goals'] += int(row['goals'])\n                count['penalties'] += int(row['penalties'])\n    except FileNotFoundError:\n        # If the file doesn't exist, start with empty counts\n        print(\"File not found. Starting with empty counts.\")\n\n    # Update counts with new goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Compile the regex pattern\n    pattern = re.compile(file_pattern)\n\n    moved_files_count = 0\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the filename matches the pattern\n        if pattern.match(filename):\n            source_path = os.path.join(source_dir, filename)\n            target_path = os.path.join(target_dir, filename)\n\n            try:\n                # Move the file\n                shutil.move(source_path, target_path)\n                moved_files_count += 1\n            except Exception as e:\n                print(f\"Error moving file {filename}: {str(e)}\")\n\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean texts\n    clean_text1 = ALPHANUMERIC.sub(' ', text1.lower()).strip()\n    clean_text2 = ALPHANUMERIC.sub(' ', text2.lower()).strip()\n    \n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(clean_text1, clean_text2)\n    \n    # Tokenize texts\n    tokens1 = clean_text1.split()\n    tokens2 = clean_text2.split()\n    \n    # Create term frequency dictionaries\n    tf1 = Counter(tokens1)\n    tf2 = Counter(tokens2)\n    \n    # Get unique words from both texts\n    all_words = set(tf1.keys()).union(tf2.keys())\n    \n    # Create term frequency vectors\n    vec1 = np.array([tf1.get(word, 0) for word in all_words])\n    vec2 = np.array([tf2.get(word, 0) for word in all_words])\n    \n    # Calculate cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n    \n    # Calculate cosine similarity, handle division by zero in case of empty vectors\n    if norm1 == 0 or norm2 == 0:\n        cosine_similarity = 0.0\n    else:\n        cosine_similarity = dot_product / (norm1 * norm2)\n    \n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Check if input is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all elements are non-negative integers\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"Input must contain non-negative integers only\")\n    \n    # If the input list is empty, return empty lists\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations\n    perms = list(permutations(numbers))\n    \n    # Calculate factorial sums for each permutation\n    factorial_sums = [\n        sum(math.factorial(num) for num in perm)\n        for perm in perms\n    ]\n    \n    # Convert permutations from tuples to lists\n    perms_as_lists = [list(perm) for perm in perms]\n    \n    return factorial_sums, perms_as_lists"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure the destination directory exists\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Normalize extensions and create file patterns\n    file_patterns = [os.path.join(SOURCE_DIR, f\"*{ext}\") for ext in EXTENSIONS]\n\n    # Iterate through all matching files\n    for pattern in file_patterns:\n        for file_path in glob.glob(pattern):\n            file_name = os.path.basename(file_path)\n            dest_path = os.path.join(DEST_DIR, file_name)\n\n            try:\n                # Check if the file already exists in the destination\n                if os.path.exists(dest_path):\n                    # If it exists, add a timestamp to the filename\n                    base_name, ext = os.path.splitext(file_name)\n                    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n                    new_file_name = f\"{base_name}_{timestamp}{ext}\"\n                    dest_path = os.path.join(DEST_DIR, new_file_name)\n\n                # Copy the file to the destination\n                shutil.copy2(file_path, dest_path)\n                transferred_files.append(file_name)\n\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file_name}: {str(e)}\")\n\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items, counts, weights = zip(*data)\n\n    # Convert counts and weights to numpy arrays for normalization\n    counts_array = np.array(counts)\n    weights_array = np.array(weights)\n\n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts_array)\n\n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights_array.reshape(-1, 1)).flatten()\n\n    # Create a pandas DataFrame with the normalized data\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n\n    return df"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Determine the maximum length of tuples in the list\n    max_length = max(len(t) for t in data_list)\n\n    # Initialize lists to store numeric values for each position\n    numeric_values = [[] for _ in range(max_length)]\n\n    # Iterate through each tuple in the list and each value in the tuple\n    for tuple_item in data_list:\n        for i, value in enumerate(tuple_item):\n            # Check if the value is numeric (int or float)\n            if isinstance(value, (int, float)):\n                if i < max_length:\n                    numeric_values[i].append(value)\n\n    # Calculate mean for each position, ignoring non-numeric values\n    means = [np.mean(values) if values else np.nan for values in numeric_values]\n\n    # Create a DataFrame with the results\n    df = pd.DataFrame({'Mean Value': means}, index=[f'Position {i}' for i in range(max_length)])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the specified columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both columns ({col1}, {col2}) not found in the DataFrame.\")\n    \n    # Check if columns contain categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not pd.api.types.is_object_dtype(data[col1]):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not pd.api.types.is_object_dtype(data[col2]):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data.\")\n    \n    # Create the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if both columns have multiple categories\n    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Check if any category has less than 5 observations, which violates chi-square test assumptions\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating the chi-square test assumptions.\")\n    \n    # Perform the chi-square test of independence\n    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    return p_value\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    dice_rolls = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(6, dtype=int)\n    for roll in dice_rolls:\n        frequency[roll - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(NUMBERS, frequency, color='blue', edgecolor='black')\n    ax.set_xlabel('Dice Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Dice Rolls')\n    ax.set_xticks(NUMBERS)\n    \n    # Add frequency labels on top of each bar\n    for i, v in enumerate(frequency):\n        ax.text(i + 1, v, str(v), ha='center', va='bottom')\n    \n    # Adjust y-axis to show all frequency labels\n    ax.set_ylim(0, max(frequency) * 1.1)\n    \n    return frequency, ax\nrolls = 1000"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        # Walk through the source directory\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                # Check if the file has '_processed' suffix\n                if re.search(r'_processed\\.[^.]+$', file):\n                    file_path = os.path.join(root, file)\n                    # Add file to the zip archive\n                    zipf.write(file_path, os.path.relpath(file_path, source_dir))\n\n    # If the archive is empty (no processed files found), remove it\n    if os.path.getsize(archive_path) == 0:\n        os.remove(archive_path)\n        return \"No processed files found to archive.\"\n\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the DataFrame has the required columns\n    if not 'Date' in df.columns or not 'Close' in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Close' columns\")\n    \n    # Convert 'Date' to datetime if it's not already\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Sort the dataframe by date\n    df = df.sort_values('Date')\n    \n    # Create features (X) based on the seconds since the minimum date\n    X = (df['Date'] - df['Date'].min()).dt.total_seconds().values.reshape(-1, 1)\n    y = df['Close'].values\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future dates using a constant time step of 24*60*60 seconds\n    last_date = df['Date'].max()\n    future_dates = [last_date + pd.Timedelta(seconds=24*60*60*i) for i in range(1, 8)]\n    \n    # Create future X values based on the seconds since the minimum date\n    future_X = np.array([(date - df['Date'].min()).total_seconds() for date in future_dates]).reshape(-1, 1)\n    \n    # Predict future prices\n    future_prices = model.predict(future_X)\n    \n    # Plot the data\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    ax.scatter(df['Date'], df['Close'], color='blue', label='Historical Data')\n    \n    # Plot regression line\n    ax.plot(df['Date'], model.predict(X), color='red', label='Regression Line')\n    \n    # Plot future predictions\n    ax.scatter(future_dates, future_prices, color='green', label='Future Predictions')\n    \n    # Customize the plot\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Price Prediction')\n    ax.legend()\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return (future_prices.tolist(), ax)"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    # Calculate Z-scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-score threshold\n    outliers = df[np.abs(z_scores) > z_threshold]\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot all data points\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Normal')\n    \n    # Highlight outliers\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outlier')\n    \n    # Set plot labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    # Add horizontal lines for upper and lower thresholds\n    mean = df['closing_price'].mean()\n    std = df['closing_price'].std()\n    ax.axhline(y=mean + z_threshold * std, color='green', linestyle='--', label='Upper Threshold')\n    ax.axhline(y=mean - z_threshold * std, color='green', linestyle='--', label='Lower Threshold')\n    \n    # Update legend\n    ax.legend()\n    \n    # Display the plot\n    plt.show()\n    \n    # Return the outliers DataFrame and the plot object\n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Check if the DataFrame has the necessary 'Close' column\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column\")\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), gridspec_kw={'height_ratios': [1, 2]})\n\n    # Box plot for closing prices\n    sns.boxplot(x=df['Close'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n    ax1.set_xlabel('Closing Price')\n\n    # Histogram for closing prices\n    sns.histplot(df['Close'], kde=True, ax=ax2)\n    ax2.set_title('Histogram of Closing Prices')\n    ax2.set_xlabel('Closing Price')\n    ax2.set_ylabel('Frequency')\n\n    # Layout adjustment\n    plt.tight_layout()\n\n    return (ax1, ax2)"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame is sorted by date and has a datetime index\n    df = df.sort_index()\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    \n    # Extract the 'Close' prices\n    close_prices = df['Close']\n\n    # Fit ARIMA model, order can be adjusted based on the data characteristics\n    model = ARIMA(close_prices, order=(1, 1, 1))  # Example order\n    results = model.fit()\n\n    # Forecast next 7 days\n    forecast = results.forecast(steps=7)\n    forecast_values = forecast.tolist()\n\n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Plot historical data\n    ax.plot(close_prices.index, close_prices.values, label='Historical')\n\n    # Plot forecast\n    last_date = close_prices.index[-1]\n    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=7)\n    ax.plot(forecast_dates, forecast_values, color='red', label='Forecast')\n\n    # Fill the area between the historical data and the forecast\n    ax.fill_between(forecast_dates, forecast_values, color='red', alpha=0.3)\n\n    # Customize the plot\n    ax.set_title('Stock Price Forecast (ARIMA)')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Close Price')\n    ax.legend()\n    ax.grid(True)\n\n    # Rotate and align the tick labels so they look better\n    fig.autofmt_xdate()\n\n    # Use tight layout to prevent clipping of labels\n    plt.tight_layout()\n\n    return forecast_values, ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of lowercase English alphabets\n    all_combinations = [''.join(combo) for combo in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    result = {combo: 0 for combo in all_combinations}\n    \n    # Count the occurrences of each two-letter combination in the word\n    word = word.lower()  # Convert the word to lowercase for case-insensitive processing\n    for i in range(len(word) - 1):\n        combo = word[i:i+2]\n        if combo in result:\n            result[combo] += 1\n    \n    return result"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create empty list to store data\n    data = []\n    \n    # Generate random sales data for each category\n    for category in categories:\n        base_sales = np.random.randint(1000, 5000)\n        trend = np.random.uniform(-0.1, 0.1)\n        noise = np.random.normal(0, 200, periods)\n        sales = base_sales + np.arange(periods) * trend * base_sales + noise\n        sales = np.maximum(sales, 0)  # Ensure non-negative sales\n        \n        for date, sale in zip(dates, sales):\n            data.append({'Date': date, 'Category': category, 'Sales': sale})\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    ax.grid(True)\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # If sales_data is not provided, generate random sales data\n    if sales_data is None:\n        np.random.seed(42)  # for reproducibility\n        sales_data = np.random.randint(1000, 5000, size=periods)\n\n    # Create a DataFrame with dates and sales data\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n\n    # Create feature matrix X (days since start) and target vector y (sales)\n    X = (df['Date'] - df['Date'].min()).dt.days.values.reshape(-1, 1)\n    y = df['Sales'].values\n\n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future dates for forecasting\n    future_dates = pd.date_range(start=df['Date'].max() + pd.Timedelta(days=1), \n                                 periods=periods, \n                                 freq=freq)\n\n    # Create feature matrix for future dates\n    X_future = (future_dates - df['Date'].min()).days.values.reshape(-1, 1)\n\n    # Forecast future sales\n    future_sales = model.predict(X_future)\n\n    return future_sales.astype(int)"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Validate the number of tasks\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n\n    # Set the random seed for reproducibility if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Ensure that the number of tasks requested does not exceed the list provided\n    if n_tasks > len(sanitized_tasks):\n        raise ValueError(\"n_tasks exceeds the number of available tasks\")\n\n    # Randomly select tasks\n    selected_tasks = random.sample(sanitized_tasks, n_tasks)\n\n    # Get the current date\n    current_date = datetime.now().date()\n\n    # Assign tasks to employees and set due dates\n    task_assignments = [{\n        'Task Name': task,\n        'Assigned To': random.choice(employees),\n        'Due Date': current_date\n    } for task in selected_tasks]\n\n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(task_assignments)\n\n    return df"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text.strip():\n        raise ValueError(\"Input text is empty\")\n\n    # Convert text to lowercase and split into words\n    words = text.lower().split()\n\n    # Replace spaces with underscores in given words and convert to lowercase\n    modified_words = [word.replace(' ', '_').lower() for word in mystrings]\n\n    # Replace occurrences of given words in the text with their modified versions\n    modified_text = text.lower()\n    for original, modified in zip(mystrings, modified_words):\n        pattern = re.compile(r'\\b' + re.escape(original.lower()) + r'\\b')\n        modified_text = pattern.sub(modified, modified_text)\n\n    # Split the modified text into words and count frequencies\n    words = modified_text.split()\n    word_counts = Counter(words)\n\n    # Extract words and their frequencies\n    unique_words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(range(len(unique_words)), frequencies, tick_label=unique_words)\n    ax.set_xticks(range(len(unique_words)))\n    ax.set_xticklabels(unique_words, rotation=45, ha='right')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency')\n\n    plt.tight_layout()\n    return ax\nmystrings = [\"New York\", \"San Francisco\"]\ntext = \"I love New York and San Francisco. New York is a big city.\""}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Ensure the target directory exists\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n\n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create the full path for the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n\n    # Create a ZipFile object\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        # Iterate through all files in the source directory by extension\n        for ext in valid_extensions:\n            for file in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add file to the zip, preserving only the base filename\n                zipf.write(file, os.path.basename(file))\n\n    # Return the full path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    converted_count = 0\n    # Set of allowed file extensions\n    allowed_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_directory):\n        file_path = os.path.join(source_directory, filename)\n        file_extension = Path(file_path).suffix.lower()\n        \n        if file_extension in allowed_extensions:\n            text_content = \"\"\n            \n            # Process files based on their extension\n            if file_extension == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    text_content = file.read()\n            elif file_extension == '.docx':\n                doc = docx.Document(file_path)\n                text_content = \"\\n\".join([para.text for para in doc.paragraphs])\n            elif file_extension == '.xlsx':\n                wb = openpyxl.load_workbook(file_path)\n                sheet = wb.active\n                text_content = \"\\n\".join([\" \".join([str(cell.value) if cell.value is not None else \"\" for cell in row]) for row in sheet.iter_rows()])\n            elif file_extension == '.csv':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    csv_reader = csv.reader(file)\n                    text_content = \"\\n\".join([\" \".join(row) for row in csv_reader])\n\n            # Create a DataFrame with a single 'Text' column\n            df = pd.DataFrame({'Text': [text_content]})\n            \n            # Save as CSV in the target directory\n            csv_filename = Path(filename).stem + '.csv'\n            csv_path = os.path.join(target_directory, csv_filename)\n            df.to_csv(csv_path, index=False)\n            \n            # Increment the counter for converted files\n            converted_count += 1\n\n    return converted_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    \n    # Check for non-numeric data types\n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    # Check for NaN values\n    if df.isna().any().any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Create a new DataFrame to store the results\n    result_df = pd.DataFrame()\n    \n    # Process each column\n    for column in df.columns:\n        # Compute cumulative sum\n        cumsum = df[column].cumsum()\n        \n        # Reshape for MinMaxScaler\n        cumsum_reshaped = cumsum.values.reshape(-1, 1)\n        \n        # Apply MinMaxScaler\n        normalized_cumsum = scaler.fit_transform(cumsum_reshaped)\n        \n        # Add to result DataFrame\n        result_df[column] = normalized_cumsum.flatten()\n    \n    return result_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    # Check if the directory exists\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n\n    result = []\n    \n    # Use os.scandir() to efficiently iterate over entries in the directory\n    for entry in os.scandir(directory_path):\n        if entry.is_file():  # Process only files, ignore subdirectories\n            # Retrieve file statistics\n            stats = entry.stat()\n            \n            # Extract the file size\n            size = stats.st_size\n            \n            # Attempt to use the creation time or fallback to the last metadata change time\n            try:\n                creation_time = datetime.fromtimestamp(stats.st_ctime, tz=timezone.utc)\n            except AttributeError:\n                creation_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc)\n            \n            # Extract the last modification time\n            modification_time = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc)\n            \n            # Format times in ISO format and append the result as a tuple\n            result.append((\n                entry.name,\n                size,\n                creation_time.isoformat(),\n                modification_time.isoformat()\n            ))\n    \n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Validate and parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    \n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n    \n    # Prepare data for DataFrame\n    countries = []\n    populations = []\n\n    for country, population in data.items():\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name encountered\")\n        \n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Invalid population value encountered\")\n        \n        countries.append(country)\n        populations.append(math.floor(population))  # Round down float populations\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Country\": countries,\n        \"Population\": populations\n    })\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(f\"Unable to write to file: {file_path}\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if the directory exists and is a directory\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n\n    # Validate and deduplicate predicates\n    valid_predicates = list(set(pred for pred in predicates if callable(pred)))\n\n    if not valid_predicates:\n        raise ValueError(\"No valid predicates provided.\")\n\n    result = {}\n\n    # Iterate through items in the directory\n    for item in os.listdir(dir_path):\n        # Evaluate each predicate for the current item name\n        predicate_results = [pred(item) for pred in valid_predicates]\n        \n        # Store the results in the dictionary\n        result[item] = predicate_results\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    try:\n        # Decode the hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to UTF-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Prepare the result dictionary\n        result = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('ascii'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        # Try ASCII encoding, if not possible, set to 'Not representable in ASCII'\n        try:\n            result['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n        except UnicodeEncodeError:\n            result['ASCII'] = 'Not representable in ASCII'\n        \n        return result\n    \n    except (binascii.Error, UnicodeDecodeError):\n        # Return an error message if the input is not a valid hexadecimal string\n        return \"Invalid hexadecimal input\""}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_checksum = md5_hash.hexdigest()\n\n        # Validate the checksum\n        if calculated_checksum != EXPECTED_MD5_CHECKSUM:\n            os.remove(TARGET_TAR_FILE)\n            return False\n\n        # Extract the contents of the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n            tar.extractall()\n        return True\n\n    except Exception as e:\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        print(f\"An error occurred: {str(e)}\")\n        return False"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Attempt to download the CSV file from the specified URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a counter to keep track of value occurrences in the specified column\n        value_counts = collections.Counter()\n        \n        # Open and process the CSV file\n        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n            csv_reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists in the CSV file\n            if column_name not in csv_reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in csv_reader:\n                value = row[column_name]\n                value_counts[value] += 1\n\n        # Return the dictionary with counts of each value\n        return dict(value_counts)\n\n    except urllib.error.URLError as e:\n        # Handle errors that occur during the file download\n        raise Exception(f\"Error downloading the file: {e}\")\n    \n    except csv.Error as e:\n        # Handle errors that occur during the processing of the CSV file\n        raise Exception(f\"Error reading the CSV file: {e}\")\n    \n    except Exception as e:\n        # Handle any other unexpected errors\n        raise Exception(f\"An unexpected error occurred: {e}\")\n    \n    finally:\n        # Ensure the downloaded CSV file is deleted after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML content from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Invalid URL or unable to fetch XML file: {e}\")\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the XML structure conforms to the expected format\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    # Extract data from each 'item' element\n    data = []\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n\n    # Ensure there is data to create a DataFrame\n    if not data:\n        raise ValueError(\"No data found in the XML file\")\n\n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Attempt to download the text file from the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            text = response.read().decode('utf-8')\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n        return None, None\n\n    # Use a regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Create a Counter object to count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    top_ten = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_ten)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(words, counts)\n    ax.set_title('Ten Most Frequently Occurring Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    # Return the Counter object and the Axes object\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Attempt to download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for bad HTTP status codes\n\n        # Validate if the content type is 'application/zip'\n        content_type = response.headers.get('Content-Type', '').lower()\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Save the downloaded ZIP file\n        file_name = os.path.join(download_path, 'downloaded_file.zip')\n        with open(file_name, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Create a directory for extraction\n        extract_path = os.path.join(download_path, 'extracted')\n        os.makedirs(extract_path, exist_ok=True)\n        \n        # Try extracting the ZIP file\n        try:\n            with ZipFile(file_name, 'r') as zip_ref:\n                zip_ref.extractall(extract_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Optionally, remove the downloaded ZIP file after extraction\n        os.remove(file_name)\n\n        # Return the path to the extracted contents\n        return extract_path\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the table with the specified ID\n        table = soup.find('table', id=table_id)\n        if table is None:\n            raise ValueError(\"Table with the specified ID not found.\")\n\n        # Check if the table has any rows\n        rows = table.find_all('tr')\n        if not rows:\n            return pd.DataFrame()  # Return an empty DataFrame if no rows are found\n\n        # Extract table headers\n        headers = [header.text.strip() for header in rows[0].find_all(['th', 'td'])]\n\n        # Extract table data\n        data = []\n        for row in rows[1:]:\n            data.append([cell.text.strip() for cell in row.find_all(['td', 'th'])])\n\n        # Create a DataFrame\n        df = pd.DataFrame(data, columns=headers)\n\n        return df\n\n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {e}\")\n    except ValueError as e:\n        raise ValueError(str(e))\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {e}\")"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nimport shutil\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Ensure the download directory exists\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    \n    # Download the file\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Failed to download file due to network issues. {str(e)}\", []\n\n    # Extract the zip file\n    try:\n        # Remove existing ZIP_DIR if it exists and create a new one\n        if ZIP_DIR.exists():\n            shutil.rmtree(ZIP_DIR)\n        ZIP_DIR.mkdir(exist_ok=True)\n        \n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        \n        # Get list of extracted files\n        extracted_files = [f.name for f in ZIP_DIR.glob('**/*') if f.is_file()]\n        return \"Success: File downloaded and extracted successfully.\", extracted_files\n    except zipfile.BadZipFile:\n        return \"Error: The downloaded file is not a valid zip file.\", []\n    except PermissionError:\n        return \"Error: Permission denied when extracting files.\", []\n    except Exception as e:\n        return f\"Error: An unexpected error occurred during extraction. {str(e)}\", []"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure the request was successful\n\n        # Parse the HTML content of the webpage\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Find all <a> tags with href attributes and convert them to absolute URLs\n        links = set()\n        for a_tag in soup.find_all('a', href=True):\n            absolute_url = urljoin(base_url, a_tag['href'])\n            links.add(absolute_url)\n\n        # Write the unique absolute URLs to a CSV file\n        with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['URL'])\n            for link in links:\n                writer.writerow([link])\n\n        # Return the number of unique absolute links\n        return len(links)\n\n    except requests.RequestException as e:\n        print(f\"Error fetching {url}: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return 0"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Determine the content source and fetch the content\n        if webpage_url.startswith('http'):\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Will raise HTTPError for bad responses\n            content = response.content\n        else:\n            with open(webpage_url, 'rb') as file:\n                content = file.read()\n\n        # Parse the HTML content\n        tree = html.fromstring(content)\n        tables = tree.xpath('//table')\n        \n        if not tables:\n            return 0  # No tables found\n\n        # Extract and parse the first table data\n        df = pd.read_html(html.tostring(tables[0]))[0]\n        if df.empty:\n            return 0  # Table is empty\n\n        # Connect to SQLite database and manage table\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n        \n        # Drop the existing table if it exists and create a new one\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Commit the changes and close the database connection\n        conn.commit()\n        conn.close()\n\n        # Return the number of rows in the DataFrame\n        return len(df)\n\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Failed to fetch the webpage: {str(e)}\")\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database operation failed: {str(e)}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {str(e)}\")"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Try OCR text extraction\n            text = pytesseract.image_to_string(img)\n            try:\n                # Convert the extracted text\n                converted_text = text.encode(from_encoding).decode(to_encoding)\n                return converted_text\n            except (UnicodeDecodeError, LookupError) as e:\n                # If encoding conversion fails, attempt to process the image comment\n                if 'comment' in img.info:\n                    try:\n                        comment = img.info['comment'].encode(from_encoding).decode(to_encoding)\n                        return comment\n                    except (UnicodeDecodeError, LookupError) as e:\n                        raise ValueError(f\"Error converting image comment: {str(e)}\")\n                else:\n                    return \"\"  # No comment available, return empty string\n    except IOError as e:\n        raise ValueError(f\"Error opening image file: {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Unexpected error: {str(e)}\")"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Send a GET request with a 5-second timeout\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Checks if the request was successful\n\n        # Retrieve the response content in raw bytes\n        raw_content = response.content\n\n        # Handle cases when the content is empty\n        if not raw_content:\n            return {}\n\n        # Detect the encoding if not provided\n        if from_encoding is None:\n            detected = chardet.detect(raw_content)\n            from_encoding = detected['encoding']\n\n            # If encoding cannot be detected, raise a ValueError\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the content using the detected or provided encoding\n        decoded_content = raw_content.decode(from_encoding)\n\n        # Re-encode the content to the desired encoding\n        re_encoded_content = decoded_content.encode(to_encoding)\n\n        # Parse and return the JSON data\n        return json.loads(re_encoded_content.decode(to_encoding))\n\n    except requests.RequestException as e:\n        # Handle any request-related exceptions\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        print(f\"JSON parsing failed: {e}\")\n        return {}\n    except ValueError as e:\n        # Handle value errors, specifically related to encoding detection\n        print(f\"Value error: {e}\")\n        raise"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Handle the case of an empty CSV file\n        if df.empty:\n            return pd.DataFrame()\n\n        # Check if the specified column exists in the DataFrame\n        if column_name not in df.columns:\n            raise ValueError(f\"The column '{column_name}' is not present in the CSV file.\")\n\n        # Convert date strings to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n        # Get today's date\n        current_date = datetime.now()\n\n        # Filter rows based on the current date\n        df = df[df[column_name] <= current_date]\n\n        # Sort the DataFrame by the date column\n        df = df.sort_values(by=column_name)\n\n        return df\n\n    except EmptyDataError:\n        # Return an empty DataFrame if the CSV file is empty\n        return pd.DataFrame()\n\n    except pd.errors.ParserError as e:\n        # Handle CSV parsing errors\n        raise ValueError(f\"Error parsing CSV file: {str(e)}\")\n\n    except Exception as e:\n        # Handle any other unexpected errors\n        raise Exception(f\"An unexpected error occurred: {str(e)}\")"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Initialize SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode().strip()\n\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            secure_socket.sendall(\"File not found\".encode())\n            return \"File not found\"\n\n        # Compute the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as file:\n            for byte_block in iter(lambda: file.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        file_hash = sha256_hash.hexdigest()\n\n        # Send the hash back to the client\n        secure_socket.sendall(file_hash.encode())\n\n        return file_hash\n\n    except ssl.SSLError as e:\n        error_message = f\"SSL Error: {str(e)}\"\n        secure_socket.sendall(error_message.encode())\n        return error_message\n    except IOError as e:\n        error_message = f\"I/O Error: {str(e)}\"\n        secure_socket.sendall(error_message.encode())\n        return error_message\n    except Exception as e:\n        error_message = f\"Unexpected Error: {str(e)}\"\n        secure_socket.sendall(error_message.encode())\n        return error_message"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setblocking(False)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n\n    # Sockets from which we expect to read\n    inputs = [server_socket]\n    # Sockets to which we expect to write\n    outputs = []\n    # Outgoing message queues (socket:Queue)\n    message_queues = {}\n\n    # Start time and duration\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    while datetime.now() < end_time:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n        for s in readable:\n            if s is server_socket:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    response = f\"{data.decode()} (Server Time: {current_time})\"\n                    message_queues[s].put(response.encode())\n                    # Add output channel for response\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    # Remove message queue\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                # No messages waiting so stop checking for writability.\n                outputs.remove(s)\n            else:\n                s.send(next_msg)\n\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Clean up the server socket\n    server_socket.close()\n    \n    duration = datetime.now() - start_time\n    return f\"Server ran for {duration.total_seconds():.2f} seconds, handling connections on {server_address}:{server_port}\""}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    try:\n        # Receive message from client socket\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n\n        # Get email details from user\n        sender_email = input(\"Enter sender's email: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        sender_password = getpass.getpass(\"Enter sender's email password: \")\n\n        # Create email message\n        email = EmailMessage()\n        email['From'] = sender_email\n        email['To'] = recipient_email\n        email['Subject'] = \"Message from Client Socket\"\n        email.set_content(message)\n\n        # Send email using SMTP\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()  # Start TLS encryption\n            server.login(sender_email, sender_password)\n            server.send_message(email)\n\n        print(\"Email sent successfully.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\n    finally:\n        # Close the client socket\n        client_socket.close()\n\n    # Function returns None implicitly"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read CSV file and check for 'Text' column or assume first column is 'Text' if header is missing\n        df = pd.read_csv(file_path, header=None, names=['Text'] if pd.read_csv(file_path, nrows=1).columns[0] != 'Text' else None)\n        \n        # Combine all text into a single string\n        combined_text = ' '.join(df['Text'].astype(str))\n        \n        # Initialize CountVectorizer with stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the combined text\n        word_counts = vectorizer.fit_transform([combined_text])\n        \n        # Sum the word counts to get frequencies and convert to a list of tuples (word, frequency)\n        sum_words = word_counts.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        \n        # Sort words by frequency and get the top 10\n        top_words = sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]\n        \n        # Extract words and their frequencies for plotting\n        words, frequencies = zip(*top_words)\n        \n        # Create the plot\n        plt.figure(figsize=(10, 5))\n        plt.bar(words, frequencies, color='blue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        \n        # Save or show the plot based on save_path\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.show()\n            return plt.gca()\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Default lists if not provided\n    default_animals = ['cat', 'dog', 'rabbit', 'hamster', 'parrot']\n    default_foods = ['apple', 'banana', 'carrot', 'lettuce', 'seed']\n\n    # Use default lists if not provided or if provided lists are empty\n    animals = animals if animals else default_animals\n    foods = foods if foods else default_foods\n\n    # If both lists are empty, return an empty DataFrame\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    # Generate all combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle the combinations to ensure variety\n    np.random.shuffle(combinations)\n\n    # Create the DataFrame with animals as rows and foods as columns\n    df = pd.DataFrame(index=animals, columns=foods)\n\n    # Fill the DataFrame with the combinations in 'animal:food' format\n    for animal, food in combinations:\n        df.at[animal, food] = f\"{animal}:{food}\"\n\n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert timezone string to pytz timezone object\n    tz = pytz.timezone(timezone)\n\n    # Convert timestamps to datetime objects in the specified timezone\n    datetimes = []\n    for time_string in time_strings:\n        dt = datetime.fromisoformat(time_string)\n        dt = dt.astimezone(tz)\n        datetimes.append(dt)\n\n    # Calculate time differences in seconds\n    time_diffs = []\n    for i in range(1, len(datetimes)):\n        diff = abs((datetimes[i] - datetimes[i-1]).total_seconds())\n        time_diffs.append(diff)\n\n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n\n    # Calculate and return the mean time difference\n    return float(np.mean(time_diffs))"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text and remove punctuation\n    processed_text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split the text into words\n    words = processed_text.split()\n    \n    # Count the frequency of each word using Counter\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10_words)\n    \n    # Create a bar plot using matplotlib\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    \n    # Set titles and labels\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cut-off labels\n    plt.tight_layout()\n    \n    # Return the list of tuples and the Axes object\n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    \n    # Find all URLs in the string\n    urls = re.findall(url_pattern, myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    # Take the first URL found\n    url = urls[0]\n    \n    # Validate the URL\n    try:\n        result = urlparse(url)\n        if all([result.scheme, result.netloc]):\n            # URL is valid, proceed with fetching the content\n            try:\n                response = requests.get(url, timeout=10)\n                response.raise_for_status()  # Raises an HTTPError for bad responses\n                \n                # Parse the HTML content\n                soup = BeautifulSoup(response.text, 'html.parser')\n                \n                # Find the title tag\n                title_tag = soup.find('title')\n                \n                if title_tag:\n                    return title_tag.string.strip()\n                else:\n                    return \"No title tag found in the webpage.\"\n            \n            except requests.RequestException as e:\n                return f\"Unable to fetch the content of the URL: {url}\"\n        else:\n            return \"No valid URL found in the provided string.\"\n    \n    except ValueError:\n        return \"No valid URL found in the provided string.\""}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n        \n        # Check if the specified key exists in the JSON data\n        if unknown_key not in data:\n            raise KeyError(f\"Key '{unknown_key}' not found in JSON data\")\n        \n        # Retrieve the URL associated with the specified key\n        url = data[unknown_key]\n        \n        # Download the file from the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raises HTTPError for bad requests (4XX or 5XX)\n        \n        # Generate a timestamp for the filename\n        timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n        \n        # Create the filename using the key and timestamp\n        filename = f\"{unknown_key}_{timestamp}.txt\"\n        \n        # Determine the directory to save the file, default is current working directory\n        if save_dir is None:\n            save_dir = os.getcwd()\n        else:\n            save_dir = os.path.abspath(save_dir)\n            os.makedirs(save_dir, exist_ok=True)  # Create the directory if it does not exist\n        \n        # Construct the full path where the file will be saved\n        file_path = os.path.join(save_dir, filename)\n        \n        # Write the downloaded content to the file\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Return the absolute path of the saved file\n        return os.path.abspath(file_path)\n    \n    except json.JSONDecodeError:\n        print(\"Error: Invalid JSON data\")\n    except KeyError as e:\n        print(f\"Error: {str(e)}\")\n    except requests.RequestException as e:\n        print(f\"Error downloading file: {str(e)}\")\n    except IOError as e:\n        print(f\"Error saving file: {str(e)}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n    \n    return None"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\nimport os\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern for phone numbers\n    phone_pattern = r'\\b(?:\\+?1[-.\\s]?)?(?:\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n    \n    # Check if the input is a URL or a local file path\n    if url.startswith('http://') or url.startswith('https://'):\n        # Fetch the content from the URL\n        response = requests.get(url)\n        content = response.text\n    elif os.path.isfile(url):\n        # Read the content from a local file\n        with open(url, 'r', encoding='utf-8') as file:\n            content = file.read()\n    else:\n        raise ValueError(\"Invalid input. Please provide a valid URL or local file path.\")\n\n    # Parse the content with BeautifulSoup\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    \n    # Extract text content from the parsed HTML\n    text_content = soup.get_text()\n    \n    # Find all phone numbers in the text content\n    phone_numbers = re.findall(phone_pattern, text_content)\n    \n    # Remove duplicates and sort the list\n    phone_numbers = sorted(list(set(phone_numbers)))\n    \n    # Save the phone numbers to the specified output file in JSON format\n    with open(output_path, 'w') as outfile:\n        json.dump(phone_numbers, outfile, indent=2)\n    \n    return phone_numbers"}
