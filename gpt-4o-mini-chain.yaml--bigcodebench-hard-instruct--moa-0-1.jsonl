{"task_id": "BigCodeBench/13", "solution": "```python\nimport subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    # Step 1: Establish a connection to the FTP server\n    try:\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    # Step 2: Log into the FTP server\n    try:\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    # Step 3: Change to the specified directory\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # Step 4: Use wget to download files\n    command = f\"wget -r --no-parent ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/\"\n    \n    # Step 5: Execute the command and capture the output\n    try:\n        subprocess.run(command, shell=True, check=True)\n    except subprocess.CalledProcessError as e:\n        raise Exception(f\"Error occurred while downloading files: {str(e)}\")\n    \n    # Step 6: List the files that were attempted to be downloaded\n    downloaded_files = os.listdir(os.path.join(os.getcwd(), ftp_dir.strip('/').replace('/', '_')))\n    \n    return downloaded_files\n```"}
{"task_id": "BigCodeBench/15", "solution": "```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands_file_path exists\n    if not os.path.isfile(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Create the output directory if it does not exist\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    # Open the CSV file for reading\n    with open(commands_file_path, mode='r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        \n        # Initialize a list to store output file paths\n        output_file_paths = []\n        \n        # Process each command in the CSV\n        for index, row in enumerate(reader):\n            command = row[0]  # Assuming each row has a command in the first column\n            output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n            \n            try:\n                # Execute the command\n                result = subprocess.run(command, shell=True, capture_output=True, text=True)\n                \n                # Write the output or error to the output file\n                with open(output_file_path, mode='w') as output_file:\n                    if result.returncode == 0:\n                        output_file.write(result.stdout)\n                    else:\n                        output_file.write(f\"Error: {result.stderr}\\nExit Code: {result.returncode}\")\n                \n            except Exception as e:\n                # Handle any exceptions that occur during command execution\n                with open(output_file_path, mode='w') as output_file:\n                    output_file.write(f\"Exception: {str(e)}\")\n            \n            # Append the path of the output file to the list\n            output_file_paths.append(output_file_path)\n\n    # Return the list of output file paths\n    return output_file_paths\n```"}
{"task_id": "BigCodeBench/17", "solution": "```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Step 1: Check if the process is currently running\n    process_running = False\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            process_running = True\n            break\n\n    # Step 2: If the process is not found, start it\n    if not process_running:\n        subprocess.Popen(process_name)  # Start the process\n        return f\"Process not found. Starting {process_name}.\"\n\n    # Step 3: If the process is found, terminate and restart it\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            proc.terminate()  # Terminate the process\n            proc.wait()  # Wait for the process to terminate\n            break\n\n    subprocess.Popen(process_name)  # Restart the process\n    return f\"Process found. Restarting {process_name}.\"\n```"}
{"task_id": "BigCodeBench/19", "solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(\"The specified directory does not exist.\")\n    \n    # Use glob to find all files in the specified directory (not including subdirectories)\n    file_pattern = os.path.join(directory, '*')  # Pattern to match all files\n    files = glob.glob(file_pattern)  # Get all files matching the pattern\n\n    # Check if files are found\n    if not files:\n        return None  # Return None if no files are found\n\n    # Create a zip file named 'files.zip' in the specified directory\n    zip_file_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        # Add all found files to the zip file\n        for file in files:\n            if os.path.isfile(file):  # Ensure it is a file\n                zip_file.write(file, os.path.basename(file))  # Add file to zip\n\n    # Return the path to the created zip file\n    return zip_file_path\n```"}
{"task_id": "BigCodeBench/34", "solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs using regular expressions\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Check if there are any words left after removing URLs\n    if not text.strip():\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate a WordCloud object using the cleaned text\n    wordcloud = WordCloud().generate(text)\n    \n    # Plot the generated word cloud using matplotlib\n    plt.figure(figsize=(10, 6))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')  # Hide axes\n    plt.show()\n    \n    # Return the WordCloud object\n    return wordcloud\n```"}
{"task_id": "BigCodeBench/37", "solution": "```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    # Step 1: Prepare the Data\n    X = df.drop(columns=[target_column])  # Features\n    y = df[target_column]  # Target variable\n\n    # Step 2: Train the Random Forest Classifier\n    rf_classifier = RandomForestClassifier()\n    rf_classifier.fit(X, y)  # Fit the model\n\n    # Step 3: Calculate Feature Importances\n    feature_importances = rf_classifier.feature_importances_\n\n    # Step 4: Sort Feature Importances\n    sorted_indices = feature_importances.argsort()[::-1]  # Indices of sorted feature importances\n    sorted_features = X.columns[sorted_indices]  # Sorted feature names\n    sorted_importances = feature_importances[sorted_indices]  # Sorted feature importances\n\n    # Step 5: Create the Bar Plot\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=sorted_importances, y=sorted_features, palette='viridis')\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n\n    # Step 6: Return the Outputs\n    return rf_classifier, plt.gca()  # Return the trained classifier and the Axes object\n```"}
{"task_id": "BigCodeBench/82", "solution": "```python\nfrom flask import Flask, render_template, redirect, url_for, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\n# Mock user database\nusers = {}\n\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = generate_password_hash(password)\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return users.get(user_id)\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.secret_key = secret_key\n    login_manager.init_app(app)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            user = next((u for u in users.values() if u.username == username), None)\n            if user and check_password_hash(user.password, password):\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                flash('Invalid username or password')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! This is a protected page.'\n\n    # Sample user for demonstration\n    users['1'] = User('1', 'testuser', 'password123')\n\n    return app\n```"}
{"task_id": "BigCodeBench/89", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Step 1: Standardize the specified column of the dataset\n    scaler = StandardScaler()\n    standardized_values = scaler.fit_transform(data[[column]])\n\n    # Step 2: Calculate Z-scores for the standardized values\n    z_scores = stats.zscore(standardized_values)\n\n    # Step 3: Identify outliers based on the Z-score threshold\n    outlier_indices = np.where(np.abs(z_scores) > outlier_z_score)[0]\n\n    # Step 4: Remove outliers from the original dataset\n    data_without_outliers = data.drop(data.index[outlier_indices]).reset_index(drop=True)\n\n    # Step 5: Create scatter plots to visualize the data\n    plt.figure(figsize=(12, 6))\n\n    # Plot 1: Data with Outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='red', label='Data with Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n\n    # Plot 2: Data without Outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n\n    # Display the plots\n    plt.tight_layout()\n    plt.show()\n\n    # Step 6: Return the original data, data without outliers, and indices of the outliers as a tuple\n    return (data, data_without_outliers, outlier_indices)\n```"}
{"task_id": "BigCodeBench/92", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef task_func(data, n_clusters=3):\n    # Validate input data\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input 'data' must be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1.\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(data)\n    cluster_labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', marker='o')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n    ax.legend()\n    ax.set_title('K-means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\n    return cluster_labels, ax\n```"}
{"task_id": "BigCodeBench/93", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n    \n    # Initialize PCA with the specified number of components\n    pca = PCA(n_components=n_components)\n    \n    # Fit the PCA model to the data and transform it\n    transformed_data = pca.fit_transform(data)\n    \n    # Create a DataFrame for the transformed data\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    # Generate a scatter plot of the transformed data\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(transformed_df['PC1'], transformed_df['PC2'])\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n        ax.set_title('PCA - Scatter Plot of Transformed Data')\n    else:\n        ax.scatter(transformed_df.index, transformed_df['PC1'])\n        ax.set_xlabel('Index')\n        ax.set_ylabel('Principal Component 1')\n        ax.set_title('PCA - Scatter Plot of Transformed Data (1D)')\n    \n    # Return the transformed DataFrame and the Axes object\n    return transformed_df, ax\n```"}
{"task_id": "BigCodeBench/99", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris_data = load_iris()\n    \n    # Convert the dataset into a pandas DataFrame\n    iris_df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n    iris_df['species'] = iris_data.target_names[iris_data.target]\n\n    # Set global font to Arial for better readability\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot using seaborn\n    pair_plot = sns.pairplot(iris_df, hue='species')\n\n    # Set the title of the plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n\n    # Return the matplotlib Figure object containing the pair plot\n    return pair_plot.fig\n```"}
{"task_id": "BigCodeBench/100", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Generate a date range for the past 30 days\n    dates = pd.date_range(end=datetime.now(), periods=30)\n\n    # Create random time series data corresponding to the generated dates\n    try:\n        values = [random.uniform(0, 100) for _ in range(30)]\n    except Exception as e:\n        raise ValueError(\"Error generating random data: \" + str(e))\n\n    # Create a new figure for the plot\n    plt.figure()\n\n    # Plot the time series data on the axes\n    try:\n        plt.plot(dates, values)\n    except Exception as e:\n        raise ValueError(\"Error plotting data: \" + str(e))\n\n    # Set the x-axis label to 'Date'\n    plt.xlabel('Date')\n\n    # Set the y-axis label to 'Value'\n    plt.ylabel('Value')\n\n    # Set the plot title to 'Random Time Series Data'\n    plt.title('Random Time Series Data')\n\n    # Apply the Arial font style to the plot\n    plt.xticks(fontname='Arial')\n    plt.yticks(fontname='Arial')\n    plt.title('Random Time Series Data', fontname='Arial')\n\n    # Return the Axes object containing the line plot\n    return plt.gca()\n```"}
{"task_id": "BigCodeBench/101", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load the Boston Housing dataset\n        data = pd.read_csv(data_url, delim_whitespace=True, skiprows=22, header=None, na_values='?')\n        \n        # Preprocess the data: drop any rows with missing values\n        data.dropna(inplace=True)\n\n        # Calculate the correlation matrix\n        corr_matrix = data.corr()\n\n        # Create the heatmap using Seaborn\n        plt.figure(figsize=(10, 8))\n        heatmap = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n\n        # Title and labels\n        plt.title('Correlation Heatmap of Boston Housing Dataset')\n        \n        # Save the heatmap to a file if a save_path is provided\n        if save_path is not None:\n            plt.savefig(save_path)\n        \n        # Show the plot\n        plt.show()\n\n        # Return the Axes object containing the heatmap plot\n        return heatmap\n\n    except Exception as e:\n        raise ValueError(\"An error occurred in generating or saving the plot.\") from e\n```"}
{"task_id": "BigCodeBench/108", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Step 1: Validate the input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Step 2: Check for the 'value' column\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Step 3: Validate data types in the 'value' column\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must contain numeric data types.\")\n    \n    # Step 4: Validate frequency string\n    valid_freqs = ['D', 'W', 'M', 'Q', 'Y']  # Example of valid frequency strings\n    if freq not in valid_freqs:\n        raise ValueError(\"Frequency 'freq' must be one of the valid strings: 'D', 'W', 'M', 'Q', 'Y'.\")\n    \n    # Step 5: Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Decomposition model must be either 'additive' or 'multiplicative'.\")\n    \n    # Step 6: Perform the decomposition\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.infer_freq(df.index))\n    \n    # Step 7: Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(10, 8))\n    decomposition_result.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    \n    decomposition_result.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    \n    decomposition_result.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    \n    decomposition_result.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residuals')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Step 8: Return the decomposition result and the axes object\n    return decomposition_result, axes\n```"}
{"task_id": "BigCodeBench/120", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime):\n        raise ValueError(\"start_date must be a datetime.datetime instance\")\n    if not isinstance(end_date, datetime):\n        raise ValueError(\"end_date must be a datetime.datetime instance\")\n    \n    # Check if start_date is later than end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date must not be later than end_date\")\n    \n    # Set the random seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days in the date range\n    num_days = (end_date - start_date).days + 1\n    \n    # Create a list to hold the random dates\n    random_dates = []\n    \n    # Generate random dates\n    for _ in range(num_days):\n        # Generate a random integer representing a day offset from start_date\n        day_offset = randint(0, num_days - 1)\n        random_date = start_date + timedelta(days=day_offset)\n        random_dates.append(random_date)\n    \n    # Convert the list of random dates into a pandas Series\n    random_dates_series = pd.Series(random_dates)\n    \n    # Return the Series containing random dates\n    return random_dates_series\n```"}
{"task_id": "BigCodeBench/123", "solution": "```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n\n    # Add the element '12' to my_list\n    my_list.append(12)\n\n    # Calculate the sum of the numbers in my_list\n    num_files_to_concatenate = sum(my_list)\n\n    # Use glob to find all files in file_dir with the specified file_ext\n    files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n\n    # Check if any files were found\n    if not files:\n        raise FileNotFoundError(f\"No files found in directory: {file_dir}\")\n\n    # Limit the number of concatenated files to available files\n    num_files_to_concatenate = min(num_files_to_concatenate, len(files))\n    \n    # Read and concatenate the CSV files\n    dataframes = [pd.read_csv(file) for file in files[:num_files_to_concatenate]]\n    concatenated_df = pd.concat(dataframes, ignore_index=True)\n\n    # Return the concatenated DataFrame\n    return concatenated_df\n```"}
{"task_id": "BigCodeBench/124", "solution": "```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Validate input\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in my_list must be numeric (int or float).\")\n\n    # Enhance 'my_list' by appending the number 12\n    my_list.append(12)\n\n    # Calculate the sum of the elements in 'my_list'\n    total_sum = sum(my_list)\n\n    # Determine the size of the random numbers list\n    random_list_size = min(total_sum, size)\n\n    # Initialize the random number generator\n    random_seed(seed)\n\n    # Measure time to generate random integers\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(random_list_size)]\n    time_taken = time.time() - start_time\n\n    # Create a histogram of the generated random numbers\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black', align='left')\n\n    # Label the axes\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n\n    # Return the time taken and the Axes object\n    return time_taken, ax\n```"}
{"task_id": "BigCodeBench/129", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        # Make the HTTP request to fetch the content of the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        \n    except requests.ConnectionError:\n        raise ConnectionError(\"Could not connect to the URL provided.\")\n    except requests.HTTPError:\n        raise requests.HTTPError(\"HTTP request failed with status code: {}\".format(response.status_code))\n\n    # Parse the HTML content using BeautifulSoup\n    try:\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Locate the first table element on the page\n        table = soup.find('table')\n        \n        # Check if the table exists\n        if table is None:\n            raise ValueError(\"No table found on the page.\")\n        \n        # Extract column headers if they exist\n        headers = []\n        header_row = table.find('tr')\n        if header_row:\n            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n        \n        # Extract the table data\n        data = []\n        for row in table.find_all('tr')[1:]:  # Skip the header row\n            cols = row.find_all('td')\n            data.append([td.get_text(strip=True) for td in cols])\n        \n        # Create a Pandas DataFrame using the extracted data\n        if headers:\n            df = pd.DataFrame(data, columns=headers)\n        else:\n            df = pd.DataFrame(data)  # No headers, unnamed columns\n\n        return df\n    \n    except Exception as e:\n        raise ValueError(\"Could not parse the page content: {}\".format(e))\n```"}
{"task_id": "BigCodeBench/139", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Check if df is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    # Identify numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    \n    # Check if there are no numeric columns\n    if not numeric_cols:\n        raise ValueError(\"No numeric columns in the DataFrame.\")\n    \n    # Prepare a list to store Axes objects\n    axes_list = []\n    \n    # Create histograms for each numeric column\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col], bins=10, alpha=0.7, color='blue')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n    \n    # Return the list of Axes objects\n    return axes_list\n```"}
{"task_id": "BigCodeBench/147", "solution": "```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    results = {}\n    threads = []\n\n    def check_port(ip):\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                sock.settimeout(1)  # Set a timeout for the connection attempt\n                result = sock.connect_ex((str(ip), port))\n                results[str(ip)] = (result == 0)  # True if port is open, False otherwise\n        except Exception:\n            results[str(ip)] = False  # In case of any exception, mark as False\n\n    # Iterate over each IP address in the given range\n    for ip in IPv4Network(ip_range):\n        thread = Thread(target=check_port, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n\n    return results\n```"}
{"task_id": "BigCodeBench/161", "solution": "```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Step 1: Open the log file for reading\n    with open(log_file, 'r') as file:\n        log_entries = []\n        \n        # Step 2: Define the regex pattern to match the log format\n        log_pattern = r'^(.*): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.*)$'\n        \n        # Step 3: Iterate through each line in the log file\n        for line in file:\n            match = re.match(log_pattern, line.strip())\n            if match:\n                message_type = match.group(1)\n                timestamp_str = match.group(2)\n                message = match.group(3)\n                \n                # Step 4: Validate the timestamp format\n                try:\n                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp: {timestamp_str}\")\n\n                # Step 5: Append valid log entries to the list\n                log_entries.append({\n                    'Type': message_type,\n                    'Timestamp': timestamp,\n                    'Message': message\n                })\n    \n    # Step 6: Check if any valid log entries were found\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Step 7: Convert the list of log entries to a DataFrame\n    df = pd.DataFrame(log_entries)\n\n    # Step 8: Define the output CSV file path\n    output_csv_file = 'structured_log_data.csv'\n\n    # Step 9: Save the DataFrame to a CSV file\n    df.to_csv(output_csv_file, index=False)\n\n    # Step 10: Return the file path of the newly created CSV file\n    return output_csv_file\n```"}
{"task_id": "BigCodeBench/162", "solution": "```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Extract words from the input text using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the lengths of the extracted words\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram subplot to visualize the distribution of word lengths\n    plt.figure(figsize=(10, 6))\n    ax = plt.subplot(111)\n    \n    # Check if there are any word lengths available\n    if word_lengths:\n        # Create the histogram\n        ax.hist(word_lengths, bins=np.arange(1, max(word_lengths) + 2) - 0.5, rwidth=rwidth)\n    else:\n        # Empty histogram if no words are present\n        ax.hist([], bins=[], rwidth=rwidth)\n\n    # Set labels and title for the histogram\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n\n    # Return the Axes object containing the histogram\n    return ax\n```"}
{"task_id": "BigCodeBench/177", "solution": "```python\nimport re\nimport pandas as pd\nfrom string import punctuation\n\ndef task_func(df):\n    # Step 1: Check if DataFrame is empty or lacks required columns\n    if df.empty or not all(col in df.columns for col in ['Title', 'Content']):\n        raise ValueError(\"DataFrame must not be empty and must contain 'Title' and 'Content' columns.\")\n\n    # Step 2: Filter articles with titles containing \"like\" or \"what\" (case-insensitive)\n    filtered_articles = df[df['Title'].str.contains('like|what', case=False, regex=True)]\n\n    # Step 3: Combine content of filtered articles into a single string\n    combined_content = ' '.join(filtered_articles['Content'].tolist())\n\n    # Step 4: Remove punctuation from the combined content\n    cleaned_content = re.sub(f\"[{punctuation}]\", \"\", combined_content)\n\n    # Step 5: Split the cleaned content into individual words\n    words = cleaned_content.split()\n\n    # Step 6: Count the frequency of each word\n    word_frequency = {}\n    for word in words:\n        word_lower = word.lower()  # Normalize to lowercase\n        if word_lower in word_frequency:\n            word_frequency[word_lower] += 1\n        else:\n            word_frequency[word_lower] = 1\n\n    # Step 7: Return the dictionary containing word frequencies\n    return word_frequency\n```"}
{"task_id": "BigCodeBench/184", "solution": "```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Step 1: Extract the text data from the specified column\n    text_data = dataframe[text_column].astype(str).tolist()\n    \n    # Step 2: Preprocess the text data\n    def preprocess_text(text):\n        # Remove numbers and punctuation, and convert to lowercase\n        text = re.sub(r'\\d+', '', text)  # Remove numbers\n        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n        text = text.lower()  # Convert to lowercase\n        # Remove stopwords\n        text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n        return text\n\n    preprocessed_data = [preprocess_text(text) for text in text_data]\n    \n    # Step 3: Vectorize the preprocessed text data\n    vectorizer = CountVectorizer()\n    word_counts = vectorizer.fit_transform(preprocessed_data)\n    \n    # Step 4: Create a DataFrame from the vectorized data\n    word_counts_df = pd.DataFrame(word_counts.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    # Step 5: Return the resulting DataFrame\n    return word_counts_df\n```"}
{"task_id": "BigCodeBench/187", "solution": "```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Check if 'Lon' and 'Lat' keys are present in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"The keys 'Lon' and 'Lat' must be present in the dictionary.\")\n    \n    # Check if the values associated with 'Lon' and 'Lat' are tuples\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"The values for 'Lon' and 'Lat' must be tuples.\")\n    \n    # Generate random coordinates for each city\n    coordinates = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        coordinates.append(Point(lon, lat))\n    \n    # Create a GeoPandas DataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': coordinates})\n    \n    # Return the GeoPandas DataFrame\n    return gdf\n```"}
{"task_id": "BigCodeBench/199", "solution": "```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object.\")\n    \n    # Validate other parameters\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of city names.\")\n    \n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of weather conditions.\")\n    \n    if not isinstance(timezones, dict) or not all(isinstance(key, str) and isinstance(value, str) for key, value in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with city names as keys and timezone strings as values.\")\n    \n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer.\")\n\n    # Set the random seed\n    set_seed(seed)\n\n    # Generate local times and weather conditions\n    weather_report = []\n    \n    for city in cities:\n        if city not in timezones:\n            continue  # Skip cities not in the timezone mapping\n        \n        # Convert UTC time to local time\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n        local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Append the data to the report\n        weather_report.append({\n            'City': city,\n            'Local Time': local_time_str,\n            'Weather Condition': weather_condition\n        })\n    \n    # Create a DataFrame from the weather report\n    report_df = pd.DataFrame(weather_report)\n    \n    return report_df\n```"}
{"task_id": "BigCodeBench/208", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    # Check if 'elements' is a positive integer\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random steps of -1 and 1\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate the cumulative sum to create the random walk\n    random_walk = np.cumsum(steps)\n\n    # Calculate descriptive statistics using pandas\n    stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'std_dev': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th_percentile': np.percentile(random_walk, 5),\n        '25th_percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th_percentile': np.percentile(random_walk, 75),\n        '95th_percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk)\n    }\n\n    # Create a plot of the random walk\n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n\n    # Return the descriptive statistics and the plot's Axes object\n    return stats, ax\n```"}
{"task_id": "BigCodeBench/211", "solution": "```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Step 1: Download the zip file from the provided URL\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Raise an error for bad responses\n\n    # Step 2: Save the downloaded zip file to a specified location\n    zip_file_path = os.path.join(destination_directory, \"downloaded.zip\")\n    with open(zip_file_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Step 3: Create the specified destination directory if it does not already exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    # Step 4: Open the downloaded zip file and extract its contents to the destination directory\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        \n        # Step 5: Create a list of the filenames of the extracted files\n        extracted_files = zip_ref.namelist()\n    \n    # Step 6: Return the list of extracted filenames\n    return extracted_files\n```"}
{"task_id": "BigCodeBench/214", "solution": "```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Create a plot to display the image\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct color representation\n    ax.axis('off')  # Hide the axes\n\n    # Show the plot\n    plt.show()\n\n    # Return the Axes object and the image\n    return ax, image\n```"}
{"task_id": "BigCodeBench/227", "solution": "```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Step 3: Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file does not exist: {audio_file}\")\n    \n    # Step 4: Convert the list L into an MxN matrix\n    matrix = np.array(L).reshape(M, N)\n    \n    # Step 5: Read the audio data from the specified audio file\n    data, sr = sf.read(audio_file)\n    \n    # Step 6: Calculate the Sound Pressure Level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n    \n    # Step 7: Normalize the MxN matrix based on the calculated SPL\n    normalized_matrix = matrix / (10 ** (SPL / 20))\n    \n    # Step 8: Generate a spectrogram from the normalized matrix\n    plt.figure(figsize=(10, 4))\n    plt.specgram(normalized_matrix.flatten(), NFFT=256, Fs=sr, Fc=0, noverlap=128, \n                 cmap='plasma', mode='mag', scale='dB', sides='default', window=np.hanning(256))\n    plt.title('Spectrogram')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    \n    # Step 9: Return the outputs\n    return normalized_matrix, plt.gcf()\n```"}
{"task_id": "BigCodeBench/239", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Step 3: Extract numeric values\n    numeric_values = []\n    for item in original:\n        for value in item:\n            if isinstance(value, (int, float)):  # Check if the value is numeric\n                numeric_values.append(value)\n    \n    # Convert list to numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Step 4: Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_array),\n        'std_dev': np.std(numeric_array),\n        'min': np.min(numeric_array),\n        'max': np.max(numeric_array)\n    }\n    \n    # Step 5: Generate a histogram\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, density=True, alpha=0.6, bins='auto')\n    \n    # Step 6: Overlay a probability density function (PDF)\n    x = np.linspace(np.min(numeric_array), np.max(numeric_array), 100)\n    pdf = stats.norm.pdf(x, statistics['mean'], statistics['std_dev'])\n    ax.plot(x, pdf, 'k', linewidth=2)\n    \n    # Step 7: Return the outputs\n    return numeric_array, statistics, ax\n```"}
{"task_id": "BigCodeBench/241", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list into a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the numpy array using Min-Max scaling\n    normalized_array = preprocessing.minmax_scale(original_array)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    \n    # Plot the original array data\n    ax.plot(original_array, label='Original Data', marker='o')\n    \n    # Plot the normalized array data on the same axes\n    ax.plot(normalized_array, label='Normalized Data', marker='x')\n    \n    # Add legend and titles for clarity\n    ax.set_title('Original vs Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    # Return the numpy array for the original data, normalized array, and the Axes object\n    return original_array, normalized_array, ax\n```"}
{"task_id": "BigCodeBench/267", "solution": "```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data[\"a\"] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\"\n    frequency = data[\"a\"]  # Using the value of \"a\" as the frequency\n    t = np.linspace(0, 1, sample_rate, endpoint=False)  # Time array\n    signal = np.sin(2 * np.pi * frequency * t)  # Generate the sine wave\n    \n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    \n    # Step 4: Plot the FFT result\n    plt.figure()\n    plt.plot(np.abs(fft_result))\n    plt.title('FFT of the Signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid()\n    axes = plt.gca()  # Get the current Axes object\n    \n    # Return the FFT result and the Axes object\n    return (fft_result, axes)\n```"}
{"task_id": "BigCodeBench/273", "solution": "```python\nimport cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    class PostRequestHandler(http.server.BaseHTTPRequestHandler):\n\n        def do_POST(self):\n            # Check Content-Type header\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_response(400)\n                self.send_header('Content-Type', 'application/json')\n                self.send_header('Content-Length', str(len(json.dumps({'error': 'Content-Type header is not application/json'}))))\n                self.end_headers()\n                self.wfile.write(json.dumps({'error': 'Content-Type header is not application/json'}).encode())\n                return\n            \n            # Read the content length from headers\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            \n            # Try to parse the JSON data\n            try:\n                json_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.send_header('Content-Type', 'application/json')\n                self.send_header('Content-Length', str(len(json.dumps({'error': 'Invalid JSON'}))))\n                self.end_headers()\n                self.wfile.write(json.dumps({'error': 'Invalid JSON'}).encode())\n                return\n            \n            # Check if 'data' key is in the parsed JSON\n            if 'data' not in json_data:\n                self.send_response(400)\n                self.send_header('Content-Type', 'application/json')\n                self.send_header('Content-Length', str(len(json.dumps({'error': 'No data key in request'}))))\n                self.end_headers()\n                self.wfile.write(json.dumps({'error': 'No data key in request'}).encode())\n                return\n            \n            # If all checks pass, respond with a success message\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', str(len(json.dumps(SUCCESS_RESPONSE))))\n            self.end_headers()\n            self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\n    return PostRequestHandler\n```"}
{"task_id": "BigCodeBench/274", "solution": "```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Parse the content type\n            content_type, params = cgi.parse_header(self.headers['Content-Type'])\n            if content_type != 'application/json':\n                self.send_response(400)\n                self.end_headers()\n                return\n            \n            # Read the incoming data\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            \n            try:\n                # Attempt to parse the JSON data\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                return\n            \n            # Check for required keys in the JSON\n            try:\n                subject = email_data['subject']\n                message = email_data['message']\n                to_email = email_data['to']\n            except KeyError:\n                self.send_response(400)\n                self.end_headers()\n                return\n            \n            # Prepare and send the email\n            try:\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_email\n                \n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, [to_email], msg.as_string())\n                \n                self.send_response(200)\n                self.send_header('Content-Type', 'application/json')\n                self.send_header('Content-Length', '0')\n                self.end_headers()\n                \n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.end_headers()\n                return\n            except Exception:\n                self.send_response(500)\n                self.end_headers()\n                return\n\n    return EmailRequestHandler\n```"}
{"task_id": "BigCodeBench/287", "solution": "```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    # Initialize a variable for total word count\n    total_word_count = 0\n    \n    # Create a dictionary to store word counts for each file\n    word_counts = {}\n    \n    # List all files in the specified directory\n    try:\n        files = os.listdir(directory)\n    except FileNotFoundError:\n        print(f\"The directory {directory} does not exist.\")\n        return 0\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0\n    \n    # Iterate through each file in the directory\n    for file in files:\n        # Check if the file has a .txt extension\n        if file.endswith('.txt'):\n            file_path = os.path.join(directory, file)\n            try:\n                # Open the file and read its contents\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    contents = f.read()\n                    # Split the contents into words and count them\n                    words = contents.split()\n                    word_count = len(words)\n                    # Update the total word count\n                    total_word_count += word_count\n                    # Store the word count for the current file\n                    word_counts[file] = word_count\n            except FileNotFoundError:\n                print(f\"The file {file_path} does not exist.\")\n            except Exception as e:\n                print(f\"An error occurred while reading {file_path}: {e}\")\n    \n    # Write the word counts dictionary to the specified JSON file\n    try:\n        with open(filename, 'w', encoding='utf-8') as json_file:\n            json.dump(word_counts, json_file)\n    except Exception as e:\n        print(f\"An error occurred while writing to {filename}: {e}\")\n    \n    # Return the total number of words counted\n    return total_word_count\n```"}
{"task_id": "BigCodeBench/302", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Step 1: Check if the DataFrame is empty or if the 'Value' column has invalid data\n    if df.empty or 'Value' not in df.columns or df['Value'].isnull().any():\n        raise ValueError(\"The DataFrame is empty or contains invalid 'Value' data.\")\n    \n    # Step 2: Split the lists in the 'Value' column into separate columns\n    value_df = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Step 3: Calculate the Pearson correlation coefficient\n    correlation_matrix = value_df.corr(method='pearson')\n    \n    # Step 4: If plot is True, create a heatmap of the correlation matrix\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n        return correlation_matrix, ax\n    \n    # Return the correlation matrix DataFrame\n    return correlation_matrix\n```"}
{"task_id": "BigCodeBench/308", "solution": "```python\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n    # Step 1: Generate random grades for each student in each subject\n    grades = {student: [random.randint(0, 100) for _ in FIELDS] for student in STUDENTS}\n    \n    # Step 2: Create a DataFrame from the grades dictionary\n    grades_df = pd.DataFrame(grades, index=FIELDS).transpose()\n    \n    # Step 3: Calculate the average grade for each student\n    grades_df['Average Grade'] = grades_df.mean(axis=1)\n    \n    # Step 4: Calculate the average grade for each subject and add as a new row\n    average_row = grades_df.mean(axis=0)\n    average_row.name = 'Average'\n    grades_df = grades_df.append(average_row)\n    \n    # Step 5: Return the final DataFrame\n    return grades_df\n\n# Call the function to generate the report\nreport = task_func()\nprint(report)\n```"}
{"task_id": "BigCodeBench/310", "solution": "```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Initialize a list to hold the simulated data\n    data = []\n\n    # Generate simulated data for PEOPLE_COUNT\n    for _ in range(PEOPLE_COUNT):\n        name = f'Person{_ + 1}'  # Generate a name like Person1, Person2, ...\n        age = random.randint(18, 65)  # Age between 18 and 65\n        height = random.uniform(150.0, 200.0)  # Height between 150 cm and 200 cm\n        weight = random.uniform(50.0, 100.0)  # Weight between 50 kg and 100 kg\n        data.append([name, age, height, weight])  # Append person's data to the list\n\n    # Calculate average age, height, and weight\n    average_age = mean(person[1] for person in data)\n    average_height = mean(person[2] for person in data)\n    average_weight = mean(person[3] for person in data)\n\n    # Append the average values as the last row\n    data.append(['Average', average_age, average_height, average_weight])\n\n    # Create the CSV file and write data\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)  # Write the header\n        writer.writerows(data)  # Write the data rows\n\n    # Return the path of the created CSV file\n    return os.path.abspath(filename)\n```"}
{"task_id": "BigCodeBench/313", "solution": "```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Create a dictionary to hold subdirectory names and corresponding file lists\n    organized_files = {}\n\n    # Iterate through each file in the specified directory\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n\n        # Check if it is a file\n        if os.path.isfile(file_path):\n            # Read the content of the file to extract the first text not in square brackets\n            with open(file_path, 'r') as file:\n                content = file.read()\n                # Use regex to find the first text not enclosed in square brackets\n                match = re.search(r'(?<!\\[)([^[]+?)(?=\\s*[\\n\\[]|$)', content)\n                if match:\n                    # Extract the matching text and strip whitespace\n                    subdirectory_name = match.group(1).strip()\n\n                    # Check if the extracted text is not empty\n                    if subdirectory_name:\n                        # Create a subdirectory if it doesn't exist\n                        subdirectory_path = os.path.join(directory, subdirectory_name)\n                        os.makedirs(subdirectory_path, exist_ok=True)\n\n                        # Move the file into the corresponding subdirectory\n                        shutil.move(file_path, os.path.join(subdirectory_path, filename))\n\n                        # Update the dictionary with the moved file\n                        if subdirectory_name not in organized_files:\n                            organized_files[subdirectory_name] = []\n                        organized_files[subdirectory_name].append(filename)\n\n    # Return the directory path and the dictionary of organized files\n    return directory, organized_files\n```"}
{"task_id": "BigCodeBench/324", "solution": "```python\nimport subprocess\nimport threading\n\ndef task_func(file_list):\n    exit_codes = []  # Create an empty list to store exit codes\n\n    def run_file(file_path):  # Define a nested function for subprocess execution\n        process = subprocess.run(file_path, shell=True)  # Execute the file as a subprocess\n        exit_codes.append(process.returncode)  # Append exit code to the list\n\n    threads = []  # Create a list to hold threads\n\n    for file in file_list:  # Loop through each file\n        thread = threading.Thread(target=run_file, args=(file,))  # Create a new thread for each file\n        thread.start()  # Start the thread\n        threads.append(thread)  # Add thread to the list\n\n    for thread in threads:  # Wait for each thread to finish\n        thread.join()\n\n    return exit_codes  # Return the list of exit codes\n```"}
{"task_id": "BigCodeBench/326", "solution": "```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    # Use glob to find all .bat files in the specified directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    # Initialize an empty list to hold results\n    results = []\n    \n    # Iterate through each .bat file found\n    for bat_file in bat_files:\n        try:\n            # Execute the .bat file using subprocess.run\n            result = subprocess.run(bat_file, check=True)\n            # If successful, append the filename and exit code to the results list\n            results.append((os.path.basename(bat_file), result.returncode))\n        except subprocess.CalledProcessError:\n            # If execution fails, append the filename and None to the results list\n            results.append((os.path.basename(bat_file), None))\n        except Exception:\n            # Handle unexpected exceptions and append None for exit code\n            results.append((os.path.basename(bat_file), None))\n    \n    # Return the results list\n    return results\n```"}
{"task_id": "BigCodeBench/341", "solution": "```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Check if df is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame.\")\n    \n    # Check if the DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The DataFrame must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' does not exist in the DataFrame.\")\n    \n    # Create a matplotlib figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # First subplot: histogram with kernel density estimate\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    axes[0].set_title(f'Histogram and KDE of {col}')\n    axes[0].set_xlabel(col)\n    axes[0].set_ylabel('Frequency')\n    \n    # Second subplot: box plot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box Plot of {col}')\n    axes[1].set_xlabel(col)\n    \n    # Return the matplotlib figure object\n    return fig\n```"}
{"task_id": "BigCodeBench/346", "solution": "```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(\"The specified script does not exist.\")\n    \n    # Prepare the command to run the script\n    command = [sys.executable, script_path] + list(args)\n    \n    # Execute the command\n    try:\n        if wait:\n            # Wait for the process to finish and capture the return code\n            result = subprocess.run(command, check=True)\n            return result.returncode  # Return the return code of the subprocess\n        else:\n            # Start the process without waiting\n            subprocess.Popen(command)\n            return None  # Return None since we are not waiting for the process\n    except subprocess.CalledProcessError as e:\n        # Raise CalledProcessError if the script raises an exception\n        raise subprocess.CalledProcessError(e.returncode, e.cmd) from e\n```"}
{"task_id": "BigCodeBench/360", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Step 1: Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    # Step 2: Read the specified sheet from the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\")\n    \n    # Step 3: Calculate mean and standard deviation for each column\n    mean_std_dict = {}\n    for column in df.columns:\n        mean = df[column].mean()\n        std_dev = df[column].std()\n        mean_std_dict[column] = {'mean': mean, 'std_dev': std_dev}\n    \n    # Step 4: Create a bar chart\n    mean_values = [mean_std_dict[col]['mean'] for col in mean_std_dict]\n    std_dev_values = [mean_std_dict[col]['std_dev'] for col in mean_std_dict]\n    bar_width = 0.35\n    index = np.arange(len(mean_std_dict))\n    \n    fig, ax = plt.subplots()\n    bars1 = ax.bar(index, mean_values, bar_width, label='Mean')\n    bars2 = ax.bar(index + bar_width, std_dev_values, bar_width, label='Standard Deviation')\n    \n    # Step 5: Customize the chart\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(mean_std_dict.keys())\n    ax.legend()\n\n    # Step 6: Return the results\n    return mean_std_dict, fig\n```"}
{"task_id": "BigCodeBench/367", "solution": "```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all items in activities are datetime objects\n    for activity in activities:\n        if not isinstance(activity, datetime):\n            raise TypeError(\"If the activities are not datetime objects.\")\n    \n    # Create a defaultdict to count activities per day of the week\n    activity_count = defaultdict(int)\n    \n    # Count activities for each day of the week\n    for activity in activities:\n        day_of_week = activity.strftime(\"%A\")  # Get the day of the week as a string\n        activity_count[day_of_week] += 1  # Increment the count for that day\n    \n    # Prepare data for the bar chart\n    days = list(activity_count.keys())  # Days of the week\n    counts = [activity_count[day] for day in days]  # Corresponding counts\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    # Return the Axes object\n    return ax\n```"}
{"task_id": "BigCodeBench/368", "solution": "```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the random seed\n    random.seed(seed)\n    \n    # List all files in the source directory\n    files = os.listdir(src_dir)\n    \n    # Ensure there are files to select from\n    if not files:\n        raise ValueError(\"No files found in the source directory.\")\n    \n    # Select a random file from the list\n    selected_file = random.choice(files)\n    \n    # Construct full path for source and destination\n    src_file_path = os.path.join(src_dir, selected_file)\n    dest_file_path = os.path.join(dest_dir, selected_file)\n    \n    # Move the selected file to the destination directory\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the moved file\n    return selected_file\n```"}
{"task_id": "BigCodeBench/374", "solution": "```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Step 1: Find all .xlsx files in the specified directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Step 2: Initialize a counter for processed files\n    processed_count = 0\n    \n    # Step 3: Iterate through each Excel file found\n    for excel_file in excel_files:\n        # Step 3a: Load the workbook\n        workbook = load_workbook(excel_file)\n        \n        # Step 3b: Iterate through each sheet in the workbook\n        for sheet in workbook.sheetnames:\n            worksheet = workbook[sheet]\n            \n            # Step 3c: Iterate through all rows and cells in the sheet\n            for row in worksheet.iter_rows():\n                for cell in row:\n                    # Step 3d: Check if the cell contains a string\n                    if isinstance(cell.value, str):\n                        # Step 3e: Replace double quotes with double backslash followed by double quote\n                        cell.value = cell.value.replace('\"', r'\\\\\\\"')\n        \n        # Step 4: Save the modified workbook back to its original file\n        workbook.save(excel_file)\n        \n        # Step 5: Increment the processed file counter\n        processed_count += 1\n    \n    # Step 6: Return the total number of processed Excel files\n    return processed_count\n```"}
{"task_id": "BigCodeBench/399", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    # Check if the frequency is negative\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    \n    # Check if the sample size is non-positive\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    # Generate an array of sample points\n    t = np.linspace(0, 2 * math.pi, sample_size)\n    \n    # Calculate sine and cosine values\n    sine_wave = np.sin(frequency * t)\n    cosine_wave = np.cos(frequency * t)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave', color='blue')\n    ax.plot(t, cosine_wave, label='Cosine Wave', color='orange')\n    \n    # Add labels and legend\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    \n    # Return the figure and axes objects\n    return fig, ax\n```"}
{"task_id": "BigCodeBench/401", "solution": "```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create a new Flask application instance with the provided app_name.\n    app = Flask(app_name)\n    \n    # Retrieve email server configuration details from environment variables.\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    mail_port = int(os.environ.get('MAIL_PORT', 25))\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    mail_username = os.environ.get('MAIL_USERNAME', None)\n    mail_password = os.environ.get('MAIL_PASSWORD', None)\n\n    # Create a Mail instance using the Flask app created earlier.\n    mail = Mail(app)\n\n    # Store the mail configuration settings in a dictionary.\n    mail_config = {\n        'MAIL_SERVER': mail_server,\n        'MAIL_PORT': mail_port,\n        'MAIL_USE_TLS': mail_use_tls,\n        'MAIL_USERNAME': mail_username,\n        'MAIL_PASSWORD': mail_password\n    }\n    \n    # Return a tuple containing the Mail instance and the dictionary of mail configurations.\n    return mail, mail_config\n```"}
{"task_id": "BigCodeBench/409", "solution": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n\n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n    \n    # Read the Excel file into a DataFrame\n    df = pd.read_excel(full_path)\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the DataFrame.\")\n\n    # Extract the specified column\n    column_data = df[column_name]\n\n    # Calculate mean, median, and standard deviation\n    mean_value = np.mean(column_data)\n    median_value = np.median(column_data)\n    std_dev_value = np.std(column_data)\n\n    # Create a dictionary to store the results\n    results = {\n        'mean': mean_value,\n        'median': median_value,\n        'standard_deviation': std_dev_value\n    }\n\n    # Return the results\n    return results\n```"}
{"task_id": "BigCodeBench/417", "solution": "```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    # Step 1: Split the input data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Step 2: Construct a Keras Sequential model\n    model = Sequential()\n    model.add(Dense(units=10, activation='sigmoid', input_dim=2))  # One hidden layer with 10 units\n\n    # Step 3: Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])\n\n    # Step 4: Fit the model to the training data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Step 5: Plot the training and validation loss\n    plt.figure()\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Step 6: Return the trained model and the Axes object\n    return model, plt.gca()\n```"}
{"task_id": "BigCodeBench/418", "solution": "```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Step 3: Split the input data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Step 4: Create a Keras Sequential model\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(units=10, activation='sigmoid', input_shape=(X_train.shape[1],)))\n\n    # Step 5: Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n\n    # Step 6: Fit the model to the training data\n    model.fit(X_train, Y_train, verbose=0)\n\n    # Step 7: Evaluate the model on the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n\n    # Step 8: Compute the ROC curve\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n\n    # Step 9: Calculate the AUC score\n    roc_auc = auc(fpr, tpr)\n\n    # Step 10: Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))\n    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    # Step 11: Return the trained model and the Axes object\n    return model, plt.gca()\n```"}
{"task_id": "BigCodeBench/424", "solution": "```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Validate the input parameters\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The file at {image_path} does not exist.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n\n    # Read the image\n    original_image = cv2.imread(image_path)\n    if original_image is None:\n        raise FileNotFoundError(f\"The file at {image_path} could not be read as an image.\")\n    \n    # Convert the image from BGR to RGB (OpenCV loads images in BGR)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n\n    # Reshape the image to a 2D array of pixels\n    pixel_values = original_image.reshape((-1, 3))  # Reshape to (num_pixels, 3)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n\n    # Replace pixel colors with the corresponding cluster centroid colors\n    segmented_image = kmeans.cluster_centers_[kmeans.labels_].reshape(original_image.shape).astype(np.uint8)\n\n    # Save each cluster region as a separate image\n    for i in range(n_clusters):\n        # Create a mask for the current cluster\n        mask = (kmeans.labels_ == i).reshape(original_image.shape[0], original_image.shape[1])\n        cluster_region = np.zeros_like(original_image)\n        cluster_region[mask] = kmeans.cluster_centers_[i]\n\n        # Save the cluster region image\n        cv2.imwrite(f\"cluster_{i}.png\", cv2.cvtColor(cluster_region, cv2.COLOR_RGB2BGR))\n\n    # Return the original and segmented images\n    return original_image, segmented_image\n```"}
{"task_id": "BigCodeBench/443", "solution": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Check the dimensions of P and T to ensure they are compatible for multiplication\n    assert P.ndim == 2, \"P must be a 2D matrix\"\n    assert T.ndim == 3, \"T must be a 3D tensor\"\n    assert P.shape[1] == T.shape[0], \"Inner dimensions of P and T must match for multiplication\"\n    \n    # Multiply the matrix P with the 3D tensor T\n    result = np.tensordot(P, T, axes=1)  # This performs the multiplication along the matching axis\n    \n    # Flatten the resulting array into a 1D array\n    flattened_result = result.flatten()\n    \n    # Apply KMeans clustering on the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(flattened_result.reshape(-1, 1))  # Reshape for KMeans (n_samples, n_features)\n    \n    # Store the clustering result\n    cluster_result = kmeans.labels_\n    \n    # Visualize the clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(flattened_result)), flattened_result, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    # Return the clustering results and the visualization\n    return cluster_result, ax\n```"}
{"task_id": "BigCodeBench/445", "solution": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Step 1: Validate the input\n    if not isinstance(points, (np.ndarray, list)):\n        raise ValueError(\"Points must be a numpy array or a list.\")\n    \n    points = np.array(points)  # Convert to numpy array if it's a list\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Points must be a 2D array with shape (n, 2).\")\n    \n    # Step 2: Apply jittering to prevent overlapping points\n    np.random.seed(seed)\n    jitter = np.random.normal(0, 0.01, points.shape)  # Small jitter\n    jittered_points = points + jitter\n\n    # Step 3: Calculate the Voronoi diagram\n    vor = Voronoi(jittered_points)\n\n    # Step 4: Create a plot for the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    # Step 5: Return the Voronoi object and the axes\n    return (vor, ax)\n```"}
{"task_id": "BigCodeBench/454", "solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if the source directory exists\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"The source directory '{src_dir}' does not exist.\")\n\n    # Check if the destination directory exists\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"The destination directory '{dest_dir}' does not exist.\")\n\n    # Search for files in the source directory that match the specified extension\n    search_pattern = os.path.join(src_dir, f\"*{ext}\")\n    files_to_move = glob.glob(search_pattern)\n\n    # Create an empty list to store the paths of successfully moved files\n    moved_files = []\n\n    # Loop through each file found in the source directory\n    for file_path in files_to_move:\n        # Extract the filename from the file path\n        filename = os.path.basename(file_path)\n        destination_file_path = os.path.join(dest_dir, filename)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(destination_file_path):\n            # Move the file if it does not exist in the destination\n            shutil.move(file_path, destination_file_path)\n            # Append the full path of the moved file to the list\n            moved_files.append(destination_file_path)\n\n    # Return the list of full paths of the files that were successfully moved\n    return moved_files\n```"}
{"task_id": "BigCodeBench/458", "solution": "```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Step 1: Load the JSON string into a dictionary\n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()  # Return empty DataFrame if JSON is invalid\n    \n    # Step 2: Check if the dictionary is empty\n    if not data_dict:\n        return pd.DataFrame()  # Return empty DataFrame if the dictionary is empty\n\n    # Step 3: Helper function to double numerical values\n    def double_numerical_values(d):\n        for key, value in d.items():\n            if isinstance(value, (int, float)):\n                d[key] = float(value) * 2\n            elif isinstance(value, list):\n                d[key] = [float(v) * 2 if isinstance(v, (int, float)) else v for v in value]\n            elif isinstance(value, str):\n                # Use regex to find and double numerical values in strings\n                match = re.match(r\"^([-+]?[0-9]*\\.?[0-9]+)$\", value)\n                if match:\n                    d[key] = float(value) * 2\n\n    # Step 4: Normalize the dictionary\n    double_numerical_values(data_dict)\n\n    # Step 5: Convert the normalized dictionary into a Pandas DataFrame\n    df = pd.DataFrame([data_dict])  # Wrap in a list to create a single row DataFrame\n\n    return df  # Return the resulting DataFrame\n```"}
{"task_id": "BigCodeBench/461", "solution": "```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Step 1: Check if the script_path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} does not exist.\")\n\n    # Step 2: Start the script in a subprocess\n    process = subprocess.Popen(['bash', script_path])\n    \n    # Step 3: Initialize variables for CPU and memory usage\n    cpu_usage = 0\n    memory_usage = 0\n    start_time = time.time()\n    \n    # Step 4: Monitor the process\n    try:\n        while True:\n            # Check if the process is still running\n            if process.poll() is not None:\n                break  # Process has completed\n            \n            # Use psutil to get process information\n            proc = psutil.Process(process.pid)\n            \n            # Accumulate CPU usage (cumulative, not average)\n            cpu_usage += proc.cpu_percent(interval=1)\n            \n            # Accumulate memory usage (RSS memory)\n            memory_usage += proc.memory_info().rss\n            \n            # Check for timeout\n            if time.time() - start_time > timeout:\n                process.terminate()  # Terminate the process\n                break\n            \n    except psutil.NoSuchProcess:\n        pass  # Process no longer exists, handle gracefully.\n    \n    except psutil.ZombieProcess:\n        pass  # Process is a zombie, handle gracefully.\n\n    # Step 5: Ensure the subprocess is terminated\n    if process.poll() is None:\n        process.terminate()\n\n    # Step 6: Prepare the output\n    output = {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }\n    \n    return output\n```"}
{"task_id": "BigCodeBench/477", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    # Step 1: Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Step 2: Generate random values for \"x\" and \"y\"\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n\n    # Step 3: Create the \"category\" column\n    if N >= len(CATEGORIES):\n        # Sample categories without replacement\n        category = np.random.choice(CATEGORIES, size=N, replace=False)\n    else:\n        # Sample categories with replacement\n        category = np.random.choice(CATEGORIES, size=N, replace=True)\n\n    # Step 4: Construct the DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': category\n    })\n\n    # Step 5: Create the scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=pd.factorize(df['category'])[0], cmap='viridis')\n\n    # Step 6: Add legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Categories\")\n    ax.add_artist(legend1)\n\n    # Step 7: Return the DataFrame and the Axes object\n    return df, ax\n```"}
{"task_id": "BigCodeBench/486", "solution": "```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Convert epoch start and end times to datetime objects\n    start_datetime = datetime.fromtimestamp(start_time)\n    end_datetime = datetime.fromtimestamp(end_time)\n\n    # Generate a time series with the specified step\n    time_series = pd.date_range(start=start_datetime, end=end_datetime, freq=step)\n\n    # Generate values with normal distribution and add linear trend\n    num_points = len(time_series)\n    values = np.random.normal(loc=0, scale=1, size=num_points) + (trend * np.arange(num_points))\n\n    # Create a DataFrame to store the time series data\n    df = pd.DataFrame({'Time': time_series, 'Value': values})\n\n    # Plot the time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'], marker='o')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Trend')\n    \n    # Return the Axes object\n    return ax\n```"}
{"task_id": "BigCodeBench/492", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Validate the input for epoch_milliseconds\n    if not isinstance(epoch_milliseconds, (int, float)):\n        raise ValueError(\"epoch_milliseconds must be a number.\")\n    \n    # Validate the products list\n    if not isinstance(products, list) or not all(isinstance(product, str) for product in products):\n        raise ValueError(\"products must be a list of strings.\")\n    \n    # Convert epoch_milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    end_date = datetime.now()\n\n    # Generate a date range from start_date to end_date\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    # Initialize a list to collect the sales data\n    sales_data = []\n\n    # Loop through each date in the date range\n    for single_date in date_range:\n        # For each product, generate random sales data\n        for product in products:\n            sales_quantity = random.randint(10, 50)  # Random sales quantity in the range [10, 50]\n            sales_data.append({\"Product\": product, \"Date\": single_date, \"Sales\": sales_quantity})\n\n    # Create a DataFrame from the sales data\n    sales_df = pd.DataFrame(sales_data)\n\n    # Return the DataFrame\n    return sales_df\n```"}
{"task_id": "BigCodeBench/501", "solution": "```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Step 1: Check if json_str is of the correct type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    # Step 2: Try to parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string\")\n\n    # Step 3: Check if the parsed JSON is an empty array\n    if isinstance(data, list) and not data:\n        # Create an empty DataFrame for an empty JSON array\n        df = pd.DataFrame()\n    else:\n        # Step 4: Convert JSON data to a DataFrame\n        df = pd.json_normalize(data)\n\n    # Step 5: Write the DataFrame to an Excel file\n    try:\n        output_path = os.path.abspath(filename)\n        writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n        writer.save()\n    except Exception as e:\n        raise Exception(f\"An error occurred while writing to the file: {str(e)}\")\n\n    # Step 6: Return the absolute path of the created Excel file\n    return output_path\n```"}
{"task_id": "BigCodeBench/502", "solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Initialize an empty list to hold the generated data\n    data = []\n    \n    # Generate data for the specified number of days in the past\n    for i in range(days_in_past):\n        # Calculate the date\n        date = datetime.now() - timedelta(days=i)\n        # Randomly select an activity\n        activity = random.choice(activities)\n        # Generate a random duration between 0 and 120\n        duration = random.randint(0, 120)\n        # Append the data as a dictionary\n        data.append({\"Date\": date.strftime(\"%Y-%m-%d\"), \"Activity\": activity, \"Duration\": duration})\n    \n    # Create a DataFrame from the list of dictionaries\n    df = pd.DataFrame(data)\n    \n    # Create a line plot using seaborn\n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Format the plot\n    plt.xticks(rotation=45)\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    \n    # Return the axes object and the DataFrame\n    return ax, df\n```"}
{"task_id": "BigCodeBench/503", "solution": "```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate a list of dates for the specified number of days in the past\n    end_date = datetime.now()\n    date_range = pd.date_range(end=end_date, periods=days_in_past)\n\n    # Create random stock prices for each stock name\n    random_prices = np.random.rand(days_in_past, len(stock_names))\n\n    # Construct a DataFrame using the dates as the index and random stock prices\n    stock_prices_df = pd.DataFrame(data=random_prices, index=date_range, columns=stock_names)\n\n    # Return the constructed DataFrame\n    return stock_prices_df\n```"}
{"task_id": "BigCodeBench/509", "solution": "```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compares two CSV files line by line and provides a detailed report of the differences.\n    \n    Parameters:\n    - file_path1: str - The path to the first CSV file.\n    - file_path2: str - The path to the second CSV file.\n    - delimiter: str - The character that separates values in the CSV (default is a comma).\n    - quotechar: str - The character used to quote fields in the CSV (default is a double quote).\n    \n    Returns:\n    - DataFrame: A pandas DataFrame with the differences.\n    \"\"\"\n    try:\n        # Read the CSV files into pandas DataFrames\n        df1 = pd.read_csv(file_path1, delimiter=delimiter, quotechar=quotechar)\n        df2 = pd.read_csv(file_path2, delimiter=delimiter, quotechar=quotechar)\n        \n        # Check if either DataFrame is empty\n        if df1.empty or df2.empty:\n            raise ValueError(\"One of the files is empty.\")\n        \n    except FileNotFoundError:\n        raise FileNotFoundError(\"One of the files cannot be found.\")\n    except ValueError as ve:\n        raise ve\n    except Exception as e:\n        raise Exception(\"An I/O error occurred: \" + str(e))\n\n    # Convert DataFrames to lists for line-by-line comparison\n    lines1 = df1.to_string(index=False).splitlines()\n    lines2 = df2.to_string(index=False).splitlines()\n\n    # Use ndiff to compare the two lists of lines\n    diff = list(ndiff(lines1, lines2))\n    \n    # Prepare a list to hold the differences\n    differences = []\n\n    # Process the differences\n    for line in diff:\n        if line.startswith(\"  \"):\n            # No change\n            status = ' '\n        elif line.startswith(\"- \"):\n            # Present in first file but not in second\n            status = '-'\n        elif line.startswith(\"+ \"):\n            # Present in second file but not in first\n            status = '+'\n        else:\n            continue  # Skip lines not starting with a diff indicator\n\n        # Extract the line content\n        content = line[2:]  # Skip the diff indicator and space\n\n        # Get the line number (1-based index)\n        line_number = len(differences) + 1\n        \n        # Append the difference to the list\n        differences.append((line_number, status, content))\n\n    # Create a DataFrame from the differences\n    diff_df = pd.DataFrame(differences, columns=['Line Number', 'Status', 'Content'])\n    \n    return diff_df\n```"}
{"task_id": "BigCodeBench/511", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Step 3: Check if the data list is empty\n    if not data:\n        # Step 4: Initialize dictionary with default values\n        stats = {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n        # Create an empty Axes for the pie chart\n        ax = plt.gca()\n        ax.pie([], labels=[], autopct='%1.1f%%')\n    else:\n        # Step 5: Convert the data into a DataFrame\n        df = pd.DataFrame(data)\n        \n        # Step 6: Calculate statistics for the specified column\n        stats = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max()\n        }\n        \n        # Step 7: Create a pie chart using the 'Age' column\n        age_counts = df['Age'].value_counts()\n        ax = age_counts.plot(kind='pie', autopct='%1.1f%%', title='Age Distribution', ylabel='', figsize=(8, 8))\n    \n    # Step 8: Return a tuple containing the dictionary and the Axes object\n    return stats, ax\n```"}
{"task_id": "BigCodeBench/513", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Step 1: Check if the data list is empty\n    if not data:\n        raise ValueError(\"The data list cannot be empty.\")\n    \n    # Step 2: Convert the data list into a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Step 3: Validate that the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' does not exist in the data.\")\n    \n    # Step 4: Ensure that the numeric values for 'steps', 'calories burned', and 'distance walked' are non-negative\n    if (df[['steps', 'calories burned', 'distance walked']] < 0).any().any():\n        raise ValueError(\"All values for 'steps', 'calories burned', and 'distance walked' must be non-negative.\")\n    \n    # Step 5: Calculate statistics for the specified column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Step 6: Generate a line chart using matplotlib\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], df[column], label=column)\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n    ax.legend()\n    \n    # Step 7: Return a tuple containing the statistics and the Axes object\n    return stats, ax\n```"}
{"task_id": "BigCodeBench/526", "solution": "```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Open and read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize a defaultdict to store numeric values\n    numeric_data = defaultdict(list)\n\n    # Iterate through each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):  # Ensure the value is numeric\n                numeric_data[key].append(value)\n\n    # Calculate mean and median for each key\n    results = {}\n    for key, values in numeric_data.items():\n        if values:  # Check if list is not empty\n            mean = np.mean(values)  # Calculate mean\n            median = np.median(values)  # Calculate median\n            results[key] = {'mean': mean, 'median': median}\n\n    # Create a DataFrame with the calculated mean and median\n    df = pd.DataFrame.from_dict(results, orient='index')\n\n    # Sort the DataFrame by the index (keys)\n    df.sort_index(inplace=True)\n\n    return df\n```"}
{"task_id": "BigCodeBench/528", "solution": "```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Step 1: Check for the .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file format is incorrect. Please provide a .csv file.\")\n    \n    # Step 2: Read the CSV file and store the rows\n    with open(file_path, mode='r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        rows = list(reader)\n    \n    # Step 3: Count occurrences of each row using Counter\n    row_counts = Counter(tuple(row) for row in rows)\n    \n    # Step 4: Filter out non-duplicate rows\n    duplicate_rows = {row: count for row, count in row_counts.items() if count > 1}\n    \n    # Step 5: Convert the duplicate rows dictionary into a pandas DataFrame\n    df = pd.DataFrame(list(duplicate_rows.items()), columns=['Row', 'Count'])\n    \n    # Step 6: Create a bar chart using matplotlib\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='Row', y='Count', ax=ax)\n    ax.set_title('Counts of Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_xlabel('Duplicate Rows')\n    plt.xticks(rotation=45)\n    \n    # Step 7: Return the dictionary of duplicates and the Axes object\n    return duplicate_rows, ax\n```"}
{"task_id": "BigCodeBench/530", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Ensure 'age' column does not contain negative values\n    if (df['age'] < 0).any():\n        raise ValueError(\"The 'age' column contains negative values.\")\n    \n    # Round down the age values to the nearest integer\n    df['age'] = np.floor(df['age']).astype(int)\n    \n    # Identify duplicate entries based on the 'name' column\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    # Record the age distribution for duplicate names\n    age_distribution = Counter(duplicates['age'])\n    \n    # Create a histogram plot if there are duplicates\n    if not duplicates.empty:\n        min_age = duplicates['age'].min()\n        max_age = duplicates['age'].max()\n        \n        # Calculate bins adjusted by 0.5\n        bins = np.arange(min_age - 0.5, max_age + 1.5, 1)\n        \n        # Plotting the histogram\n        plt.figure(figsize=(10, 6))\n        plt.hist(duplicates['age'], bins=bins, edgecolor='black')\n        plt.xlabel('Age')\n        plt.ylabel('Count')\n        plt.title('Age Distribution for Duplicate Names')\n        plt.xticks(np.arange(min_age, max_age + 1, 1))\n        plt.grid(axis='y', alpha=0.75)\n        \n        # Return the Counter and the Axes object\n        return age_distribution, plt.gca()\n    else:\n        # Return the Counter and None if there are no duplicates\n        return age_distribution, None\n```"}
{"task_id": "BigCodeBench/532", "solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Step 1: Extract the 'value' column from the DataFrame\n    values = df['value']\n    \n    # Step 2: Identify and count the duplicate values in the 'value' column\n    value_counts = Counter(values)\n    \n    # Step 3: Create a histogram of the 'value' column\n    plt.figure(figsize=(10, 6))  # Optional: Set the figure size for better visibility\n    counts, bins_edges, _ = plt.hist(values, bins=bins, color='green', alpha=0.6, edgecolor='black')\n\n    # Step 4: Fit a normal distribution to the data\n    mean = np.mean(values)\n    std_dev = np.std(values)\n    x = np.linspace(min(values), max(values), 100)\n    y = norm.pdf(x, mean, std_dev) * (bins_edges[1] - bins_edges[0]) * len(values)  # Scale the normal curve\n\n    # Step 5: Overlay the normal distribution curve on the histogram\n    plt.plot(x, y, color='black', linewidth=2)\n\n    # Step 6: Set the title and labels of the plot\n    plt.title(\"Distribution\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n\n    # Step 7: Return the Counter object and the Axes object\n    return value_counts, plt.gca()  # gca() gets the current Axes instance\n```"}
{"task_id": "BigCodeBench/553", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Determine the number of columns based on the length of list b\n    num_columns = len(b)\n    \n    # Generate a DataFrame with random values\n    data = np.random.rand(len(a), num_columns)\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_columns])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n    \n    # Return the Axes object of the plotted bar chart\n    return ax\n```"}
{"task_id": "BigCodeBench/560", "solution": "```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Ensure data is a pandas DataFrame and contains monthly data for one year\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame\")\n    \n    if data.shape[0] != 12:\n        raise ValueError(\"Data must contain exactly 12 rows for monthly data\")\n    \n    # Extract the year from the index or column (assuming it's in the first column)\n    year = data.index.year[0] if hasattr(data.index, 'year') else data.iloc[0, 0].year\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(data.index.month, data.iloc[:, 0], color='blue')\n    \n    # Set the title and labels\n    plt.title(f'Monthly Data for {year}')\n    plt.xlabel('Month')\n    plt.ylabel('Value')\n    \n    # Return the Axes object\n    return plt.gca()  # Get current Axes object\n```"}
{"task_id": "BigCodeBench/567", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input string data to a numeric format\n    numeric_data = pd.to_numeric(data.split(','), errors='coerce')\n    \n    # Calculate the minimum and maximum values\n    min_value = numeric_data.min()\n    max_value = numeric_data.max()\n    \n    # Create bins for the histogram\n    bins = np.arange(min_value, max_value + 2) - 0.5\n    \n    # Create a histogram using matplotlib\n    fig, ax = plt.subplots()\n    ax.hist(numeric_data, bins=bins, edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Return the Axes object of the created histogram\n    return ax\n```"}
{"task_id": "BigCodeBench/574", "solution": "```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Step 1: Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n\n    # Step 2: Create clean sine wave\n    clean_sine_wave = np.sin(x)\n\n    # Step 3: Add random noise to the sine wave\n    noisy_sine_wave = clean_sine_wave + np.random.normal(0, noise_level, array_length)\n\n    # Step 4: Define model function for fitting\n    def model_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n\n    # Step 5: Use curve_fit to fit the noisy data to the model function\n    initial_guess = [1, 1, 0, 0]  # Initial guess for amplitude, frequency, phase, offset\n    params, _ = curve_fit(model_func, x, noisy_sine_wave, p0=initial_guess)\n\n    # Step 6: Generate fitted curve using the optimized parameters\n    fitted_curve = model_func(x, *params)\n\n    # Step 7: Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(x, noisy_sine_wave, label='Noisy Sine Wave', color='blue', linestyle='dashed')\n    ax.plot(x, fitted_curve, label='Fitted Curve', color='red')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    ax.set_xlabel('x values')\n    ax.set_ylabel('y values')\n    ax.legend()\n\n    # Step 8: Return the Axes object\n    return ax\n\n# Example call to the function (can be commented out if not needed)\n# task_func(100, 0.2)\n```"}
{"task_id": "BigCodeBench/579", "solution": "```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    # Initialize an empty list to store all text from the CSV file\n    text_data = []\n\n    # Read the CSV file with error handling\n    try:\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                # Assuming the text data is in the first column of each row\n                text_data.extend(row)  # Extend the list with row content\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file {csv_file}.\")\n\n    # Normalize the text to ASCII and concatenate all text\n    normalized_text = ' '.join(unicodedata.normalize('NFKD', text) for text in text_data)\n\n    # Count the words\n    words = normalized_text.split()\n    word_counts = Counter(words)\n\n    # Identify the 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Create the bar plot\n    words, counts = zip(*most_common_words)  # Unzip the words and counts\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('10 Most Common Words')\n\n    # Return the output as a tuple\n    return (ax, most_common_words)\n```"}
{"task_id": "BigCodeBench/582", "solution": "```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a histogram of the generated random numbers\n    count, bins, ignored = plt.hist(data, bins=30, density=True, alpha=0.5, color='g', label='Histogram')\n    \n    # Calculate the probability density function (PDF)\n    mu, std = np.mean(data), np.std(data)\n    pdf = stats.norm.pdf(bins, mu, std)\n    \n    # Plot the PDF on the same figure\n    plt.plot(bins, pdf, 'k', linewidth=2, label='PDF')\n    \n    # Add title and labels\n    plt.title('Histogram and PDF of Normally Distributed Data')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid()\n    \n    # Return the matplotlib figure object\n    return plt.gcf()\n```"}
{"task_id": "BigCodeBench/583", "solution": "```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\n\ndef task_func():\n    # Step 1: Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Step 2: Generate 8 random bytes for the filename\n    random_bytes = get_random_bytes(8)\n    hex_number = random_bytes.hex()\n\n    # Step 3: Create a unique filename for the encrypted private key\n    filename = f\"private_key_{hex_number}.txt\"\n\n    # Step 4: Generate a random encryption password\n    encryption_password = get_random_bytes(16)  # 16 bytes for AES-128\n\n    # Step 5: Create an AES cipher object\n    cipher = AES.new(encryption_password, AES.MODE_GCM)\n    nonce = cipher.nonce  # Get the nonce from the cipher\n\n    # Step 6: Encrypt the private RSA key\n    encrypted_private_key, tag = cipher.encrypt_and_digest(private_key.save_pkcs1(format='PEM'))\n\n    # Step 7: Save the encrypted private key to the file\n    with open(filename, 'wb') as file:\n        file.write(nonce + tag + encrypted_private_key)\n\n    # Step 8: Return the results\n    return public_key, filename, encryption_password, nonce\n```"}
{"task_id": "BigCodeBench/587", "solution": "```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Step 1: Generate RSA keys\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Step 2: Generate a random AES key\n    aes_key = os.urandom(32)  # AES-256 requires a 32-byte key\n    iv = os.urandom(16)  # Initialization vector for AES\n\n    # Step 3: Read the content of the file\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n\n    # Step 4: Encrypt the file content using AES\n    # Create a Cipher object\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad the file data\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n\n    # Encrypt the padded data\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Step 5: Save the encrypted file\n    encrypted_file_name = f'encrypted_{os.path.basename(file_path)}'\n    with open(encrypted_file_name, 'wb') as enc_file:\n        enc_file.write(iv + encrypted_data)  # Prepend IV for decryption later\n\n    # Step 6: Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Step 7: Save the encrypted AES key\n    encrypted_aes_key_file_name = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_file_name, 'wb') as aes_key_file:\n        aes_key_file.write(encrypted_aes_key)\n\n    # Step 8: Return the required outputs\n    return public_key, encrypted_file_name, encrypted_aes_key_file_name\n```"}
{"task_id": "BigCodeBench/590", "solution": "```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    # Validate the URL\n    if not url or not isinstance(url, str):\n        raise ValueError(\"Provided URL is invalid or empty.\")\n    \n    try:\n        # Fetch HTML content from the URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except urllib.error.URLError:\n        raise urllib.error.URLError(\"There is an issue with network connectivity or the server.\")\n    \n    # Parse the HTML content using PyQuery\n    document = pq(html_content)\n    anchor_tags = document('a')\n    \n    # Extract text and href attributes from anchor tags\n    data = []\n    for anchor in anchor_tags.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        data.append({'text': text, 'href': href})\n    \n    # Capture fetch time\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    df['fetch_time'] = fetch_time\n    \n    return df\n```"}
{"task_id": "BigCodeBench/592", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Calculate total data points based on hours\n    total_data_points = hours * 60  # 1 data point every minute\n\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create a new CSV file with an appropriate name\n    output_file = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Write the header row to the CSV file\n    with open(output_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n\n        # Generate data points\n        for i in range(total_data_points):\n            # Generate timestamp\n            timestamp = datetime.now() + timedelta(minutes=i)\n            # Generate random sensor values\n            temperature = randint(-20, 40)  # Random temperature between -20 and 40\n            humidity = randint(0, 100)      # Random humidity between 0 and 100\n            pressure = randint(950, 1050)   # Random pressure in hPa\n\n            # Write each generated data row to the CSV file\n            writer.writerow([timestamp.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n```"}
{"task_id": "BigCodeBench/593", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Step 1: Prepare to generate traffic data\n    data = []\n    time_format = \"%H:%M\"\n    \n    # Step 2: Generate random traffic data for each hour\n    for hour in range(hours):\n        time_stamp = (datetime.now().replace(hour=hour % 24, minute=0, second=0, microsecond=0)).strftime(time_format)\n        vehicle_counts = {vehicle: randint(0, 100) for vehicle in VEHICLE_TYPES}\n        vehicle_counts['Time'] = time_stamp\n        data.append(vehicle_counts)\n\n    # Step 3: Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Step 4: Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Step 5: Define CSV file path\n    csv_file_path = os.path.join(output_dir, f\"traffic_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n\n    # Step 6: Save data to CSV\n    df.to_csv(csv_file_path, index=False)\n\n    # Step 7: Plot the data\n    plt.figure(figsize=(10, 6))\n    for vehicle in VEHICLE_TYPES:\n        plt.plot(df['Time'], df[vehicle], label=vehicle)\n    \n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data Over Time')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n\n    # Step 8: Show the plot and return the axes object\n    ax = plt.gca()\n    plt.show()\n    \n    return (csv_file_path, ax)\n```"}
{"task_id": "BigCodeBench/594", "solution": "```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create a CSV file path\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n\n    # Open a CSV file for writing\n    with open(csv_file_path, mode='w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        # Write the header\n        writer.writerow(['Time', 'Condition'])\n        \n        # Generate weather data for the specified hours\n        start_time = datetime.now()\n        for hour in range(hours):\n            # Calculate the current hour's timestamp\n            current_time = start_time + timedelta(hours=hour)\n            # Randomly select a weather condition\n            condition = choice(WEATHER_CONDITIONS)\n            # Write the row to the CSV\n            writer.writerow([current_time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n\n    # Create the backup directory if it does not exist\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n    \n    # Copy the CSV file to the backup directory\n    shutil.copy(csv_file_path, BACKUP_DIR)\n\n    # Return the path of the generated CSV file\n    return csv_file_path\n```"}
{"task_id": "BigCodeBench/618", "solution": "```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Step 1: Create a DataFrame to store the match results\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in TEAMS],  # Random goals scored\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in TEAMS]  # Random penalties converted to fines\n    }\n    df = pd.DataFrame(data)\n    \n    # Step 2: Create visualizations using Seaborn\n    plt.figure(figsize=(12, 6))\n\n    # Plot for Goals\n    plt.subplot(1, 2, 1)\n    goals_plot = sns.barplot(x='Team', y='Goals', data=df, palette='viridis')\n    goals_plot.set_title('Goals Scored by Each Team')\n    goals_plot.set_ylabel('Goals')\n    goals_plot.set_xlabel('Teams')\n\n    # Plot for Penalty Costs\n    plt.subplot(1, 2, 2)\n    penalty_plot = sns.barplot(x='Team', y='Penalty Cost', data=df, palette='rocket')\n    penalty_plot.set_title('Penalty Costs Incurred by Each Team')\n    penalty_plot.set_ylabel('Penalty Cost (in $)')\n    penalty_plot.set_xlabel('Teams')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Step 3: Return the DataFrame and list of plot objects\n    return df, [goals_plot, penalty_plot]\n\n# Example usage\nresult_df, plots = task_func(5, 3)\n```"}
{"task_id": "BigCodeBench/636", "solution": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Step 1: Create a Pandas DataFrame with specified number of rows and random integers between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Step 2: Count the number of non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Step 3: Create a bar plot to visualize the count of non-zero values for each column\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Count of Non-Zero Values in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    \n    # Step 4: Output a tuple containing the DataFrame and the Axes object\n    return df, ax\n```"}
{"task_id": "BigCodeBench/637", "solution": "```python\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Generate random student names/IDs\n    student_names = [f'Student_{i+1}' for i in range(num_students)]\n    \n    # Create a DataFrame with random grades for multiple courses\n    courses = ['Math', 'Science', 'English', 'History']\n    grades = np.random.randint(0, 101, size=(num_students, len(courses)))  # Random grades between 0 and 100\n    df = pd.DataFrame(grades, columns=courses)\n    df['Student'] = student_names\n    \n    # Calculate average grades for each course\n    average_grades = df[courses].mean()\n    \n    # Count number of students with passing grades (>= 60)\n    passing_counts = (df[courses] >= 60).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    width = 0.35  # Width of the bars\n    indices = np.arange(len(courses))  # X locations for groups\n    \n    # Create bars for average grades and passing counts\n    bars1 = ax.bar(indices - width/2, average_grades, width, label='Average Grades', color='blue')\n    bars2 = ax.bar(indices + width/2, passing_counts, width, label='Passing Count', color='green')\n    \n    # Add labels and title\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Scores / Counts')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xticks(indices)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    \n    # Return the DataFrame and Axes object\n    return df, ax\n```"}
{"task_id": "BigCodeBench/654", "solution": "```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Step 1: Identify indices in the first column that match target_value\n    indices = np.where(array[:, 0] == target_value)[0]\n    \n    # Step 2: Extract corresponding values from the second column\n    values = array[indices, 1]\n    \n    # Step 3: Define the exponential decay function\n    def exp_decay(x, a, b):\n        return a * np.exp(-b * x)\n    \n    # Step 4: Fit the exponential decay function to the extracted values\n    x_data = indices  # Using the indices as x data\n    popt, pcov = optimize.curve_fit(exp_decay, x_data, values, p0=(1, 1e-6))\n    \n    # Step 5: Create a plot to visualize the original data and fitted curve\n    plt.figure()\n    plt.scatter(x_data, values, label='Data', color='red')\n    plt.plot(x_data, exp_decay(x_data, *popt), label='Fitted Curve', color='blue')\n    plt.xlabel('Indices')\n    plt.ylabel('Values')\n    plt.title('Exponential Decay Fit')\n    plt.legend()\n    \n    # Step 6: Return the optimized parameters and the Axes object\n    return popt, plt.gca()\n```"}
{"task_id": "BigCodeBench/655", "solution": "```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(texts, num_topics):\n    # Step 1: Preprocess the input texts\n    preprocessed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        cleaned_text = cleaned_text.lower()\n        # Remove stopwords\n        words = cleaned_text.split()\n        filtered_words = [word for word in words if word not in STOPWORDS]\n        preprocessed_texts.append(' '.join(filtered_words))\n\n    # Step 2: Vectorize the preprocessed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)\n\n    # Step 3: Apply NMF to extract the specified number of topics\n    nmf_model = NMF(n_components=num_topics, random_state=1)\n    nmf_model.fit(tfidf_matrix)\n\n    # Step 4: Get the words corresponding to each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        topic_words = [feature_names[i] for i in topic.argsort()[-10:]]  # Get top 10 words for each topic\n        topics.append(topic_words)\n\n    return topics\n```"}
{"task_id": "BigCodeBench/657", "solution": "```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # Step 1: Check if stopwords are provided; if not, use NLTK's English stopwords\n    if stopwords is None:\n        nltk.download('stopwords', quiet=True)\n        from nltk.corpus import stopwords as nltk_stopwords\n        stopwords = set(nltk_stopwords.words('english'))\n\n    # Step 2: Clean and process the texts\n    processed_texts = []\n    for text in texts:\n        # a. Remove non-alphanumeric characters and convert to lowercase\n        cleaned_text = ALPHANUMERIC.sub(' ', text).lower()\n        \n        # b. Tokenize the cleaned text into words\n        tokens = cleaned_text.split()\n        \n        # c. Remove stop words\n        filtered_tokens = [token for token in tokens if token not in stopwords]\n        \n        # d. Append the filtered tokens to processed_texts\n        processed_texts.append(filtered_tokens)\n\n    # Step 3: Train the Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    # Step 4: Return the trained Word2Vec model\n    return model\n```"}
{"task_id": "BigCodeBench/678", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Create a \"processed\" subdirectory within the provided path\n    processed_path = os.path.join(path, \"processed\")\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # List and sort all JSON files in the specified directory\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n    \n    # Initialize an empty DataFrame to hold combined data\n    combined_df = pd.DataFrame()\n    \n    # Process each JSON file\n    for json_file in json_files:\n        file_path = os.path.join(path, json_file)\n        \n        # Read the contents of the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        \n        # Convert JSON data into a DataFrame\n        temp_df = pd.DataFrame(data)\n        \n        # Add a \"Source\" column to the DataFrame\n        temp_df['Source'] = json_file[:-5]  # remove the '.json' extension\n        \n        # Append the temporary DataFrame to the combined DataFrame\n        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n        \n        # Move the processed JSON file to the \"processed\" subdirectory\n        shutil.move(file_path, os.path.join(processed_path, json_file))\n    \n    # Return the combined DataFrame\n    return combined_df\n```"}
{"task_id": "BigCodeBench/720", "solution": "```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Create a directory named 'task_func_data' if it does not exist\n    directory = 'task_func_data'\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Generate random sensor data for temperature and humidity\n    temperature = round(random.uniform(-30.0, 50.0), 2)  # Random temperature between -30.0 and 50.0\n    humidity = round(random.uniform(0.0, 100.0), 2)      # Random humidity between 0.0 and 100.0\n\n    # Open a file named 'Output.txt' in append mode within the 'task_func_data' directory\n    file_path = os.path.join(directory, 'Output.txt')\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the generated sensor data to the file in CSV format\n        writer.writerow([datetime.now().isoformat(), temperature, humidity])\n\n    # Return the path to the CSV file before deleting the file\n    return_path = file_path\n\n    # Delete the 'Output.txt' file after returning the path\n    os.remove(file_path)\n\n    # Return the path of the deleted file\n    return return_path\n```"}
{"task_id": "BigCodeBench/723", "solution": "```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Step 1: Fetch the content from the provided URL\n    response = urllib.request.urlopen(url)\n    web_content = response.read()\n    \n    # Step 2: Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(web_content, 'html.parser')\n    \n    # Step 3: Extract the desired data from the parsed HTML\n    data_to_scrape = []\n    paragraphs = soup.find_all('p')\n    for paragraph in paragraphs:\n        data_to_scrape.append(paragraph.get_text())\n    \n    # Step 4: Write the scraped data into the CSV file\n    with open(CSV_FILE_PATH, mode='w', newline='', encoding='utf-8') as csv_file:\n        writer = csv.writer(csv_file)\n        # Writing headers (optional)\n        writer.writerow(['Paragraphs'])\n        # Writing data\n        for data in data_to_scrape:\n            writer.writerow([data])\n    \n    # Step 5: Return the CSV file path\n    return CSV_FILE_PATH\n```"}
{"task_id": "BigCodeBench/752", "solution": "```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Step 1: Validate inputs\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a DataFrame.\")\n    if data.empty:\n        raise ValueError(\"data cannot be empty.\")\n    if target_column not in data.columns:\n        raise ValueError(\"target_column must be a column in data.\")\n    if not np.issubdtype(data.dtypes, np.number).all():\n        raise ValueError(\"data must contain only numeric values.\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state must be an integer.\")\n    if not (0 < test_size < 1):\n        raise ValueError(\"test_size must be between 0 and 1.\")\n\n    # Step 2: Split data into features and target variable\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    # Step 3: Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Step 4: Create and fit the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Step 5: Evaluate the model using the test set\n    score = model.score(X_test, y_test)\n\n    # Step 6: Return the model's score\n    return float(score)\n\n# Example usage:\n# rng = np.random.default_rng()\n# data = pd.DataFrame({'x1': rng.random(500)})\n# data['y'] = data['x1'] * 2 + 1\n# result = task_func(data, 'y', random_state=9, test_size=0.1)\n# print(result)\n```"}
{"task_id": "BigCodeBench/760", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n               latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n               other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n               rng_seed=None):\n    # Step 1: Set the random seed if provided\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Step 2: Create a list of IDs from 1 to 100\n    ids = list(range(1, 101))\n    \n    # Step 3: Combine the names and randomly select from them\n    all_names = latin_names + other_names\n    selected_names = np.random.choice(all_names, 100, replace=True)\n\n    # Step 4: Generate random dates of birth\n    date_of_births = []\n    for _ in range(100):\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 29)  # To avoid issues with month length\n        dob = datetime.datetime(year, month, day)\n        date_of_births.append(dob)\n\n    # Step 5: Construct email addresses\n    emails = []\n    for name, dob in zip(selected_names, date_of_births):\n        year_of_birth = dob.year\n        # Correct improperly encoded Latin characters\n        name = codecs.decode(name.encode('utf-8'), 'utf-8', 'ignore')\n        email = f\"{name.lower()}{year_of_birth}@{email_domain}\"\n        emails.append(email)\n\n    # Step 6: Create a DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': selected_names,\n        'Date of Birth': date_of_births,\n        'Email': emails\n    })\n\n    return df\n\n# To test the function\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\nprint(df)\n```"}
{"task_id": "BigCodeBench/763", "solution": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Step 1: Read the content of the JSON file specified by input_file\n    with open(input_file, 'r') as infile:\n        data = json.load(infile)\n\n    # Step 2: Initialize a dictionary to store sums, counts, and values for each key\n    stats = defaultdict(lambda: {'sum': 0, 'count': 0, 'values': []})\n\n    # Step 3: Iterate over each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            # Only process numeric values\n            if isinstance(value, (int, float)):\n                stats[key]['sum'] += value\n                stats[key]['count'] += 1\n                stats[key]['values'].append(value)\n\n    # Step 4: Create a result dictionary for storing means and medians\n    result = {}\n\n    # Step 5: Calculate mean and median for each key\n    for key, value in stats.items():\n        mean = value['sum'] / value['count'] if value['count'] > 0 else None\n        median = np.median(value['values']) if value['values'] else None\n        result[key] = {'mean': mean, 'median': median}\n\n    # Step 6: Write the results to a CSV file\n    with open(output_file, 'w', newline='') as outfile:\n        writer = csv.writer(outfile)\n        # Write header\n        writer.writerow(['Field', 'Mean', 'Median'])\n        # Write data\n        for key, stats in result.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n\n    return result\n```"}
{"task_id": "BigCodeBench/765", "solution": "```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    # Initialize an empty list to store copied file paths\n    copied_files = []\n    \n    # Ensure the target directory exists; if not, create it\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n    \n    # Iterate over each file path in the kwargs dictionary\n    for file_path in kwargs.values():\n        # Check if the file exists\n        if os.path.exists(file_path):\n            # Check if the file is not empty\n            if os.path.getsize(file_path) > 0:\n                # Copy the file to the target directory\n                shutil.copy(file_path, target_dir)\n                # Append the full path of the copied file to the list\n                copied_files.append(os.path.join(target_dir, Path(file_path).name))\n    \n    # Return the list of copied file paths\n    return copied_files\n```"}
{"task_id": "BigCodeBench/771", "solution": "```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Initialize an empty list to store names of new files\n    new_files = []\n    \n    # Use pathlib to get all files in the specified directory\n    dir_path = Path(directory)\n    \n    # Iterate through each file in the directory\n    for file in dir_path.iterdir():\n        # Check if the file is a CSV and matches the specified pattern\n        if file.is_file() and re.match(pattern, file.name):\n            # Read the content of the original CSV file\n            with open(file, 'r', newline='', encoding='utf-8') as original_file:\n                reader = csv.reader(original_file)\n                content = list(reader)  # Read content into a list\n            \n            # Alter the filename based on the matched pattern\n            new_filename = re.sub(pattern, r'\\1_new.csv', file.name)  # Example alteration\n            \n            # Create a new CSV file with the altered filename\n            new_file_path = dir_path / new_filename\n            with open(new_file_path, 'w', newline='', encoding='utf-8') as new_file:\n                writer = csv.writer(new_file)\n                writer.writerows(content)  # Write the preserved content into the new file\n            \n            # Append the new filename to the list of new files\n            new_files.append(new_file_path.name)\n    \n    # Return the list of newly created CSV files\n    return new_files\n```"}
{"task_id": "BigCodeBench/777", "solution": "```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    # Step 1: List all files in the given directory\n    files = os.listdir(directory)\n    \n    # Step 2: Filter files that match the provided regex pattern\n    zip_files = [f for f in files if re.match(pattern, f)]\n    \n    # Step 3: Initialize a list to store extracted directories\n    extracted_dirs = []\n    \n    # Step 4: Process each zip file\n    for zip_file in zip_files:\n        # Step 4a: Split the filename at the last occurrence of \"-\"\n        prefix = zip_file.rsplit(\"-\", 1)[0]\n        \n        # Step 4b: Define the extraction directory\n        extraction_dir = os.path.join(directory, prefix)\n        \n        # Step 4c: Create the extraction directory if it doesn't exist\n        if not os.path.exists(extraction_dir):\n            os.makedirs(extraction_dir)\n        \n        # Step 4d: Extract the contents of the zip file\n        with zipfile.ZipFile(os.path.join(directory, zip_file), 'r') as zip_ref:\n            zip_ref.extractall(extraction_dir)\n        \n        # Step 4e: Append the extraction directory to the list\n        extracted_dirs.append(extraction_dir)\n    \n    # Step 5: Return the list of directories where files were extracted\n    return extracted_dirs\n```"}
{"task_id": "BigCodeBench/785", "solution": "```python\nimport subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Step 1: Use glob to find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    \n    # Step 2: Create the archive directory if it does not exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n    \n    # Step 3: Define the archive file name and path\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n    \n    # Step 4: Archive the matched files into a single archive file using subprocess\n    subprocess.run(['tar', '-czf', archive_file] + files_to_archive, check=True)\n    \n    # Step 5: Delete the original files that were archived\n    for file in files_to_archive:\n        os.remove(file)\n    \n    # Step 6: Return the archive file path\n    return archive_file\n```"}
{"task_id": "BigCodeBench/800", "solution": "```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv(CSV_FILE_PATH, content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    total_goals = 0\n    total_penalties = 0\n\n    # Read the existing data from the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            total_goals += int(row['goals'])\n            total_penalties += int(row['penalties'])\n\n    # Update totals with provided goals and penalties\n    total_goals += goals\n    total_penalties += penalties\n\n    # Write updated totals back to the CSV file\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['team', 'goals', 'penalties'])  # Write header\n        writer.writerow(['Total', total_goals, total_penalties])  # Write totals\n\n    # Return the counts as a Counter object\n    return Counter({'goals': total_goals, 'penalties': total_penalties})\n\n# Example usage:\nsetup_csv()\nresult = task_func(5, 3)  # Update with 5 new goals and 3 new penalties\nprint(result)  # Output the Counter object\n```"}
{"task_id": "BigCodeBench/826", "solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Initialize a counter for the number of moved files\n    moved_files_count = 0\n    \n    # Get a list of all files in the source directory\n    files = os.listdir(source_dir)\n    \n    # Iterate over each file in the source directory\n    for file in files:\n        # Check if the file name matches the specified pattern\n        if re.match(file_pattern, file):\n            # Construct the full path for the source and target files\n            source_file_path = os.path.join(source_dir, file)\n            target_file_path = os.path.join(target_dir, file)\n            \n            # Move the file to the target directory\n            shutil.move(source_file_path, target_file_path)\n            \n            # Increment the counter for moved files\n            moved_files_count += 1\n            \n    # Return the count of moved files\n    return moved_files_count\n```"}
{"task_id": "BigCodeBench/845", "solution": "```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Step 1: Clean both texts\n    cleaned_text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    cleaned_text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Step 2: Tokenize the cleaned texts\n    tokens1 = cleaned_text1.split()\n    tokens2 = cleaned_text2.split()\n\n    # Step 3: Count term frequencies\n    term_freq1 = Counter(tokens1)\n    term_freq2 = Counter(tokens2)\n\n    # Step 4: Create a set of all unique terms\n    all_terms = set(term_freq1.keys()).union(set(term_freq2.keys()))\n\n    # Step 5: Create frequency vectors for cosine similarity\n    vector1 = np.array([term_freq1.get(term, 0) for term in all_terms])\n    vector2 = np.array([term_freq2.get(term, 0) for term in all_terms])\n\n    # Step 6: Calculate cosine similarity\n    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2)) if np.linalg.norm(vector1) != 0 and np.linalg.norm(vector2) != 0 else 0.0\n\n    # Step 7: Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    # Step 8: Return the results as a tuple\n    return (cosine_similarity, levenshtein_ratio)\n```"}
{"task_id": "BigCodeBench/854", "solution": "```python\nfrom functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Check if the input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list.\")\n    \n    # Check if all elements are integers\n    if not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    \n    # Check for negative numbers\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n    \n    # Handle empty list case\n    if len(numbers) == 0:\n        return [], []\n    \n    # Generate all permutations\n    perms = list(permutations(numbers))\n    \n    # Calculate sums of factorials for each permutation\n    sums_of_factorials = []\n    for perm in perms:\n        sum_fact = sum(math.factorial(n) for n in perm)\n        sums_of_factorials.append(sum_fact)\n    \n    # Return the lists\n    return sums_of_factorials, perms\n```"}
{"task_id": "BigCodeBench/857", "solution": "```python\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    # Create a list to hold names of successfully transferred files\n    transferred_files = []\n\n    # Construct the search pattern for the specified extensions\n    search_pattern = os.path.join(SOURCE_DIR, '*')\n\n    # Use glob to find all files in the source directory\n    files_to_transfer = glob.glob(search_pattern)\n\n    # Iterate over the list of files found\n    for file_path in files_to_transfer:\n        # Get the filename from the file path\n        file_name = os.path.basename(file_path)\n        \n        # Check if the file has one of the specified extensions\n        if any(file_name.endswith(ext) for ext in EXTENSIONS):\n            try:\n                # Attempt to copy the file to the destination directory\n                shutil.copy(file_path, os.path.join(DEST_DIR, file_name))\n                # If successful, append the filename to the transferred_files list\n                transferred_files.append(file_name)\n            except Exception as e:\n                # If there is an issue, issue a warning with the error message\n                warnings.warn(f\"Could not transfer {file_name}: {str(e)}\")\n\n    # Return the list of successfully transferred files\n    return transferred_files\n```"}
{"task_id": "BigCodeBench/865", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Step 3: Extract items, counts, and weights from the input data\n    items = [item[0] for item in data]\n    counts = [item[1] for item in data]\n    weights = [item[2] for item in data]\n    \n    # Step 4: Apply z-score normalization to counts\n    normalized_counts = zscore(counts)\n    \n    # Step 5: Apply min-max scaling to weights\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1, 1)).flatten()\n    \n    # Step 6: Create a pandas DataFrame with the results\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    # Step 7: Return the constructed DataFrame\n    return df\n```"}
{"task_id": "BigCodeBench/870", "solution": "```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Check if the input list is empty\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Initialize a list to store the values for each position\n    position_values = [[] for _ in range(len(data_list[0]))]\n\n    # Iterate over the tuples in data_list\n    for tup in data_list:\n        # Iterate over each element by its index\n        for index, value in enumerate(tup):\n            # Check if the current element is a numeric value\n            if isinstance(value, (int, float)):\n                position_values[index].append(value)\n\n    # Calculate the mean for each position\n    mean_values = []\n    for values in position_values:\n        if values:  # If there are numeric values for this position\n            mean_values.append(np.mean(values))\n        else:  # If no numeric values, append NaN\n            mean_values.append(np.nan)\n\n    # Create a DataFrame with the computed mean values\n    df = pd.DataFrame(mean_values, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(mean_values))])\n\n    # Return the DataFrame\n    return df\n\n# Example usage\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)\nprint(df)\n```"}
{"task_id": "BigCodeBench/879", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    # Check if col1 and col2 are in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns '{col1}' or '{col2}' not found in DataFrame.\")\n    \n    # Check if both columns have multiple categories\n    if data[col1].nunique() < 2 or data[col2].nunique() < 2:\n        raise ValueError(\"Both columns must have multiple categories.\")\n    \n    # Construct a contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check for categories with less than 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Check if columns are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not pd.api.types.is_object_dtype(data[col1]):\n        raise TypeError(f\"Column '{col1}' must be categorical.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not pd.api.types.is_object_dtype(data[col2]):\n        raise TypeError(f\"Column '{col2}' must be categorical.\")\n    \n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Return the p-value\n    return p\n```"}
{"task_id": "BigCodeBench/897", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    # Step 1: Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Step 2: Simulate the dice rolls\n    simulated_rolls = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Step 3: Calculate the frequency of each outcome\n    frequency = np.array([simulated_rolls.count(i) for i in NUMBERS])\n    \n    # Step 4: Create the histogram of results\n    fig, ax = plt.subplots()\n    ax.hist(simulated_rolls, bins=np.arange(1, 8) - 0.5, edgecolor='black', align='mid')\n    \n    # Step 5: Set the title and labels of the histogram\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    \n    # Step 6: Return the frequency array and the axes object\n    return frequency, ax\n```"}
{"task_id": "BigCodeBench/906", "solution": "```python\nimport zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Step 1: Create the full path for the archive in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Step 2: Create a zip file at the archive path\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        # Step 3: Iterate over all files in the source directory\n        for filename in os.listdir(source_dir):\n            # Step 4: Check if the file ends with '_processed'\n            if filename.endswith('_processed'):\n                # Step 5: Create the full path for the file to be added\n                file_path = os.path.join(source_dir, filename)\n                # Step 6: Add the file to the zip archive\n                zipf.write(file_path, arcname=filename)\n\n    # Step 7: Return the path to the created zip archive\n    return archive_path\n```"}
{"task_id": "BigCodeBench/914", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure the dataframe has the right structure\n    if 'Date' not in df.columns or 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Close' columns\")\n\n    # Prepare the data for linear regression\n    df['Date'] = pd.to_datetime(df['Date'])  # Convert 'Date' to datetime\n    df['Days'] = (df['Date'] - df['Date'].min()).dt.days  # Create a 'Days' column for regression\n    \n    X = df[['Days']]  # Feature: number of days\n    y = df['Close']   # Target variable: closing prices\n\n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps for the next 7 days\n    last_day = df['Days'].max()\n    future_days = np.array([last_day + i + 1 for i in range(7)]).reshape(-1, 1)\n\n    # Predict the stock closing prices for the next 7 days\n    predicted_prices = model.predict(future_days).tolist()\n\n    # Create a plot\n    plt.figure(figsize=(10, 5))\n    plt.plot(df['Date'], df['Close'], label='Historical Prices', color='blue')\n    future_dates = [df['Date'].min() + pd.Timedelta(days=int(day)) for day in future_days.flatten()]\n    plt.plot(future_dates, predicted_prices, label='Predicted Prices', color='orange', linestyle='dashed')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Price Prediction')\n    plt.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Get the current Axes object\n    ax = plt.gca()\n\n    # Return a tuple with the predicted prices and the Axes object\n    return predicted_prices, ax\n```"}
{"task_id": "BigCodeBench/915", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\n\ndef task_func(df, z_threshold=2):\n    # Step 1: Calculate the Z-Scores of the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Step 2: Identify outliers based on the Z-Score threshold\n    outlier_condition = np.abs(df['z_score']) > z_threshold\n    \n    # Step 3: Create a new DataFrame containing only the outliers\n    outliers_df = df[outlier_condition][['closing_price']]\n    \n    # Step 4: Plot the outliers\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df.index, df['closing_price'], label='Closing Prices', color='blue')\n    plt.scatter(outliers_df.index, outliers_df['closing_price'], label='Outliers', color='red')\n    plt.xlabel('Index')\n    plt.ylabel('Closing Price')\n    plt.title('Outliers in Closing Prices')\n    plt.legend()\n    \n    # Step 5: Show the plot\n    plt.show()\n    \n    # Step 6: Return the DataFrame with outliers and the plot object\n    return outliers_df, plt.gca()\n\n# Example usage:\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\n```"}
{"task_id": "BigCodeBench/916", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Create a figure with two subplots (one for box plot, one for histogram)\n    fig, (ax_box, ax_hist) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    # Create a box plot on the first axes\n    sns.boxplot(ax=ax_box, data=df, y='Closing Prices')  # Assuming the column name for closing prices is 'Closing Prices'\n    ax_box.set_title('Box Plot of Closing Prices')  # Set title for box plot\n\n    # Create a histogram on the second axes\n    sns.histplot(ax=ax_hist, data=df, x='Closing Prices', bins=30, kde=True)  # Adding a density estimate\n    ax_hist.set_title('Histogram of Closing Prices')  # Set title for histogram\n\n    # Return the axes objects as a tuple\n    return (ax_box, ax_hist)\n```"}
{"task_id": "BigCodeBench/917", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Step 1: Prepare the data for ARIMA model\n    df = df.asfreq('D')  # Ensure daily frequency\n    df = df['Close'].dropna()  # Drop any NaN values\n\n    # Step 2: Fit the ARIMA model\n    model = ARIMA(df, order=(5, 1, 0))  # Example orders (p, d, q)\n    fitted_model = model.fit()\n\n    # Step 3: Generate the forecast for the next 7 days\n    forecast = fitted_model.forecast(steps=7)\n    forecasted_prices = forecast.tolist()  # Convert forecast to a list\n\n    # Step 4: Create a plot to visualize the forecast\n    plt.figure(figsize=(10, 5))\n    plt.plot(df.index, df, label='Historical Prices', color='blue')\n    \n    # Create a date range for the forecast\n    forecast_index = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7, freq='D')\n    plt.plot(forecast_index, forecast, label='Forecasted Prices', color='orange', marker='o')\n\n    # Step 5: Customize the plot\n    plt.title('Share Closing Prices Forecast')\n    plt.xlabel('Date')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.grid()\n\n    # Get the current Axes object\n    ax = plt.gca()\n\n    # Step 6: Return the forecasted prices and the Axes object\n    return forecasted_prices, ax\n```"}
{"task_id": "BigCodeBench/928", "solution": "```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations\n    combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize a dictionary with combinations as keys and 0 as initial counts\n    combination_counts = {combo: 0 for combo in combinations}\n    \n    # Count occurrences of each two-letter combination in the word\n    word_counts = Counter(word[i:i+2] for i in range(len(word) - 1))\n    \n    # Update the dictionary with counts from the word\n    for combo in combination_counts.keys():\n        combination_counts[combo] = word_counts[combo]\n    \n    # Return the final dictionary\n    return combination_counts\n```"}
{"task_id": "BigCodeBench/942", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Step 1: Generate a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Step 2: Create a DataFrame to hold the sales data\n    sales_data = {\n        'Date': [],\n        'Category': [],\n        'Sales': []\n    }\n\n    # Step 3: Populate the DataFrame with random sales data for each category\n    for date in date_range:\n        for category in categories:\n            sales_data['Date'].append(date)\n            sales_data['Category'].append(category)\n            sales_data['Sales'].append(np.random.randint(100, 1000))  # Random sales between 100 and 1000\n\n    # Step 4: Convert the dictionary to a DataFrame\n    df = pd.DataFrame(sales_data)\n\n    # Step 5: Create a pivot table to summarize sales by date and category\n    pivot_df = df.pivot_table(values='Sales', index='Date', columns='Category', aggfunc='sum')\n\n    # Step 6: Plotting the sales data\n    ax = pivot_df.plot(kind='bar', figsize=(12, 6))\n    ax.set_title('Sales Report by Category Over Time')\n    ax.set_ylabel('Sales')\n    ax.set_xlabel('Date')\n    plt.xticks(rotation=45)\n    plt.tight_layout()  # Adjust layout to make room for labels\n\n    # Step 7: Show the plot\n    plt.show()\n\n    # Step 8: Return the DataFrame and the Axes object\n    return df, ax\n\n# Example of calling the function\nsales_report_df, plot_ax = task_func()\n```"}
{"task_id": "BigCodeBench/945", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Step 1: Generate sales data if not provided\n    if sales_data is None:\n        date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n        np.random.seed(0)  # For reproducibility\n        sales_data = np.random.randint(100, 200, size=periods)\n    \n    # Step 2: Prepare the data for regression\n    X = np.arange(periods).reshape(-1, 1)  # Independent variable (time as integer index)\n    y = sales_data  # Dependent variable (sales data)\n\n    # Step 3: Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Step 4: Forecast future sales\n    future_X = np.arange(periods, periods + periods).reshape(-1, 1)\n    forecasted_sales = model.predict(future_X)\n\n    return forecasted_sales\n```"}
{"task_id": "BigCodeBench/952", "solution": "```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Raise ValueError if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n    \n    # Set random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Sanitize the task names\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n    \n    # Randomly select n_tasks from sanitized_tasks\n    selected_tasks = random.sample(sanitized_tasks, min(n_tasks, len(sanitized_tasks)))\n    \n    # Assign tasks to random employees\n    assignments = []\n    current_date = datetime.now().date()\n    \n    for task in selected_tasks:\n        assigned_employee = random.choice(employees)\n        assignments.append({\"Task Name\": task, \"Assigned To\": assigned_employee, \"Due Date\": current_date})\n    \n    # Create DataFrame\n    df = pd.DataFrame(assignments)\n    \n    # Return the DataFrame\n    return df\n```"}
{"task_id": "BigCodeBench/955", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    # Check if the text parameter is empty\n    if not text:\n        raise ValueError(\"Input text cannot be empty.\")\n    \n    # Replace spaces in the mystrings parameter with underscores\n    modified_strings = mystrings.replace(\" \", \"_\")\n    \n    # Convert all characters to lowercase for case-insensitivity\n    modified_strings = modified_strings.lower()\n    \n    # Split the modified strings into words\n    words = modified_strings.split('_')\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Prepare for plotting\n    unique_words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n    \n    # Create the frequency plot\n    plt.figure(figsize=(10, 6))\n    plt.bar(unique_words, frequencies, color='blue')\n    plt.xlabel('Unique Words')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Unique Words')\n    plt.xticks(rotation=45)\n    \n    # Return the Axes object of the plot\n    ax = plt.gca()\n    plt.tight_layout()\n    return ax\n```"}
{"task_id": "BigCodeBench/963", "solution": "```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Step 1: Check if the source_directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Step 2: Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Step 3: Create a pattern for glob to find files with valid extensions\n    files_to_zip = []\n    for ext in valid_extensions:\n        pattern = os.path.join(source_directory, '*' + ext)\n        files_to_zip.extend(glob.glob(pattern))\n    \n    # Step 4: Create the zip file in the target_directory\n    zip_file_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        # Step 5: Add each file to the zip file\n        for file in files_to_zip:\n            zip_file.write(file, os.path.basename(file))\n    \n    # Step 6: Return the full path to the created zip file\n    return zip_file_path\n```"}
{"task_id": "BigCodeBench/964", "solution": "```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # List of supported file extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Counter for successfully converted files\n    converted_count = 0\n    \n    # Iterate through each file in the source directory\n    for file_name in os.listdir(source_directory):\n        file_path = Path(source_directory) / file_name\n        file_extension = file_path.suffix.lower()\n        \n        # Check if the file extension is supported\n        if file_extension in supported_extensions:\n            content = \"\"\n            \n            # Read the content based on the file type\n            if file_extension == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            elif file_extension == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([para.text for para in doc.paragraphs])\n            elif file_extension == '.xlsx':\n                df = pd.read_excel(file_path)\n                content = '\\n'.join(df.astype(str).agg(' '.join, axis=1).tolist())\n            elif file_extension == '.csv':\n                df = pd.read_csv(file_path)\n                content = '\\n'.join(df.astype(str).agg(' '.join, axis=1).tolist())\n            \n            # Prepare the DataFrame to save as CSV\n            df_to_save = pd.DataFrame({'Text': [content]})\n            output_file_path = Path(target_directory) / (file_path.stem + '.csv')\n            \n            # Save the DataFrame as a CSV file, overwriting if it exists\n            df_to_save.to_csv(output_file_path, index=False)\n            \n            # Increment the converted count\n            converted_count += 1\n            \n    # Return the total count of successfully converted files\n    return converted_count\n```"}
{"task_id": "BigCodeBench/969", "solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Check for NaN values in the DataFrame\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n\n    # Check for non-numeric data types\n    if not np.all(np.issubdtype(df.dtypes, np.number)):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    # Compute the cumulative sum for each numeric column\n    cumulative_sum = df.cumsum()\n\n    # Normalize the cumulative sums using MinMax normalization\n    scaler = MinMaxScaler()\n    normalized_cumsum = scaler.fit_transform(cumulative_sum)\n\n    # Create a new DataFrame with normalized cumulative sums\n    normalized_df = pd.DataFrame(normalized_cumsum, columns=df.columns)\n\n    # Return the new DataFrame\n    return normalized_df\n```"}
{"task_id": "BigCodeBench/971", "solution": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Check if the provided directory exists\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # Initialize an empty list to store the results\n    results = []\n\n    # Use Path.iterdir() to get a list of files in the specified directory\n    for file in Path(directory_path).iterdir():\n        if file.is_file():  # Ensure we only consider files, not directories\n            # Get file name\n            file_name = file.name\n            \n            # Determine file size in bytes\n            file_size = file.stat().st_size\n            \n            # Retrieve creation time and last modification time\n            creation_time = file.stat().st_ctime  # Creation time\n            modification_time = file.stat().st_mtime  # Last modification time\n            \n            # Convert times to ISO format\n            creation_time_iso = datetime.fromtimestamp(creation_time, timezone.utc).isoformat()\n            modification_time_iso = datetime.fromtimestamp(modification_time, timezone.utc).isoformat()\n            \n            # Create a tuple for each file\n            file_info = (file_name, file_size, creation_time_iso, modification_time_iso)\n            \n            # Append the tuple to the results list\n            results.append(file_info)\n\n    # Return the results list from the function\n    return results\n```"}
{"task_id": "BigCodeBench/985", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Step 1: Validate the provided JSON data\n    if not json_data or not isinstance(json_data, dict):\n        raise ValueError(\"JSON data is malformed or empty.\")\n    \n    # Step 2: Prepare lists to hold country names and populations\n    countries = []\n    populations = []\n\n    # Step 3: Loop through the JSON data to validate and extract information\n    for country, population in json_data.items():\n        # Check if country name is a string\n        if not isinstance(country, str):\n            raise ValueError(f\"Country name '{country}' is not a string.\")\n        \n        # Check if population is numeric and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(f\"Population for '{country}' is non-numeric or negative.\")\n        \n        # Round down population if it is a float\n        populations.append(math.floor(population) if isinstance(population, float) else population)\n        countries.append(country)\n\n    # Step 4: Create a DataFrame with the validated data\n    df = pd.DataFrame({\"Country\": countries, \"Population\": populations})\n\n    # Step 5: Save the DataFrame to a CSV file\n    output_file_path = os.path.join(output_dir, file_name)\n    \n    try:\n        df.to_csv(output_file_path, index=False)\n    except Exception as e:\n        raise IOError(f\"Could not write to the specified directory: {e}\")\n\n    # Step 6: Return the file path and the DataFrame\n    return output_file_path, df\n```"}
{"task_id": "BigCodeBench/988", "solution": "```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if predicates list is empty or not provided\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Check if dir_path exists and is a directory\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n    \n    # Initialize a dictionary to store results\n    results = {}\n    \n    # List all items in the directory\n    items = os.listdir(dir_path)\n    \n    # Evaluate each item against the predicates\n    for item in items:\n        # Initialize a list to store evaluation results for the current item\n        evaluation_results = []\n        for predicate in unique_predicates:\n            # Check if the item name matches the predicate\n            evaluation_results.append(bool(re.search(predicate, item)))\n        \n        # Store the item and its evaluation results in the dictionary\n        results[item] = evaluation_results\n    \n    # Return the results dictionary\n    return results\n```"}
{"task_id": "BigCodeBench/990", "solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Step 1: Convert the hexadecimal string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n    \n    # Step 2: Decode bytes into a UTF-8 string\n    utf8_string = byte_data.decode('utf-8')\n    \n    # Step 3: Create a dictionary to store the encoded outputs\n    encoded_dict = {\n        'hex': hex_string,\n        'base64': base64.b64encode(byte_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n    }\n    \n    # Step 4: Attempt to encode to ASCII\n    try:\n        encoded_dict['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n    except UnicodeEncodeError:\n        encoded_dict['ASCII'] = 'Not representable in ASCII'\n    \n    # Step 5: URL encode the UTF-8 string\n    encoded_dict['URL'] = urllib.parse.quote(utf8_string)\n    \n    # Step 6: Apply ROT13 encoding\n    encoded_dict['ROT13'] = codecs.encode(utf8_string, 'rot_13')\n    \n    # Step 7: Return the dictionary with all the encoded values\n    return encoded_dict\n```"}
{"task_id": "BigCodeBench/998", "solution": "```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Step 1: Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n    \n    # Step 2: Calculate the MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    \n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        # Read the file in chunks to avoid memory overload\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            md5_hash.update(byte_block)\n    \n    # Step 3: Compare the calculated MD5 checksum with the expected checksum\n    if md5_hash.hexdigest() == EXPECTED_MD5_CHECKSUM:\n        # Step 4: If they match, extract the contents of the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()  # Extract all contents to the current directory\n        return True\n    else:\n        # Step 5: If they do not match, delete the downloaded file\n        os.remove(TARGET_TAR_FILE)\n        return False\n```"}
{"task_id": "BigCodeBench/999", "solution": "```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    # Step 1: Download the CSV file from the provided URL\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise IOError(f\"Failed to download the file from {url}. Error: {e}\")\n\n    # Step 2: Open and read the downloaded CSV file\n    try:\n        with open(csv_file_path, mode='r', newline='') as csv_file:\n            # Step 3: Use the CSV library to read the contents\n            reader = csv.DictReader(csv_file)\n            # Check if the specified column exists in the CSV\n            if column_name not in reader.fieldnames:\n                os.remove(csv_file_path)  # Delete the downloaded file\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Step 4: Count occurrences of each value in the specified column\n            value_counts = collections.Counter()\n            for row in reader:\n                value = row[column_name]\n                value_counts[value] += 1\n\n    except Exception as e:\n        raise Exception(f\"An error occurred while processing the CSV file. Error: {e}\")\n    finally:\n        # Step 6: Delete the downloaded CSV file\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n\n    # Step 7: Return the dictionary of counts\n    return dict(value_counts)\n```"}
{"task_id": "BigCodeBench/1003", "solution": "```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    # Step 1: Try to fetch the XML file from the specified URL\n    try:\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n    except Exception:\n        raise ValueError(\"Invalid URL or XML file cannot be fetched.\")\n\n    # Step 2: Try to parse the fetched XML\n    try:\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"XML file has invalid syntax.\")\n\n    # Step 3: Validate the structure of the parsed XML\n    if root.tag != \"items\":\n        raise ValueError(\"XML structure does not match expected format.\")\n    \n    items = root.findall(\"item\")\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    # Step 4: Extract data from each 'item' element\n    data = []\n    for item in items:\n        row = {}\n        for child in item:\n            row[child.tag] = child.text\n        data.append(row)\n    \n    # Step 5: Construct a DataFrame\n    df = pd.DataFrame(data)\n    \n    return df\n```"}
{"task_id": "BigCodeBench/1004", "solution": "```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Step 1: Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n    \n    # Step 2: Process the text to extract words\n    words = re.findall(r'\\b\\w+\\b', text)  # Identify words using a regular expression\n    \n    # Step 3: Count the frequency of each word\n    word_count = Counter(words)  # Count occurrences of each word\n    \n    # Step 4: Find the ten most common words\n    most_common_words = word_count.most_common(10)  # Get the ten most common words\n    \n    # Step 5: Prepare data for plotting\n    words, frequencies = zip(*most_common_words)  # Unzip the words and their frequencies\n    \n    # Step 6: Create the bar chart\n    fig, ax = plt.subplots()  # Create a figure and an axes\n    ax.bar(words, frequencies)  # Create a bar chart\n    ax.set_title('Top 10 Most Common Words')  # Set the title of the chart\n    ax.set_xlabel('Words')  # Set the x-axis label\n    ax.set_ylabel('Frequency')  # Set the y-axis label\n    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n    \n    # Step 7: Return the Counter object and the Axes object\n    return word_count, ax  # Return the word count and the axes of the plot\n```"}
{"task_id": "BigCodeBench/1006", "solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    # Step 1: Create the download path directory if it doesn't exist\n    if not os.path.exists(download_path):\n        os.makedirs(download_path)\n    \n    # Step 2: Define the path for the downloaded file\n    zip_file_path = os.path.join(download_path, 'downloaded_file.zip')\n    \n    # Step 3: Attempt to download the file\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    \n    # Step 4: Check the content type of the response\n    if response.headers.get('Content-Type') != 'application/zip':\n        return \"Error: The URL does not point to a ZIP file.\"\n    \n    # Step 5: Save the downloaded content to a file\n    with open(zip_file_path, 'wb') as zip_file:\n        zip_file.write(response.content)\n\n    # Step 6: Attempt to extract the contents of the ZIP file\n    try:\n        with ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n    except BadZipFile:\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    \n    # Step 7: Return the path to the directory containing the extracted contents\n    return download_path\n```"}
{"task_id": "BigCodeBench/1008", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    # Step 1: Make an HTTP GET request to the specified URL\n    response = requests.get(url)\n    \n    # Step 2: Raise HTTPError for unsuccessful responses\n    response.raise_for_status()\n    \n    # Step 3: Parse the HTML content with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Step 4: Locate the table by the specified table_id\n    table = soup.find('table', id=table_id)\n    \n    # Step 5: Raise ValueError if the table is not found\n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n    \n    # Step 6: Check for <tr> tags within the table\n    rows = table.find_all('tr')\n    \n    # Step 7: If no rows are present, return an empty DataFrame\n    if not rows:\n        return pd.DataFrame()\n    \n    # Step 8: Extract headers and data\n    headers = [header.text.strip() for header in rows[0].find_all('th')]\n    data = []\n    \n    # Step 9: Extract data from each row\n    for row in rows[1:]:\n        cols = row.find_all('td')\n        data.append([col.text.strip() for col in cols])\n    \n    # Step 10: Create a DataFrame from the extracted data\n    df = pd.DataFrame(data, columns=headers)\n    \n    # Step 11: Return the DataFrame\n    return df\n```"}
{"task_id": "BigCodeBench/1012", "solution": "```python\nimport requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Create download and unzip directories if they do not exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    zip_file_path = DOWNLOAD_DIR / filename\n    status_message = \"Success\"\n    extracted_files = []\n\n    try:\n        # Attempt to download the zip file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)\n        \n        # Save the downloaded zip file\n        with open(zip_file_path, 'wb') as zip_file:\n            zip_file.write(response.content)\n    except requests.exceptions.RequestException:\n        status_message = \"Error: Network-related exception occurred.\"\n        return (status_message, extracted_files)\n\n    try:\n        # Attempt to extract the downloaded zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            extracted_files = zip_ref.namelist()  # Get list of extracted file names\n    except (zipfile.BadZipFile, FileNotFoundError, OSError):\n        status_message = \"Error: File-related exception occurred.\"\n        extracted_files = []  # Ensure the list is empty if extraction fails\n\n    return (status_message, extracted_files)\n```"}
{"task_id": "BigCodeBench/1013", "solution": "```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Step 1: Make an HTTP GET request to the provided URL\n    response = requests.get(url)\n    \n    # Step 2: Check if the request was successful (status code 200)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to retrieve the webpage: {response.status_code}\")\n    \n    # Step 3: Parse the webpage content using BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Step 4: Find all <a> tags (hyperlinks) in the parsed HTML\n    links = soup.find_all('a')\n    \n    # Step 5: Extract href attributes and convert to absolute URLs\n    unique_urls = set()\n    for link in links:\n        href = link.get('href')\n        if href:  # Check if href is not None\n            absolute_url = urljoin(base_url, href)  # Convert to absolute URL\n            unique_urls.add(absolute_url)  # Store in a set to avoid duplicates\n    \n    # Step 6: Write the unique absolute URLs to the specified CSV file\n    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        for url in unique_urls:\n            writer.writerow([url])  # Write each URL in a new row\n    \n    # Step 7: Return the count of unique absolute links scraped\n    return len(unique_urls)\n```"}
{"task_id": "BigCodeBench/1015", "solution": "```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the specified URL\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.RequestException as e:\n        raise e  # Raise the exception if there is a network issue\n    \n    # Parse the HTML content\n    tree = html.fromstring(response.content)\n    tables = tree.xpath('//table')  # Find all tables in the HTML\n\n    # Check if any table is found\n    if not tables:\n        return 0  # Return 0 if no tables found\n\n    # Convert the first table into a DataFrame\n    df = pd.read_html(html.tostring(tables[0]))[0]\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        return 0  # Return 0 if the DataFrame is empty\n\n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        # Replace the existing data in the table \"my_table\"\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        conn.commit()  # Commit the changes\n        return len(df)  # Return the number of rows stored\n    except sqlite3.DatabaseError as e:\n        raise e  # Raise the exception if there is a database issue\n    finally:\n        # Close the database connection\n        conn.close()\n```"}
{"task_id": "BigCodeBench/1019", "solution": "```python\nfrom PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    # Step 1: Open the image file\n    try:\n        with Image.open(filename) as img:\n            # Step 2: Extract text from the image using OCR\n            extracted_text = pytesseract.image_to_string(img)\n    except Exception:\n        extracted_text = \"\"\n\n    # Step 3: Attempt to convert the extracted text from `from_encoding` to `to_encoding`\n    if extracted_text.strip():\n        try:\n            # Decode the extracted text from `from_encoding` and convert to `to_encoding`\n            decoded_text = extracted_text.encode(from_encoding).decode(to_encoding)\n            return decoded_text\n        except (ValueError, UnicodeDecodeError, LookupError) as e:\n            raise ValueError(\"Encoding error during text conversion: \" + str(e))\n\n    # Step 4: If OCR fails, check for image comments\n    try:\n        comment = img.info.get('comment', '')\n        if comment:\n            # Convert the comment text from `from_encoding` to `to_encoding`\n            decoded_comment = comment.encode(from_encoding).decode(to_encoding)\n            return decoded_comment\n    except (ValueError, UnicodeDecodeError, LookupError) as e:\n        raise ValueError(\"Encoding error during comment conversion: \" + str(e))\n\n    # Step 5: If both OCR and comment extraction fail, return an empty string\n    return \"\"\n```"}
{"task_id": "BigCodeBench/1020", "solution": "```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    # Set a timeout for the HTTP GET request\n    timeout = 5\n\n    # Make the GET request and retrieve the response content in raw bytes\n    response = requests.get(url, timeout=timeout)\n    raw_content = response.content\n\n    # Check if the response content is empty\n    if not raw_content:\n        return {}\n\n    # Detect the encoding of the response content\n    detected_encoding = chardet.detect(raw_content)['encoding']\n\n    # Raise an exception if the encoding is undetectable\n    if detected_encoding is None:\n        raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n    # Decode the response content from the detected encoding and re-encode it to the specified to_encoding\n    decoded_content = raw_content.decode(detected_encoding)\n    re_encoded_content = decoded_content.encode(to_encoding)\n\n    # Parse the re-encoded data as JSON\n    json_data = json.loads(re_encoded_content)\n\n    # Return the parsed JSON data as a dictionary\n    return json_data\n```"}
{"task_id": "BigCodeBench/1022", "solution": "```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the specified CSV file exists at the given path\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The specified CSV file was not found: {csv_file_path}\")\n    \n    # Attempt to read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()  # Return an empty DataFrame if the file is empty\n\n    # Validate the presence of the specified date column\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not present in the CSV file.\")\n\n    # Convert the values in the specified date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter the DataFrame to include only rows where the date is less than or equal to the current date\n    current_date = datetime.now()\n    df_filtered = df[df[column_name] <= current_date]\n\n    # Sort the resulting DataFrame by the specified date column in ascending order\n    df_sorted = df_filtered.sort_values(by=column_name)\n\n    # Return the processed DataFrame after filtering and sorting\n    return df_sorted\n```"}
{"task_id": "BigCodeBench/1039", "solution": "```python\nimport ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    # Wrap the client socket with SSL/TLS\n    secure_socket = ssl.wrap_socket(client_socket, certfile=cert_file, keyfile=key_file)\n    \n    try:\n        # Wait for the client to send the file path\n        file_path = secure_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the requested file exists\n        if os.path.exists(file_path):\n            # Open the file in binary mode and read its content\n            with open(file_path, 'rb') as file:\n                file_content = file.read()\n                # Calculate the SHA256 hash of the file content\n                file_hash = hashlib.sha256(file_content).hexdigest()\n            return file_hash\n        else:\n            return 'File not found'\n    except Exception as e:\n        # Return an error message if an exception occurs\n        return f'Error: {str(e)}'\n```"}
{"task_id": "BigCodeBench/1040", "solution": "```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Create a TCP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen()  # Listen for incoming connections\n    server_socket.setblocking(False)  # Set the socket to non-blocking mode\n\n    # Initialize data structures for managing sockets\n    inputs = [server_socket]  # List of sockets to monitor for incoming connections\n    outputs = []  # List of sockets to send data to\n    message_queues = {}  # Dictionary to hold message queues for each client\n\n    # Record the start time of the server\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    # Main server loop\n    while datetime.now() < end_time:\n        # Use select to wait for activity on the sockets\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server_socket:\n                # Accept a new connection\n                client_socket, client_address = server_socket.accept()\n                print(f\"Connection from {client_address}\")\n                client_socket.setblocking(False)  # Set the client socket to non-blocking\n                inputs.append(client_socket)  # Add to the list of inputs\n                message_queues[client_socket] = queue.Queue()  # Initialize message queue for this client\n            else:\n                # Handle incoming messages from clients\n                try:\n                    data = s.recv(buffer_size)\n                    if data:\n                        # Append current server time to the message\n                        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                        response = f\"{data.decode()} - Received at {current_time}\"\n                        message_queues[s].put(response)  # Queue the response for sending\n                        if s not in outputs:\n                            outputs.append(s)  # Add to writable list if it has a message to send\n                    else:\n                        # No data means the client has closed the connection\n                        print(f\"Closing connection to {s.getpeername()}\")\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n                except Exception as e:\n                    print(f\"Error handling data from {s.getpeername()}: {e}\")\n                    # Clean up the socket on error\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                # Send messages back to clients\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                # No messages to send; remove from writable list\n                outputs.remove(s)\n            else:\n                # Send the message\n                s.send(next_msg.encode())\n\n        for s in exceptional:\n            # Handle exceptional conditions\n            print(f\"Exceptional condition on {s.getpeername()}\")\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Output status message\n    run_duration_message = f\"Server ran for {run_duration} seconds.\"\n    print(run_duration_message)\n\n# To run the server, uncomment the next line:\n# task_func()\n```"}
{"task_id": "BigCodeBench/1042", "solution": "```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Step 1: Receive a message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Step 2: Prompt the user for the sender's email address\n    sender_email = input(\"Enter your email address: \")\n    \n    # Step 3: Prompt the user for the recipient's email address\n    recipient_email = input(\"Enter the recipient's email address: \")\n    \n    # Step 4: Prompt the user for the sender's email password securely\n    sender_password = getpass.getpass(\"Enter your email password: \")\n\n    # Step 5: Set up an SMTP connection using the specified server and port\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n        # Step 6: Upgrade the connection to a secure encrypted SSL/TLS connection\n        smtp.starttls()\n        \n        # Step 7: Log in to the SMTP server using the provided credentials\n        smtp.login(sender_email, sender_password)\n        \n        # Step 8: Create an email message\n        email_message = EmailMessage()\n        email_message.set_content(message)  # Set the received message as the email content\n        email_message['Subject'] = 'Message from Client'\n        email_message['From'] = sender_email\n        email_message['To'] = recipient_email\n        \n        # Step 9: Send the email\n        smtp.send_message(email_message)\n\n    # Step 10: No return value; function outputs None\n    return None\n```"}
{"task_id": "BigCodeBench/1053", "solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Load the CSV file\n        df = pd.read_csv(file_path, header=0)  # Assumes the first row is the header\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred while reading the CSV file: {e}\")\n        return None\n    \n    # Check if 'Text' column exists, if not assume first column\n    if 'Text' in df.columns:\n        text_data = df['Text']\n    else:\n        text_data = df.iloc[:, 0]  # Take the first column if no header\n    \n    # Join all text data into a single string\n    all_text = ' '.join(text_data)\n    \n    # Vectorize the text data, excluding stop words\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    word_count = vectorizer.fit_transform([all_text])\n    \n    # Sum the counts of each word\n    word_counts = word_count.toarray().sum(axis=0)\n    words = vectorizer.get_feature_names_out()\n    \n    # Create a DataFrame for word counts\n    word_count_df = pd.DataFrame({'Word': words, 'Count': word_counts})\n    \n    # Get the ten most common words\n    top_ten_words = word_count_df.nlargest(10, 'Count')\n    \n    # Generate the histogram\n    plt.figure(figsize=(10, 6))\n    plt.bar(top_ten_words['Word'], top_ten_words['Count'], color='blue')\n    plt.title('Top 10 Most Common Words')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=45)\n    \n    # Save or show the plot\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()  # Close the plot to free memory\n        return None\n    else:\n        plt.tight_layout()\n        plt.show()\n        return plt.gca()  # Return the Axes object\n```"}
{"task_id": "BigCodeBench/1057", "solution": "```python\nimport pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Handle special cases for empty lists\n    if animals is None and foods is None:\n        return pd.DataFrame()  # Return empty DataFrame if both are None\n    if animals is None:\n        animals = ['dog', 'cat', 'bird']  # Default list for animals\n    if foods is None:\n        foods = ['meat', 'fish', 'seeds']  # Default list for foods\n    \n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the resulting pairs randomly\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame with 'animal:food' format\n    data = [[f\"{animal}:{food}\" for food in foods] for animal in animals]\n    df = pd.DataFrame(data, index=animals, columns=foods)\n    \n    # Populate the DataFrame with the shuffled combinations\n    for idx, (animal, food) in enumerate(combinations):\n        row_idx = idx // len(foods)\n        col_idx = idx % len(foods)\n        df.iat[row_idx, col_idx] = f\"{animal}:{food}\"\n    \n    return df\n```"}
{"task_id": "BigCodeBench/1077", "solution": "```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Check if the length of time_strings is less than 2\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Create a timezone object from the provided timezone string\n    tz = pytz.timezone(timezone)\n    \n    # Convert each timestamp string to a datetime object in the specified timezone\n    converted_times = []\n    for time_string in time_strings:\n        # Parse the time string into a naive datetime object\n        naive_time = datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n        # Localize the naive datetime object to the specified timezone\n        localized_time = tz.localize(naive_time)\n        converted_times.append(localized_time)\n    \n    # Calculate the absolute time differences in seconds between consecutive timestamps\n    time_differences = []\n    for i in range(1, len(converted_times)):\n        # Calculate the absolute difference in seconds\n        time_diff = abs((converted_times[i] - converted_times[i - 1]).total_seconds())\n        time_differences.append(time_diff)\n    \n    # If there are no time differences, return 0.0\n    if len(time_differences) == 0:\n        return 0.0\n    \n    # Calculate the average of the time differences using numpy\n    average_difference = np.mean(time_differences)\n    \n    # Return the average time difference in seconds as a float\n    return average_difference\n```"}
{"task_id": "BigCodeBench/1085", "solution": "```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Step 1: Convert the entire text to lowercase\n    text = text.lower()\n    \n    # Step 2: Remove all punctuation from the text\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Step 3: Split the cleaned text into individual words\n    words = text.split()\n    \n    # Step 4: Count the frequency of each word using Counter\n    word_counts = Counter(words)\n    \n    # Step 5: Identify the top 10 most common words and their counts\n    top_10_words = word_counts.most_common(10)\n    \n    # Step 6: Plot a bar chart of the top 10 most common words\n    words, counts = zip(*top_10_words)  # Unzip the tuples into two lists\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    \n    # Step 7: Return the list of tuples and the Axes object\n    return top_10_words, ax\n```"}
{"task_id": "BigCodeBench/1124", "solution": "```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Step 1: Use a regular expression to search for a valid URL in the provided string.\n    url_pattern = r'(https?://[^\\s]+)'\n    match = re.search(url_pattern, myString)\n\n    # Step 2: Check if a valid URL was found.\n    if not match:\n        return \"No valid URL found in the provided string.\"\n    \n    # Step 3: Extract the URL from the match object.\n    url = match.group(0)\n    \n    # Step 4: Attempt to fetch the content of the webpage using the requests library.\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses (4xx and 5xx)\n    except requests.exceptions.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Step 5: Parse the HTML content using BeautifulSoup.\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Step 6: Search for the <title> tag in the parsed HTML.\n    title_tag = soup.find('title')\n    \n    # Step 7: Check if the <title> tag was found.\n    if title_tag is None:\n        return \"No title tag found in the webpage.\"\n\n    # Step 8: Return the text content of the <title> tag.\n    return title_tag.get_text()\n```"}
{"task_id": "BigCodeBench/1129", "solution": "```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    parsed_data = json.loads(json_data)\n    \n    # Retrieve the URL associated with the specified unknown_key\n    url = parsed_data.get(unknown_key)\n    \n    if url is None:\n        raise KeyError(f\"The key '{unknown_key}' was not found in the JSON data.\")\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an error if the download fails\n\n    # Generate a timestamp for the filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Construct the filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the directory to save the file\n    if save_dir is not None:\n        os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n        file_path = os.path.join(save_dir, filename)\n    else:\n        file_path = os.path.join(os.getcwd(), filename)  # Use current working directory\n\n    # Save the downloaded content to the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n\n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)\n```"}
{"task_id": "BigCodeBench/1137", "solution": "```python\nimport bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    # Step 1: Fetch content from the URL or read from a local file\n    if url.startswith('http://') or url.startswith('https://'):\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure we notice HTTP errors\n        content = response.text\n    else:\n        with open(url, 'r', encoding='utf-8') as file:\n            content = file.read()\n\n    # Step 2: Parse the HTML content using BeautifulSoup\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n\n    # Step 3: Use a regular expression to find phone numbers\n    phone_numbers = []\n    # Define a regex pattern for phone numbers (this can be adjusted based on expected formats)\n    pattern = r'\\+?\\d[\\d -]{8,}\\d'  # Matches international and local formats\n\n    # Extract text from the parsed HTML and search for phone numbers\n    text = soup.get_text()\n    found_numbers = re.findall(pattern, text)\n\n    # Step 4: Store found numbers in the list\n    phone_numbers.extend(found_numbers)\n\n    # Step 5: Save the list of phone numbers to the specified output path in JSON format\n    with open(output_path, 'w', encoding='utf-8') as json_file:\n        json.dump(phone_numbers, json_file, ensure_ascii=False, indent=4)\n\n    # Step 6: Return the list of found phone numbers\n    return phone_numbers\n```"}
