{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Download all files from a specific directory on an FTP server using wget in a subprocess.\n\n    Args:\n        ftp_server (str): The FTP server address. Default is 'ftp.dlptest.com'.\n        ftp_user (str): The FTP server username. Default is 'dlpuser'.\n        ftp_password (str): The FTP server password. Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n        ftp_dir (str): The directory path on the FTP server from which files need to be downloaded. Default is '/ftp/test'.\n\n    Returns:\n        List[str]: A list of filenames that were attempted to be downloaded from the FTP server.\n\n    Raises:\n        Exception: If there is a failure in connecting to the FTP server, logging into the FTP server, or changing to the specified directory.\n    \"\"\"\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.connect()\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        # Log into the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Get the list of files in the directory\n    files = ftp.nlst()\n\n    # Close the FTP connection\n    ftp.quit()\n\n    # Use wget to download all files\n    downloaded_files = []\n    for file in files:\n        try:\n            # Construct the wget command\n            wget_cmd = f\"wget -q -r -nH --cut-dirs=1 -P {os.getcwd()} ftp://{ftp_user}:{ftp_password}@{ftp_server}/{ftp_dir}/{file}\"\n            # Run the wget command in a subprocess\n            subprocess.run(wget_cmd, shell=True, check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download file {file} from FTP server {ftp_server}: {str(e)}\")\n\n    return downloaded_files"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Execute a list of shell commands read from a CSV file and save the outputs in separate files.\n    \n    Parameters:\n    commands_file_path (str): The path to the CSV file containing the shell commands.\n    output_dir_path (str): The path to the directory where the output files will be saved.\n    \n    Returns:\n    list: A list of paths to the output files created in the output directory.\n    \n    Raises:\n    FileNotFoundError: If the commands_file_path does not exist.\n    \"\"\"\n    \n    # Check if the commands_file_path exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Ensure the output directory exists\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    # Initialize an empty list to store the output file paths\n    output_file_paths = []\n    \n    # Open the CSV file and read the commands\n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        for index, command in enumerate(reader):\n            if command:  # Check if the command is not empty\n                command = command[0]  # Assuming the command is the first element in the row\n                output_file_path = os.path.join(output_dir_path, f'command_{index + 1}_output.txt')\n                output_file_paths.append(output_file_path)\n                \n                try:\n                    # Execute the command and capture the output\n                    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(result.stdout)\n                except subprocess.CalledProcessError as e:\n                    # If the command fails, write the error message and exit code to the file\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(f\"Command failed with error: {e.stderr}\\nExit code: {e.returncode}\")\n    \n    # Return the list of output file paths\n    return output_file_paths"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    Check if a particular process is running based on its name.\n    If it is not running, start it using the process name as a command.\n    If it is running, terminate the process and restart it by executing the process name as a command.\n\n    Args:\n        process_name (str): The name of the process to check.\n\n    Returns:\n        str: A message indicating the action taken.\n    \"\"\"\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If the process is running, terminate it and restart it\n            proc.terminate()\n            time.sleep(1)  # Wait for 1 second to ensure the process is terminated\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # If the process is not running, start it\n    subprocess.Popen([process_name])\n    return f\"Process not found. Starting {process_name}.\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located in the specified directory \n    and returns the path to the created zip file.\n\n    Args:\n        directory (str): The path to the directory containing files to be zipped.\n\n    Returns:\n        str: The path to the generated zip file. Returns None if the directory does not contain any files.\n\n    Raises:\n        FileNotFoundError: If the specified directory does not exist.\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The specified directory does not exist: {directory}\")\n\n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n\n    # If there are no files, return None\n    if not files:\n        return None\n\n    # Define the path for the zip file\n    zip_path = os.path.join(directory, 'files.zip')\n\n    # Create the zip file and add all files to it\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n\n    # Return the path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    This function generates a word cloud from the given text after removing URLs.\n    \n    Args:\n    text (str): The input text.\n    \n    Returns:\n    WordCloud object: The generated word cloud.\n    \n    Raises:\n    ValueError: If there are no words available to generate a word cloud after removing URLs.\n    \"\"\"\n    \n    # Remove URLs from the text\n    text_without_urls = re.sub(r'http\\S+', '', text)\n    \n    # Remove punctuation and convert to lower case\n    text_without_urls = re.sub(r'[^\\w\\s]', '', text_without_urls).lower()\n    \n    # Split the text into words\n    words = text_without_urls.split()\n    \n    # Check if there are any words available\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud\ntext = \"This is a sample text with a URL https://www.example.com. This text will be used to generate a word cloud.\""}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Separate features and target variable\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the RandomForestClassifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Get feature importances\n    feature_importances = rf_classifier.feature_importances_\n    \n    # Sort feature importances in descending order\n    sorted_feature_importances = sorted(zip(feature_importances, X.columns), reverse=True)\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=[importance for importance, _ in sorted_feature_importances], y=[feature for _, feature in sorted_feature_importances])\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n    \n    return rf_classifier, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password = generate_password_hash(password)\n\n        def __repr__(self):\n            return f\"User('{self.username}')\"\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    users = [User(1, 'user1', 'password1'), User(2, 'user2', 'password2')]\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users:\n            if user.id == int(user_id):\n                return user\n        return None\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.username == form.username.data and user.check_password(form.password.data):\n                    login_user(user)\n                    return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', username=current_user.username)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(data, column, outlier_z_score):\n    # Ensure the data is in the correct format\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Data must be a pandas DataFrame\")\n\n    # Standardize the specified column\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n    \n    # Identify outliers\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers\n    data_without_outliers = data.drop(outlier_indices)\n    \n    # Plot original data with outliers\n    plt.figure(figsize=(10, 6))\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], label='Data with Outliers', color='blue')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n    \n    # Plot data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], label='Data without Outliers', color='green')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Return original data, data without outliers, and indices of outliers\n    return data, data_without_outliers, outlier_indices\ndata = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100]})\ncolumn = 'A'\noutlier_z_score = 2"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on a dataset and generate a scatter plot visualizing the clusters and their centroids.\n\n    Args:\n        data (pd.DataFrame): The input dataset.\n        n_clusters (int, optional): The number of clusters. Defaults to 3.\n\n    Returns:\n        tuple:\n            np.ndarray: An array of cluster labels assigned to each sample.\n            plt.Axes: An Axes object with the scatter plot showing the clusters and centroids.\n\n    Raises:\n        ValueError: If 'data' is not a pd.DataFrame.\n        ValueError: If 'n_clusters' is not an integer greater than 1.\n    \"\"\"\n    # Check if data is a pd.DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pd.DataFrame\")\n\n    # Check if n_clusters is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"n_clusters must be an integer greater than 1\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(data)\n\n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels)\n\n    # Plot centroids\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='*', s=200)\n\n    # Adding legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    ax.legend(loc='upper right')\n\n    # Show plot\n    plt.show()\n\n    return cluster_labels, ax\ndata = pd.DataFrame({\n    'feature1': [1, 2, 3, 4, 5],\n    'feature2': [1, 2, 3, 4, 5]\n})"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset and record the result.\n    Also, generates a scatter plot of the transformed data.\n\n    Parameters:\n    - data (pd.DataFrame): The input dataset.\n    - n_components (int, optional): The number of principal components to retain. Defaults to 2.\n\n    Returns:\n    - pd.DataFrame: The transformed data with principal components.\n    - Axes: The matplotlib Axes object containing the scatter plot.\n\n    Raises:\n    - ValueError: If n_components is not a positive integer.\n    \"\"\"\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Create a DataFrame with the transformed data\n    transformed_df = pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n    \n    # Generate a scatter plot of the transformed data\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n        ax.set_xlabel(\"PC1\")\n        ax.set_ylabel(\"PC2\")\n    else:\n        ax.plot(transformed_df.iloc[:, 0], marker='o')\n        ax.set_xlabel(\"Sample Index\")\n        ax.set_ylabel(\"PC1\")\n    ax.set_title(\"Scatter Plot of Transformed Data\")\n    \n    # Return the transformed DataFrame and the Axes object\n    return transformed_df, ax\ndata = pd.DataFrame(np.random.rand(100, 5))"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    g = sns.pairplot(iris_df, hue='species', markers=['o', 's', 'D'])\n\n    # Set the title and labels\n    g.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n    for i, j in zip(*plt.np.triu_indices_from(g.axes, 1)):\n        g.axes[i, j].set_visible(False)\n    for i in range(len(iris.feature_names)):\n        for j in range(len(iris.feature_names)):\n            if i != j:\n                g.axes[i, j].set_xlabel(iris.feature_names[j])\n                g.axes[i, j].set_ylabel(iris.feature_names[i])\n\n    # Return the figure object\n    return g.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days.\n\n    Args:\n        seed (int, optional): Seed for reproducibility. Defaults to 42.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing a line plot of the time series data.\n\n    Raises:\n        ValueError: If there is an issue generating the data or plot.\n    \"\"\"\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.today()\n        start_date = end_date - timedelta(days=30)\n        dates = pd.date_range(start=start_date, end=end_date, freq='D')\n\n        # Generate random values for each date\n        values = [random.random() for _ in range(len(dates))]\n\n        # Create a pandas DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Set the 'Date' column as the index\n        df.set_index('Date', inplace=True)\n\n        # Create a line plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(df.index, df['Value'], marker='o')\n\n        # Set the title and labels\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n\n        # Set the font to Arial for better readability\n        plt.rcParams['font.family'] = 'Arial'\n\n        # Rotate x-axis labels for better readability\n        plt.xticks(rotation=45)\n\n        # Ensure the plot fits within the figure\n        plt.tight_layout()\n\n        # Return the Axes object\n        return ax\n\n    except Exception as e:\n        # Raise a ValueError if there is an issue generating the data or plot\n        raise ValueError(\"Error generating data or plot: {}\".format(str(e)))"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_boston\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    \"\"\"\n    Draw the correlation heatmap of the Boston Housing dataset using Seaborn, with an option to save it to a specified file.\n\n    Args:\n        data_url (str): The URL of the Boston Housing dataset. Defaults to \"http://lib.stat.cmu.edu/datasets/boston\".\n        seed (int): The seed for the random number generator. Defaults to 42.\n        save_path (str): The file path to save the plot. Defaults to None.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n\n    Raises:\n        ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    try:\n        # Load the Boston Housing dataset\n        boston = load_boston()\n        data = pd.DataFrame(boston.data, columns=boston.feature_names)\n        data['PRICE'] = boston.target\n\n        # Set the seed for the random number generator\n        np.random.seed(seed)\n\n        # Create a correlation matrix\n        corr_matrix = data.corr()\n\n        # Create a heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n\n        # Save the plot to a file if specified\n        if save_path:\n            try:\n                plt.savefig(save_path)\n            except Exception as e:\n                raise ValueError(f\"Error saving the plot: {e}\")\n\n        # Return the Axes object\n        return ax\n\n    except Exception as e:\n        # Raise a ValueError if an error occurs\n        raise ValueError(f\"Error generating or saving the plot: {e}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport numpy as np\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the time series data.\n        freq (str, optional): The frequency of the time series. Defaults to 'D' (daily).\n        decomposition_model (str, optional): The decomposition model to use. Defaults to 'multiplicative'.\n\n    Returns:\n        tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n\n    Raises:\n        ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n        ValueError: If 'freq' is not a valid frequency string.\n        ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Check if required columns are present\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Check if 'value' column contains valid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data.\")\n    \n    # Check if 'freq' is a valid frequency string\n    try:\n        pd.date_range(start='1/1/2020', periods=1, freq=freq)\n    except ValueError:\n        raise ValueError(\"'freq' must be a valid frequency string.\")\n    \n    # Check if 'decomposition_model' is 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=pd.Timedelta(freq).days)\n    \n    # Plot the decomposition\n    fig, ax = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\n    decomposition.observed.plot(ax=ax[0], legend=False)\n    ax[0].set_ylabel('Observed')\n    decomposition.trend.plot(ax=ax[1], legend=False)\n    ax[1].set_ylabel('Trend')\n    decomposition.seasonal.plot(ax=ax[2], legend=False)\n    ax[2].set_ylabel('Seasonal')\n    decomposition.resid.plot(ax=ax[3], legend=False)\n    ax[3].set_ylabel('Residual')\n    \n    plt.tight_layout()\n    \n    return (decomposition, ax)\ndf = pd.DataFrame({\n    'date': pd.date_range(start='1/1/2020', periods=365, freq='D'),\n    'value': np.random.randn(365).cumsum()\n})"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generates a pandas Series of random dates within a specified date range.\n\n    Args:\n        start_date (datetime): The start date of the range (inclusive). Defaults to datetime(2020, 1, 1).\n        end_date (datetime): The end date of the range (inclusive). Defaults to datetime(2020, 12, 31).\n        seed (int): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.Series: A Series object containing random dates within the specified range.\n\n    Raises:\n        ValueError: If 'start_date' or 'end_date' is not a datetime.datetime instance, or if 'start_date' is later than 'end_date'.\n    \"\"\"\n\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n\n    # Check if start_date is not later than end_date\n    if start_date > end_date:\n        raise ValueError(\"'start_date' cannot be later than 'end_date'.\")\n\n    # Set the seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the number of days in the range\n    num_days = (end_date - start_date).days + 1\n\n    # Generate a list of random dates\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    # Create a pandas Series from the list of random dates\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies a list by adding the element '12', then concatenates a number of CSV files \n    from a directory into a single DataFrame. The number of files concatenated is determined \n    by the sum of the numbers in the list.\n\n    Args:\n        my_list (list): A list of numbers.\n        file_dir (str, optional): The directory containing the CSV files. Defaults to './data_files/'.\n        file_ext (str, optional): The file extension of the CSV files. Defaults to '.csv'.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame concatenating data from the selected CSV files.\n\n    Raises:\n        TypeError: If 'my_list' is not a list.\n        FileNotFoundError: If no files are found in the specified directory.\n    \"\"\"\n\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The provided 'my_list' is not a list\")\n\n    # Add the element '12' to the list\n    my_list.append(12)\n\n    # Calculate the number of files to concatenate\n    num_files_to_concat = sum(my_list)\n\n    # Get the list of CSV files in the directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(file_pattern)\n\n    # Check if there are any files in the directory\n    if not files:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n\n    # Ensure we do not attempt to concatenate more files than available\n    num_files_to_concat = min(num_files_to_concat, len(files))\n\n    # Read and concatenate the selected CSV files into a DataFrame\n    dfs = [pd.read_csv(file) for file in files[:num_files_to_concat]]\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n\n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    This function enhances 'my_list' by appending the number 12, then generates a list of random integers \n    based on the sum of elements in 'my_list', limited by 'size'. It measures the time taken for this process \n    and plots a histogram of the generated random numbers.\n\n    Args:\n        my_list (list): A list of numbers.\n        size (int, optional): The maximum size of the random numbers list. Defaults to 100.\n        seed (int, optional): The seed for the random number generator. Defaults to 100.\n\n    Returns:\n        tuple: A tuple containing the time taken to generate the list (in seconds, as a float) and \n        the matplotlib Axes object for the histogram.\n\n    Raises:\n        TypeError: If 'my_list' is not a list.\n        ValueError: If 'my_list' contains elements that are not numeric (int or float).\n    \"\"\"\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n\n    # Check if all elements in my_list are numeric\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric elements\")\n\n    # Append 12 to my_list\n    my_list.append(12)\n\n    # Calculate the sum of elements in my_list\n    total_sum = sum(my_list)\n\n    # Calculate the size of the random numbers list\n    list_size = min(total_sum, size)\n\n    # Set the seed for the random number generator\n    random_seed(seed)\n\n    # Measure the time taken to generate the list\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(list_size)]\n    end_time = time.time()\n\n    # Calculate the time taken\n    time_taken = end_time - start_time\n\n    # Plot a histogram of the generated random numbers\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 102), align='left', rwidth=0.8)\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Random Numbers')\n\n    # Return the time taken and the histogram axes\n    return time_taken, ax\nmy_list = [1, 2, 3]"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from a web page and extracts data into a Pandas DataFrame.\n    \n    Parameters:\n    url (str): The URL of the web page to scrape. Defaults to 'http://example.com'.\n    \n    Returns:\n    pd.DataFrame: A DataFrame containing the scraped table data, with rows corresponding to table rows and columns named after the table headers, if available.\n    \n    Raises:\n    ConnectionError: If there is an issue connecting to the URL.\n    requests.HTTPError: If the HTTP request to the URL fails.\n    ValueError: If no table data is found on the page or if the page content cannot be parsed.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Find the first table on the page\n        table = soup.find('table')\n        \n        if table is None:\n            raise ValueError(\"No table data found on the page.\")\n        \n        # Extract the table data into a DataFrame\n        df = pd.read_html(str(table))[0]\n        \n        return df\n    \n    except requests.exceptions.ConnectionError as e:\n        raise ConnectionError(\"Error connecting to the URL: {}\".format(e))\n    except requests.exceptions.HTTPError as e:\n        raise requests.HTTPError(\"HTTP request to the URL failed: {}\".format(e))\n    except Exception as e:\n        raise ValueError(\"Error parsing the page content: {}\".format(e))\nurl = 'http://example.com'"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    This function takes a pandas DataFrame as input, draws histograms of numeric columns, \n    and returns the plots. Each histogram represents the distribution of values in one \n    numeric column, with the column name as the plot title, 'Value' as the x-axis label, \n    and 'Frequency' as the y-axis label.\n\n    Parameters:\n    df (pandas DataFrame): The input DataFrame.\n\n    Returns:\n    list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n\n    Raises:\n    ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    \n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n    \n    # Select numeric columns\n    numeric_df = df.select_dtypes(include=[np.number])\n    \n    # Check if there are any numeric columns\n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns in the DataFrame\")\n    \n    # Create a figure with subplots for each numeric column\n    fig, axes = plt.subplots(nrows=len(numeric_df.columns), ncols=1, figsize=(8, 6*len(numeric_df.columns)))\n    \n    # If there is only one numeric column, axes will be a single Axes object, not a list\n    if len(numeric_df.columns) == 1:\n        axes = [axes]\n    \n    # Draw a histogram for each numeric column\n    for i, col in enumerate(numeric_df.columns):\n        axes[i].hist(numeric_df[col], bins=10)\n        axes[i].set_title(col)\n        axes[i].set_xlabel('Value')\n        axes[i].set_ylabel('Frequency')\n    \n    # Return the list of Axes objects\n    return axes\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [5, 4, 3, 2, 1],\n    'C': ['a', 'b', 'c', 'd', 'e']\n})"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n\n    Args:\n        ip_range (str): IP address range in CIDR notation (e.g., '192.168.1.0/24')\n        port (int): Port number to check\n\n    Returns:\n        dict: A dictionary mapping IP addresses to their port status (True if open)\n    \"\"\"\n    def check_port(ip, port, result):\n        \"\"\"\n        Checks if a specified port is open on a given IP address.\n\n        Args:\n            ip (str): IP address to check\n            port (int): Port number to check\n            result (dict): Dictionary to store the result\n        \"\"\"\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(1)\n            try:\n                s.connect((ip, port))\n                result[ip] = True\n            except (socket.timeout, ConnectionRefusedError):\n                result[ip] = False\n\n    result = {}\n    threads = []\n\n    for ip in IPv4Network(ip_range):\n        ip = str(ip)\n        thread = Thread(target=check_port, args=(ip, port, result))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    return result\nip_range = '192.168.1.0/24'\nport = 80"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\nimport os\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information from a log file and stores it in a CSV file.\n\n    Args:\n        log_file (str): The path to the log file.\n\n    Returns:\n        str: The file path to the newly created CSV file.\n\n    Raises:\n        ValueError: If the timestamp in any log entry is invalid or if no valid log entries are found.\n    \"\"\"\n    # Define the regex pattern to extract log components\n    log_pattern = re.compile(r'(\\w+): \\[(.*?)\\] - (.*)')\n    \n    # Initialize lists to store extracted data\n    types = []\n    timestamps = []\n    messages = []\n    \n    # Read the log file and extract data\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                log_type, log_timestamp, log_message = match.groups()\n                try:\n                    datetime.strptime(log_timestamp, '%Y-%m-%d %H:%M:%S')\n                    types.append(log_type)\n                    timestamps.append(log_timestamp)\n                    messages.append(log_message)\n                except ValueError:\n                    raise ValueError(\"Invalid timestamp in log entry\")\n    \n    # Check if any valid log entries were found\n    if not types:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create a DataFrame from the extracted data\n    data = {\n        'Type': types,\n        'Timestamp': timestamps,\n        'Message': messages\n    }\n    df = pd.DataFrame(data)\n    \n    # Define the output CSV file path\n    csv_file = os.path.splitext(log_file)[0] + '.csv'\n    \n    # Write the DataFrame to a CSV file\n    df.to_csv(csv_file, index=False)\n    \n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a given text.\n\n    Args:\n        text (str): The input text to be analyzed.\n        rwidth (float, optional): The relative width of the bars in the histogram. Defaults to 0.8.\n\n    Returns:\n        matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \"\"\"\n    # Remove non-word characters and convert to lowercase\n    text = re.sub(r'\\W+', ' ', text).lower()\n    \n    # Split text into words\n    words = text.split()\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # If there are no words, return an empty plot\n    if not word_lengths:\n        fig, ax = plt.subplots()\n        ax.set_title('Word Length Distribution')\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        return ax\n    \n    # Create histogram subplot\n    fig, ax = plt.subplots()\n    bins = np.arange(1, max(word_lengths) + 2) - 0.5  # Center bins on integers\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, edgecolor='black')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    # Return Axes object\n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nimport pandas as pd\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") \n    from a DataFrame and analyzes the frequency of each word in the content of these articles, \n    excluding punctuation.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing 'Title' and 'Content' columns.\n\n    Returns:\n        dict: A dictionary with keys as words and values as their corresponding frequency, \n              excluding any punctuation marks.\n\n    Raises:\n        ValueError: If the DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\n    \"\"\"\n    # Check if DataFrame is empty or does not contain necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain 'Title' and 'Content' columns\")\n\n    # Ensure you have the necessary NLTK data files\n    nltk.download('punkt')\n\n    # Extract articles with titles containing 'like' or 'what'\n    filtered_df = df[df['Title'].str.contains('like|what', case=False, regex=True)]\n\n    # Concatenate content of extracted articles\n    content = filtered_df['Content'].str.cat(sep=' ')\n\n    # Remove punctuation\n    content_no_punct = re.sub('['+punctuation+']', '', content)\n\n    # Tokenize content\n    tokens = nltk.word_tokenize(content_no_punct)\n\n    # Convert tokens to lowercase\n    tokens = [token.lower() for token in tokens]\n\n    # Count frequency of each word\n    word_freq = nltk.FreqDist(tokens)\n\n    return dict(word_freq)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Function to clean text\n    def clean_text(text):\n        if isinstance(text, str):\n            # Remove numbers and punctuation\n            text = re.sub(r'[\\d\\W]', ' ', text)\n            # Convert to lowercase\n            text = text.lower()\n            # Remove stopwords\n            text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n            return text\n        return ''\n\n    # Apply the clean_text function to the specified column\n    dataframe['cleaned_text'] = dataframe[text_column].apply(clean_text)\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the cleaned text data\n    X = vectorizer.fit_transform(dataframe['cleaned_text'])\n\n    # Convert the result to a DataFrame\n    word_counts_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return word_counts_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Creates a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges.\n\n    Args:\n        dic (dict): A dictionary containing 'Lon' and 'Lat' keys with tuple values representing the range of coordinates.\n        cities (list): A list of city names.\n\n    Returns:\n        GeoDataFrame: A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects).\n\n    Raises:\n        ValueError: If 'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples.\n    \"\"\"\n\n    # Check if 'Lon' and 'Lat' keys are present in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys\")\n\n    # Check if values of 'Lon' and 'Lat' are tuples\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values of 'Lon' and 'Lat' must be tuples\")\n\n    # Generate random coordinates\n    lons = np.random.uniform(dic['Lon'][0], dic['Lon'][1], size=len(cities))\n    lats = np.random.uniform(dic['Lat'][0], dic['Lat'][1], size=len(cities))\n\n    # Create a GeoPandas DataFrame\n    data = {'City': cities, 'Coordinates': [Point(lon, lat) for lon, lat in zip(lons, lats)]}\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n\n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n\n    Parameters:\n    utc_datetime (datetime): The UTC datetime for which the weather report is generated.\n    cities (list): A list of city names. Defaults to ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'].\n    weather_conditions (list): A list of possible weather conditions. Defaults to ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'].\n    timezones (dict): A dictionary mapping city names to their respective timezones. Defaults to a dictionary with the default cities and their timezones.\n    seed (int): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing the weather report.\n\n    Raises:\n    ValueError: If utc_datetime is not a datetime object or if any of the other parameters are not in the expected format.\n    \"\"\"\n\n    # Check if utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    # Check if cities, weather_conditions, and timezones are in the expected format\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary of city: timezone pairs\")\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n\n    # Set the seed for the random number generator\n    set_seed(seed)\n\n    # Create a list to store the weather reports\n    weather_reports = []\n\n    # Iterate over each city\n    for city in cities:\n        # Get the local time for the city\n        local_time = utc_datetime.astimezone(pytz.timezone(timezones[city]))\n\n        # Get a random weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Create a dictionary to store the weather report\n        weather_report = {\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        }\n\n        # Add the weather report to the list\n        weather_reports.append(weather_report)\n\n    # Create a pandas DataFrame from the list of weather reports\n    df = pd.DataFrame(weather_reports)\n\n    # Return the DataFrame\n    return df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate and draw a random sequence of \"elements\" number of steps.\n    The steps are either -1 or 1, and the sequence is plotted as a random walk.\n    Returns the descriptive statistics of the random walk and the plot of the random walk.\n\n    Parameters:\n    elements (int): The number of steps in the random walk.\n    seed (int, optional): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n    dict: A dictionary containing the descriptive statistics of the random walk.\n    matplotlib.axes.Axes: The Axes object with the plotted random walk.\n\n    Raises:\n    ValueError: If elements is not a positive integer.\n    \"\"\"\n    # Check if elements is a positive integer\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n\n    # Set the seed for the random number generator\n    np.random.seed(seed)\n\n    # Generate the random sequence of steps\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate the cumulative sum of the steps\n    walk = np.cumsum(steps)\n\n    # Calculate the descriptive statistics\n    stats = pd.Series(walk).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    stats_dict = {\n        'count': stats['count'],\n        'mean': stats['mean'],\n        'std': stats['std'],\n        'min': stats['min'],\n        '5th percentile': stats['5%'],\n        '25th percentile': stats['25%'],\n        'median': stats['50%'],\n        '75th percentile': stats['75%'],\n        '95th percentile': stats['95%'],\n        'max': stats['max']\n    }\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(walk)\n    ax.set_title(\"Random Walk\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Position\")\n\n    # Return the descriptive statistics and the plot\n    return stats_dict, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from a given URL, extracts its contents to the specified directory, \n    and returns a list of the extracted files.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory where the zip file will be saved and its contents extracted.\n        headers (dict, optional): Optional headers for the HTTP request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_file_path = os.path.join(destination_directory, 'downloaded.zip')\n    with open(zip_file_path, 'wb') as f:\n        f.write(response.content)\n\n    # Extract the zip file\n    extracted_files = []\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = [os.path.join(destination_directory, file) for file in zip_ref.namelist()]\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Args:\n        seed (int, optional): Seed for the random number generator. Defaults to 42.\n        image_size (tuple, optional): Size of the image. Defaults to (100, 100, 3).\n        range_low (int, optional): Lower bound of the random values. Defaults to 0.\n        range_high (int, optional): Upper bound of the random values. Defaults to 255.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axes object of the plot.\n        image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Remove the axis ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Show the plot\n    plt.show()\n\n    # Return the axis and the image\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) of a specified audio file,\n    and generates a spectrogram from the matrix.\n\n    Args:\n        L (list): The list of values to create the matrix from.\n        M (int): The number of rows in the matrix.\n        N (int): The number of columns in the matrix.\n        audio_file (str): The path to the audio file.\n\n    Returns:\n        numpy.ndarray: The normalized MxN matrix.\n        matplotlib.figure.Figure: The figure object for the generated spectrogram.\n\n    Raises:\n        FileNotFoundError: If the specified audio file does not exist.\n    \"\"\"\n\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(\"The specified audio file does not exist.\")\n\n    # Read the audio file\n    data, sr = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / np.max(np.abs(matrix)) * SPL\n\n    # Generate the spectrogram from the normalized matrix\n    fig, ax = plt.subplots()\n    ax.specgram(normalized_matrix, Fs=sr, scale='log', mode='magnitude')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Spectrogram')\n\n    # Return the normalized matrix and the figure object\n    return normalized_matrix, fig\nL = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nM = 3\nN = 4\naudio_file = 'path_to_your_audio_file.wav'"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    \"\"\"\n    Extract numeric values from a list of tuples, compute basic statistics, \n    and generate a histogram with an overlaid probability density function (PDF).\n\n    Parameters:\n    original (list): A list of tuples containing numeric values.\n\n    Returns:\n    np.array: A numpy array of the extracted numeric values.\n    dict: Basic statistics for the array including mean, standard deviation, minimum, and maximum.\n    Axes: A matplotlib Axes object showing the histogram with overlaid PDF.\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([value for tup in original for value in tup if isinstance(value, (int, float))])\n\n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Generate a histogram with an overlaid probability density function (PDF)\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n    kernel = stats.gaussian_kde(numeric_values)\n    x = np.linspace(numeric_values.min(), numeric_values.max(), 100)\n    ax.plot(x, kernel(x), 'r', label='PDF')\n    ax.legend()\n\n    return numeric_values, statistics, ax\noriginal = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using Min-Max Scaler\n    normalized_array = preprocessing.MinMaxScaler().fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Create a figure and axis object\n    fig, ax = plt.subplots()\n    \n    # Plot the original array\n    ax.plot(original_array, label='Original')\n    \n    # Plot the normalized array\n    ax.plot(normalized_array, label='Normalized')\n    \n    # Set title and labels\n    ax.set_title('Original and Normalized Arrays')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    # Return the original array, normalized array, and axes object\n    return original_array, normalized_array, ax\noriginal = [1, 2, 3, 4, 5]"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Performs a series of operations on the input dictionary data:\n    1. Adds a new key \"a\" with the value 1 to the dictionary.\n    2. Generates a signal based on the values in \"data\".\n    3. Runs a Fast Fourier Transform (FFT) on the signal.\n    4. Plots and returns the FFT of the signal.\n\n    Args:\n        data (dict): The input dictionary.\n        sample_rate (int, optional): The sample rate of the signal. Defaults to 8000.\n\n    Returns:\n        tuple: A tuple containing the FFT of the signal and the plot of the FFT.\n    \"\"\"\n\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data[\"a\"] = 1\n\n    # Step 2: Generate a signal based on the values in \"data\"\n    signal = np.array(list(data.values()))\n\n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    frequencies = fftpack.fftfreq(len(signal), d=1/sample_rate)\n\n    # Step 4: Plot the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(frequencies, np.abs(fft_signal))\n    ax.set_xlabel('Frequency [Hz]')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('FFT of the Signal')\n\n    # Display the plot\n    plt.show()\n\n    # Return the FFT of the signal and the plot\n    return fft_signal, ax\ndata = {\"b\": 2, \"c\": 3, \"d\": 4}"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\ndef task_func():\n    class PostHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check the content type\n            if self.headers['Content-Type'] != 'application/json':\n                self.send_response(400)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                self.wfile.write(json.dumps({'error': 'Content-Type header is not application/json'}).encode())\n                return\n\n            # Read the length of the request and read the data\n            content_length = int(self.headers['Content-Length'])\n            body = self.rfile.read(content_length)\n\n            try:\n                data = json.loads(body)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                self.wfile.write(json.dumps({'error': 'Invalid JSON'}).encode())\n                return\n\n            # Check if the 'data' key is present\n            if 'data' not in data:\n                self.send_response(400)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                self.wfile.write(json.dumps({'error': 'No data key in request'}).encode())\n                return\n\n            # Send a success response\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\n    return PostHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Read the length of the data posted\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n\n            try:\n                # Parse JSON data\n                email_data = json.loads(post_data.decode('utf-8'))\n\n                # Check for required keys\n                if not all(key in email_data for key in ('subject', 'message', 'to')):\n                    raise ValueError(\"Missing required keys in JSON data\")\n\n                # Prepare email\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n\n                # Send email\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n\n                # Successful response\n                self.send_response(200)\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', '0')\n                self.end_headers()\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON\")\n            except ValueError as e:\n                self.send_error(400, str(e))\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication Failed\")\n            except Exception as e:\n                self.send_error(500, str(e))\n\n    return EmailRequestHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    \"\"\"\n    Counts the number of words in .txt files within a specified directory,\n    exports the counts to a JSON file, and returns the total number of words.\n\n    Args:\n        filename (str): The name of the JSON output file.\n        directory (str): The path to the directory containing the .txt files.\n\n    Returns:\n        int: The total number of words in .txt files.\n    \"\"\"\n    # Initialize a counter for words\n    word_counter = Counter()\n    \n    # Initialize a dictionary to store word counts for each file\n    file_word_counts = {}\n    \n    # Walk through the directory to find .txt files\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    # Split the file contents into words\n                    words = f.read().split()\n                    \n                    # Count the number of words in the file\n                    word_count = len(words)\n                    \n                    # Update the total word count\n                    word_counter.update(words)\n                    \n                    # Store the word count for the current file\n                    file_word_counts[file] = word_count\n    \n    # Calculate the total number of words\n    total_words = sum(word_counter.values())\n    \n    # Export the word counts to a JSON file\n    with open(filename, 'w', encoding='utf-8') as json_file:\n        json.dump(file_word_counts, json_file, ensure_ascii=False, indent=4)\n    \n    # Return the total number of words\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with 'Date' and 'Value' columns.\n        plot (bool, optional): Whether to plot the correlation heatmap. Defaults to False.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n        Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n\n    Raises:\n        ValueError: If the DataFrame input is empty or has invalid 'Value' column.\n    \"\"\"\n    # Check if DataFrame is empty or does not contain the required columns\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the 'Value' column\")\n\n    # Check if 'Value' column contains lists\n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"'Value' column must contain lists\")\n\n    # Split lists in 'Value' column into separate columns\n    df_expanded = pd.concat([df['Date'], df['Value'].apply(pd.Series)], axis=1)\n\n    # Calculate Pearson correlation coefficients\n    correlation_matrix = df_expanded.corr(method='pearson')\n\n    # Plot heatmap if requested\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n        ax.set_title(\"Correlation Heatmap\")\n        plt.show()\n        return correlation_matrix, ax\n\n    return correlation_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {student: {field: random.randint(0, 100) for field in all_fields} for student in STUDENTS}\n    \n    # Create DataFrame from the data\n    df = pd.DataFrame(data).T\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject\n    df.loc['Average'] = df.mean()\n    \n    return df\nadditional_fields = ['Computer Science', 'Art']"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate simulated data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{_ + 1}\"\n        age = random.randint(18, 65)\n        height = round(random.uniform(150, 200), 2)\n        weight = round(random.uniform(50, 120), 2)\n        data.append([name, age, height, weight])\n    \n    # Calculate averages\n    avg_age = mean([row[1] for row in data])\n    avg_height = mean([row[2] for row in data])\n    avg_weight = mean([row[3] for row in data])\n    \n    # Append averages to the data\n    data.append(['Average', round(avg_age, 2), round(avg_height, 2), round(avg_weight, 2)])\n    \n    # Write to CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        csvwriter = csv.writer(csvfile)\n        csvwriter.writerow(COLUMNS)\n        csvwriter.writerows(data)\n    \n    # Return the path of the created CSV file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    \"\"\"\n    Organize files in a directory based on the first text that is not enclosed in square brackets.\n    \n    Args:\n        directory (str): The path to the directory containing files to be organized.\n    \n    Returns:\n        tuple: A tuple containing the directory path and a dictionary where keys are the created subdirectories and values are lists of files moved to them.\n    \"\"\"\n    \n    # Create a dictionary to store the subdirectories and files moved to them\n    moved_files = {}\n    \n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Get the full path of the file\n        filepath = os.path.join(directory, filename)\n        \n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(filepath):\n            # Use regular expression to find the first text not enclosed in square brackets in the filename\n            match = re.search(r'\\[[^]]*\\]|([^\\[\\]]+)', filename)\n            \n            # If a match is found, move the file to a subdirectory named after the matched text\n            if match:\n                # Get the matched text\n                text = match.group(1)\n                \n                # Create the subdirectory if it does not exist\n                subdirectory = os.path.join(directory, text)\n                if not os.path.exists(subdirectory):\n                    os.makedirs(subdirectory)\n                \n                # Move the file to the subdirectory\n                shutil.move(filepath, subdirectory)\n                \n                # Add the file to the dictionary\n                if text not in moved_files:\n                    moved_files[text] = []\n                moved_files[text].append(filename)\n    \n    # Return the directory path and the dictionary of moved files\n    return directory, moved_files\ndirectory = '/path/to/your/directory'"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    # Create a list to store the exit codes\n    exit_codes = [None] * len(file_list)\n\n    # Define a function to run a subprocess\n    def run_subprocess(file, index):\n        try:\n            # Run the subprocess\n            process = subprocess.Popen(['python', file], capture_output=True)\n            # Wait for the subprocess to finish\n            process.wait()\n            # Store the exit code\n            exit_codes[index] = process.returncode\n        except Exception as e:\n            # Handle any exceptions\n            print(f\"Error running {file}: {e}\")\n\n    # Create and start a thread for each file\n    threads = []\n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(file, i))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Return the list of exit codes\n    return exit_codes\nfile_list = [\"file1.py\", \"file2.py\", \"file3.py\"]"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    This function finds all .bat files in a given directory, runs them, \n    and returns their file names and exit codes.\n\n    Args:\n        directory_path (str): The path to the directory where the .bat files are located.\n\n    Returns:\n        list of tuples: A list where each tuple contains the file name and its exit code.\n                         The exit code is None if the file could not be executed.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        print(f\"The directory '{directory_path}' does not exist.\")\n        return results\n\n    # Ensure the directory path ends with a slash\n    if not directory_path.endswith(os.sep):\n        directory_path += os.sep\n\n    # Find all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    # Run each .bat file and store the results\n    for file in bat_files:\n        try:\n            # Run the .bat file using the subprocess module\n            result = subprocess.run([file], capture_output=True, text=True, check=True)\n            exit_code = result.returncode\n        except subprocess.CalledProcessError as e:\n            exit_code = e.returncode\n        except Exception as e:\n            # If the file could not be executed, set exit code to None\n            exit_code = None\n            print(f\"An error occurred while running '{file}': {str(e)}\")\n\n        # Append the file name and exit code to the results list\n        results.append((os.path.basename(file), exit_code))\n\n    return results\ndirectory_path = 'path_to_your_directory'"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    \"\"\"\n    Generate a matplotlib figure with two subplots: a histogram with kernel density estimate and a box plot,\n    representing the distribution of the values in the specified column of a pandas DataFrame.\n\n    Parameters:\n    df (pandas DataFrame): The input DataFrame.\n    col (str): The name of the column to plot.\n\n    Returns:\n    matplotlib.figure.Figure: A matplotlib figure object containing the histogram and box plot.\n\n    Raises:\n    ValueError: If the input df is not a pandas DataFrame, is empty, or does not contain the specified column.\n    \"\"\"\n\n    # Check if input df is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame\")\n\n    # Check if DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n\n    # Check if column exists in DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column '{col}'\")\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Create a histogram with kernel density estimate on the first subplot\n    sns.histplot(df[col], ax=axs[0], kde=True)\n    axs[0].set_title(f\"Histogram of {col}\")\n\n    # Create a box plot on the second subplot\n    sns.boxplot(df[col], ax=axs[1])\n    axs[1].set_title(f\"Box Plot of {col}\")\n\n    # Return the figure\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n\n    Args:\n        script_path (str): Path to the Python script to run.\n        wait (bool, optional): Whether to wait for the process to complete. Defaults to True.\n        *args: Variable number of arguments to pass to the script.\n\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n\n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"Script '{script_path}' does not exist\")\n\n    # Construct the command to run the script\n    command = [sys.executable, script_path] + list(args)\n\n    # Run the script as a subprocess\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # If wait is True, wait for the process to complete and return the return code\n    if wait:\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, command, output=stdout, stderr=stderr)\n        return process.returncode\n    # If wait is False, return None immediately\n    else:\n        return None"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Load data from an Excel spreadsheet (.xlsx), calculate the mean and standard deviation of each column, \n    and draw a bar chart. The bar chart will be returned as a matplotlib figure object.\n\n    Args:\n        file_location (str): The path to the Excel file.\n        sheet_name (str): The name of the sheet in the Excel file.\n\n    Returns:\n        dict: A dictionary with mean and standard deviation of each column.\n        matplotlib.figure.Figure: The figure object containing the bar chart.\n\n    Raises:\n        FileNotFoundError: If the Excel file does not exist at the specified path.\n        ValueError: If the specified sheet does not exist in the workbook.\n    \"\"\"\n\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n\n    # Load the data\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"The sheet named '{sheet_name}' does not exist in the workbook.\") from e\n\n    # Calculate mean and standard deviation for each column\n    stats = {}\n    for column in df.columns:\n        mean_val = df[column].mean()\n        std_val = df[column].std()\n        stats[column] = {'mean': mean_val, 'std': std_val}\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    index = np.arange(len(df.columns))\n    bar_width = 0.35\n\n    means = [stats[col]['mean'] for col in df.columns]\n    stds = [stats[col]['std'] for col in df.columns]\n\n    rects1 = ax.bar(index, means, bar_width, label='Mean')\n    rects2 = ax.bar(index + bar_width, stds, bar_width, label='Standard Deviation')\n\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(df.columns)\n    ax.legend()\n\n    fig.tight_layout()\n\n    return stats, fig\nfile_location = 'path_to_excel_file.xlsx'\nsheet_name = 'Sheet1'"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    \"\"\"\n    Returns a bar chart of the number of activities performed on each day of the week.\n\n    Args:\n        activities (list): A list of datetime objects representing the activities.\n\n    Returns:\n        matplotlib.axes.Axes: Axes object representing the bar chart.\n\n    Raises:\n        TypeError: If the activities are not datetime objects.\n    \"\"\"\n\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n\n    # Create a dictionary to store the count of activities per day of the week\n    day_counts = defaultdict(int)\n\n    # Map day of the week (0=Monday, 6=Sunday) to its name\n    day_names = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n    # Iterate over the activities and count the number of activities per day of the week\n    for activity in activities:\n        day_counts[day_names[activity.weekday()]] += 1\n\n    # Prepare data for plotting\n    days = sorted(day_counts.keys(), key=lambda x: day_names.index(x))\n    counts = [day_counts[day] for day in days]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Return the Axes object\n    return ax\nactivities = [datetime(2023, 1, 1), datetime(2023, 1, 2), datetime(2023, 1, 3), datetime(2023, 1, 4), datetime(2023, 1, 5)]"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n\n    Args:\n        src_dir (str): The source directory.\n        dest_dir (str): The destination directory.\n        seed (int, optional): The seed for the random number generator. Defaults to 100.\n\n    Returns:\n        str: The name of the file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n    \"\"\"\n    # Set the seed for the random number generator to ensure reproducibility\n    random.seed(seed)\n    \n    # Check if the source directory exists\n    if not os.path.isdir(src_dir):\n        raise ValueError(f\"Source directory '{src_dir}' does not exist.\")\n    \n    # Check if the destination directory exists, if not create it\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # If there are no files, raise an error\n    if not files:\n        raise ValueError(f\"No files found in the source directory '{src_dir}'.\")\n    \n    # Select a random file from the list\n    file_to_move = random.choice(files)\n    \n    # Construct full file paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n    \n    # Move the file\n    shutil.move(src_path, dest_path)\n    \n    # Return the name of the file moved\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n\n    Args:\n        directory_path (str): The path to the directory containing the Excel files. Defaults to './xlsx_files/'.\n\n    Returns:\n        int: The number of Excel files processed.\n    \"\"\"\n    # Initialize a counter to keep track of the number of files processed\n    file_count = 0\n\n    # Ensure the directory path ends with a slash\n    if not directory_path.endswith('/'):\n        directory_path += '/'\n\n    # Iterate over all Excel files in the specified directory\n    for file_path in glob.glob(os.path.join(directory_path, '*.xlsx')):\n        # Load the Excel file using openpyxl\n        wb = load_workbook(file_path)\n\n        # Iterate over all sheets in the workbook\n        for sheet_name in wb.sheetnames:\n            ws = wb[sheet_name]\n\n            # Iterate over all cells in the worksheet\n            for row in ws.iter_rows(values_only=False):\n                for cell in row:\n                    if cell.value:\n                        # Use regular expression to replace all double quotes with double backslash + double quote\n                        cell.value = re.sub(r'\"', r'\\\\\"', str(cell.value))\n\n        # Save the modified workbook\n        wb.save(file_path)\n\n        # Increment the file count\n        file_count += 1\n\n    # Return the number of files processed\n    return file_count"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Creates a diagram of a sine wave and cosine wave with a given frequency and returns the plot.\n\n    Args:\n        frequency (float): The frequency of the waves.\n        sample_size (int, optional): The number of samples to generate. Defaults to 10000.\n\n    Returns:\n        tuple: A tuple containing the figure object and the axes object of the plot.\n\n    Raises:\n        ValueError: If the frequency is negative.\n        ValueError: If the sample size is non-positive.\n    \"\"\"\n\n    # Check if frequency is negative\n    if frequency < 0:\n        raise ValueError(\"Frequency cannot be negative.\")\n\n    # Check if sample size is non-positive\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Generate x values\n    x = np.linspace(0, 1, sample_size)\n\n    # Generate sine and cosine waves\n    sine_wave = np.sin(2 * np.pi * frequency * x)\n    cosine_wave = np.cos(2 * np.pi * frequency * x)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the sine and cosine waves\n    ax.plot(x, sine_wave, label='Sine Wave')\n    ax.plot(x, cosine_wave, label='Cosine Wave')\n\n    # Set title and labels\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n\n    # Add legend\n    ax.legend()\n\n    # Return the figure and axis\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the generated Flask application.\n\n    Args:\n        app_name (str): The name of the Flask application.\n\n    Returns:\n        tuple: A tuple containing the Flask-Mail instance and the app's mail configurations.\n    \"\"\"\n    # Create a Flask application instance\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables or use defaults\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    mail_port = int(os.environ.get('MAIL_PORT', 25))\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    mail_username = os.environ.get('MAIL_USERNAME', None)\n    mail_password = os.environ.get('MAIL_PASSWORD', None)\n\n    # Configure Flask-Mail\n    app.config['MAIL_SERVER'] = mail_server\n    app.config['MAIL_PORT'] = mail_port\n    app.config['MAIL_USE_TLS'] = mail_use_tls\n    app.config['MAIL_USERNAME'] = mail_username\n    app.config['MAIL_PASSWORD'] = mail_password\n\n    # Initialize Flask-Mail instance\n    mail = Mail(app)\n\n    # Return the Flask-Mail instance and the app's mail configurations\n    return mail, app.config"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of the data from a specific column in an Excel file.\n\n    Args:\n        excel_file_path (str): The path to the Excel file.\n        file_name (str): The name of the Excel file.\n        column_name (str): The name of the column to calculate the statistics for.\n\n    Returns:\n        dict: A dictionary with the mean, median, and standard deviation.\n\n    Raises:\n        FileNotFoundError: If the Excel file does not exist at the specified path.\n        ValueError: If the specified column is not found in the Excel file.\n    \"\"\"\n\n    # Construct the full file path\n    full_path = os.path.join(excel_file_path, file_name)\n\n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n\n    # Read the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise ValueError(f\"Failed to read the Excel file: {str(e)}\")\n\n    # Check if the column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' is not found in the Excel file.\")\n\n    # Extract the data from the specified column\n    data = df[column_name]\n\n    # Calculate the mean, median, and standard deviation\n    mean_value = np.mean(data)\n    median_value = np.median(data)\n    std_value = np.std(data)\n\n    # Return the results as a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'standard_deviation': std_value\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    # Fit the model to the training data, evaluating on the test set\n    history = model.fit(X_train, Y_train, epochs=50, batch_size=10, verbose=0, validation_data=(X_test, Y_test))\n\n    # Plot the model's training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    # Return the trained model and the Axes object\n    return model, ax"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X.shape[1], activation='sigmoid')\n    ])\n\n    # Compile model with binary cross-entropy loss and an SGD optimizer specifying a learning rate\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n\n    # Fit model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n\n    # Compute ROC curve and AUC\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n\n    # Return the trained Keras model and the matplotlib Axes object for the ROC curve plot\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    and saves each region as a separate image. The function returns numpy arrays of the original and segmented images.\n\n    Args:\n        image_path (str): The path to the input image file. Defaults to 'image.jpg'.\n        n_clusters (int): The number of clusters to segment the image into. Defaults to 3.\n        random_seed (int): The random seed for K-means clustering. Defaults to 42.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays. The first array represents the original RGB image,\n            and the second array represents the segmented image, with each pixel's color replaced by\n            the centroid of the cluster it belongs to.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist at the specified path.\n        ValueError: If 'n_clusters' is not a positive integer.\n    \"\"\"\n\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n\n    # Read the image\n    original_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n\n    # Reshape the image to a 2D array with each row representing a pixel's RGB value\n    pixel_values = original_image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n\n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_image = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_image = segmented_image.reshape(original_image.shape)\n    segmented_image = np.uint8(segmented_image)\n\n    # Save each region as a separate image\n    for i in range(n_clusters):\n        region = np.zeros(original_image.shape, dtype=np.uint8)\n        for j in range(original_image.shape[0]):\n            for k in range(original_image.shape[1]):\n                if kmeans.labels_[j * original_image.shape[1] + k] == i:\n                    region[j, k] = segmented_image[j, k]\n        cv2.imwrite(f\"region_{i}.jpg\", cv2.cvtColor(region, cv2.COLOR_RGB2BGR))  # Convert RGB to BGR\n\n    # Return the original and segmented images\n    return original_image, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    This function calculates the product of a matrix 'P' and a 3D tensor 'T', \n    flattens the result, applies KMeans clustering to the flattened data, \n    and visualizes it.\n\n    Parameters:\n    P (np.ndarray): A 2D matrix.\n    T (np.ndarray): A 3D tensor.\n    n_clusters (int): The number of clusters for KMeans clustering. Default is 3.\n    random_state (int): The random state for KMeans clustering. Default is 0.\n    n_init (int): The number of initializations for KMeans clustering. Default is 10.\n\n    Returns:\n    cluster_result (np.ndarray): The result of KMeans clustering.\n    ax (matplotlib.axes.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    \n    # Ensure the dimensions are compatible for matrix multiplication\n    assert P.shape[1] == T.shape[0], \"Incompatible dimensions for matrix multiplication\"\n\n    # Calculate the product of matrix 'P' and 3D tensor 'T'\n    product = np.dot(P, T)\n    \n    # Flatten the product\n    flattened_product = product.flatten()\n    \n    # Reshape the flattened product to a 2D array with one feature per column\n    flattened_product = flattened_product.reshape(-1, 1)\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_product)\n    \n    # Visualize the KMeans clustering\n    plt.scatter(np.arange(len(cluster_result)), flattened_product, c=cluster_result, cmap='viridis')\n    ax = plt.gca()\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Sample index')\n    ax.set_ylabel('Flattened value')\n    \n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n\n    Parameters:\n    points (array_like): A list of points in 2D, where each point is represented as a tuple of two floats.\n    seed (int, optional): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n    tuple: A tuple containing the Voronoi object and the axes of the plotted Voronoi diagram.\n\n    Raises:\n    ValueError: If the input is invalid, for example wrong type or shape.\n    \"\"\"\n\n    # Set the seed for the random number generator\n    np.random.seed(seed)\n\n    # Check if the input is valid\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input must be a 2D numpy array with each row representing a 2D point.\")\n\n    # Apply jittering to the points\n    jitter = np.random.normal(0, 1e-9, points.shape)\n    points_jittered = points + jitter\n\n    # Calculate the Voronoi diagram\n    vor = Voronoi(points_jittered)\n\n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False, line_colors='black', line_width=2, line_alpha=0.6, point_size=2)\n\n    # Set the limits of the plot to the range of the points\n    ax.set_xlim(min(points[:, 0]) - 1, max(points[:, 0]) + 1)\n    ax.set_ylim(min(points[:, 1]) - 1, max(points[:, 1]) + 1)\n\n    # Return the Voronoi object and the axes\n    return vor, ax\npoints = np.array([[0, 0], [1, 0], [1, 1], [0, 1]])"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n\n    Args:\n        src_dir (str): The source directory.\n        dest_dir (str): The destination directory.\n        ext (str): The file extension to move.\n\n    Returns:\n        list: A list of the full paths of files that were successfully moved.\n\n    Raises:\n        FileNotFoundError: If either the source or destination directory does not exist.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist\")\n\n    # Initialize an empty list to store the paths of moved files\n    moved_files = []\n\n    # Find all files in the source directory with the specified extension\n    for file in glob.glob(os.path.join(src_dir, f\"*{ext}\")):\n        # Get the file name\n        file_name = os.path.basename(file)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(os.path.join(dest_dir, file_name)):\n            # Move the file to the destination directory\n            shutil.move(file, dest_dir)\n            # Add the file path to the list of moved files\n            moved_files.append(os.path.join(dest_dir, file_name))\n\n    # Return the list of moved files\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    \"\"\"\n    Load a JSON string into a dictionary, normalize the dictionary by doubling the numerical values, \n    and then create a Pandas DataFrame from the dictionary.\n\n    Args:\n        json_str (str): The input JSON string.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame created from the dictionary.\n    \"\"\"\n\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n\n    # Load the JSON string into a dictionary\n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    # Function to double numerical values\n    def double_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, list):\n            return [double_value(v) for v in value]\n        elif isinstance(value, str):\n            # Use regex to check if the string can be interpreted as a number\n            number_match = re.match(r'^([\\+\\-]?\\d+(\\.\\d*)?|\\.\\d+)$', value)\n            if number_match:\n                number = float(value)\n                return number * 2\n        return value\n\n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {key: double_value(value) for key, value in data_dict.items()}\n\n    # Create a Pandas DataFrame from the dictionary\n    df = pd.DataFrame.from_dict(normalized_dict, orient='index').transpose()\n\n    return df\njson_example = '{\"a\": 1, \"b\": \"2\", \"c\": [3, 4.5], \"d\": \"not a number\"}'"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\nfrom typing import Dict\ndef task_func(script_path: str, timeout=10) -> Dict[str, float]:\n    \"\"\"\n    Execute a given bash script and return the CPU and memory usage of the script's process.\n\n    Args:\n        script_path (str): The path to the bash script to execute.\n        timeout (int, optional): The maximum time in seconds to wait for the script to complete. Defaults to 10.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the accumulated CPU usage in percentage and the accumulated memory usage in bytes.\n    \"\"\"\n\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path '{script_path}' does not exist.\")\n\n    # Initialize the CPU and memory usage accumulators\n    cpu_usage = 0.0\n    mem_usage = 0.0\n\n    # Start the script as a subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Get the process object using the process ID\n    proc = psutil.Process(process.pid)\n\n    # Initialize the start time\n    start_time = time.time()\n\n    try:\n        # Loop until the script completes or the timeout is reached\n        while process.poll() is None:\n            # Check if timeout is reached\n            if time.time() - start_time > timeout:\n                process.terminate()  # Terminate the process if it runs beyond the timeout\n                break\n\n            # Update CPU and memory usage\n            try:\n                cpu_usage += proc.cpu_percent(interval=0.1)  # Non-blocking call\n                mem_usage += proc.memory_info().rss  # Resident Set Size in bytes\n            except psutil.NoSuchProcess:\n                break  # Exit if the process is no longer available\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        process.terminate()\n\n    # Ensure the process is terminated\n    process.wait()\n\n    # Return the accumulated CPU and memory usage as a dictionary\n    return {'CPU Usage': cpu_usage, 'Memory Usage': mem_usage}"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    # Set random seed\n    np.random.seed(seed)\n\n    # Generate random values for \"x\" and \"y\"\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n\n    # Determine the number of categories to use\n    num_categories = len(CATEGORIES)\n    if N >= num_categories:\n        # Ensure each category appears at least once\n        categories = np.random.choice(CATEGORIES, N, replace=True)\n        # Ensure each category appears at least once\n        categories[:num_categories] = CATEGORIES\n        np.random.shuffle(categories)\n    else:\n        # Randomly sample without replacement if N is less than the number of categories\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n\n    # Draw scatter plot\n    fig, ax = plt.subplots()\n    for cat in CATEGORIES:\n        subset = df[df['category'] == cat]\n        ax.scatter(subset['x'], subset['y'], label=cat)\n\n    # Set title and labels\n    ax.set_title(\"Scatter Plot of x vs y\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n\n    # Return DataFrame and Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generate a time series with a specified start time, end time, step, and trend.\n    \n    Parameters:\n    start_time (datetime or str): The start time of the time series. Can be a datetime object or a string in the format '%Y-%m-%d %H:%M:%S'.\n    end_time (datetime or str): The end time of the time series. Can be a datetime object or a string in the format '%Y-%m-%d %H:%M:%S'.\n    step (str): The step (frequency) of the time series. For example, 'H' for hourly, 'D' for daily, etc.\n    trend (float): The linear trend to add to the values.\n    seed (int, optional): The seed for the random number generator. Defaults to 42.\n    \n    Returns:\n    ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    \n    # Convert start_time and end_time to datetime objects if they are not\n    if not isinstance(start_time, datetime):\n        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    if not isinstance(end_time, datetime):\n        end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate the time series\n    timestamps = pd.date_range(start=start_time, end=end_time, freq=step)\n    values = np.random.normal(size=len(timestamps))\n    \n    # Add linear trend\n    time_index = np.arange(len(timestamps))\n    values += trend * time_index\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n    \n    # Plot the time series\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax\nstart_time = '2023-01-01 00:00:00'\nend_time = '2023-01-31 00:00:00'\nstep = 'D'\ntrend = 0.1"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time up to the current time.\n\n    Args:\n        epoch_milliseconds (int): The epoch time in milliseconds.\n        random_seed (int, optional): The random seed for reproducibility. Defaults to 0.\n        products (list, optional): The list of product names. Defaults to [\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"].\n\n    Returns:\n        pd.DataFrame: A DataFrame containing sales data with columns 'Product' (string), 'Date' (datetime), and 'Sales' (integer).\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"Epoch time must be a non-negative integer\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"Random seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"Products must be a list of 5 strings\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime object\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Get current date\n    end_date = datetime.now()\n\n    # Generate sales data for each day between start_date and end_date\n    data = []\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append({\"Product\": product, \"Date\": current_date, \"Sales\": sales})\n        current_date += timedelta(days=1)\n\n    # Create DataFrame from data\n    df = pd.DataFrame(data)\n\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert JSON strings to an Excel file, including handling empty JSON arrays.\n\n    Args:\n        json_str (str): The JSON string to be converted.\n        filename (str): The filename of the output Excel file.\n        sheet_name (str, optional): The name of the sheet in the Excel file. Defaults to \"sheet1\".\n\n    Returns:\n        str: The absolute path of the created Excel file.\n\n    Raises:\n        ValueError: If `json_str` is not valid JSON.\n        TypeError: If `json_str` is not a string, bytes, or bytearray.\n        Exception: For other general errors related to file writing.\n    \"\"\"\n    # Check if json_str is a string, bytes, or bytearray\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    # Try to parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON: {}\".format(e))\n\n    # Check if the JSON string represents an empty array\n    if data == []:\n        # Create an empty DataFrame\n        df = pd.DataFrame()\n    else:\n        # Convert the JSON data to a Pandas DataFrame\n        df = pd.DataFrame(data)\n\n    # Create an Excel file with the specified filename\n    try:\n        # Ensure the filename has the correct extension\n        if not filename.endswith('.xls') and not filename.endswith('.xlsx'):\n            filename += '.xlsx'\n        \n        # Save DataFrame to Excel\n        df.to_excel(filename, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(\"Error writing to Excel file: {}\".format(e))\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\njson_str = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\nfilename = 'example.xlsx'\nsheet_name = 'MySheet'"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate dates\n    end_date = datetime.today()\n    start_date = end_date - timedelta(days=days_in_past)\n    dates = [start_date + timedelta(days=x) for x in range(days_in_past + 1)]\n\n    # Generate random data for each activity\n    data = []\n    for date in dates:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append({\"Date\": date, \"Activity\": activity, \"Duration\": duration})\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Create lineplot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df, ax=ax)\n    ax.set_title('Daily Activity Durations')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Return tuple containing ax and df\n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generate a DataFrame of random stock prices for a specified number of days in the past.\n\n    Parameters:\n    days_in_past (int): The number of days in the past to generate prices for. Defaults to 7.\n    stock_names (list): A list of stock names. Defaults to [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"].\n    random_seed (int): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing random stock prices for the specified number of days.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate a date range for the specified number of days in the past\n    end_date = datetime.today()\n    date_range = [end_date - timedelta(days=x) for x in range(days_in_past)]\n\n    # Generate random stock prices\n    prices = np.random.rand(days_in_past, len(stock_names))\n\n    # Create a DataFrame with the date range as the index and the stock names as columns\n    df = pd.DataFrame(prices, index=date_range, columns=stock_names)\n\n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files and create a difference report.\n\n    Args:\n        file_path1 (str): The path to the first CSV file.\n        file_path2 (str): The path to the second CSV file.\n        delimiter (str): The delimiter used in the CSV files. Defaults to ','.\n        quotechar (str): The quote character used in the CSV files. Defaults to '\"'.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with the differences between the two files.\n\n    Raises:\n        FileNotFoundError: If either of the files cannot be found.\n        ValueError: If either of the files is empty.\n        Exception: For other IO related errors.\n    \"\"\"\n\n    try:\n        # Read the files\n        with open(file_path1, newline='', encoding='utf-8') as file1, open(file_path2, newline='', encoding='utf-8') as file2:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            \n            # Convert readers to lists of lines\n            lines1 = [','.join(line) for line in reader1]\n            lines2 = [','.join(line) for line in reader2]\n            \n            # Check if files are empty\n            if not lines1:\n                raise ValueError(f\"{file_path1} is empty\")\n            if not lines2:\n                raise ValueError(f\"{file_path2} is empty\")\n            \n            # Generate the diff\n            diff = list(ndiff(lines1, lines2))\n            \n            # Prepare the DataFrame\n            data = []\n            line_number = 1\n            for line in diff:\n                status = line[0]\n                content = line[2:]\n                if status != ' ':\n                    data.append([line_number, status, content])\n                line_number += 1\n            \n            df = pd.DataFrame(data, columns=['Line Number', 'Status', 'Content'])\n            return df\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Value error: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if the data list is empty\n    if not data:\n        # If empty, return a dictionary with sum as 0 and mean, min, max as NaN\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        # Create a pie chart with a single slice representing the empty data\n        fig, ax = plt.subplots()\n        ax.pie([1], labels=['No Data'], autopct='%1.1f%%')\n        ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n        return stats, ax\n\n    # Convert the data list to a Pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"The column '{column}' does not exist in the data.\")\n\n    # Calculate statistics for the given column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create a pie chart using the Age column as labels\n    if 'Age' in df.columns:\n        age_counts = df['Age'].value_counts()\n        labels = age_counts.index\n        sizes = age_counts.values\n    else:\n        labels = ['No Data']\n        sizes = [1]\n\n    fig, ax = plt.subplots()\n    ax.pie(sizes, labels=labels, autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return stats, ax\ndata = [\n    {'Name': 'John', 'Age': 25, 'Salary': 50000},\n    {'Name': 'Jane', 'Age': 30, 'Salary': 60000},\n    {'Name': 'Bob', 'Age': 35, 'Salary': 70000}\n]"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyze a list of fitness data, calculate the sum, the mean, the minimum, \n    the maximum of a certain column and draw a line chart. Additionally, validate \n    that the numeric values for steps, calories burned, and distance walked are non-negative.\n\n    Args:\n        column (str): The column name to analyze.\n        data (list): A list of dictionaries containing fitness data.\n\n    Returns:\n        tuple: A tuple containing a dictionary with the sum, mean, min, max of the column,\n               and the Axes object of the plotted line chart.\n\n    Raises:\n        KeyError: If the specified column is not valid.\n        ValueError: If the data list is empty or if any of the numeric values for steps, \n                    calories burned, and distance walked are negative.\n    \"\"\"\n    # Validate the data\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Validate the column\n    if column not in df.columns:\n        raise KeyError(\"Invalid column\")\n\n    # Validate the numeric values\n    for col in ['steps', 'calories_burned', 'distance_walked']:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in {col} column\")\n\n    # Calculate the sum, mean, min, max of the column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n\n    return stats, ax\ndata = [\n    {'Date': '2022-01-01', 'steps': 1000, 'calories_burned': 500, 'distance_walked': 5},\n    {'Date': '2022-01-02', 'steps': 1200, 'calories_burned': 600, 'distance_walked': 6},\n    {'Date': '2022-01-03', 'steps': 1500, 'calories_burned': 700, 'distance_walked': 7},\n    {'Date': '2022-01-04', 'steps': 1800, 'calories_burned': 800, 'distance_walked': 8},\n    {'Date': '2022-01-05', 'steps': 2000, 'calories_burned': 900, 'distance_walked': 9}\n]\ncolumn = 'steps'"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize dictionaries to store means and medians\n    means = defaultdict(list)\n    medians = defaultdict(list)\n    \n    # Iterate over each dictionary in the list\n    for item in data:\n        for key, value in item.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):\n                means[key].append(value)\n                medians[key].append(value)\n    \n    # Calculate means and medians for each key\n    mean_values = {key: np.mean(values) for key, values in means.items()}\n    median_values = {key: np.median(values) for key, values in medians.items()}\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'mean': mean_values,\n        'median': median_values\n    })\n    \n    # Set the index to the variable names (keys)\n    df.index = df.index.set_names('variable')\n    \n    # Sort the DataFrame by the index (variable names)\n    df = df.sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    \"\"\"\n    Identifies duplicate rows from a CSV file, converts duplicated rows into a pandas DataFrame, \n    then plots using matplotlib.\n\n    Args:\n    file_path (str): The path to the CSV file.\n\n    Returns:\n    dict: A dictionary with duplicate rows as keys and their counts as values.\n    Axes: A matplotlib Axes object with the bar chart of duplicate rows.\n    \"\"\"\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must be a CSV file.\")\n\n    # Initialize an empty list to store rows\n    rows = []\n\n    # Read the CSV file\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        for row in reader:\n            rows.append(tuple(row))  # Convert each row to a tuple for hashing\n\n    # Count the occurrences of each row\n    row_counts = Counter(rows)\n\n    # Filter out rows that appear only once\n    duplicate_rows = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicate rows to a pandas DataFrame\n    df = pd.DataFrame(list(duplicate_rows.items()), columns=['Row', 'Count'])\n\n    # Plot a bar chart of the duplicate rows\n    fig, ax = plt.subplots()\n    ax.bar(df['Row'], df['Count'])\n    ax.set_xlabel('Row')\n    ax.set_ylabel('Count')\n    ax.set_title('Duplicate Rows')\n    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n\n    return duplicate_rows, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    Identify duplicate entries in a DataFrame and record the age distribution for the duplicate names.\n\n    Args:\n        df (pd.DataFrame): DataFrame with 'name' and 'age' columns.\n\n    Returns:\n        Counter: Age distribution among duplicate names.\n        plt.Axes or None: Histogram plot displaying age distribution, or None if there are no duplicates.\n\n    Raises:\n        ValueError: If the DataFrame is empty or if age is negative.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if age is negative\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n\n    # Round down age to the nearest integer\n    df['age'] = np.floor(df['age'])\n\n    # Identify duplicate names\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n\n    # If there are no duplicates, return None for the histogram\n    if duplicates.empty:\n        return Counter(), None\n\n    # Record age distribution for duplicate names\n    age_distribution = Counter(duplicates['age'])\n\n    # Create a histogram plot\n    fig, ax = plt.subplots()\n    sns.histplot(data=duplicates, x='age', ax=ax, bins=np.arange(duplicates['age'].min() - 0.5, duplicates['age'].max() + 1.5, 1))\n\n    return age_distribution, ax\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n    'age': [25.2, 30.7, 25.8, 22.0, 30.3]\n})"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    \"\"\"\n    Identify and count duplicate values in a DataFrame's 'value' column.\n    This function also plots a histogram for all values in the 'value' column\n    and overlays a normal distribution curve on the histogram.\n\n    Parameters:\n    df (DataFrame): Input DataFrame with a 'value' column.\n    bins (int): Number of bins in the histogram (default is 4).\n\n    Returns:\n    tuple: A tuple containing:\n    Counter: A Counter object with the count of each duplicate value.\n    Axes: A matplotlib.axes.Axes object that represents the plot\n    of the histogram with the 'value' column data. If applicable,\n    a normal distribution curve fitted to the data is overlaid.\n    \"\"\"\n\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n\n    # Prepare data for plotting\n    values = df['value']\n    mu, sigma = np.mean(values), np.std(values)\n\n    # Create histogram plot\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(values, bins=bins, density=True, alpha=0.6, color='green', label='Histogram')\n\n    # Overlay normal distribution curve\n    y = norm.pdf(bins, mu, sigma)\n    ax.plot(bins, y, 'k-', linewidth=2, label='Normal Distribution')\n\n    # Set plot attributes\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.legend()\n\n    # Return the results\n    return (value_counts, ax)"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generate a pandas DataFrame with random values based on lists 'a' and 'b', \n    and plot it as a bar chart.\n\n    Parameters:\n    a (list): List of row indices for the DataFrame.\n    b (list): List that determines the number of columns in the DataFrame.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    # Ensure the length of 'b' does not exceed the number of predefined columns\n    num_columns = min(len(b), len(COLUMNS))\n    \n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), num_columns), index=a, columns=COLUMNS[:num_columns])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 7))\n    plt.title('Random Data Bar Chart')\n    plt.xlabel('Indices from List \"a\"')\n    plt.ylabel('Random Values')\n    plt.xticks(rotation=45)\n    plt.legend(title='Columns')\n    plt.tight_layout()\n    \n    return ax\na = ['Row1', 'Row2', 'Row3']\nb = [3, 4, 5]"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Ensure data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame\")\n\n    # Check if data has the required columns\n    required_columns = ['month', 'value']\n    if not all(col in data.columns for col in required_columns):\n        raise ValueError(\"Input data must have 'month' and 'value' columns\")\n\n    # Convert 'month' column to datetime objects\n    data['month'] = pd.to_datetime(data['month'], format='%Y-%m')\n\n    # Extract the year from the 'month' column\n    year = data['month'].dt.year.unique()[0]\n\n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(data['month'], data['value'])\n\n    # Set title and labels\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n\n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n\n    # Show the plot\n    plt.show()\n\n    # Return the Axes object\n    return ax\ndata = pd.DataFrame({\n    'month': ['2021-01', '2021-02', '2021-03', '2021-04'],\n    'value': [10, 20, 30, 40]\n})"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the string data to a pandas Series\n    data = pd.Series(data.split(',')).astype(int)\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, edgecolor='black')\n    \n    # Set the title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax\ndata = \"1,2,3,4,5,6,7,8,9\""}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    Generate a noisy sine wave and fit a curve to it using curve_fit.\n\n    Parameters:\n    array_length (int): The length of the noisy sine wave array. Default is 100.\n    noise_level (float): The level of noise to add to the sine wave. Default is 0.2.\n\n    Returns:\n    Axes object: A plot showing the noisy sine wave and its adjusted curve.\n    \"\"\"\n\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n\n    # Generate sine wave\n    y_true = np.sin(x)\n\n    # Add noise to the sine wave\n    y_noisy = y_true + noise_level * np.random.randn(array_length)\n\n    # Define the model function\n    def sine_func(x, a, b, c, d):\n        \"\"\"\n        A sine function with four parameters: amplitude, frequency, phase shift, and offset.\n        \"\"\"\n        return a * np.sin(b * x + c) + d\n\n    # Initial guess for the parameters\n    initial_guess = [1, 1, 0, 0]\n\n    # Fit the curve\n    popt, _ = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n\n    # Generate the fitted curve\n    y_fit = sine_func(x, *popt)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(x, y_noisy, 'bo', label='Noisy data')\n    ax.plot(x, y_fit, 'r-', label='Fitted curve')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n\n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    \"\"\"\n    Reads a CSV file, normalizes the text to ASCII, counts the words, and returns the 10 most common words \n    along with their frequencies as a matplotlib bar plot and a list of tuples.\n\n    Args:\n        csv_file (str): The path to the CSV file.\n\n    Returns:\n        tuple: A tuple containing matplotlib.axes.Axes object for the bar plot and a list of the 10 most common words\n        with their frequencies.\n\n    Raises:\n        FileNotFoundError: If the CSV file cannot be found at the specified path.\n        IOError: If there is an error in reading the file.\n    \"\"\"\n    try:\n        # Read the CSV file\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = []\n            for row in reader:\n                text_data.extend(row)\n        \n        # Normalize text to ASCII\n        normalized_text = ' '.join(text_data)\n        normalized_text = unicodedata.normalize('NFKD', normalized_text).encode('ascii', 'ignore').decode('ascii')\n        \n        # Convert text to lowercase\n        normalized_text = normalized_text.lower()\n        \n        # Count the words\n        words = normalized_text.split()\n        word_counts = Counter(words)\n        \n        # Get the 10 most common words\n        common_words = word_counts.most_common(10)\n        \n        # Create a bar plot\n        words, frequencies = zip(*common_words)\n        fig, ax = plt.subplots()\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        \n        # Show the plot\n        plt.show()\n        \n        return (ax, common_words)\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} could not be found.\")\n    except IOError as e:\n        raise IOError(f\"An error occurred while reading the file {csv_file}: {e}\")\ncsv_file = 'example.csv'"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    \"\"\"\n    Generate a list of normally distributed random numbers, plot their histogram, and plot the PDF of the normal distribution.\n\n    Parameters:\n    size (int): The number of random numbers to generate. Default is 1000.\n\n    Returns:\n    matplotlib.figure.Figure: A figure object containing the histogram and PDF plot.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(size=size)\n    \n    # Create a figure and an axis\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    \n    # Calculate the PDF\n    mu, sigma = stats.norm.fit(data)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    \n    # Plot the PDF\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    # Add labels and title\n    title = f\"Fit results: mu = {mu:.2f}, sigma = {sigma:.2f}\"\n    ax.set_title(title)\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    # Return the figure object\n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate random filename for encrypted private key\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(12)\n\n    # Encrypt private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    encrypted_private_key, tag = cipher.encrypt_and_digest(private_key.save_pkcs1())\n\n    # Save encrypted private key to file\n    with open(filename, \"wb\") as f:\n        f.write(nonce + tag + encrypted_private_key)\n\n    # Return public key, filename, password, and nonce\n    return public_key, filename, password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n\n    # Generate a random AES key\n    aes_key = os.urandom(32)\n\n    # Read the file to be encrypted\n    with open(file_path, 'rb') as file:\n        data = file.read()\n\n    # Pad the data to be encrypted with AES\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(data) + padder.finalize()\n\n    # Encrypt the data with AES\n    iv = os.urandom(16)  # Initialization vector\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as file:\n        file.write(iv + encrypted_data)\n\n    # Encrypt the AES key with the public RSA key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Save the encrypted AES key\n    encrypted_aes_key_path = file_path + '.key.enc'\n    with open(encrypted_aes_key_path, 'wb') as file:\n        file.write(encrypted_aes_key)\n\n    return public_key, encrypted_file_path, encrypted_aes_key_path\nfile_path = 'path_to_your_file.txt'"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    \"\"\"\n    Extracts the text and href attributes of all anchor tags from a given URL's HTML content,\n    and returns this data in a pandas DataFrame along with the time of data extraction.\n\n    Args:\n        url (str): The URL of the HTML content to extract.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with columns 'text', 'href', and 'fetch_time'.\n\n    Raises:\n        ValueError: If the provided URL is invalid or empty.\n        URLError: If there is an issue with network connectivity or the server.\n    \"\"\"\n\n    # Validate the URL\n    if not url or not isinstance(url, str):\n        raise ValueError(\"Invalid or empty URL\")\n\n    try:\n        # Fetch the HTML content from the URL\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"There is an issue with network connectivity or the server.\") from e\n\n    # Parse the HTML content using PyQuery\n    doc = pq(html_content)\n\n    # Extract all anchor tags\n    anchor_tags = doc('a')\n\n    # Extract text and href attributes\n    data = []\n    for tag in anchor_tags:\n        text = tag.text or \"\"\n        href = tag.attrib.get('href', \"\")\n        data.append((text, href))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['text', 'href'])\n\n    # Add fetch time column\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df['fetch_time'] = fetch_time\n\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint, uniform\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generate sensor data for the specified number of hours and save it in a CSV file.\n\n    Args:\n        hours (int): Number of hours to generate data for.\n        output_dir (str, optional): Directory to save the CSV file. Defaults to './output'.\n\n    Returns:\n        str: Path to the generated CSV file.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the output file path\n    output_file = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Start time for the data\n    start_time = datetime.now()\n\n    # Open the CSV file for writing\n    with open(output_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Time'] + SENSORS)\n\n        # Generate data for each hour\n        for hour in range(hours):\n            # Calculate the current time\n            current_time = start_time + timedelta(hours=hour)\n            # Generate random sensor values\n            temperature = uniform(20.0, 30.0)  # Temperature in Celsius\n            humidity = uniform(30.0, 70.0)      # Humidity in percentage\n            pressure = uniform(980.0, 1030.0)   # Pressure in hPa\n            # Write the data row\n            writer.writerow([current_time.strftime('%Y-%m-%d %H:%M:%S')] + [temperature, humidity, pressure])\n\n    print(f\"Sensor data for {hours} hours has been saved to {output_file}\")\n    return output_file\nhours = 24"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate traffic data\n    traffic_data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = [current_time.strftime('%Y-%m-%d %H:%M')]\n        for vehicle in VEHICLE_TYPES:\n            row.append(randint(0, 100))  # Random count between 0 and 100\n        traffic_data.append(row)\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(traffic_data)\n\n    # Read data from CSV to plot\n    df = pd.read_csv(csv_file_path)\n\n    # Plot data\n    fig, ax = plt.subplots(figsize=(10, 5))\n    for vehicle in VEHICLE_TYPES:\n        df.plot(x='Time', y=vehicle, ax=ax, label=vehicle)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    ax.grid(True)\n\n    # Save plot to file (optional)\n    plot_file_path = os.path.join(output_dir, 'traffic_plot.png')\n    plt.savefig(plot_file_path)\n\n    return csv_file_path, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create backup directory if it doesn't exist\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Generate weather data\n    weather_data = []\n    start_time = datetime.now()\n    for i in range(hours):\n        time = start_time + timedelta(hours=i)\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append([time.strftime('%Y-%m-%d %H:%M'), condition])\n\n    # Save weather data to CSV file\n    output_file = os.path.join(output_dir, 'weather_data.csv')\n    with open(output_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file\n    backup_file = os.path.join(backup_dir, 'weather_data.csv')\n    shutil.copy(output_file, backup_file)\n\n    return output_file"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Generate random data\n    data = {\n        'Team': [TEAMS[randint(0, len(TEAMS) - 1)] for _ in range(goals + penalties)],\n        'Type': ['Goals'] * goals + ['Penalty Cost'] * penalties,\n        'Value': [randint(0, 5) for _ in range(goals)] + [randint(1, 5) for _ in range(penalties)]\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Convert penalties to fines\n    df.loc[df['Type'] == 'Penalty Cost', 'Value'] = df['Value'] * PENALTY_COST\n    \n    # Plotting\n    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n    \n    # Goals plot\n    sns.barplot(x='Team', y='Value', data=df[df['Type'] == 'Goals'], ax=axes[0])\n    axes[0].set_title('Goals by Team')\n    axes[0].set_ylabel('Goals')\n    \n    # Penalty Cost plot\n    sns.barplot(x='Team', y='Value', data=df[df['Type'] == 'Penalty Cost'], ax=axes[1])\n    axes[1].set_title('Penalty Costs by Team')\n    axes[1].set_ylabel('Penalty Cost ($)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df, [axes[0], axes[1]]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Generate a Pandas DataFrame with random integer values between 0 and 9,\n    count the non-zero values in each column, and visualize this information using a bar plot.\n\n    Args:\n        rows (int): The number of rows in the DataFrame.\n\n    Returns:\n        tuple: A tuple containing the generated DataFrame and the matplotlib Axes object.\n    \"\"\"\n    # Generate a DataFrame with random integer values between 0 and 9\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count the non-zero values in each column\n    non_zero_counts = df.apply(lambda x: (x != 0).sum())\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n\n    # Set title and labels\n    ax.set_title('Non-Zero Values in Each Column')\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Non-Zero Values')\n\n    # Return the DataFrame and the Axes object\n    return df, ax\nrows = 10"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define course names\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    \n    # Generate random student IDs\n    student_ids = sample(range(1000, 9999), num_students)\n    \n    # Create a DataFrame with random grades for each student in each course\n    data = {course: np.random.randint(0, 101, size=num_students) for course in courses}\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate average grade in each course\n    average_grades = df.mean()\n    \n    # Calculate number of students with a passing grade (>= 60) in each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(average_grades.index, average_grades.values, label='Average Grade', color='blue', width=0.4)\n    ax.bar(passing_counts.index, passing_counts.values, label='Passing Grades', color='green', width=0.4)\n    \n    # Set plot title and labels\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_ylabel('Counts / Grades')\n    ax.set_xticklabels(courses, rotation=45)\n    ax.legend()\n    \n    # Show plot\n    plt.tight_layout()\n    \n    return df, ax\nnum_students = 10"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n\n    Parameters:\n    array (numpy.ndarray): Input array with shape (n, 2) where n is the number of data points.\n    target_value (int): The value to match in the first column of the array.\n\n    Returns:\n    tuple: Containing the optimized parameters of the fitting function (popt) and the matplotlib Axes object.\n    \"\"\"\n    # Filter the array to get rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n    \n    # Extract the indices and the corresponding values to fit\n    indices = np.arange(len(filtered_array))\n    values = filtered_array[:, 1]\n    \n    # Define the exponential decay function to fit\n    def exp_decay(x, a, b, c):\n        \"\"\"\n        Exponential decay function.\n\n        Parameters:\n        x (numpy.ndarray): Input array of x values.\n        a (float): Amplitude of the exponential decay.\n        b (float): Rate of the exponential decay.\n        c (float): Offset of the exponential decay.\n\n        Returns:\n        numpy.ndarray: The exponential decay function evaluated at x.\n        \"\"\"\n        return a * np.exp(-b * x) + c\n    \n    # Initial guess for the parameters\n    p0 = [1, 0.1, 0]\n    \n    # Fit the exponential decay function\n    popt, _ = optimize.curve_fit(exp_decay, indices, values, p0=p0)\n    \n    # Plot the original data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(indices, values, label='Data')\n    ax.plot(indices, exp_decay(indices, *popt), label='Fitted curve', color='red')\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Exponential Decay Fit')\n    \n    return (popt, ax)\narray = np.array([[1, 2], [1, 3], [2, 4], [1, 5], [2, 6]])\ntarget_value = 1"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    \"\"\"\n    Performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF).\n\n    Args:\n        texts (list of str): A list of text documents.\n        num_topics (int): The number of topics to extract.\n\n    Returns:\n        list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess the input texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces) and convert to lowercase\n        text = ALPHANUMERIC.sub(' ', text).lower()\n        # Remove stopwords\n        text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n        processed_texts.append(text)\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract the specified number of topics\n    nmf = NMF(n_components=num_topics, random_state=1)\n    nmf_matrix = nmf.fit_transform(tfidf_matrix)\n\n    # Extract the most significant words for each topic\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        topic_words = []\n        for word_idx, word_weight in enumerate(topic):\n            if word_weight > 0.1:  # Threshold for significant words\n                word = vectorizer.get_feature_names_out()[word_idx]\n                topic_words.append(word)\n        topics.append(topic_words)\n\n    return topics\ntexts = [\"Example text one\", \"Another example text\", \"More text here\"]\nnum_topics = 2"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n\n    Parameters:\n    texts (list): A list of texts to process.\n    stopwords (set, optional): A set of stopwords to remove. Defaults to None.\n\n    Returns:\n    Word2Vec: A trained Word2Vec model.\n    \"\"\"\n    # Initialize stopwords if not provided\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    # Preprocess texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space, and lowercase\n        cleaned_text = ALPHANUMERIC.sub(' ', text).lower()\n        # Tokenize and remove stopwords\n        tokens = [word for word in cleaned_text.split() if word not in stopwords]\n        cleaned_texts.append(tokens)\n\n    # Train Word2Vec model\n    model = Word2Vec(cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Process JSON files in a directory.\n\n    Reads each JSON file alphabetically into a DataFrame, inserts a \"Source\" column \n    that specifies the filename, and moves the processed files to a \"processed\" \n    subdirectory.\n\n    Args:\n        path (str): The directory containing the JSON files to be processed.\n\n    Returns:\n        df (pandas.DataFrame): A DataFrame containing the data from all processed files.\n    \"\"\"\n\n    # Ensure the directory exists\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The directory {path} does not exist.\")\n\n    # Create the processed directory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    # Initialize an empty DataFrame to store all data\n    df = pd.DataFrame()\n\n    # List all JSON files in the directory and sort them alphabetically\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()\n\n    for file_name in json_files:\n        file_path = os.path.join(path, file_name)\n\n        # Read the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n\n        # Convert the JSON data to a DataFrame\n        temp_df = pd.DataFrame(data)\n\n        # Add the \"Source\" column with the filename\n        temp_df['Source'] = file_name\n\n        # Append the DataFrame to the main DataFrame\n        df = pd.concat([df, temp_df], ignore_index=True)\n\n        # Move the processed file to the processed directory\n        shutil.move(file_path, os.path.join(processed_path, file_name))\n\n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Create the directory if it doesn't exist\n    directory = 'task_func_data'\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Define the file path\n    file_path = os.path.join(directory, 'Output.txt')\n\n    # Generate random sensor data\n    temperature = random.uniform(20.0, 30.0)\n    humidity = random.uniform(40.0, 60.0)\n\n    # Write data to CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        # Write header if the file is empty\n        if os.stat(file_path).st_size == 0:\n            writer.writerow([\"Timestamp\", \"Temperature (C)\", \"Humidity (%)\"])\n        writer.writerow([datetime.now(), temperature, humidity])\n\n    # Return the file path before deletion\n    return_path = file_path\n\n    # Delete the file after use\n    os.remove(file_path)\n\n    return return_path"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n\n    Args:\n        url (str): The URL to scrape data from.\n\n    Returns:\n        str: The path of the CSV file where the scraped data is saved.\n    \"\"\"\n\n    try:\n        # Send a GET request to the URL\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        print(f\"Failed to retrieve the URL: {e}\")\n        return None\n\n    # If the GET request is successful, the status code will be 200\n    if response.getcode() == 200:\n        # Get the content of the response\n        web_content = response.read()\n\n        # Create a BeautifulSoup object and specify the parser\n        soup = BeautifulSoup(web_content, 'html.parser')\n\n        # Find the table in the HTML content\n        table = soup.find('table')\n\n        # Check if the table is found\n        if table is None:\n            raise ValueError(\"No table found on the page\")\n\n        # Extract headers\n        headers = []\n        for header in table.find_all('th'):\n            headers.append(header.text.strip())\n\n        # Extract rows\n        rows = []\n        for row in table.find_all('tr'):\n            cells = row.find_all('td')\n            if cells:\n                rows.append([cell.text.strip() for cell in cells])\n\n        # Write data to CSV file\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            csvwriter = csv.writer(csvfile)\n            csvwriter.writerow(headers)\n            csvwriter.writerows(rows)\n\n        # Return the path of the CSV file\n        return os.path.abspath(CSV_FILE_PATH)\n\n    else:\n        print(\"Failed to retrieve the URL\")\n        return None\nurl = 'http://example.com/table_page'"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport numpy as np\nimport pandas as pd\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    \"\"\"\n    Train a linear regression model and return the model score of the test set.\n\n    Parameters:\n    data (pd.DataFrame): DataFrame used as training data.\n    target_column (str): Target column in the DataFrame.\n    test_size (float, optional): Proportion of the data to include in the test set. Defaults to 0.2.\n    random_state (int, optional): Seed used to shuffle the data before splitting. Defaults to 0.\n\n    Returns:\n    float: The model's score.\n\n    Raises:\n    ValueError: If data is not a DataFrame.\n    ValueError: If data is empty.\n    ValueError: If target_column is not a column of data.\n    ValueError: If data contains values that are not numeric.\n    ValueError: If random_state is not an integer.\n    ValueError: If test_size is not between 0 and 1.\n    \"\"\"\n\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a DataFrame\")\n\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data cannot be empty\")\n\n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(\"Target column must be a column of data\")\n\n    # Check if data contains non-numeric values\n    if not data.apply(lambda s: pd.api.types.is_numeric_dtype(s)).all():\n        raise ValueError(\"Data must contain only numeric values\")\n\n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be an integer\")\n\n    # Check if test_size is between 0 and 1\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size must be between 0 and 1\")\n\n    # Split data into training and test sets\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Return the model's score on the test set\n    return model.score(X_test, y_test)\nrng = np.random.default_rng()\ndata = pd.DataFrame({'x1': rng.random(500)})"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'],\n              rng_seed=None):\n    # Set random seed\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate IDs\n    ids = np.arange(1, 101)\n\n    # Combine names lists\n    names = latin_names + other_names\n\n    # Generate random names\n    random_names = np.random.choice(names, size=100)\n\n    # Generate random dates of birth\n    dates_of_birth = []\n    for _ in range(100):\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 29)\n        date_of_birth = datetime.date(year, month, day)\n        dates_of_birth.append(date_of_birth)\n\n    # Generate emails\n    emails = []\n    for name, date_of_birth in zip(random_names, dates_of_birth):\n        # Correct encoding of Latin characters\n        name = codecs.decode(name, 'utf-8')\n        email = f\"{name.lower().replace(' ', '.')}{date_of_birth.year}@{email_domain}\"\n        emails.append(email)\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': random_names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n\n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Load the data from the JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a dictionary to store the results\n    results = defaultdict(list)\n    \n    # Iterate over the data and collect values for each key\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                results[key].append(value)\n    \n    # Calculate the mean and median for each key\n    stats = {}\n    for key, values in results.items():\n        mean = np.mean(values)\n        median = np.median(values)\n        stats[key] = {\"mean\": mean, \"median\": median}\n    \n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['key', 'mean', 'median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for key, value in stats.items():\n            writer.writerow({'key': key, 'mean': value['mean'], 'median': value['median']})\n    \n    # Return the results as a dictionary\n    return stats"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, and if it has content, \n    then copies it to a target directory.\n\n    Args:\n        kwargs (dict): A dictionary containing file paths as values.\n        target_dir (str, optional): The target directory to copy files to. Defaults to \"non_none_files\".\n\n    Returns:\n        dict: A dictionary containing a list of full file paths that were copied.\n    \"\"\"\n    # Ensure the target directory exists\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n    \n    copied_files = []\n    \n    # Iterate over the dictionary items\n    for file_path in kwargs.values():\n        file_path = Path(file_path)\n        # Check if the file exists and is not empty\n        if file_path.exists() and file_path.stat().st_size > 0:\n            # Copy the file to the target directory\n            target_path = Path(target_dir) / file_path.name\n            shutil.copy2(file_path, target_path)\n            copied_files.append(str(target_path))\n    \n    return {\"copied_files\": copied_files}\nkwargs = {\"key1\": \"path/to/file1.txt\", \"key2\": \"path/to/file2.txt\", \"key3\": \"path/to/file3.txt\"}"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n\n    Args:\n        directory (str): The path to the directory containing the CSV files.\n        pattern (str, optional): The regular expression pattern to match the CSV files. Defaults to r'^(.*?)-\\d+\\.csv$'.\n\n    Returns:\n        list: A list of strings, where each string is the filename of a new CSV file created by the function.\n    \"\"\"\n    # Initialize an empty list to store the new file names\n    new_files = []\n\n    # Compile the regex pattern\n    regex = re.compile(pattern)\n\n    # Iterate over the files in the directory\n    path = Path(directory)\n    for file_path in path.iterdir():\n        if file_path.is_file() and file_path.suffix == '.csv':\n            match = regex.match(file_path.name)\n            if match:\n                # Extract the base name from the pattern\n                base_name = match.group(1)\n                # Create a new filename by appending '.csv' to the base name\n                new_filename = f\"{base_name}.csv\"\n                new_file_path = path / new_filename\n\n                # Copy the content of the original file to the new file\n                with file_path.open('r', newline='') as original_file, new_file_path.open('w', newline='') as new_file:\n                    reader = csv.reader(original_file)\n                    writer = csv.writer(new_file)\n                    for row in reader:\n                        writer.writerow(row)\n\n                # Add the new file name to the list\n                new_files.append(str(new_file_path))\n\n    # Return the list of new file names\n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzip all zip files in a directory whose name matches a certain pattern.\n    The pattern is used to split the filename at the last \"-\" and use the prefix part of the filename as the directory to extract.\n    \n    Args:\n        directory (str): The directory where the zip files are located.\n        pattern (str): The pattern to match the zip file names. Default is '^(.*?)-\\d+\\.zip$'.\n    \n    Returns:\n        list: A list of directories where the files were extracted.\n    \"\"\"\n    # Compile the pattern\n    regex = re.compile(pattern)\n    \n    # Initialize an empty list to store the extracted directories\n    extracted_dirs = []\n    \n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        match = regex.match(filename)\n        if match:\n            # Extract the prefix part of the filename\n            prefix = match.group(1)\n            \n            # Create the directory if it doesn't exist\n            extract_dir = os.path.join(directory, prefix)\n            if not os.path.exists(extract_dir):\n                os.makedirs(extract_dir)\n            \n            # Extract the zip file to the directory\n            zip_file_path = os.path.join(directory, filename)\n            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n            \n            # Add the extracted directory to the list\n            extracted_dirs.append(extract_dir)\n    \n    return extracted_dirs\ndirectory = '/path/to/zip/files'\npattern = r'^(.*?)-\\d+\\.zip$'"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport zipfile\nimport shutil\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archives all files matching a given pattern into a single ZIP file and then deletes the original files.\n\n    Args:\n        pattern (str): The pattern to match files against.\n\n    Returns:\n        str: The archive file path.\n    \"\"\"\n\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n\n    if not files_to_archive:\n        raise FileNotFoundError(f\"No files found matching the pattern: {pattern}\")\n\n    # Create a unique archive file name\n    archive_file_name = f\"archive_{os.path.basename(pattern)}_{os.getpid()}.zip\"\n    archive_file_path = os.path.join(ARCHIVE_DIR, archive_file_name)\n\n    # Create the ZIP archive\n    with zipfile.ZipFile(archive_file_path, 'w') as zf:\n        for file in files_to_archive:\n            zf.write(file, os.path.basename(file))\n\n    # Delete the original files\n    for file in files_to_archive:\n        os.remove(file)\n\n    return archive_file_path\npattern = \"*.txt\""}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count the total number of goals and penalties from a CSV file and update it with the given goals and penalties.\n\n    Args:\n        goals (int): The number of goals to update.\n        penalties (int): The number of penalties to update.\n        csv_file_path (str, optional): The path to the CSV file. Defaults to 'match_data.csv'.\n\n    Returns:\n        Counter: A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Initialize Counter object\n    count = Counter({'goals': 0, 'penalties': 0})\n\n    # Read CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n\n    # Update counts with given goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from the source directory to the target directory based on a specified pattern.\n\n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        file_pattern (str): The regular expression pattern to match file names. Defaults to r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'.\n\n    Returns:\n        int: The number of files that were successfully moved from the source directory to the target directory.\n    \"\"\"\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Compile the regex pattern for matching filenames\n    pattern = re.compile(file_pattern)\n\n    # Initialize a counter for moved files\n    moved_files_count = 0\n\n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the filename matches the pattern\n        if pattern.match(filename):\n            # Construct full file path\n            source_file_path = os.path.join(source_dir, filename)\n            target_file_path = os.path.join(target_dir, filename)\n\n            # Check if the file already exists in the target directory\n            if os.path.exists(target_file_path):\n                print(f\"File {filename} already exists in the target directory. Skipping...\")\n                continue\n\n            # Move the file\n            try:\n                shutil.move(source_file_path, target_file_path)\n                moved_files_count += 1\n            except Exception as e:\n                print(f\"Error moving file {filename}: {str(e)}\")\n\n    return moved_files_count\nsource_dir = '/path/to/source/directory'\ntarget_dir = '/path/to/target/directory'"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the similarity values between two texts based on the cosine similarity and the Levenshtein ratio.\n\n    Args:\n        text1 (str): The first text.\n        text2 (str): The second text.\n\n    Returns:\n        tuple: A tuple containing the cosine similarity and Levenshtein ratio as floats.\n    \"\"\"\n    # Clean the texts by removing non-alphanumeric characters except spaces and converting to lowercase\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Split the texts into words\n    words1 = text1.split()\n    words2 = text2.split()\n\n    # Compute term frequency for each text\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n\n    # Get unique words from both texts\n    unique_words = set(tf1.keys()).union(set(tf2.keys()))\n\n    # Create term frequency vectors\n    tf_vector1 = np.array([tf1.get(word, 0) for word in unique_words])\n    tf_vector2 = np.array([tf2.get(word, 0) for word in unique_words])\n\n    # Compute cosine similarity\n    dot_product = np.dot(tf_vector1, tf_vector2)\n    magnitude1 = np.linalg.norm(tf_vector1)\n    magnitude2 = np.linalg.norm(tf_vector2)\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 * magnitude2 != 0 else 0\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio\ntext1 = \"Hello, world!\"\ntext2 = \"Hello, everyone!\""}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    \"\"\"\n    This function generates all permutations of a given list of numbers and \n    calculates the sum of the factorials of each number in each permutation.\n\n    Args:\n        numbers (list): A list of integers.\n\n    Returns:\n        tuple: A tuple containing two lists. The first list contains the sums of \n        the factorials of each number in each permutation. The second list contains \n        all permutations of numbers.\n\n    Raises:\n        TypeError: If numbers is not a list of integers.\n        ValueError: If input numbers are negative.\n    \"\"\"\n    # Check if input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers.\")\n\n    # Check if input list is empty\n    if len(numbers) == 0:\n        return [], []\n\n    # Check if all elements in the list are integers\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n\n    # Check if all elements in the list are non-negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n\n    # Generate all permutations of the input list\n    perms = list(permutations(numbers))\n\n    # Calculate the sum of the factorials of each number in each permutation\n    sums = [sum(math.factorial(num) for num in perm) for perm in perms]\n\n    return sums, perms"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory to another based on specified file extensions.\n\n    Args:\n        SOURCE_DIR (str): The source directory containing files to be transferred.\n        DEST_DIR (str): The destination directory where files will be transferred.\n        EXTENSIONS (list): A list of file extensions to be transferred.\n\n    Returns:\n        transferred_files (list): A list containing the names of files that were successfully transferred.\n    \"\"\"\n    transferred_files = []\n\n    # Ensure the destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    # Iterate over each extension\n    for ext in EXTENSIONS:\n        # Find all files with the current extension in the source directory\n        for file_path in glob.glob(os.path.join(SOURCE_DIR, f'*{ext}')):\n            try:\n                # Construct the destination path\n                file_name = os.path.basename(file_path)\n                dest_path = os.path.join(DEST_DIR, file_name)\n\n                # Transfer the file\n                shutil.move(file_path, dest_path)\n\n                # Record the successful transfer\n                transferred_files.append(file_name)\n            except Exception as e:\n                # Issue a warning for files that could not be transferred\n                warnings.warn(f\"Could not transfer {file_name}: {e}\")\n\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item[0] for item in data]\n    counts = np.array([item[1] for item in data]).reshape(-1, 1)\n    weights = np.array([item[2] for item in data]).reshape(-1, 1)\n\n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts, axis=0)\n\n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights)\n\n    # Create a DataFrame with the results\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts.flatten(),\n        'Normalized Weight': normalized_weights.flatten()\n    })\n\n    return df\ndata = [('item1', 10, 0.5), ('item2', 20, 0.8), ('item3', 30, 1.2)]"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Transpose the list of tuples to get a list of values at each position\n    transposed_data = list(zip(*data_list))\n\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over the transposed data\n    for values in transposed_data:\n        # Filter out non-numeric values\n        numeric_values = [value for value in values if isinstance(value, (int, float))]\n\n        # Calculate the mean of the numeric values\n        mean_value = np.mean(numeric_values) if numeric_values else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame({'Mean Value': mean_values}, index=[f'Position {i}' for i in range(len(mean_values))])\n\n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    \"\"\"\n    Perform a chi-square test of independence of variables in a contingency table.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame containing categorical data.\n    col1 (str): The name of the first categorical column.\n    col2 (str): The name of the second categorical column.\n\n    Returns:\n    float: The p-value of the chi-square test of independence.\n\n    Raises:\n    ValueError: If 'data' is empty, if 'col1' or 'col2' are not in 'data',\n                if one or both of the columns do not have multiple categories,\n                or if some categories have less than 5 observations.\n    TypeError: If one or both of the columns contain non-categorical data.\n    \"\"\"\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n\n    # Check if columns are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both columns are not in data\")\n\n    # Check if columns are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both columns contain non-categorical data\")\n\n    # Check if columns have multiple categories\n    if data[col1].nunique() < 2 or data[col2].nunique() < 2:\n        raise ValueError(\"One or both columns do not have multiple categories\")\n\n    # Check if categories have less than 5 observations\n    if (data[col1].value_counts() < 5).any() or (data[col2].value_counts() < 5).any():\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Construct the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform the chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return p\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    # Set the seed for reproducibility\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate the dice rolls\n    outcomes = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate the frequency of each outcome\n    frequency = np.array([outcomes.count(num) for num in NUMBERS])\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    # Return the frequency array and the histogram\n    return frequency, ax\nrolls = 1000\nseed = 42"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    \n    The function identifies processed files by the '_processed' suffix in the filename.\n    \n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        archive_name (str, optional): The name of the archive file. Defaults to 'archive.zip'.\n    \n    Returns:\n        str: The path to the created archive.\n    \"\"\"\n    \n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Define the pattern to match processed files\n    processed_pattern = re.compile(r'.*_processed\\..+')\n\n    # Create the full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a zip file and add processed files to it\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                if processed_pattern.match(file):\n                    file_path = os.path.join(root, file)\n                    archive.write(file_path, os.path.relpath(file_path, source_dir))\n\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    \"\"\"\n    Predicts the stock closing prices for the next 7 days using simple linear regression and plots the data.\n\n    Args:\n        df (pd.DataFrame): A DataFrame with 'Timestamp' and 'Close' columns.\n\n    Returns:\n        tuple: A tuple containing a list of predicted prices for the next 7 days and the matplotlib Axes object containing the plot.\n    \"\"\"\n    # Ensure the Timestamp is in datetime format\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n    # Convert Timestamp to numerical format for regression\n    df['Timestamp_num'] = df['Timestamp'].astype(np.int64) // 10**9\n    \n    # Prepare data for linear regression\n    X = df[['Timestamp_num']]\n    y = df['Close']\n    \n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    last_timestamp = df['Timestamp'].max()\n    future_timestamps = [last_timestamp + pd.Timedelta(seconds=24*60*60*i) for i in range(1, 8)]\n    future_timestamps_num = [ts.timestamp() for ts in future_timestamps]\n    \n    # Predict future prices\n    future_prices = model.predict(np.array(future_timestamps_num).reshape(-1, 1))\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df['Timestamp'], df['Close'], label='Historical Prices')\n    ax.plot(future_timestamps, future_prices, label='Predicted Prices', linestyle='--')\n    ax.set_xlabel('Timestamp')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return (future_prices.tolist(), ax)\ndf = pd.DataFrame({\n    'Timestamp': pd.date_range(start='2023-01-01', periods=30, freq='D'),\n    'Close': np.random.rand(30) * 100\n})"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a given DataFrame using the Z-Score method.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame containing the 'closing_price' column.\n    z_threshold (float, optional): The Z-Score threshold for identifying outliers. Defaults to 2.\n\n    Returns:\n    tuple: A tuple containing the following elements:\n        pandas.DataFrame: A DataFrame containing the outliers in the 'closing_price' column.\n        matplotlib.axes._axes.Axes: The plot object displaying the outliers.\n    \"\"\"\n    # Calculate Z-Scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[(np.abs(z_scores) > z_threshold)]\n    \n    # Plotting the data\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['closing_price'], label='Original Data', color='blue')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    # Return the outliers DataFrame and the plot object\n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram.\n\n    Args:\n        df (pd.DataFrame): A pandas DataFrame containing the stock closing prices.\n\n    Returns:\n        tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot\n            and the second for the histogram.\n    \"\"\"\n    # Ensure the DataFrame contains a column named 'Close'\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a column named 'Close'\")\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Box Plot\n    sns.boxplot(x=df['Close'], ax=axs[0])\n    axs[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(df['Close'], ax=axs[1], kde=True)\n    axs[1].set_title('Histogram of Closing Prices')\n\n    # Layout so plots do not overlap\n    fig.tight_layout()\n\n    # Return the axes objects\n    return axs\ndf = pd.DataFrame({'Close': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]})"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has a datetime index\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    \n    # Fit ARIMA model\n    model = ARIMA(df['close'], order=(5,1,0))  # Example order, can be tuned\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecast_values = forecast[0].tolist()\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    df['close'].plot(ax=ax, label='Historical Prices')\n    forecast_df = pd.DataFrame(forecast_values, index=pd.date_range(start=df.index[-1] + pd.DateOffset(days=1), periods=7), columns=['Forecast'])\n    forecast_df.plot(ax=ax, label='Forecasted Prices', linestyle='--')\n    ax.legend()\n    ax.set_title('Share Closing Prices Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    \n    return forecast_values, ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    This function generates a dictionary containing all possible two-letter combinations of the lowercase English alphabets.\n    The dictionary values represent the frequency of these two-letter combinations in the given word.\n    \n    Args:\n        word (str): The input word to count two-letter combinations.\n    \n    Returns:\n        dict: A dictionary with keys as two-letter alphabet combinations and values as their counts in the word.\n    \"\"\"\n    \n    # Generate all possible two-letter combinations\n    alphabet = string.ascii_lowercase\n    two_letter_combinations = [''.join(pair) for pair in itertools.product(alphabet, repeat=2)]\n    \n    # Initialize a Counter with all combinations set to 0\n    combo_counter = Counter({combo: 0 for combo in two_letter_combinations})\n    \n    # Count the actual occurrences of two-letter combinations in the word\n    for i in range(len(word.lower()) - 1):\n        combo = word.lower()[i:i+2]\n        combo_counter[combo] += 1\n    \n    return dict(combo_counter)"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Creates and visualizes a sales report for different categories over a period of time.\n\n    Args:\n        start_date (str): The start date of the period. Defaults to '2016-01-01'.\n        periods (int): The number of periods. Defaults to 13.\n        freq (str): The frequency of the periods. Defaults to 'WOM-2FRI'.\n        categories (list): The list of categories. Defaults to ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports'].\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the sales data with the following columns: 'Date', 'Category', 'Sales'.\n        matplotlib.axes.Axes: The Matplotlib Axes object for the plot.\n    \"\"\"\n    # Create a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the date range and categories\n    df = pd.DataFrame({'Date': np.repeat(dates, len(categories)), \n                       'Category': np.tile(categories, len(dates))})\n    \n    # Generate random sales data\n    np.random.seed(0)  # For reproducibility\n    df['Sales'] = np.random.randint(100, 1000, size=len(df))\n    \n    # Plot the sales data\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        category_df = df[df['Category'] == category]\n        ax.plot(category_df['Date'], category_df['Sales'], marker='o', label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates a time series of sales data starting from a specified date, \n    then uses linear regression to forecast future sales based on the provided or generated sales data.\n\n    Args:\n        start_date (str): The start date of the time series. Defaults to '2016-01-01'.\n        periods (int): The number of periods in the time series. Defaults to 13.\n        freq (str): The frequency of the time series. Defaults to 'WOM-2FRI'.\n        sales_data (list or numpy array): The sales data. If None, random sales data will be generated. Defaults to None.\n\n    Returns:\n        numpy array: A numpy array containing the forecasted future sales for the same number of periods as the input data.\n    \"\"\"\n\n    # Generate a time series of sales data\n    if sales_data is None:\n        np.random.seed(0)  # For reproducibility\n        sales_data = np.random.randint(100, 200, size=periods)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the date range and sales data\n    df = pd.DataFrame({'date': date_range, 'sales': sales_data})\n    df['time'] = np.arange(len(df))\n    \n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[['time']], df['sales'])\n    \n    # Forecast future sales\n    future_times = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    forecasted_sales = model.predict(future_times)\n    \n    # Return the forecasted future sales as a numpy array\n    return forecasted_sales"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n\n    Args:\n        task_list (list): A list of task names.\n        n_tasks (int): The number of tasks to assign.\n        employees (list, optional): A list of employee names. Defaults to a predefined list.\n        seed (int, optional): A seed for random number generation. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing columns 'Task Name', 'Assigned To', and 'Due Date', with each row representing an assigned task.\n\n    Raises:\n        ValueError: If n_tasks is negative.\n    \"\"\"\n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_task_list = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Randomly select n_tasks from the sanitized task list\n    selected_tasks = random.sample(sanitized_task_list, n_tasks)\n\n    # Assign each task to a random employee\n    assignments = []\n    for task in selected_tasks:\n        assigned_to = random.choice(employees)\n        due_date = datetime.today().strftime('%Y-%m-%d')\n        assignments.append([task, assigned_to, due_date])\n\n    # Create a DataFrame from the assignments\n    df = pd.DataFrame(assignments, columns=['Task Name', 'Assigned To', 'Due Date'])\n\n    return df\ntask_list = [\"Task One\", \"Task Two\", \"Task Three\", \"Task Four\", \"Task Five\"]\nn_tasks = 3"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    \"\"\"\n    Replaces spaces in given words with underscores, then plots the frequency of each unique word.\n    \n    Parameters:\n    mystrings (None): This parameter is not used in the function.\n    text (str): The input text to be processed.\n    \n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plot.\n    \n    Raises:\n    ValueError: If the input text is empty.\n    \"\"\"\n    \n    # Check if input text is empty\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Replace spaces with underscores\n    text = text.replace(\" \", \"_\")\n    \n    # Split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count frequency of each word\n    freq = Counter(words)\n    \n    # Plot frequency of each word\n    fig, ax = plt.subplots()\n    ax.bar(freq.keys(), freq.values())\n    ax.set_xlabel('Word')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency')\n    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to make room for rotated labels\n    \n    # Return the Axes object\n    return ax\ntext = \"This is a sample text with multiple words and some repeated words\""}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n\n    Args:\n        source_directory (str): The path to the source directory.\n        target_directory (str): The path to the target directory.\n        zip_name (str): The name of the zip file to be created.\n\n    Returns:\n        str: The full path to the created zip file.\n\n    Raises:\n        OSError: If the source_directory does not exist.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create target directory if it doesn't exist\n    target_dir_path = Path(target_directory)\n    target_dir_path.mkdir(parents=True, exist_ok=True)\n\n    # Create zip file path\n    zip_file_path = target_dir_path / f\"{zip_name}.zip\"\n\n    # Find files with valid extensions in source directory\n    files_to_zip = []\n    for file in glob.glob(f\"{source_directory}/*\"):\n        file_path = Path(file)\n        if file_path.is_file() and file_path.suffix in valid_extensions:\n            files_to_zip.append(file_path)\n\n    # Create zip file\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        for file in files_to_zip:\n            zip_file.write(file, file.name)\n\n    return str(zip_file_path)\nzip_name = \"my_zip_file\""}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    \n    Args:\n        source_directory (str): The path to the source directory containing files to be converted.\n        target_directory (str): The path to the target directory where converted files will be saved.\n    \n    Returns:\n        int: The number of files successfully converted to CSV.\n    \n    Raises:\n        FileNotFoundError: If the source directory does not exist.\n    \"\"\"\n    \n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory {source_directory} does not exist.\")\n    \n    # Create the target directory if it does not exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # Initialize a counter for the number of files converted\n    converted_count = 0\n    \n    # Iterate over all files in the source directory\n    for file_name in os.listdir(source_directory):\n        file_path = os.path.join(source_directory, file_name)\n        \n        # Check if the file has the correct extension\n        if file_name.endswith(('.txt', '.docx', '.xlsx', '.csv')):\n            # Read the content of the file based on its type\n            if file_name.endswith('.txt'):\n                with open(file_path, 'r') as file:\n                    content = file.read()\n            elif file_name.endswith('.docx'):\n                doc = docx.Document(file_path)\n                content = '\\n'.join([para.text for para in doc.paragraphs])\n            elif file_name.endswith('.xlsx'):\n                df = pd.read_excel(file_path)\n                content = df.to_csv(index=False)\n            elif file_name.endswith('.csv'):\n                df = pd.read_csv(file_path)\n                content = df.to_csv(index=False)\n            \n            # Create a DataFrame with the content\n            df_content = pd.DataFrame({'Text': [content]})\n            \n            # Save the DataFrame to a CSV file in the target directory\n            target_file_path = os.path.join(target_directory, f\"{os.path.splitext(file_name)[0]}.csv\")\n            df_content.to_csv(target_file_path, index=False)\n            \n            # Increment the counter\n            converted_count += 1\n    \n    return converted_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    This function computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \n    Parameters:\n    df (pd.DataFrame): The input DataFrame.\n    \n    Returns:\n    pd.DataFrame: A DataFrame where each column contains the normalized cumulative sum of the respective column in the input DataFrame, retaining the original column names.\n    \n    Raises:\n    TypeError: If the DataFrame contains non-numeric data types.\n    ValueError: If the DataFrame is empty or contains NaN values.\n    \"\"\"\n    \n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check for NaN values in the DataFrame\n    if df.isnull().any().any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    \n    # Check if all columns are numeric\n    if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    \n    # Compute the cumulative sum for each numeric column\n    cumsum_df = df.cumsum()\n    \n    # MinMax normalize the cumulative sum for each column\n    scaler = MinMaxScaler()\n    normalized_cumsum = scaler.fit_transform(cumsum_df)\n    \n    # Convert the normalized array back to a DataFrame with original column names\n    normalized_cumsum_df = pd.DataFrame(normalized_cumsum, columns=df.columns)\n    \n    return normalized_cumsum_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size, \n    creation time, and last modification time without recursing into subdirectories.\n\n    Args:\n        directory_path (str): Path to the directory to analyze.\n\n    Returns:\n        list of tuples: Each tuple contains (file name, file size in bytes, \n            creation time in ISO format, modification time in ISO format).\n\n    Raises:\n        ValueError: If the provided directory does not exist.\n    \"\"\"\n    # Check if the directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"Directory does not exist\")\n\n    # Initialize an empty list to store the results\n    results = []\n\n    # Iterate over the files in the directory\n    for file in os.listdir(directory_path):\n        # Construct the full path to the file\n        file_path = os.path.join(directory_path, file)\n\n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(file_path):\n            # Get the file size in bytes\n            file_size = os.path.getsize(file_path)\n\n            # Get the creation time and modification time\n            creation_time = datetime.fromtimestamp(os.path.getctime(file_path), timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(os.path.getmtime(file_path), timezone.utc).isoformat()\n\n            # Append the result to the list\n            results.append((file, file_size, creation_time, modification_time))\n\n    # Return the list of results\n    return results"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on the provided JSON data.\n\n    Args:\n        json_data (str): The JSON data containing country names and populations.\n        output_dir (str, optional): The directory where the CSV file will be written. Defaults to \".\".\n        file_name (str, optional): The name of the CSV file. Defaults to \"country_population_report.csv\".\n\n    Returns:\n        str: The file path of the generated CSV report.\n        pd.DataFrame: The country-population data loaded from the input JSON, with columns: \"Country\", \"Population\".\n\n    Raises:\n        ValueError: If the JSON data is malformed, empty, contains non-string country names, non-numeric or negative populations.\n        IOError: If the file cannot be written to the specified directory.\n    \"\"\"\n\n    # Validate and parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    \n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n    \n    # Prepare DataFrame\n    df_data = []\n    for country, population in data.items():\n        # Check if the country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Country names must be strings.\")\n        \n        # Check if the population is a non-negative number\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Population must be numeric and non-negative.\")\n        \n        # Round the population down to the nearest integer\n        population = math.floor(population)\n        \n        # Add the country and population to the DataFrame\n        df_data.append({\"Country\": country, \"Population\": population})\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(df_data, columns=[\"Country\", \"Population\"])\n    \n    # Create the output directory if it does not exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create the full file path\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Try to write the DataFrame to the CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n    \n    # Return the file path and the DataFrame\n    return file_path, df\njson_data = '{\"USA\": 331002651, \"China\": 1439323776, \"India\": 1380004385}'\noutput_dir = \".\"\nfile_name = \"country_population_report.csv\""}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n\n    Args:\n        dir_path (str): The path to the directory to be evaluated.\n        predicates (list): A list of functions or regex patterns that take a string (file/directory name) as input.\n\n    Returns:\n        dict: A dictionary with directory items as keys and the results of condition checks as values.\n\n    Raises:\n        ValueError: If no valid predicates are provided.\n        FileNotFoundError: If the specified directory does not exist or is not a directory.\n    \"\"\"\n    # Check if the directory exists and is a directory\n    if not Path(dir_path).is_dir():\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n\n    # Check if predicates are provided\n    if not predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Deduplicate predicates\n    predicates = list(set(predicates))\n\n    # Initialize the result dictionary\n    result = {}\n\n    # Iterate over the directory items\n    for item in os.listdir(dir_path):\n        # Initialize the result for this item\n        item_result = []\n        \n        # Evaluate the item against each predicate\n        for predicate in predicates:\n            if callable(predicate):\n                item_result.append(predicate(item))\n            elif isinstance(predicate, str):\n                try:\n                    # Assume predicate is a regex pattern\n                    item_result.append(bool(re.match(predicate, item)))\n                except re.error:\n                    item_result.append(False)\n            else:\n                item_result.append(False)\n        \n        # Add the item result to the result dictionary\n        result[item] = item_result\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Decode the hexadecimal string to bytes\n    bytes_string = binascii.unhexlify(hex_string)\n\n    # Convert the bytes to a UTF-8 string\n    utf8_string = bytes_string.decode('utf-8')\n\n    # Encode the UTF-8 string into different formats\n    encodings = {\n        'hex': hex_string,\n        'base64': base64.b64encode(utf8_string.encode('utf-8')).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_string if all(ord(c) < 128 for c in utf8_string) else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot13').decode('utf-8')\n    }\n\n    return encodings"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    \"\"\"\n    Downloads a tar.gz file from a specified URL, validates its MD5 checksum against a predefined expected value,\n    and extracts the contents if the checksum matches.\n\n    Args:\n        url (str): The URL of the tar.gz file to download.\n\n    Returns:\n        bool: True if the file is successfully downloaded, its MD5 checksum matches the expected value, and it is extracted.\n              False if the checksum does not match the expected value or if the download fails.\n    \"\"\"\n    try:\n        # Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_checksum = hashlib.md5(open(TARGET_TAR_FILE, 'rb').read()).hexdigest()\n        \n        # Validate the MD5 checksum against the expected value\n        if md5_checksum == EXPECTED_MD5_CHECKSUM:\n            # Extract the contents of the tar.gz file\n            with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n                tar.extractall()\n            return True\n        else:\n            # Delete the downloaded file if the checksum does not match\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        # Handle any exceptions (e.g., download failure, file I/O errors)\n        print(f\"An error occurred: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    Download a CSV file from a given URL, save it to a specified path, \n    and count the occurrences of each value in a particular column.\n\n    Args:\n        url (str): The URL of the CSV file to download.\n        column_name (str): The name of the column to count values in.\n        csv_file_path (str): The path to save the downloaded CSV file.\n\n    Returns:\n        dict: A dictionary mapping the values from the specified column to their\n            corresponding occurrence counts.\n\n    Raises:\n        ValueError: If the specified column_name does not exist in the CSV file.\n        Exception: If an error occurs during file download or processing.\n    \"\"\"\n\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise Exception(f\"Failed to download CSV file from {url}: {e}\")\n\n    try:\n        # Read the CSV file and count occurrences of each value in the specified column\n        value_counts = collections.defaultdict(int)\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            for row in reader:\n                value = row[column_name]\n                value_counts[value] += 1\n    except Exception as e:\n        raise e\n    finally:\n        # Delete the downloaded CSV file\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n\n    return dict(value_counts)\nurl = \"https://example.com/data.csv\"\ncolumn_name = \"column1\"\ncsv_file_path = \"data.csv\""}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    \"\"\"\n    Fetches and parses an XML file from a specified URL, then converts it into a Pandas DataFrame.\n\n    Args:\n        url (str): The URL of the XML file to fetch and parse.\n\n    Returns:\n        pandas.DataFrame: A DataFrame constructed from the parsed XML data.\n\n    Raises:\n        ValueError: If the URL is invalid or the XML file cannot be fetched from the URL,\n                     if the XML file has invalid syntax, or if the XML structure does not conform to the expected format.\n    \"\"\"\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n    except Exception as e:\n        raise ValueError(f\"Failed to fetch XML from URL: {e}\")\n\n    try:\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the XML structure conforms to the expected format\n    if root.tag != 'root' or not all(child.tag == 'item' for child in root):\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Extract data from XML and convert to DataFrame\n    data = []\n    columns = set()\n    for item in root:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n            columns.add(child.tag)\n        data.append(item_data)\n\n    # Convert list of dictionaries to DataFrame\n    df = pd.DataFrame(data, columns=columns)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency of each word,\n    and then plots a bar chart showing the ten most frequently occurring words.\n\n    Args:\n        url (str): The URL of the text file to download.\n\n    Returns:\n        tuple: A tuple containing two elements:\n            Counter: A Counter object from the collections module, containing word frequencies in the text.\n            Axes: A matplotlib Axes object that represents the plotted bar chart of the ten most common words.\n\n    Note:\n        The function assumes the URL points to a plain text file and may not handle binary files or non-text content correctly.\n        Words are identified using a basic regular expression and are case-sensitive.\n        The function does not remove common stopwords; all words are counted as is.\n        Requires internet access to download the file from the URL.\n    \"\"\"\n    # Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_freq = Counter(words)\n\n    # Plot a bar chart showing the ten most frequently occurring words\n    top_10_words = word_freq.most_common(10)\n    words, freqs = zip(*top_10_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, freqs)\n    ax.set_xlabel('Word')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=90)\n\n    # Return the Counter object and the Axes object\n    return word_freq, ax\nurl = 'http://example.com/textfile.txt'"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download path exists\n        if not os.path.exists(download_path):\n            os.makedirs(download_path)\n        \n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        \n        # Check if the content type is zip\n        content_type = response.headers.get('content-type')\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Save the file to the download path\n        file_path = os.path.join(download_path, \"downloaded_file.zip\")\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Extract the ZIP file\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        return download_path\n    \n    except requests.RequestException as e:\n        return f\"Error: Unable to download the file from the provided URL. {e}\"\n    \n    except Exception as e:\n        return f\"Error: {e}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table based on the given 'table_id' on a webpage into a Pandas DataFrame.\n    \n    Args:\n        url (str): The URL of the webpage containing the HTML table.\n        table_id (str): The ID of the HTML table to be extracted.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the data extracted from the specified HTML table.\n    \n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request fails (e.g., due to connection issues or a non-successful status code like 404 or 500).\n        ValueError: If no table with the specified 'table_id' is found on the webpage.\n    \"\"\"\n    \n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for unsuccessful HTTP requests\n    except requests.exceptions.HTTPError as e:\n        raise e\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find the table with the specified ID\n    table = soup.find('table', id=table_id)\n    \n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n    \n    # Check if the table has any rows\n    if not table.find('tr'):\n        return pd.DataFrame()\n    \n    # Convert the table to a DataFrame\n    df = pd.read_html(StringIO(table.prettify()))[0]\n    \n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads and extracts a zip file from a specified URL.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        filename (str): The filename to save the zip file as.\n\n    Returns:\n        tuple: A tuple containing a status message and a list of filenames in the unzipped directory,\n               or an empty list if extraction fails.\n    \"\"\"\n    # Ensure directories exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n\n    try:\n        # Download the zip file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        with open(DOWNLOAD_DIR / filename, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        # Extract the zip file\n        with zipfile.ZipFile(DOWNLOAD_DIR / filename, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get the list of filenames in the unzipped directory\n        filenames = [f.name for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        # Return a tuple containing a status message and the list of filenames\n        return (\"Success\", filenames)\n\n    except requests.exceptions.RequestException as e:\n        # Network-related exceptions\n        return (\"Error: Download failed - \" + str(e), [])\n\n    except (zipfile.BadZipFile, OSError) as e:\n        # File-related exceptions\n        return (\"Error: File-related exception - \" + str(e), [])\n\n    except Exception as e:\n        # Other exceptions\n        return (\"Error: An error occurred - \" + str(e), [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n\n    Args:\n        url (str): The URL of the webpage to scrape.\n        base_url (str, optional): The base URL to use for converting relative URLs to absolute URLs. Defaults to \"https://www.example.com\".\n        csv_file (str, optional): The file path to save the scraped URLs to. Defaults to \"scraped_data.csv\".\n\n    Returns:\n        int: The number of unique absolute links scraped from the webpage.\n    \"\"\"\n\n    # Send a GET request to the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an error for bad status codes\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all hyperlinks in the page\n    links = set()\n    for a_tag in soup.find_all('a', href=True):\n        # Convert relative URLs to absolute URLs\n        absolute_url = urljoin(base_url, a_tag['href'])\n        links.add(absolute_url)\n\n    # Write the unique absolute URLs to a CSV file\n    with open(csv_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['URL'])  # Write header\n        for link in links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    This function parses HTML table data from a specified URL or local file and stores it into an SQLite database.\n    \n    Parameters:\n    webpage_url (str): The URL of the webpage containing the HTML table or the path to a local HTML file.\n    database_name (str): The name of the SQLite database to store the parsed data. Defaults to \"my_database.db\".\n    \n    Returns:\n    int: The number of rows in the parsed HTML table.\n    \n    Raises:\n    requests.RequestException: If there is a network issue in accessing the URL.\n    sqlite3.DatabaseError: If there is an issue connecting to or writing to the SQLite database.\n    \"\"\"\n    \n    try:\n        # Fetch HTML content from URL or local file\n        if webpage_url.startswith(\"http\"):\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            html_content = response.content\n        else:\n            with open(webpage_url, \"r\") as file:\n                html_content = file.read()\n\n        # Parse HTML content using lxml\n        tree = html.fromstring(html_content)\n\n        # Find all tables in the HTML content\n        tables = tree.xpath(\"//table\")\n\n        # Check if there are any tables\n        if not tables:\n            return 0\n\n        # Extract the first table (assuming there's only one)\n        table = tables[0]\n\n        # Convert the table to a pandas DataFrame\n        df = pd.read_html(html.tostring(table, encoding=\"unicode\"))[0]\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n\n        # Drop the existing table (if it exists)\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n\n        # Create a new table with the same structure as the DataFrame\n        df.to_sql(\"my_table\", conn, if_exists=\"replace\", index=False)\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n\n        # Return the number of rows in the parsed HTML table\n        return len(df)\n\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Error fetching HTML content: {e}\")\n\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Error connecting to or writing to SQLite database: {e}\")\n\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    Opens an image file, extracts text using OCR, and converts the text encoding, \n    with a fallback to image comment processing.\n\n    Args:\n        filename (str): The path to the image file. Defaults to IMAGE_PATH.\n        from_encoding (str): The encoding of the text in the image. Defaults to \"cp1251\".\n        to_encoding (str): The target encoding for the extracted text. Defaults to \"utf8\".\n\n    Returns:\n        str: The text extracted from the image or the image comment, converted to the target encoding.\n        If OCR extraction and comment processing both fail, returns an empty string.\n\n    Raises:\n        ValueError: If UnicodeDecodeError or LookupError occurs during conversion.\n        ValueError: If incorrect encodings are provided for the text or comment conversion.\n    \"\"\"\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Attempt to extract text using OCR\n            try:\n                ocr_text = pytesseract.image_to_string(img, lang='eng')\n                # Convert the extracted text to the target encoding\n                converted_text = ocr_text.encode(from_encoding).decode(to_encoding)\n                return converted_text\n            except (UnicodeDecodeError, LookupError) as e:\n                # Fallback to processing image comments if OCR fails\n                try:\n                    comment = img.info.get(\"comment\", \"\")\n                    if comment:\n                        converted_comment = comment.decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                        return converted_comment\n                    else:\n                        return \"\"\n                except (UnicodeDecodeError, LookupError) as e:\n                    raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\")\n    except Exception as e:\n        raise ValueError(\"Incorrect encodings provided for the text or comment conversion\")"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    Fetches data from a specified REST API URL and processes it for JSON parsing.\n    \n    Args:\n    url (str): The URL of the REST API. Defaults to API_URL.\n    from_encoding (str): The encoding of the response content. Defaults to None.\n    to_encoding (str): The target encoding for the response content. Defaults to \"utf8\".\n    \n    Returns:\n    dict: The JSON-parsed data after re-encoding. Returns an empty dictionary if the content is empty.\n    \n    Raises:\n    ValueError: \"Unable to detect encoding for non-empty content\" if it fails to detect the encoding for non-empty response content.\n    \"\"\"\n    \n    try:\n        # Set a timeout of 5 seconds for the API request\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Retrieve the response content in raw bytes\n        content = response.content\n        \n        # Handle cases with empty content\n        if not content:\n            return {}\n        \n        # Detect the encoding of the response content if not specified\n        if from_encoding is None:\n            # Use the chardet library to detect the encoding\n            detected_encoding = chardet.detect(content)\n            from_encoding = detected_encoding.get('encoding')\n            \n            # Raise an exception if the encoding cannot be detected\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n        \n        # Decode the content from the detected encoding\n        decoded_content = content.decode(from_encoding)\n        \n        # Re-encode the content to the target encoding\n        re_encoded_content = decoded_content.encode(to_encoding)\n        \n        # Parse the re-encoded content as JSON\n        parsed_data = json.loads(re_encoded_content.decode(to_encoding))\n        \n        return parsed_data\n    \n    except requests.RequestException as e:\n        # Handle any request exceptions (e.g., timeout, connection error)\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        print(f\"JSON decode error: {e}\")\n        return {}\n    except ValueError as e:\n        # Handle value errors (e.g., encoding detection failure)\n        print(f\"Value error: {e}\")\n        raise"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n    column_name (str): The name of the date column in the CSV file.\n    date_format (str): The format of the date values in the CSV file. Defaults to \"%Y-%m-%d\".\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing the processed data.\n\n    Raises:\n    FileNotFoundError: If the specified CSV file is not found at the given path.\n    ValueError: If the specified column is not present in the CSV file.\n    \"\"\"\n\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # If the file is empty, return an empty DataFrame\n        return pd.DataFrame()\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} does not exist in the CSV file.\")\n\n    # Convert the date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Filter rows based on the current date\n    current_date = datetime.now()\n    df = df[df[column_name] <= current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    Secures a client socket using SSL/TLS and sends back the SHA256 hash of a file requested by the client.\n\n    Args:\n        client_socket (socket): The client socket to secure.\n        cert_file (str): The path to the SSL/TLS certificate file.\n        key_file (str): The path to the SSL/TLS key file.\n        buffer_size (int, optional): The buffer size for reading the file. Defaults to 1024.\n\n    Returns:\n        str: The SHA256 hash of the requested file. If the requested file does not exist, returns 'File not found'.\n    \"\"\"\n\n    try:\n        # Create an SSL/TLS context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(cert_file, key_file)\n\n        # Wrap the client socket with SSL/TLS\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8')\n\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n\n        # Open the file and calculate its SHA256 hash\n        with open(file_path, 'rb') as file:\n            hash = hashlib.sha256()\n            while True:\n                chunk = file.read(buffer_size)\n                if not chunk:\n                    break\n                hash.update(chunk)\n            file_hash = hash.hexdigest()\n\n        # Send the file hash back to the client\n        ssl_socket.sendall(file_hash.encode('utf-8'))\n\n        # Return the file hash\n        return file_hash\n\n    except Exception as e:\n        # Handle any exceptions during processing\n        if 'ssl_socket' in locals():\n            ssl_socket.sendall(f'Error: {str(e)}'.encode('utf-8'))\n        return f'Error: {str(e)}'"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Run a non-blocking echo server that appends the server's current time to received data and sends it back to the client.\n\n    Args:\n        server_address (str): The address of the server. Defaults to \"localhost\".\n        server_port (int): The port of the server. Defaults to 12345.\n        buffer_size (int): The size of the buffer. Defaults to 1024.\n        run_duration (int): The duration of the server's operation in minutes. Defaults to 5.\n\n    Returns:\n        str: A status message indicating the server's operation and run duration.\n    \"\"\"\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n    server.bind((server_address, server_port))\n    server.listen(5)\n\n    # Create lists to hold sockets to be monitored\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n\n    # Set the start time\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration * 60)\n\n    while inputs and datetime.now() < end_time:\n        # Wait for incoming connections or messages\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n        # Handle incoming connections\n        for s in readable:\n            if s is server:\n                # Accept new connections\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                # Receive data from the client\n                data = s.recv(buffer_size)\n                if data:\n                    # Append the server's current time to the received data\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    response = f\"{data.decode()} (Server Time: {current_time})\"\n                    message_queues[s].put(response)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Remove the socket from the list if no data is received\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        # Handle messages in the queue\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                # Send the data back to the client\n                s.send(next_msg.encode())\n\n        # Handle exceptional conditions\n        for s in exceptional:\n            # Remove the socket from the list\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Close all sockets\n    for s in inputs:\n        s.close()\n\n    # Return a status message\n    run_time = datetime.now() - start_time\n    return f\"Server operated for {run_time.total_seconds()} seconds.\""}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive data from the client socket\n    data = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Prompt for email details\n    sender_email = input(\"Enter sender's email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    sender_password = getpass.getpass(\"Enter sender's email password: \")\n    \n    # Create the email\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client socket'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    \n    # Send the email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()  # Secure the connection\n        server.login(sender_email, sender_password)\n        server.send_message(msg)\n        server.quit()\n        print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    # Close the client socket\n    client_socket.close()"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    This function reads a CSV file, splits the text into words, and creates a histogram of the frequency of the top ten most common words, excluding a predefined set of stopwords.\n\n    Args:\n        file_path (str): The path to the CSV file.\n        save_path (str, optional): The path to save the histogram. If not provided, the histogram is displayed on the screen. Defaults to None.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object of the plot if save_path is not provided.\n        None: If save_path is provided, the plot is saved to the specified path, and the function returns None.\n    \"\"\"\n    try:\n        # Check if the file exists\n        try:\n            with open(file_path):\n                pass\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The file at path {file_path} was not found.\")\n\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n\n        # Ensure the DataFrame has at least one column\n        if df.empty or df.columns.size == 0:\n            raise Exception(\"The CSV file is empty or has no columns.\")\n\n        # Use the 'Text' column if present, otherwise use the first column\n        text_column = 'Text' if 'Text' in df.columns else df.columns[0]\n        text_data = df[text_column]\n\n        # Initialize CountVectorizer with stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        X = vectorizer.fit_transform(text_data)\n\n        # Get word frequencies\n        word_frequencies = X.toarray().sum(axis=0)\n        word_freq_dict = dict(zip(vectorizer.get_feature_names_out(), word_frequencies))\n\n        # Sort words by frequency and get top 10\n        top_words = sorted(word_freq_dict.items(), key=lambda item: item[1], reverse=True)[:10]\n\n        # Plotting\n        words, frequencies = zip(*top_words)\n        fig, ax = plt.subplots()\n        ax.barh(words, frequencies)\n        ax.set_xlabel('Frequency')\n        ax.set_ylabel('Words')\n        ax.set_title('Top 10 Most Common Words')\n\n        # Save or display the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close(fig)\n            return None\n        else:\n            plt.show()\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Predefined lists in case of missing parameters\n    predefined_animals = ['Lion', 'Tiger', 'Bear']\n    predefined_foods = ['Meat', 'Fish', 'Vegetables']\n    \n    # Handle missing or empty lists\n    if animals is None or len(animals) == 0:\n        animals = predefined_animals\n    if foods is None or len(foods) == 0:\n        foods = predefined_foods\n    \n    # If both lists are empty, return an empty DataFrame\n    if len(animals) == 0 and len(foods) == 0:\n        return pd.DataFrame()\n    \n    # Generate all possible combinations\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations randomly\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    \n    # Pivot the DataFrame to have animals as rows and foods as columns\n    df['Value'] = df['Animal'] + ':' + df['Food']\n    df = df.pivot(index='Animal', columns='Food', values='Value')\n    \n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates the average time difference in seconds between each consecutive pair of timestamps in a given list,\n    after converting them to a specified timezone.\n\n    Args:\n        time_strings (list): A list of timestamp strings in the format '%Y-%m-%d %H:%M:%S'.\n        timezone (str): A string representing the timezone to convert the timestamps to.\n\n    Returns:\n        float: The mean (average) time difference in seconds between each consecutive pair of timestamps.\n        If there are less than two timestamps in the list, the function returns 0.0.\n    \"\"\"\n    # Convert time strings to datetime objects in the specified timezone\n    tz = pytz.timezone(timezone)\n    dt_objects = [datetime.strptime(t, '%Y-%m-%d %H:%M:%S').replace(tzinfo=pytz.utc).astimezone(tz) for t in time_strings]\n\n    # If there are less than two timestamps, return 0.0\n    if len(dt_objects) < 2:\n        return 0.0\n\n    # Calculate the absolute time difference in seconds between each consecutive pair of timestamps\n    time_diffs = [(dt_objects[i+1] - dt_objects[i]).total_seconds() for i in range(len(dt_objects) - 1)]\n\n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n\n    # Calculate the average time difference using numpy's mean function\n    avg_time_diff = np.mean(time_diffs)\n\n    return avg_time_diff\ntime_strings = [\"2023-01-01 12:00:00\", \"2023-01-01 13:00:00\", \"2023-01-01 14:00:00\"]\ntimezone = \"America/New_York\""}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Analyzes the frequency of words in a given text and plots the top 10 most common words.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        list: A list of tuples containing the 10 most common words and their counts.\n        Axes: The matplotlib Axes object of the bar chart.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Plot the top 10 most common words\n    fig, ax = plt.subplots()\n    ax.bar([word for word, count in top_10_words], [count for word, count in top_10_words])\n    ax.set_xlabel('Word')\n    ax.set_ylabel('Count')\n    ax.set_title('Top 10 Most Common Words')\n    ax.tick_params(axis='x', rotation=90)\n    \n    # Return the list of top 10 words and the Axes object\n    return top_10_words, ax\ntext = \"This is a sample text. This text is just a sample.\""}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    \"\"\"\n    Extracts a URL from a given string and retrieves the title of the web page from that URL.\n    \n    Args:\n    myString (str): The input string that may contain a URL.\n    \n    Returns:\n    str: The title of the webpage at the extracted URL if successful, otherwise one of the following error messages:\n         \"No valid URL found in the provided string.\"\n         \"Unable to fetch the content of the URL: {url}\"\n         \"No title tag found in the webpage.\"\n    \"\"\"\n    \n    # Regular expression to find URLs\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    urls = re.findall(url_pattern, myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    url = urls[0]\n    \n    # Check if the URL is valid\n    try:\n        result = urlparse(url)\n        if all([result.scheme, result.netloc]):\n            pass\n        else:\n            return \"No valid URL found in the provided string.\"\n    except ValueError:\n        return \"No valid URL found in the provided string.\"\n    \n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the HTML content\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find the title tag\n    title = soup.find('title')\n    \n    if title is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title.text.strip()"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, \n    downloads the file from the URL, and saves it with a timestamped filename.\n\n    Args:\n        json_data (str): The JSON string to parse.\n        unknown_key (str): The key to find in the JSON data.\n        save_dir (str, optional): The directory to save the file in. Defaults to None.\n\n    Returns:\n        str: The absolute path of the downloaded file.\n    \"\"\"\n    \n    # Parse the JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        print(f\"Error: Failed to parse JSON data - {e}\")\n        return None\n    \n    # Find the URL associated with the specified key\n    url = data.get(unknown_key)\n    \n    # Check if URL is found\n    if url is None:\n        print(f\"Error: No URL found for key '{unknown_key}'\")\n        return None\n    \n    # Download file from URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad status codes\n    except requests.RequestException as e:\n        print(f\"Error: Failed to download file from URL '{url}' - {e}\")\n        return None\n    \n    # Create timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Save file to specified directory or current working directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    file_path = os.path.join(save_dir, filename)\n    \n    # Ensure the save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    try:\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n    except Exception as e:\n        print(f\"Error: Failed to save file to '{file_path}' - {e}\")\n        return None\n    \n    # Output absolute path of downloaded file\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n\n    Args:\n        url (str): The URL or local file path to extract phone numbers from.\n        output_path (str): The file path to save the extracted phone numbers to.\n\n    Returns:\n        list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Check if URL is a valid URL\n    if not url.startswith('http'):\n        # If not, assume it's a local file and read its contents\n        with open(url, 'r') as f:\n            text = f.read()\n    else:\n        # If it's a URL, fetch its contents using requests\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure the request was successful\n        text = response.text\n\n    # Parse the HTML content using BeautifulSoup (if it's HTML)\n    if url.startswith('http'):\n        soup = bs4.BeautifulSoup(text, 'html.parser')\n        text = soup.get_text()\n\n    # Extract phone numbers using regular expression\n    phone_regex = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n    phone_numbers = phone_regex.findall(text)\n\n    # Save phone numbers to JSON file\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers"}
