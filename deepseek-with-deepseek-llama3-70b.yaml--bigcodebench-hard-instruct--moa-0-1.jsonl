{"task_id": "BigCodeBench/13", "solution": "Here's a self-contained Python function that meets the specified requirements. This function uses `ftplib` to connect to the FTP server, navigate to the specified directory, and then uses `wget` via a subprocess to download all files from that directory. It handles exceptions as specified and returns a list of filenames that were attempted to be downloaded.\n\n```python\nimport subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # List all files in the directory\n    files = ftp.nlst()\n    \n    # Close the FTP connection\n    ftp.quit()\n    \n    # Use wget to download all files\n    for file in files:\n        try:\n            subprocess.run(['wget', '-r', '-nH', '--cut-dirs=1', '--user', ftp_user, '--password', ftp_password, '--no-parent', f'ftp://{ftp_server}{ftp_dir}/{file}'], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download file {file}: {str(e)}\")\n    \n    return files\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        downloaded_files = task_func()\n        print(f\"Files attempted to be downloaded: {downloaded_files}\")\n    except Exception as e:\n        print(e)\n```\n\nThis function first attempts to connect and log in to the FTP server using `ftplib`. It then changes to the specified directory and lists all files present. After closing the FTP connection, it uses `wget` with the `-r`, `-nH`, `--cut-dirs=1`, `--user`, `--password`, and `--no-parent` options to download each file. The function returns a list of filenames that were successfully attempted to be downloaded. If any step fails, it raises an exception with the appropriate error message."}
{"task_id": "BigCodeBench/15", "solution": "Here's the complete Python function that meets the specified requirements:\n\n```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Ensure the output directory exists\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    output_files = []\n    \n    # Read commands from CSV file\n    with open(commands_file_path, mode='r') as file:\n        reader = csv.reader(file)\n        for idx, command in enumerate(reader):\n            if command:  # Check if the command is not empty\n                output_file_path = os.path.join(output_dir_path, f'command_{idx}_output.txt')\n                output_files.append(output_file_path)\n                \n                try:\n                    # Execute the command\n                    result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(result.stdout)\n                except subprocess.CalledProcessError as e:\n                    # Handle command execution failure\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(f\"Command failed with error: {e.stderr}\\nExit code: {e.returncode}\")\n    \n    return output_files\n\n# Example usage:\n# output_files = task_func('commands.csv', 'output_directory')\n# print(output_files)\n```\n\nThis function reads commands from a CSV file, executes each command, and saves the output to a unique file in the specified output directory. If a command fails, the error message and exit code are written to the respective output file. The function also handles the case where the commands file does not exist by raising a `FileNotFoundError`."}
{"task_id": "BigCodeBench/17", "solution": "Here's a self-contained Python function that accomplishes the task described:\n\n```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is running, terminate it\n            proc.terminate()\n            time.sleep(1)  # wait for the process to terminate\n            # Restart the process\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    # Process is not running, start it\n    subprocess.Popen([process_name])\n    return f\"Process not found. Starting {process_name}.\"\n\n# Example usage:\n# print(task_func(\"notepad.exe\"))\n```\n\nThis function uses the `psutil` library to iterate through all running processes and check if any of them match the given `process_name`. If a matching process is found, it is terminated using the `terminate()` method, and then the process is restarted using `subprocess.Popen()`. If no matching process is found, the process is simply started. The function returns a string indicating the action taken.\n\nNote: This code assumes that the process name is enough to identify and start the process. In some cases, you might need to provide the full path to the executable or additional arguments. Also, be cautious when terminating processes, as this can lead to data loss or other unintended consequences if not handled properly."}
{"task_id": "BigCodeBench/19", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The specified directory does not exist: {directory}\")\n    \n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n    \n    # If there are no files, return None\n    if not files:\n        return None\n    \n    # Create the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n    \n    # Create the zip file and add all files to it\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_path\n```\n\nThis function does the following:\n1. Checks if the specified directory exists and raises a `FileNotFoundError` if it does not.\n2. Collects all files in the directory using `glob` and filters out subdirectories.\n3. Returns `None` if there are no files in the directory.\n4. Creates a zip file named `files.zip` in the specified directory and adds all files to it.\n5. Returns the path to the created zip file."}
{"task_id": "BigCodeBench/34", "solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    text_without_urls = re.sub(r'http\\S+', '', text)\n    \n    # Split the text into words\n    words = text_without_urls.split()\n    \n    # Check if there are any words available to generate a word cloud\n    if len(words) == 0:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud\n\n# Example usage:\n# text = \"This is a sample text with a URL https://www.example.com. This text will be used to generate a word cloud.\"\n# task_func(text)\n```\n\nThis function:\n1. Removes URLs from the input text using a regular expression.\n2. Checks if there are any words left after removing URLs.\n3. Generates a word cloud from the remaining words.\n4. Plots the word cloud using `matplotlib`.\n5. Returns the `WordCloud` object.\n\nYou can test this function by uncommenting the example usage at the end and providing a string with or without URLs."}
{"task_id": "BigCodeBench/37", "solution": "Here's a self-contained function that accomplishes the task described. This function trains a Random Forest classifier on the input DataFrame, calculates the feature importances, and plots them in a bar plot sorted by importance.\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    # Separate the features and the target variable\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Initialize and train the Random Forest classifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Calculate feature importances\n    feature_importances = pd.DataFrame({\n        'Feature': X.columns,\n        'Importance': rf_classifier.feature_importances_\n    })\n    \n    # Sort the feature importances by descending order\n    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n    \n    # Plotting\n    ax = sns.barplot(x='Importance', y='Feature', data=feature_importances)\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    \n    # Show the plot\n    plt.show()\n    \n    return rf_classifier, ax\n\n# Example usage:\n# Assuming 'df' is your DataFrame and 'target_column' is the name of your target column\n# rf_classifier, ax = task_func(df, 'target_column')\n```\n\nThis function takes a DataFrame `df` and the name of the target column `target_column` as inputs. It trains a Random Forest classifier, computes the feature importances, and plots them in a bar plot using Seaborn. The function returns the trained Random Forest classifier and the Axes object of the plotted data."}
{"task_id": "BigCodeBench/82", "solution": "```python\nfrom flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    users = [User(1, 'user1', 'password1'), User(2, 'user2', 'password2')]\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users:\n            if user.id == int(user_id):\n                return user\n        return None\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.username == form.username.data and user.check_password(form.password.data):\n                    login_user(user)\n                    return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', username=current_user.username)\n\n    return app\n```\n\nThis code defines a Flask application with the following features:\n- User authentication using Flask-Login.\n- A login form using Flask-WTF.\n- Routes for login, logout, and a protected page.\n- Dynamic configuration for security (SECRET_KEY) and template rendering (template_folder).\n\nYou can customize the `users` list to fetch users from a database in a real application. The example provided uses a hardcoded list of users for simplicity."}
{"task_id": "BigCodeBench/89", "solution": "Here's a self-contained function that accomplishes the task described:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Standardize the column\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = stats.zscore(standardized_data)\n    \n    # Identify and remove outliers\n    outlier_indices = np.where(np.abs(z_scores) > outlier_z_score)[0]\n    data_without_outliers = data.drop(outlier_indices)\n    \n    # Visualize the data\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], label='Data with Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_without_outliers, outlier_indices\n\n# Example usage:\n# Assuming 'data' is a pandas DataFrame and 'column' is the name of the column to process\n# import pandas as pd\n# data = pd.DataFrame({\n#     'column_name': [1, 2, 3, 100, 5, 6, 7, 8, 9, 10]\n# })\n# result = task_func(data, 'column_name', 3)\n```\n\nThis function takes a pandas DataFrame, the name of the column to process, and a Z-score threshold for identifying outliers. It standardizes the specified column, calculates the Z-scores, identifies and removes the outliers, and visualizes the data before and after the outlier removal. The function returns a tuple containing the original data, the data without outliers, and the indices of the outliers."}
{"task_id": "BigCodeBench/92", "solution": "Here's a self-contained Python function that performs K-means clustering on a dataset, validates the input, and generates a scatter plot visualizing the clusters and their centroids:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on a dataset and generate a scatter plot visualizing the clusters and their centroids.\n\n    Args:\n        data (pd.DataFrame): Input dataset.\n        n_clusters (int, optional): Number of clusters. Defaults to 3.\n\n    Returns:\n        tuple: A tuple containing the cluster labels and the Axes object with the scatter plot.\n\n    Raises:\n        ValueError: If 'data' is not a pd.DataFrame.\n        ValueError: If 'n_clusters' is not an integer greater than 1.\n    \"\"\"\n\n    # Check if 'data' is a pd.DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pd.DataFrame\")\n\n    # Check if 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(data)\n\n    # Generate a scatter plot visualizing the clusters and their centroids\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis')\n    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='*', s=200)\n\n    # Adding legend for clusters\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n\n    return cluster_labels, ax\n\n# Example usage:\n# Assuming 'df' is a DataFrame with at least two columns\n# df = pd.DataFrame({\n#     'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n#     'feature2': [1, 3, 2, 5, 4, 7, 6, 9, 8, 10]\n# })\n# labels, ax = task_func(df, n_clusters=3)\n# plt.show()\n```\n\nThis function `task_func` takes a DataFrame `data` and an integer `n_clusters` as input. It validates the inputs, performs K-means clustering, and then plots the clusters and their centroids. The function returns the cluster labels and the `Axes` object containing the scatter plot."}
{"task_id": "BigCodeBench/93", "solution": "Here's a self-contained Python function that performs Principal Component Analysis (PCA) on a given dataset, handles the specified exception for `n_components` being not a positive integer, and generates a scatter plot of the transformed data. The function returns the transformed data as a DataFrame and the matplotlib Axes object containing the scatter plot.\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert transformed data to DataFrame for output\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    transformed_df = pd.DataFrame(data=transformed_data, columns=columns)\n    \n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(transformed_df['PC1'], transformed_df['PC2'])\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n    else:\n        ax.plot(transformed_df.iloc[:, 0], marker='o')\n        ax.set_xlabel('Sample Index')\n        ax.set_ylabel(f'Principal Component {1}')\n    \n    ax.set_title('PCA Transformed Data')\n    plt.show()\n    \n    return transformed_df, ax\n\n# Example usage:\n# data = pd.DataFrame(np.random.rand(10, 5))  # Replace with your actual dataset\n# transformed_df, ax = task_func(data, n_components=2)\n```\n\nThis function takes a dataset (`data`) and an optional parameter (`n_components`) specifying the number of principal components to retain. It performs PCA, transforms the data, and then plots the first two principal components if `n_components` is 2 or more. If `n_components` is 1, it plots the single principal component against the sample index. The function returns the transformed data as a DataFrame and the matplotlib Axes object containing the scatter plot."}
{"task_id": "BigCodeBench/99", "solution": "Here's the self-contained code that accomplishes the task described:\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Series(iris.target).map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    g = sns.pairplot(iris_df, hue='species', markers=[\"o\", \"s\", \"D\"])\n    g.fig.suptitle('Iris Dataset Pair Plot', y=1.02)  # Set the title\n\n    # Label the axes\n    for i, j in zip(*plt.np.triu_indices_from(g.axes, 1)):\n        g.axes[i, j].set_visible(False)\n    for i in range(len(iris.feature_names)):\n        for j in range(len(iris.feature_names)):\n            if i != j:\n                g.axes[i, j].set_xlabel(iris.feature_names[j])\n                g.axes[i, j].set_ylabel(iris.feature_names[i])\n\n    # Return the figure object\n    return g.fig\n\n# Example usage\nif __name__ == \"__main__\":\n    fig = task_func()\n    plt.show()\n```\n\nThis code defines a function `task_func` that loads the iris dataset, sets the global font to Arial, creates a seaborn pair plot, sets the title, and labels the axes appropriately. The function returns a matplotlib Figure object containing the plot. The example usage at the end demonstrates how to call the function and display the plot."}
{"task_id": "BigCodeBench/100", "solution": "Here is the self-contained Python function that meets the specified requirements:\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate dates for the past 30 days\n        end_date = datetime.today()\n        start_date = end_date - timedelta(days=30)\n        dates = pd.date_range(start=start_date, end=end_date, freq='D')\n\n        # Generate random values for each date\n        values = [random.random() for _ in range(len(dates))]\n\n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Plotting\n        plt.rcParams['font.family'] = 'Arial'\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'], marker='o')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Example usage:\n# ax = task_func(seed=42)\n# plt.show()\n```\n\nThis function sets a seed for random number generation to ensure reproducibility. It generates a series of dates for the past 30 days and corresponding random values. It then creates a plot using Matplotlib, setting the font to Arial and labeling the axes and title as specified. If any error occurs during the process, it raises a `ValueError` with a descriptive message."}
{"task_id": "BigCodeBench/101", "solution": "Here's a self-contained function that loads the Boston Housing dataset from a specified URL, generates a correlation heatmap using Seaborn, and provides an option to save the plot to a specified file. The function also handles potential errors by raising a `ValueError` if any issue occurs during the process.\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load the Boston Housing dataset\n        data = pd.read_csv(data_url, sep=\"\\s+\", header=None)\n        data.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n\n        # Calculate the correlation matrix\n        corr_matrix = data.corr()\n\n        # Generate the heatmap\n        plt.figure(figsize=(12, 10))\n        ax = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n\n        # Save the plot if a save path is provided\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Example usage:\n# ax = task_func(save_path=\"correlation_heatmap.png\")\n# plt.show()\n```\n\nThis function does the following:\n1. Loads the Boston Housing dataset from the specified URL using `pandas.read_csv`.\n2. Parses the data correctly by specifying the separator and column names.\n3. Computes the correlation matrix for the dataset.\n4. Creates a heatmap using Seaborn.\n5. Optionally saves the heatmap to a specified file path.\n6. Raises a `ValueError` if any exception occurs during the process."}
{"task_id": "BigCodeBench/108", "solution": "Here is the self-contained function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    df (DataFrame): Input DataFrame with a 'value' column.\n    freq (str): Frequency string (e.g., 'D' for daily, 'M' for monthly). Defaults to 'D'.\n    decomposition_model (str): Decomposition model ('additive' or 'multiplicative'). Defaults to 'multiplicative'.\n\n    Returns:\n    tuple: A tuple containing the decomposition result (DecomposeResult object) and the matplotlib Axes object.\n\n    Raises:\n    ValueError: If 'df' is not a DataFrame, lacks required columns, or contains invalid data types.\n    ValueError: If 'freq' is not a valid frequency string.\n    ValueError: If 'decomposition_model' is not 'additive' or 'multiplicative'.\n    \"\"\"\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a DataFrame\")\n\n    # Check if df has required columns\n    required_columns = ['value']\n    if not all(col in df.columns for col in required_columns):\n        raise ValueError(\"Input 'df' must have a 'value' column\")\n\n    # Check if df contains invalid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"Input 'df' must have numeric data types in 'value' column\")\n\n    # Check if freq is a valid frequency string\n    valid_frequencies = ['D', 'M', 'Q', 'A']\n    if freq not in valid_frequencies:\n        raise ValueError(\"Invalid frequency string 'freq'. Valid options are 'D', 'M', 'Q', 'A'.\")\n\n    # Check if decomposition_model is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid decomposition model 'decomposition_model'. Valid options are 'additive' or 'multiplicative'.\")\n\n    # Perform seasonal decomposition\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=int(freq[:-1]))\n\n    # Plot the decomposition result\n    fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n    decomposition_result.observed.plot(ax=ax[0], legend=False)\n    ax[0].set_ylabel('Observed')\n    decomposition_result.trend.plot(ax=ax[1], legend=False)\n    ax[1].set_ylabel('Trend')\n    decomposition_result.seasonal.plot(ax=ax[2], legend=False)\n    ax[2].set_ylabel('Seasonal')\n    decomposition_result.resid.plot(ax=ax[3], legend=False)\n    ax[3].set_ylabel('Residual')\n    plt.tight_layout()\n\n    return decomposition_result, ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'date': pd.date_range(start='2023-01-01', periods=365, freq='D'),\n#     'value': np.random.randn(365)\n# })\n# df.set_index('date', inplace=True)\n# result, ax = task_func(df, freq='D', decomposition_model='additive')\n# plt.show()\n```\n\nThis function includes all the necessary checks and raises `ValueError` exceptions as specified. It also performs the seasonal decomposition and plots the results using `matplotlib`. The function returns a tuple containing the decomposition result (`DecomposeResult` object) and the `matplotlib` Axes object."}
{"task_id": "BigCodeBench/120", "solution": "Here's the self-contained code for the function `task_func` that generates a pandas Series of random dates within a specified date range, including both `start_date` and `end_date`, with an optional seed for reproducibility:\n\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range, including both start_date and end_date, with an optional seed for reproducibility.\n\n    Args:\n        start_date (datetime.datetime): The start date of the range (inclusive). Defaults to datetime(2020, 1, 1).\n        end_date (datetime.datetime): The end date of the range (inclusive). Defaults to datetime(2020, 12, 31).\n        seed (int): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.Series: A Series object containing random dates within the specified range, with each date being a datetime.datetime object. The series length matches the number of days in the specified range.\n\n    Raises:\n        ValueError: If 'start_date' or 'end_date' is not a datetime.datetime instance, or if 'start_date' is later than 'end_date'.\n    \"\"\"\n    # Check if start_date and end_date are datetime instances\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both start_date and end_date must be datetime.datetime instances\")\n\n    # Check if start_date is not later than end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n\n    # Set the seed for the random number generator\n    random_seed(seed)\n\n    # Calculate the number of days in the range\n    num_days = (end_date - start_date).days + 1\n\n    # Generate a list of random dates within the range\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    # Create a pandas Series from the list of random dates\n    series = pd.Series(random_dates)\n\n    return series\n\n# Example usage:\n# series_of_dates = task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 1, 10), seed=42)\n# print(series_of_dates)\n```\n\nThis function first checks if the provided `start_date` and `end_date` are instances of `datetime.datetime` and ensures that `start_date` is not later than `end_date`. It then calculates the number of days between the two dates (inclusive) and uses this to generate a list of random dates within the range. The list is converted to a pandas Series and returned. The seed is set using `random_seed` to ensure reproducibility."}
{"task_id": "BigCodeBench/123", "solution": "Here's the self-contained Python function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The 'my_list' parameter must be a list.\")\n    \n    # Add '12' to the list\n    my_list.append(12)\n    \n    # Calculate the number of files to concatenate\n    num_files_to_concat = sum(my_list)\n    \n    # Get the list of CSV files in the directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(file_pattern)\n    \n    # Check if there are any files in the directory\n    if not files:\n        raise FileNotFoundError(f\"No files found in the directory '{file_dir}' with extension '{file_ext}'.\")\n    \n    # Ensure we do not try to concatenate more files than available\n    num_files_to_concat = min(num_files_to_concat, len(files))\n    \n    # Read and concatenate the selected CSV files into a DataFrame\n    dfs = [pd.read_csv(file) for file in files[:num_files_to_concat]]\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n    \n    return concatenated_df\n\n# Example usage:\n# df = task_func([1, 2, 3], './data_files/', '.csv')\n```\n\nThis function performs the following steps:\n1. Checks if `my_list` is a list and raises a `TypeError` if it is not.\n2. Appends the integer `12` to `my_list`.\n3. Calculates the number of files to concatenate based on the sum of the numbers in `my_list`.\n4. Retrieves the list of CSV files from the specified directory.\n5. Raises a `FileNotFoundError` if no files are found in the directory.\n6. Ensures that the number of files to concatenate does not exceed the number of files available.\n7. Reads and concatenates the selected CSV files into a single pandas DataFrame.\n8. Returns the concatenated DataFrame."}
{"task_id": "BigCodeBench/124", "solution": "Here is the self-contained Python function that meets all the specified requirements:\n\n```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Check if my_list contains only numeric elements\n    if not all(isinstance(item, (int, float)) for item in my_list):\n        raise ValueError(\"my_list must contain only numeric elements (int or float)\")\n    \n    # Append 12 to my_list\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    sum_of_elements = sum(my_list)\n    \n    # Determine the size of the random numbers list\n    random_list_size = min(sum_of_elements, size)\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Generate a list of random integers\n    random_numbers = [randint(1, 100) for _ in range(random_list_size)]\n    \n    # End timing\n    end_time = time.time()\n    \n    # Calculate the time taken\n    time_taken = end_time - start_time\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Random Numbers')\n    \n    # Return the time taken and the Axes object\n    return (time_taken, ax)\n\n# Example usage:\n# result = task_func([1, 2, 3], size=50)\n# plt.show()  # This would display the histogram plot\n```\n\nThis function `task_func` performs the following steps:\n1. Checks if `my_list` is a list and if it contains only numeric elements, raising appropriate exceptions if not.\n2. Appends the number 12 to `my_list`.\n3. Calculates the sum of the elements in `my_list` and determines the size of the random numbers list based on this sum, with an upper limit set by `size`.\n4. Seeds the random number generator with the provided seed.\n5. Measures the time taken to generate a list of random integers between 1 and 100.\n6. Plots a histogram of the generated random numbers with appropriate labels and titles.\n7. Returns a tuple containing the time taken to generate the list of random numbers and the matplotlib Axes object for the histogram."}
{"task_id": "BigCodeBench/129", "solution": "Here's a self-contained function that meets the specified requirements:\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Failed to connect to {url}: {e}\")\n\n    try:\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Find the first table in the parsed content\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page.\")\n        \n        # Extract headers if they exist\n        headers = []\n        header_row = table.find('tr')\n        if header_row:\n            headers = [header.text.strip() for header in header_row.find_all('th')]\n        \n        # Extract rows from the table\n        rows = []\n        for row in table.find_all('tr')[1:]:  # Skip the header row if it exists\n            cells = row.find_all('td')\n            if cells:\n                rows.append([cell.text.strip() for cell in cells])\n        \n        # Create a DataFrame\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n        \n        return df\n    except Exception as e:\n        raise ValueError(f\"Failed to parse the page content: {e}\")\n\n# Example usage:\n# df = task_func('http://example.com')\n# print(df)\n```\n\nThis function performs the following steps:\n1. Sends a GET request to the specified URL and handles potential connection errors.\n2. Parses the HTML content of the page using BeautifulSoup.\n3. Locates the first table in the HTML content.\n4. Extracts headers from the table if they are present.\n5. Extracts data from each row of the table, skipping the header row if it exists.\n6. Constructs a Pandas DataFrame from the extracted data, using the headers if available.\n7. Raises appropriate exceptions for connection issues, HTTP errors, and parsing errors."}
{"task_id": "BigCodeBench/139", "solution": "Here's a self-contained function that meets the requirements you've described:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n    \n    Parameters:\n    df (pd.DataFrame): Input DataFrame.\n    \n    Returns:\n    list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \n    Raises:\n    ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    \n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n    \n    # Select numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    # Check if there are numeric columns\n    if not numeric_cols.any():\n        raise ValueError(\"No numeric columns in the DataFrame\")\n    \n    # Create a figure with subplots for each numeric column\n    fig, axs = plt.subplots(len(numeric_cols), figsize=(8, 6*len(numeric_cols)))\n    \n    # If there's only one numeric column, axs is a single Axes object, not a list\n    if len(numeric_cols) == 1:\n        axs = [axs]\n    \n    # Create histograms for each numeric column\n    for i, col in enumerate(numeric_cols):\n        axs[i].hist(df[col], bins=10)\n        axs[i].set_title(col)\n        axs[i].set_xlabel('Value')\n        axs[i].set_ylabel('Frequency')\n    \n    # Layout so plots do not overlap\n    fig.tight_layout()\n    \n    # Return the list of Axes objects\n    return axs\n\n# Example usage:\n# df = pd.DataFrame({\n#     'A': [1, 2, 3, 4, 5],\n#     'B': [5, 4, 3, 2, 1],\n#     'C': ['a', 'b', 'c', 'd', 'e']\n# })\n# histograms = task_func(df)\n# plt.show()\n```\n\nThis function `task_func` takes a DataFrame as input, checks if it's non-empty and contains numeric columns, and then generates histograms for each numeric column. It returns a list of Matplotlib Axes objects, each representing a histogram for a numeric column. The example usage at the end demonstrates how you might call this function and display the histograms."}
{"task_id": "BigCodeBench/147", "solution": "Here's a self-contained Python function that scans a specified IP address range and checks if a specified port is open on each IP. The function uses threading to speed up the scanning process and returns a dictionary with IP addresses as keys and a boolean indicating the port's status:\n\n```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    # Dictionary to store the results\n    results = {}\n\n    # Function to check if a port is open on a single IP address\n    def check_port(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(1)\n            try:\n                sock.connect((ip, port))\n                results[ip] = True\n            except (socket.timeout, ConnectionRefusedError, socket.error):\n                results[ip] = False\n\n    # List of IP addresses to check\n    ip_addresses = [str(ip) for ip in IPv4Network(ip_range, strict=False)]\n\n    # Create and start threads for each IP address\n    threads = []\n    for ip in ip_addresses:\n        thread = Thread(target=check_port, args=(ip,))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Return the results dictionary\n    return results\n\n# Example usage:\n# ip_range = '192.168.1.0/24'\n# port = 80\n# print(task_func(ip_range, port))\n```\n\nThis function, `task_func`, takes an IP range in CIDR notation and a port number as input. It uses the `ipaddress` module to iterate over each IP address in the range and the `socket` module to check if the specified port is open on each IP. The function uses threading to perform the checks concurrently, which can significantly speed up the scanning process for larger IP ranges. The result is a dictionary where each key is an IP address and the value is a boolean indicating whether the port is open (`True`) or closed (`False`)."}
{"task_id": "BigCodeBench/161", "solution": "Here's a self-contained Python function that reads a log file, extracts the required information, and stores it in a CSV file. It also handles the specified exceptions.\n\n```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information from a log file and stores the data in a CSV format.\n    \n    Args:\n        log_file (str): The path to the log file.\n    \n    Returns:\n        str: The file path to the newly created CSV file which contains the structured log data.\n    \n    Raises:\n        ValueError: If the timestamp in any log entry is invalid or if no valid log entries are found.\n    \"\"\"\n\n    # Regular expression pattern to match log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.*)'\n\n    # Initialize an empty list to store the extracted log data\n    log_data = []\n\n    # Open the log file and read it line by line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use the regular expression pattern to match the log entry\n            match = re.match(pattern, line)\n            \n            # If the log entry matches the pattern, extract the data\n            if match:\n                # Extract the message type, timestamp, and message\n                msg_type, timestamp, message = match.groups()\n                \n                # Try to parse the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    # If the timestamp is invalid, raise a ValueError\n                    raise ValueError(f\"Invalid timestamp: {timestamp}\")\n                \n                # Append the extracted data to the log_data list\n                log_data.append({\n                    'Message Type': msg_type,\n                    'Timestamp': timestamp,\n                    'Message': message.strip()\n                })\n    \n    # If no valid log entries are found, raise a ValueError\n    if not log_data:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create a Pandas DataFrame from the log_data list\n    df = pd.DataFrame(log_data)\n    \n    # Create a CSV file path by replacing the log file extension with '.csv'\n    csv_file = log_file.replace('.log', '.csv')\n    \n    # Write the DataFrame to the CSV file\n    df.to_csv(csv_file, index=False)\n    \n    # Return the CSV file path\n    return csv_file\n\n# Example usage:\n# csv_path = task_func('path_to_log_file.log')\n# print(f\"CSV file created at: {csv_path}\")\n```\n\nThis function reads a log file, uses a regular expression to match the log format, validates the timestamps, and then stores the extracted data in a pandas DataFrame. The DataFrame is then written to a CSV file, and the function returns the path to this CSV file. If there are any invalid timestamps or no valid log entries, it raises a `ValueError`."}
{"task_id": "BigCodeBench/162", "solution": "Here is a self-contained Python function that analyzes and visualizes the distribution of word lengths in a given text using `matplotlib` for plotting the histogram:\n\n```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a text.\n\n    Parameters:\n    text (str): The input text to analyze.\n    rwidth (float, optional): The relative width of the histogram bars. Defaults to 0.8.\n\n    Returns:\n    matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \"\"\"\n    # Use regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Filter out words with length 0 (if any)\n    words = [word for word in words if len(word) > 0]\n    \n    # Calculate the lengths of all words\n    word_lengths = [len(word) for word in words]\n    \n    # If there are no words, return an empty plot\n    if not word_lengths:\n        fig, ax = plt.subplots()\n        ax.set_title('Word Length Distribution')\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        return ax\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    bins = np.arange(1, max(word_lengths) + 2) - 0.5\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, edgecolor='black')\n    \n    # Set plot labels and title\n    ax.set_title('Word Length Distribution')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    \n    # Return the Axes object containing the histogram\n    return ax\n\n# Example usage:\n# text = \"This is an example sentence to test the function.\"\n# ax = task_func(text)\n# plt.show()\n```\n\nThis function takes a text string as input and generates a histogram of the word lengths. It uses regular expressions to extract words from the text, filters out any words with length 0, calculates their lengths, and then plots the distribution of these lengths. The `rwidth` parameter controls the width of the bars in the histogram. If there are no words in the input text, it returns an empty plot with appropriate labels."}
{"task_id": "BigCodeBench/177", "solution": "Here's a self-contained function that meets the specified requirements:\n\n```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\n\n# Ensure you have the necessary NLTK data files\nnltk.download('punkt')\n\ndef task_func(df):\n    # Check if DataFrame is empty or missing necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Filter articles with titles containing \"like\" or \"what\" (case-insensitive)\n    filtered_df = df[df['Title'].str.contains('like|what', case=False, na=False)]\n\n    # Initialize a list to store words\n    words = []\n\n    # Process each article's content\n    for content in filtered_df['Content']:\n        # Remove punctuation from the content\n        content_no_punct = re.sub('['+punctuation+']', '', content)\n        # Tokenize the content into words\n        content_words = nltk.word_tokenize(content_no_punct)\n        # Convert to lowercase and add to the list\n        words.extend([word.lower() for word in content_words])\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    return dict(word_freq)\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Title': ['I like Python', 'What is Java', 'Python vs Java'],\n#     'Content': ['Python is great like no other', 'Java is what I use for backend', 'Comparing Python and Java']\n# })\n# print(task_func(df))\n```\n\nThis function first checks if the provided DataFrame is empty or lacks the required columns. It then filters the DataFrame to include only those articles whose titles contain the keywords \"like\" or \"what\" (case-insensitive). It removes punctuation from the content, tokenizes it into words, converts them to lowercase, and counts the frequency of each word. The function finally returns a dictionary with words as keys and their frequencies as values."}
{"task_id": "BigCodeBench/184", "solution": "Here's a self-contained function that accomplishes the task described:\n\n```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Preprocessing function\n    def preprocess_text(text):\n        # Remove numbers and punctuation\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Convert to lowercase and split into words\n        words = text.lower().split()\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n        return ' '.join(words)\n\n    # Apply preprocessing to the specified column\n    dataframe['cleaned_text'] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe['cleaned_text'])\n\n    # Convert the vectorized data to a DataFrame\n    vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return vectorized_df\n\n# Example usage:\n# df = pd.DataFrame({\n#     'text': [\"Hello, how are you?\", \"I am fine, thank you!\", \"What's up?\"]\n# })\n# result_df = task_func(df, 'text')\n# print(result_df)\n```\n\nThis function `task_func` takes a DataFrame and a column name as input. It first preprocesses the text by removing numbers, punctuation, and stopwords. Then, it applies the `CountVectorizer` from `sklearn` to convert the cleaned text into a numeric format, resulting in a DataFrame where each word is a column and their counts are rows."}
{"task_id": "BigCodeBench/187", "solution": "```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges.\n\n    Parameters:\n    dic (dict): A dictionary containing the ranges for longitude and latitude. Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    cities (list): A list of city names. Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n    GeoDataFrame: A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects)\n    \"\"\"\n    # Check if 'Lon' and 'Lat' keys are present in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys\")\n\n    # Check if values for 'Lon' and 'Lat' are tuples\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values for 'Lon' and 'Lat' must be tuples\")\n\n    # Generate random coordinates for each city\n    coordinates = [Point(np.random.uniform(dic['Lon'][0], dic['Lon'][1]), np.random.uniform(dic['Lat'][0], dic['Lat'][1])) for _ in cities]\n\n    # Create a GeoPandas DataFrame\n    data = {'City': cities, 'Coordinates': coordinates}\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n\n    return gdf\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        gdf = task_func()\n        print(gdf)\n    except ValueError as e:\n        print(e)\n```\n\nThis function starts by validating the input dictionary to ensure it contains the necessary keys and that their values are tuples. It then generates random coordinates within the specified ranges for each city and constructs a GeoDataFrame with the city names and their corresponding Point geometries. The example usage at the bottom demonstrates how to call the function and handle potential `ValueError` exceptions."}
{"task_id": "BigCodeBench/199", "solution": "Here is the self-contained Python function that generates a weather report for specified cities at a given UTC datetime. The function includes error handling for incorrect input types and formats, and it uses a random seed for reproducibility of weather conditions.\n\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n\n    Args:\n        utc_datetime (datetime): The UTC datetime for which the weather report is generated.\n        cities (list, optional): A list of city names. Defaults to ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'].\n        weather_conditions (list, optional): A list of possible weather conditions. Defaults to ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'].\n        timezones (dict, optional): A dictionary mapping city names to their respective timezones. Defaults to {'New York': 'America/New_York', 'London': 'Europe/London', 'Beijing': 'Asia/Shanghai', 'Tokyo': 'Asia/Tokyo', 'Sydney': 'Australia/Sydney'}.\n        seed (int, optional): The seed for the random number generator. Defaults to 42.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the weather report.\n\n    Raises:\n        ValueError: If utc_datetime is not a datetime object or if any of the other parameters are not in the expected format.\n    \"\"\"\n\n    # Validate input types and formats\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All elements in cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All elements in weather_conditions must be strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n\n    # Set the random seed for reproducibility\n    set_seed(seed)\n\n    # Initialize an empty list to store the weather report data\n    weather_data = []\n\n    # Loop through each city to generate the weather report\n    for city in cities:\n        # Get the timezone for the city\n        timezone = pytz.timezone(timezones[city])\n        # Convert UTC datetime to local datetime\n        local_datetime = utc_datetime.replace(tzinfo=pytz.utc).astimezone(timezone)\n        # Format the local datetime\n        local_time_str = local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')\n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        # Append the data to the list\n        weather_data.append([city, local_time_str, weather_condition])\n\n    # Create a DataFrame from the weather data\n    df = pd.DataFrame(weather_data, columns=['City', 'Local Time', 'Weather Condition'])\n\n    return df\n\n# Example usage:\n# utc_now = datetime.utcnow()\n# print(task_func(utc_now))\n```\n\nThis function takes a UTC datetime and generates a weather report for the specified cities. It uses predefined weather conditions and timezones, and it sets a random seed for consistent results. The function validates the input types and formats, and it raises a `ValueError` if any of the inputs are incorrect. The output is a pandas DataFrame containing the weather report with columns for the city name, local time, and weather condition."}
{"task_id": "BigCodeBench/208", "solution": "Here's a self-contained Python function that generates a random walk sequence of steps, calculates descriptive statistics, and plots the random walk. The function includes error handling for invalid input and uses `numpy`, `matplotlib`, and `pandas` for calculations and plotting.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random walk\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = pd.Series(walk).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    stats_dict = {\n        'count': stats['count'],\n        'mean': stats['mean'],\n        'std': stats['std'],\n        'min': stats['min'],\n        '5th percentile': stats['5%'],\n        '25th percentile': stats['25%'],\n        'median': stats['50%'],\n        '75th percentile': stats['75%'],\n        '95th percentile': stats['95%'],\n        'max': stats['max']\n    }\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    \n    # Return descriptive statistics and plot\n    return stats_dict, ax\n\n# Example usage:\n# stats, plot = task_func(1000)\n# print(stats)\n# plt.show()\n```\n\nThis function `task_func` takes an integer `elements` and an optional `seed` for reproducibility. It generates a random walk by randomly choosing steps of -1 or 1, computes the cumulative sum to simulate the walk, and then calculates and returns descriptive statistics along with a plot of the walk. The plot is created using `matplotlib`, and the descriptive statistics are computed using `pandas`."}
{"task_id": "BigCodeBench/211", "solution": "Here's a self-contained Python function that downloads a zip file from a given URL, extracts its contents to the specified directory, and returns a list of the extracted files. The function also keeps the downloaded zip file in the destination directory:\n\n```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Download a zip file from a URL, extract its contents to the specified directory, and return the list of extracted files.\n    The downloaded zip file is kept in the destination directory.\n\n    Args:\n        url (str): The URL of the zip file to download.\n        destination_directory (str): The directory where the zip file's contents will be extracted.\n        headers (dict, optional): HTTP headers to include in the request. Defaults to None.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers, stream=True)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Save the zip file to the destination directory\n    zip_file_path = os.path.join(destination_directory, 'downloaded.zip')\n    with open(zip_file_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            file.write(chunk)\n\n    # Extract the zip file\n    extracted_files = []\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n\n    return extracted_files\n\n# Example usage:\n# files = task_func('http://example.com/path/to/file.zip', '/path/to/destination')\n# print(files)\n```\n\nThis function performs the following steps:\n1. Checks if the destination directory exists and creates it if it doesn't.\n2. Downloads the zip file from the provided URL.\n3. Saves the downloaded zip file to the destination directory with the name `downloaded.zip`.\n4. Extracts the contents of the zip file to the destination directory.\n5. Collects a list of the extracted files.\n6. Returns the list of extracted files.\n\nThe downloaded zip file is kept in the destination directory for reference or further use."}
{"task_id": "BigCodeBench/214", "solution": "Here's a self-contained Python function that generates a random RGB image using the specified parameters. The function includes error handling for the case where `range_low` is not less than `range_high`, and it uses Matplotlib to display the image.\n\n```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Args:\n        seed (int, optional): Seed for the random number generator. Defaults to 42.\n        image_size (tuple, optional): Size of the image. Defaults to (100, 100, 3).\n        range_low (int, optional): Lower bound of the random values. Defaults to 0.\n        range_high (int, optional): Upper bound of the random values. Defaults to 255.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axes object of the plot.\n        image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set the seed for the random number generator\n    random.seed(seed)\n\n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis labels\n\n    # Return the axis and the image\n    return ax, image\n\n# Example usage\nif __name__ == \"__main__\":\n    ax, image = task_func()\n    plt.show()\n```\n\nThis function sets a seed for reproducibility, generates a random RGB image using `numpy.random.randint`, and displays the image using Matplotlib. The function returns the Axes object and the image array. If the `range_low` is not less than `range_high`, it raises a `ValueError`."}
{"task_id": "BigCodeBench/227", "solution": "```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file '{audio_file}' does not exist.\")\n\n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the Sound Pressure Level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / np.max(np.abs(matrix)) * spl\n\n    # Generate the spectrogram\n    D = librosa.stft(data)\n    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n\n    # Plot the spectrogram with logarithmic frequency scale and linear time scale\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(S_db, sr=samplerate, x_axis='time', y_axis='log', ax=ax)\n    ax.set_title('Spectrogram with Logarithmic Frequency Scale')\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n\n    return normalized_matrix, fig\n\n# Example usage:\n# L = [...list of values...]\n# M = 100\n# N = 100\n# audio_file = 'path_to_audio_file.wav'\n# normalized_matrix, fig = task_func(L, M, N, audio_file)\n# plt.show()\n```\n\n### Explanation:\n1. **File Check**: The function first checks if the specified audio file exists. If not, it raises a `FileNotFoundError`.\n2. **Audio Reading**: It reads the audio file using the `soundfile` library.\n3. **SPL Calculation**: The Sound Pressure Level (SPL) is calculated using the provided formula.\n4. **Matrix Creation and Normalization**: A matrix of size MxN is created from the list `L` and normalized by scaling it based on the SPL.\n5. **Spectrogram Generation**: The Short-Time Fourier Transform (STFT) is computed using `librosa.stft`, and the amplitude is converted to decibels. The spectrogram is displayed with a logarithmic frequency scale and a linear time scale.\n6. **Return Values**: The function returns the normalized matrix and the figure object for the spectrogram.\n\nMake sure to replace `'path_to_audio_file.wav'` with the actual path to your audio file when using the function."}
{"task_id": "BigCodeBench/239", "solution": "Here's a self-contained Python function that accomplishes the task described:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = [value for tup in original for value in tup if isinstance(value, (int, float))]\n    numeric_array = np.array(numeric_values)\n\n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_array),\n        'standard_deviation': np.std(numeric_array),\n        'minimum': np.min(numeric_array),\n        'maximum': np.max(numeric_array)\n    }\n\n    # Generate a histogram with an overlaid probability density function (PDF)\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, density=True, alpha=0.6, bins='auto')\n    ax.set_title('Histogram with Overlaid PDF')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Probability Density')\n\n    # Fit a normal distribution to the data and plot the PDF\n    mean, std_dev = stats.norm.fit(numeric_array)\n    x = np.linspace(numeric_array.min(), numeric_array.max(), 100)\n    pdf = stats.norm.pdf(x, mean, std_dev)\n    ax.plot(x, pdf, 'r--', label='Normal Distribution PDF')\n    ax.legend()\n\n    return numeric_array, statistics, ax\n\n# Example usage\n# original_data = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n# array, stats, ax = task_func(original_data)\n# print(\"Numeric Array:\", array)\n# print(\"Statistics:\", stats)\n# plt.show()\n```\n\nThis function `task_func` takes a list of tuples as input, extracts numeric values from these tuples, computes basic statistics (mean, standard deviation, minimum, and maximum), and generates a histogram with an overlaid probability density function (PDF). The histogram is plotted with `density=True` to normalize the histogram, `alpha=0.6` for transparency, and `bins='auto'` for automatic bin selection. The function returns a numpy array of the numeric values, a dictionary of basic statistics, and a matplotlib Axes object containing the histogram and PDF plot."}
{"task_id": "BigCodeBench/241", "solution": "Here's a self-contained function that meets the specified requirements:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Create a numeric array from the \"original\" list\n    original_array = np.array(original)\n    \n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])[0]\n    \n    # Draw the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array, label='Normalized')\n    ax.legend()\n    ax.set_title('Original and Normalized Arrays')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Show the plot\n    plt.show()\n    \n    return original_array, normalized_array, ax\n\n# Example usage\noriginal_list = [1, 2, 3, 4, 5]\noriginal_array, normalized_array, ax = task_func(original_list)\n```\n\nThis function does the following:\n1. Converts the input list `original` into a numpy array `original_array`.\n2. Normalizes the `original_array` using `sklearn.preprocessing.normalize`.\n3. Creates a plot with matplotlib, plotting both the original and normalized arrays.\n4. Returns the original numpy array, the normalized numpy array, and the matplotlib axes object.\n\nYou can test this function with different lists of data to see how it handles and visualizes the normalization process."}
{"task_id": "BigCodeBench/267", "solution": "Here is the self-contained code that performs the specified operations:\n\n```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\"\n    signal = np.array(list(data.values()))\n    \n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal)) * sample_rate\n    \n    # Step 4: Plot and return the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.plot(freqs, np.abs(fft_signal))\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('FFT of the Signal')\n    \n    # Return the FFT of the signal and the plot axes\n    return (fft_signal, ax)\n\n# Example usage:\n# data = {'b': 2, 'c': 3}\n# fft_result, plot_axes = task_func(data)\n# plt.show()  # To display the plot\n```\n\nThis function does the following:\n1. Adds a new key \"a\" with the value 1 to the input dictionary `data`.\n2. Converts the dictionary values into a numpy array to form a signal. This step assumes that the dictionary values can be directly used as a time-series signal.\n3. Computes the FFT of the signal using `scipy.fftpack`.\n4. Plots the FFT result using `matplotlib` and returns both the FFT result and the plot axes.\n\nNote: The function assumes that the dictionary values can be directly used as a time-series signal. If the dictionary values represent something else (e.g., parameters for generating a signal), additional steps would be needed to generate the actual signal before performing the FFT."}
{"task_id": "BigCodeBench/273", "solution": "Here's a self-contained Python class that handles HTTP POST requests and validates incoming data according to the specifications provided:\n\n```python\nimport cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    class DataHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check the Content-Type header\n            if 'content-type' not in self.headers or self.headers['content-type'] != 'application/json':\n                self.send_error(400, \"Content-Type header is not application/json\")\n                return\n\n            # Read the length of the request and parse the JSON data\n            length = int(self.headers['content-length'])\n            try:\n                data = self.rfile.read(length).decode('utf-8')\n                json_data = json.loads(data)\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON\")\n                return\n\n            # Check if the 'data' key is present in the JSON object\n            if 'data' not in json_data:\n                self.send_error(400, \"No data key in request\")\n                return\n\n            # If everything is valid, send a success response\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            response = json.dumps(SUCCESS_RESPONSE)\n            self.wfile.write(response.encode('utf-8'))\n\n        def send_error(self, code, message):\n            self.send_response(code)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            error_response = json.dumps({**ERROR_RESPONSE, 'message': message})\n            self.wfile.write(error_response.encode('utf-8'))\n\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, DataHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()\n\nif __name__ == '__main__':\n    task_func()\n```\n\nThis code defines a `DataHandler` class that inherits from `http.server.BaseHTTPRequestHandler`. It overrides the `do_POST` method to handle POST requests, validate the incoming data, and respond accordingly. The `send_error` method is a helper function to send error responses in a consistent format. The `task_func` function sets up and starts the HTTP server on port 8000."}
{"task_id": "BigCodeBench/274", "solution": "Here's a self-contained Python class that handles HTTP POST requests to process incoming email data and sends an email using the provided SMTP server details. The class extends `http.server.BaseHTTPRequestHandler` to handle the HTTP requests.\n\n```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Read the content-length header\n            content_length = int(self.headers['Content-Length'])\n            # Read the request body\n            post_data = self.rfile.read(content_length)\n            \n            try:\n                # Parse the JSON data\n                email_data = json.loads(post_data)\n                \n                # Check for required keys\n                if not all(key in email_data for key in ('subject', 'message', 'to')):\n                    raise ValueError(\"Missing required keys in JSON data\")\n                \n                # Prepare the email\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n                \n                # Send the email\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n                \n                # Send a success response\n                self.send_response(200)\n                self.send_header('Content-type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'Email sent successfully')\n            \n            except json.JSONDecodeError:\n                self.send_error(400, 'Invalid JSON')\n            except ValueError as e:\n                self.send_error(400, str(e))\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, 'Authentication Failed')\n            except Exception as e:\n                self.send_error(500, str(e))\n\n    return EmailRequestHandler\n\n# Example usage:\n# handler_class = task_func('smtp.example.com', 587, 'user@example.com', 'password')\n# server = http.server.HTTPServer(('localhost', 8080), handler_class)\n# server.serve_forever()\n```\n\nThis code defines a class `EmailRequestHandler` that handles POST requests. It reads the request body, parses the JSON data, checks for required keys, constructs an email message, and sends it using the provided SMTP server details. It handles various exceptions and sends appropriate HTTP responses based on the error encountered."}
{"task_id": "BigCodeBench/287", "solution": "Here's a self-contained Python function that accomplishes the task described. This function will count the number of words in all `.txt` files within a specified directory, export the counts to a JSON file, and then return the total number of words.\n\n```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    # Initialize a counter for words\n    word_counter = Counter()\n    \n    # Walk through the directory to find .txt files\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    # Read the file content and split into words\n                    content = f.read()\n                    words = content.split()\n                    word_counter[file] = len(words)\n    \n    # Calculate the total number of words\n    total_words = sum(word_counter.values())\n    \n    # Export the counts to a JSON file\n    with open(filename, 'w', encoding='utf-8') as json_file:\n        json.dump(word_counter, json_file, ensure_ascii=False, indent=4)\n    \n    return total_words\n\n# Example usage:\n# total_words = task_func('word_counts.json', '/path/to/directory')\n# print(total_words)\n```\n\n### Explanation:\n1. **Imports**: The necessary modules (`collections.Counter`, `os`, and `json`) are imported.\n2. **Counter Initialization**: A `Counter` object is initialized to keep track of word counts for each file.\n3. **Directory Traversal**: The `os.walk` function is used to traverse the directory and find all `.txt` files.\n4. **File Reading and Word Counting**: For each `.txt` file, the content is read, split into words, and the word count is added to the `Counter`.\n5. **Total Word Calculation**: The total number of words is calculated by summing the values in the `Counter`.\n6. **JSON Export**: The word counts are exported to a JSON file specified by `filename`.\n7. **Return Value**: The function returns the total number of words.\n\nYou can call this function with the desired JSON filename and directory path to get the total word count and generate the JSON file with word counts for each `.txt` file."}
{"task_id": "BigCodeBench/302", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Process a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Parameters:\n        df (pandas DataFrame): Input DataFrame with 'Date' and 'Value' columns.\n        plot (bool, optional): Whether to visualize the correlation matrix using a heatmap. Defaults to False.\n\n    Returns:\n        DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n        Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n\n    Raises:\n        ValueError: If the DataFrame input is empty or has invalid 'Value'.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if the 'Value' column exists and contains lists\n    if 'Value' not in df.columns or not df['Value'].apply(lambda x: isinstance(x, list)).all():\n        raise ValueError(\"Invalid 'Value' column\")\n\n    # Split the lists in the 'Value' column into separate columns\n    df_split = df['Value'].apply(pd.Series)\n\n    # Calculate the Pearson correlation coefficient between the columns\n    corr_df = df_split.corr()\n\n    # Optionally visualize the correlation matrix using a heatmap\n    if plot:\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(corr_df, annot=True, cmap='coolwarm', square=True)\n        plt.title('Correlation Heatmap')\n        plt.show()\n\n    return corr_df, plt.gca() if plot else None\n```\n\nThis function performs the following steps:\n1. Checks if the DataFrame is empty or if the 'Value' column contains invalid data (non-list entries).\n2. Splits the lists in the 'Value' column into separate columns.\n3. Calculates the Pearson correlation coefficient matrix.\n4. Optionally plots the correlation matrix as a heatmap with the title \"Correlation Heatmap\" if the `plot` parameter is set to `True`.\n5. Returns the correlation matrix and optionally the Axes object containing the heatmap plot."}
{"task_id": "BigCodeBench/308", "solution": "Here's a self-contained Python function that generates a report on students' grades in different subjects, calculates the average grade for each student and each subject, and outputs a pandas DataFrame:\n\n```python\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {student: [random.randint(0, 100) for _ in all_fields] for student in STUDENTS}\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, index=all_fields)\n    \n    # Calculate average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate average grade for each subject\n    df.loc['Average'] = df.mean()\n    \n    # Ensure 'Average Grade' column is at the end\n    cols = df.columns.tolist()\n    cols.remove('Average Grade')\n    cols.append('Average Grade')\n    df = df[cols]\n    \n    return df\n\n# Example usage\nadditional_fields = ['Art', 'Music']\nreport = task_func(additional_fields)\nprint(report)\n```\n\n### Explanation:\n1. **Data Generation**: The function generates random grades for each student in each subject. The grades are integers between 0 and 100.\n2. **DataFrame Creation**: A pandas DataFrame is created where each row represents a subject and each column represents a student.\n3. **Average Calculation**:\n   - The average grade for each student is calculated and added as a new column named 'Average Grade'.\n   - The average grade for each subject is calculated and added as a new row named 'Average'.\n4. **Column Ordering**: The 'Average Grade' column is moved to the end to maintain a consistent format.\n5. **Output**: The function returns the DataFrame containing the grades, the average grade for each student, and the average grade for each subject.\n\nThis function can be called with additional fields to include more subjects in the report."}
{"task_id": "BigCodeBench/310", "solution": "Here's a self-contained Python function that generates a CSV file with simulated data for 100 people, including their names, ages, heights, and weights. It also calculates and appends the average age, height, and weight at the end of the file. The function returns the path of the created CSV file.\n\n```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate random data for 100 people\n    people_data = []\n    for i in range(PEOPLE_COUNT):\n        name = f\"Person {i + 1}\"\n        age = random.randint(18, 65)\n        height = round(random.uniform(150, 200), 2)\n        weight = round(random.uniform(50, 120), 2)\n        people_data.append([name, age, height, weight])\n    \n    # Calculate averages\n    ages = [person[1] for person in people_data]\n    heights = [person[2] for person in people_data]\n    weights = [person[3] for person in people_data]\n    average_age = round(mean(ages), 2)\n    average_height = round(mean(heights), 2)\n    average_weight = round(mean(weights), 2)\n    \n    # Append averages to the data\n    people_data.append(['Average', average_age, average_height, average_weight])\n    \n    # Write data to CSV file\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(people_data)\n    \n    return os.path.abspath(filename)\n\n# Example usage\ncsv_filename = 'people_data.csv'\ncreated_file_path = task_func(csv_filename)\nprint(f\"CSV file created at: {created_file_path}\")\n```\n\nThis code defines a function `task_func` that takes a filename as input, generates the simulated data, calculates the averages, and writes everything to a CSV file. The function then returns the absolute path of the created CSV file. The example usage at the end demonstrates how to call this function and print the path of the created file."}
{"task_id": "BigCodeBench/313", "solution": "Here's a Python function that organizes files in a directory based on the first text that is not enclosed in square brackets, moving them to subdirectories named after this text. If no matching text is found, the file is not moved. The function returns a tuple containing the directory path and a dictionary mapping subdirectory names to lists of files moved into them.\n\n```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Regular expression pattern to match the first text not enclosed in square brackets\n    pattern = r'^\\[[^]]*\\]\\s*([^\\[]+)'\n\n    # Dictionary to store the created subdirectories and their corresponding files\n    subdirectories = {}\n\n    # Iterate over the files in the directory\n    for filename in os.listdir(directory):\n        # Construct the full file path\n        filepath = os.path.join(directory, filename)\n\n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(filepath):\n            # Use the regular expression pattern to extract the first text not enclosed in square brackets\n            match = re.search(pattern, filename)\n\n            # If a match is found, move the file to the corresponding subdirectory\n            if match:\n                # Get the matched text (the first text not enclosed in square brackets)\n                subdirectory_name = match.group(1).strip()\n\n                # Create the subdirectory if it doesn't exist\n                subdirectory_path = os.path.join(directory, subdirectory_name)\n                if not os.path.exists(subdirectory_path):\n                    os.makedirs(subdirectory_path)\n\n                # Move the file to the subdirectory\n                shutil.move(filepath, subdirectory_path)\n\n                # Add the file to the list of files in the subdirectory\n                if subdirectory_name not in subdirectories:\n                    subdirectories[subdirectory_name] = []\n                subdirectories[subdirectory_name].append(filename)\n\n    # Return the directory path with organized files and the dictionary of subdirectories and files\n    return directory, subdirectories\n\n# Example usage:\n# result = task_func('/path/to/directory')\n# print(result)\n```\n\nThis function `task_func` takes a directory path as input, processes each file in the directory to find the first text not enclosed in square brackets, creates subdirectories based on this text, and moves the files into the appropriate subdirectories. It returns a tuple containing the directory path and a dictionary mapping subdirectory names to lists of files moved into them."}
{"task_id": "BigCodeBench/324", "solution": "```python\nimport subprocess\nimport threading\n\ndef task_func(file_list):\n    # Initialize list to store exit codes\n    exit_codes = [None] * len(file_list)\n\n    # Function to run a subprocess\n    def run_subprocess(index, file_path):\n        try:\n            # Run the subprocess and capture the exit code\n            exit_codes[index] = subprocess.call(['python', file_path])\n        except Exception as e:\n            # Handle any exceptions and set exit code to 1\n            print(f\"Error running {file_path}: {e}\")\n            exit_codes[index] = 1\n\n    # Create and start a thread for each file\n    threads = []\n    for i, file_path in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(i, file_path))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n\n# Example usage\nif __name__ == \"__main__\":\n    files = ['script1.py', 'script2.py', 'script3.py']\n    exit_codes = task_func(files)\n    print(\"Exit codes:\", exit_codes)\n```\n\n### Explanation:\n1. **Initialization**:\n   - `exit_codes` is initialized with `None` values for each file in the list. This list will store the exit codes of the subprocesses.\n\n2. **Thread Function (`run_subprocess`)**:\n   - This function takes an `index` and a `file_path` as arguments.\n   - It runs the file using `subprocess.call` with `python` as the interpreter.\n   - The exit code of the subprocess is stored in the `exit_codes` list at the corresponding index.\n   - Any exceptions during the subprocess execution are caught, and the exit code is set to 1.\n\n3. **Creating and Starting Threads**:\n   - For each file in the `file_list`, a new thread is created and started. Each thread runs the `run_subprocess` function with the appropriate index and file path.\n\n4. **Joining Threads**:\n   - The main thread waits for all the subprocess threads to complete by calling `join` on each thread.\n\n5. **Return Exit Codes**:\n   - Once all threads have completed, the function returns the list of exit codes.\n\nThis approach ensures that all files are run concurrently and the exit codes are collected and returned once all subprocesses have finished."}
{"task_id": "BigCodeBench/326", "solution": "Here's a Python function that accomplishes the task of finding and running all `.bat` files in a given directory, returning their file names and exit codes:\n\n```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Find and run all .bat files in a given directory, returning their file names and exit codes.\n\n    Args:\n        directory_path (str): The path to the directory containing the .bat files.\n\n    Returns:\n        list of tuples: A list where each tuple contains the file name and its exit code.\n        The exit code is None if the file could not be executed.\n    \"\"\"\n    # Initialize an empty list to store the results\n    results = []\n\n    # Use glob to find all .bat files in the directory\n    for file in glob.glob(os.path.join(directory_path, \"*.bat\")):\n        try:\n            # Run the .bat file using subprocess and capture the exit code\n            exit_code = subprocess.call([file])\n            results.append((os.path.basename(file), exit_code))\n        except Exception as e:\n            # If an error occurs, append the file name with an exit code of None\n            results.append((os.path.basename(file), None))\n            print(f\"Error running {file}: {str(e)}\", file=sys.stderr)\n\n    return results\n\n# Example usage\ndirectory_path = r\"C:\\path\\to\\directory\"\nresults = task_func(directory_path)\nfor file, exit_code in results:\n    print(f\"{file}: {exit_code}\")\n```\n\nThis function uses the `glob` module to find all `.bat` files in the specified directory. It then attempts to run each file using the `subprocess.call` function, which returns the exit code of the process. If an error occurs while running a file, the function catches the exception, prints an error message, and appends the file name with an exit code of `None` to the results list. Finally, the function returns the list of results.\n\nIn the example usage, replace `r\"C:\\path\\to\\directory\"` with the actual path to the directory containing the `.bat` files. The function will print the file name and exit code for each file."}
{"task_id": "BigCodeBench/341", "solution": "Here's a self-contained function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame.\")\n    \n    # Check if df is not empty\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(\"The specified column must exist in the DataFrame.\")\n    \n    # Create a figure with two subplots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    # Plot histogram with kernel density estimate on the first subplot\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    axes[0].set_title(f'Histogram and KDE of {col}')\n    \n    # Plot box plot on the second subplot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box Plot of {col}')\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Return the figure\n    return fig\n\n# Example usage:\n# df = pd.DataFrame({'A': [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]})\n# fig = task_func(df, 'A')\n# plt.show()\n```\n\nThis function `task_func` takes a pandas DataFrame `df` and a column name `col` as inputs. It first checks if the inputs meet the specified conditions (DataFrame, not empty, and contains the column). If any condition is not met, it raises a `ValueError`. If all conditions are met, it creates a figure with two subplots: a histogram with a kernel density estimate on the first subplot and a box plot on the second subplot. Finally, it returns the matplotlib figure object containing these plots."}
{"task_id": "BigCodeBench/346", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n\n    Args:\n        script_path (str): The path to the Python script.\n        wait (bool, optional): Whether to wait for the process to complete. Defaults to True.\n        *args: Variable number of arguments to pass to the script.\n\n    Returns:\n        int: The return code of the subprocess. If 'wait' is False, returns None.\n\n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(\"The script does not exist.\")\n    \n    # Prepare the command to run the script with arguments\n    command = [sys.executable, script_path] + list(args)\n    \n    # Run the script as a subprocess\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # If wait is True, wait for the process to complete and check for errors\n    if wait:\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, command, output=stdout, stderr=stderr)\n        return process.returncode\n    else:\n        return None\n\n# Example usage:\n# task_func('path_to_script.py', True, 'arg1', 'arg2')\n```\n\n### Explanation:\n1. **Script Existence Check**: The function first checks if the script file exists using `os.path.isfile(script_path)`. If it does not exist, it raises a `ValueError`.\n2. **Command Preparation**: The command to run the script is prepared using `sys.executable` (which is the path to the Python interpreter) and appending the script path and arguments.\n3. **Subprocess Execution**: The script is executed using `subprocess.Popen`. The `stdout` and `stderr` are captured for potential error handling.\n4. **Waiting and Error Handling**: If `wait` is `True`, the function waits for the process to complete using `process.communicate()`. If the return code is non-zero, it raises a `subprocess.CalledProcessError` with the details of the error. If `wait` is `False`, the function immediately returns `None`.\n\nThis function is designed to be self-contained and handles the specified exceptions and behaviors."}
{"task_id": "BigCodeBench/360", "solution": "Here's a self-contained Python function that accomplishes the task described:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    # Load the data from the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"The sheet named '{sheet_name}' does not exist in the workbook.\") from e\n    \n    # Calculate mean and standard deviation for each column\n    stats = df.describe().loc[['mean', 'std']].T.to_dict()\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    index = np.arange(len(df.columns))\n    bar_width = 0.35\n    \n    rects1 = ax.bar(index, [stats[col]['mean'] for col in df.columns], bar_width, label='Mean')\n    rects2 = ax.bar(index + bar_width, [stats[col]['std'] for col in df.columns], bar_width, label='Standard Deviation')\n    \n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(df.columns)\n    ax.legend()\n    \n    fig.tight_layout()\n    \n    return stats, fig\n\n# Example usage:\n# stats, figure = task_func('path_to_excel_file.xlsx', 'Sheet1')\n# plt.show()  # To display the figure\n```\n\nThis function does the following:\n1. Checks if the specified Excel file exists.\n2. Attempts to load the specified sheet from the Excel file.\n3. Calculates the mean and standard deviation for each column in the DataFrame.\n4. Creates a bar chart using `matplotlib` to visualize the mean and standard deviation of each column.\n5. Returns a dictionary containing the mean and standard deviation for each column and a `matplotlib` figure object.\n\nMake sure to replace `'path_to_excel_file.xlsx'` and `'Sheet1'` with the actual file path and sheet name when calling the function."}
{"task_id": "BigCodeBench/367", "solution": "Here's the self-contained code for the function `task_func` that takes a list of activities (which should be datetime objects) and returns a bar chart of the number of activities performed on each day of the week:\n\n```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Initialize a dictionary to count activities per day of the week\n    day_counts = defaultdict(int)\n    for activity in activities:\n        day_of_week = activity.strftime('%A')  # Get the full name of the day of the week\n        day_counts[day_of_week] += 1\n\n    # Define the order of days for the x-axis\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    # Prepare data for plotting\n    x_labels = days_of_week\n    y_values = [day_counts[day] for day in days_of_week]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(x_labels, y_values)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Return the Axes object\n    return ax\n\n# Example usage:\n# activities = [datetime(2023, 1, 1), datetime(2023, 1, 2), datetime(2023, 1, 3)]\n# ax = task_func(activities)\n# plt.show()\n```\n\nThis function first checks if all elements in the `activities` list are instances of `datetime`. If not, it raises a `TypeError`. It then counts the number of activities for each day of the week using a `defaultdict`. Finally, it creates a bar chart using `matplotlib`, with the x-axis labeled 'Day of the Week', the y-axis labeled 'Number of Activities', and the title 'Weekly Activity'. The function returns the `Axes` object representing the bar chart."}
{"task_id": "BigCodeBench/368", "solution": "Here's a self-contained Python function that moves a random file from a source directory to a specified destination directory and returns the name of the file moved:\n\n```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n\n    Args:\n    src_dir (str): The source directory.\n    dest_dir (str): The destination directory.\n    seed (int): The seed for the random number generator. Defaults to 100.\n\n    Returns:\n    str: The name of the file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n    \"\"\"\n    # Set the seed for the random number generator\n    random.seed(seed)\n    \n    # Check if the source directory exists\n    if not os.path.isdir(src_dir):\n        raise ValueError(\"Source directory does not exist\")\n    \n    # Check if the destination directory exists, if not, create it\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files in the source directory\n    if not files:\n        raise ValueError(\"No files found in the source directory\")\n    \n    # Select a random file from the list of files\n    file_to_move = random.choice(files)\n    \n    # Construct full file paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n    \n    # Move the file\n    shutil.move(src_path, dest_path)\n    \n    # Return the name of the file moved\n    return file_to_move\n\n# Example usage:\n# result = task_func('/source/directory', '/destination/directory', seed=123)\n# print(result)  # Output will be the name of the file moved, e.g., 'file1.txt'\n```\n\nThis function includes checks to ensure that the source directory exists and contains files, and it creates the destination directory if it does not already exist. It then selects a random file from the source directory, moves it to the destination directory, and returns the name of the moved file."}
{"task_id": "BigCodeBench/374", "solution": "Here's a self-contained Python function that accomplishes the task of protecting all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash. The function returns the number of Excel files processed.\n\n```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory by prefixing them with a double backslash.\n    \n    Args:\n        directory_path (str): The path to the directory containing the Excel files. Defaults to './xlsx_files/'.\n    \n    Returns:\n        int: The number of Excel files processed.\n    \"\"\"\n    # Ensure the directory path ends with a slash\n    if not directory_path.endswith('/'):\n        directory_path += '/'\n    \n    # Initialize a counter for the number of Excel files processed\n    num_files_processed = 0\n    \n    # Find all .xlsx files in the directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Iterate over each Excel file\n    for file_path in excel_files:\n        # Load the workbook\n        wb = load_workbook(file_path)\n        \n        # Iterate over all sheets in the workbook\n        for sheet_name in wb.sheetnames:\n            sheet = wb[sheet_name]\n            \n            # Iterate over all cells in the sheet\n            for row in sheet.iter_rows():\n                for cell in row:\n                    if cell.value:\n                        # Protect double quotes by prefixing them with a double backslash\n                        cell.value = re.sub(r'\"', r'\\\\\"', str(cell.value))\n        \n        # Save the modified workbook\n        wb.save(file_path)\n        \n        # Increment the files processed counter\n        num_files_processed += 1\n    \n    # Return the number of files processed\n    return num_files_processed\n\n# Example usage\nif __name__ == \"__main__\":\n    processed_count = task_func('./xlsx_files/')\n    print(f\"Number of Excel files processed: {processed_count}\")\n```\n\nThis function ensures the directory path ends with a slash, finds all `.xlsx` files in the specified directory, loads each Excel file using `openpyxl`, iterates over all sheets and cells in the workbook, replaces double quotes with double backslashes followed by a double quote, saves the modified workbook, and counts and returns the number of files processed. Make sure to install the `openpyxl` library if you haven't already:\n\n```sh\npip install openpyxl\n```\n\nThis script assumes that the directory contains only `.xlsx` files and that the files are not open in Excel while the script is running to avoid permission errors."}
{"task_id": "BigCodeBench/399", "solution": "Here's a self-contained Python function that creates a plot of a sine wave and a cosine wave with a given frequency and sample size. The function raises `ValueError` if the frequency is negative or if the sample size is non-positive.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    # Check for negative frequency\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative\")\n    \n    # Check for non-positive sample size\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n    \n    # Generate the time array\n    t = np.linspace(0, 2 * np.pi, sample_size)\n    \n    # Generate the sine and cosine waves\n    sine_wave = np.sin(frequency * t)\n    cosine_wave = np.cos(frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine wave')\n    ax.plot(t, cosine_wave, label='Cosine wave')\n    \n    # Add labels and title\n    ax.set_xlabel('Time [s]')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves with Frequency {} Hz'.format(frequency))\n    ax.legend()\n    \n    # Return the figure and axes objects\n    return fig, ax\n\n# Example usage:\n# fig, ax = task_func(2, 1000)\n# plt.show()\n```\n\nThis function uses `numpy` to generate the sine and cosine waves and `matplotlib` to plot them. The `linspace` function from `numpy` is used to create an array of time points over one period of the sine and cosine waves. The `plot` function from `matplotlib` is used to plot the waves, and labels, a title, and a legend are added for clarity. The function returns the `Figure` and `Axes` objects, which can be used to further customize the plot or to display it using `plt.show()`."}
{"task_id": "BigCodeBench/401", "solution": "Here's a self-contained function that initializes a Flask-Mail instance and retrieves the email server details from environment variables, using the specified defaults if the variables are not set:\n\n```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create a Flask application instance\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, using defaults if not set\n    mail_settings = {\n        'MAIL_SERVER': os.getenv('MAIL_SERVER', 'localhost'),\n        'MAIL_PORT': int(os.getenv('MAIL_PORT', 25)),\n        'MAIL_USE_TLS': os.getenv('MAIL_USE_TLS', 'False').lower() in ('true', '1', 't'),\n        'MAIL_USERNAME': os.getenv('MAIL_USERNAME', None),\n        'MAIL_PASSWORD': os.getenv('MAIL_PASSWORD', None)\n    }\n\n    # Configure the Flask-Mail settings\n    app.config.update(mail_settings)\n\n    # Initialize the Flask-Mail instance\n    mail = Mail(app)\n\n    # Return a tuple containing the Flask-Mail instance and the app's mail configurations\n    return mail, mail_settings\n```\n\nThis function does the following:\n1. Creates a Flask application instance with the given `app_name`.\n2. Retrieves the email server details from environment variables, using the specified defaults if the environment variables are not set.\n3. Configures the Flask-Mail settings using the retrieved details.\n4. Initializes the Flask-Mail instance.\n5. Returns a tuple containing the Flask-Mail instance and the app's mail configurations."}
{"task_id": "BigCodeBench/409", "solution": "Here is the self-contained code for the function `task_func` that calculates the mean, median, and standard deviation of a specific column in an Excel file, and handles the specified exceptions:\n\n```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {full_path} does not exist.\")\n    \n    # Load the Excel file into a DataFrame\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise ValueError(f\"Error reading Excel file: {e}\")\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Extract the data from the specified column\n    data = df[column_name]\n    \n    # Calculate the mean, median, and standard deviation\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    std_val = np.std(data)\n    \n    # Return the results in a dictionary\n    result = {\n        'mean': mean_val,\n        'median': median_val,\n        'standard_deviation': std_val\n    }\n    \n    return result\n\n# Example usage:\n# result = task_func('/path/to/excel/files', 'example.xlsx', 'ColumnName')\n# print(result)\n```\n\n### Explanation:\n1. **File Path Construction**: The function constructs the full path to the Excel file by joining the `excel_file_path` and `file_name`.\n2. **File Existence Check**: It checks if the file exists using `os.path.exists` and raises a `FileNotFoundError` if the file does not exist.\n3. **Loading the Excel File**: The Excel file is loaded into a pandas DataFrame using `pd.read_excel`. If there's an error reading the file, a `ValueError` is raised.\n4. **Column Existence Check**: It checks if the specified column exists in the DataFrame and raises a `ValueError` if the column is not found.\n5. **Data Extraction and Calculation**: The data from the specified column is extracted, and the mean, median, and standard deviation are calculated using `np.mean`, `np.median`, and `np.std` respectively.\n6. **Result Dictionary**: The results are returned in a dictionary with keys 'mean', 'median', and 'standard_deviation'.\n\nYou can call this function with the appropriate parameters to get the desired statistics for a specific column in an Excel file."}
{"task_id": "BigCodeBench/417", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model with one dense hidden layer and sigmoid activation\n    model = Sequential()\n    model.add(Dense(1, input_dim=2, activation='sigmoid'))\n\n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    # Fit the model to the training data, evaluating on the test set\n    history = model.fit(X_train, Y_train, epochs=50, verbose=0, validation_data=(X_test, Y_test))\n\n    # Plot the model's training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    # Return the trained model and the Axes object\n    return model, ax\n\n# Example usage:\n# model, ax = task_func(X, Y)\n# plt.show()  # To display the plot\n```\n\nThis function takes input data `X` and target labels `Y`, splits them into training and test sets, constructs and compiles a Keras Sequential model, fits the model to the training data while evaluating it on the test set, and finally plots the training and validation loss over epochs. The function returns the trained model and the `Axes` object of the plot for further customization."}
{"task_id": "BigCodeBench/418", "solution": "Here's a self-contained Python function that accomplishes the task described:\n\n```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(1, input_dim=X_train.shape[1], activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    # Fit the model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n\n    # Compute ROC curve and AUC\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    ax = plt.gca()\n\n    return model, ax\n```\n\nThis function takes input features `X` and labels `Y`, splits them into training and test sets, builds and trains a simple Keras model, and then plots the ROC curve with the AUC score included in the plot legend. The function returns the trained Keras model and the matplotlib Axes object for the ROC curve plot."}
{"task_id": "BigCodeBench/424", "solution": "Here is the complete implementation of the `task_func` function that meets the specified requirements:\n\n```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n    \n    # Read the image\n    original_image = cv2.imread(image_path)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n    \n    # Reshape the image to a 2D array with each row representing a pixel's RGB value\n    pixel_values = original_image.reshape((-1, 3))\n    pixel_values = np.float32(pixel_values)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n    \n    # Replace each pixel's color with the centroid of the cluster it belongs to\n    segmented_image = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_image = segmented_image.reshape(original_image.shape)\n    segmented_image = np.uint8(segmented_image)\n    \n    # Save each region as a separate image\n    for i in range(n_clusters):\n        region_mask = kmeans.labels_ == i\n        region_image = np.zeros_like(original_image)\n        region_image[region_mask.reshape(original_image.shape[:2])] = original_image[region_mask.reshape(original_image.shape[:2])]\n        cv2.imwrite(f\"region_{i}.jpg\", cv2.cvtColor(region_image, cv2.COLOR_RGB2BGR))  # Convert back to BGR for saving\n    \n    # Return the original and segmented images as numpy arrays\n    return original_image, segmented_image\n\n# Example usage:\n# original, segmented = task_func('path_to_image.jpg', n_clusters=3)\n```\n\nThis function reads an RGB image, applies K-means clustering to segment the image into `n_clusters` regions, saves each region as a separate image, and returns the original and segmented images as numpy arrays. It also handles exceptions for file not found and invalid `n_clusters` values."}
{"task_id": "BigCodeBench/443", "solution": "Here is the self-contained code that performs the required operations:\n\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', \n    flatten the result, apply KMeans clustering to the flattened data, \n    and visualize it.\n\n    Parameters:\n    P (np.ndarray): The input matrix.\n    T (np.ndarray): The input 3D tensor.\n    n_clusters (int): The number of clusters for KMeans clustering (default: 3).\n    random_state (int): The random seed for KMeans clustering (default: 0).\n    n_init (int): The number of initializations for KMeans clustering (default: 10).\n\n    Returns:\n    cluster_result (np.ndarray): The result of KMeans clustering.\n    ax (matplotlib.axes.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Ensure the dimensions are compatible for matrix multiplication\n    assert P.shape[1] == T.shape[0], \"Incompatible shapes for matrix multiplication\"\n\n    # Calculate the product of matrix 'P' and 3D tensor 'T'\n    product = np.dot(P, T)\n\n    # Flatten the product\n    flattened = product.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened.reshape(-1, 1))\n\n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(cluster_result)), flattened, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Data Points')\n    ax.set_ylabel('Flattened Data Values')\n\n    return cluster_result, ax\n\n# Example usage\n# P = np.random.rand(3, 4)\n# T = np.random.rand(4, 5, 6)\n# cluster_result, ax = task_func(P, T)\n# plt.show()\n```\n\nThis code defines the `task_func` function, which takes the input matrix `P`, 3D tensor `T`, and optional parameters for KMeans clustering. It calculates the product of `P` and `T`, flattens the result, applies KMeans clustering, and visualizes the clustering using matplotlib. The function returns the cluster result and the visualization axis. The example usage section is commented out but can be used to test the function with random data."}
{"task_id": "BigCodeBench/445", "solution": "Here is a self-contained Python function that calculates the Voronoi diagram for a set of 2D points, applies jittering to avoid duplicate points, and plots the diagram. The function includes error handling for invalid input types or shapes and returns a tuple containing the Voronoi object and the axes of the plot.\n\n```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Check if points is a numpy array and has the correct shape\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input must be a 2D numpy array with 2 columns\")\n    \n    # Apply jittering to avoid duplicate points\n    np.random.seed(seed)\n    jitter = np.random.normal(0, 1e-9, points.shape)\n    points_jittered = points + jitter\n    \n    # Calculate the Voronoi diagram\n    vor = Voronoi(points_jittered)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    plt.axis('equal')\n    plt.show()\n    \n    return (vor, ax)\n\n# Example usage:\n# points = np.array([[0, 0], [1, 2], [3, 3], [4, 1], [7, 2]])\n# task_func(points)\n```\n\nThis function `task_func` takes a numpy array of 2D points and an optional seed for the random number generator used in jittering. It first checks if the input is valid, applies a small jitter to the points to prevent any exact duplicates, computes the Voronoi diagram, and then plots it using `matplotlib`. The function returns a tuple containing the Voronoi object and the axes of the plot."}
{"task_id": "BigCodeBench/454", "solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"The source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"The destination directory '{dest_dir}' does not exist.\")\n\n    # Find files with the specified extension in the source directory\n    files_to_move = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize a list to store the paths of moved files\n    moved_files = []\n\n    # Iterate over the files to move\n    for file in files_to_move:\n        # Get the file name\n        file_name = os.path.basename(file)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(os.path.join(dest_dir, file_name)):\n            # Move the file\n            shutil.move(file, dest_dir)\n            # Add the file path to the list of moved files\n            moved_files.append(os.path.join(dest_dir, file_name))\n\n    # Return the list of moved files\n    return moved_files\n```\n\nThis function `task_func` takes three arguments: `src_dir` (the source directory), `dest_dir` (the destination directory), and `ext` (the file extension to look for). It checks if both directories exist, searches for files with the specified extension in the source directory, and moves them to the destination directory if they do not already exist there. It returns a list of the full paths of files that were successfully moved."}
{"task_id": "BigCodeBench/458", "solution": "Here's a self-contained Python function that accomplishes the described task:\n\n```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Check if the JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n    \n    # Load JSON string into a dictionary\n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    \n    # Function to double numerical values\n    def double_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, list):\n            return [double_value(v) for v in value]\n        elif isinstance(value, str):\n            # Use regex to check if the string can be interpreted as a number\n            number_match = re.match(r'^([\\+\\-]?\\d+(\\.\\d*)?|\\.\\d+)([eE][\\+\\-]?\\d+)?$', value)\n            if number_match:\n                number = float(value)\n                return number * 2\n        return value\n    \n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {key: double_value(value) for key, value in data_dict.items()}\n    \n    # Create a Pandas DataFrame from the dictionary\n    df = pd.DataFrame.from_dict(normalized_dict, orient='index').transpose()\n    \n    return df\n\n# Example usage:\njson_example = '{\"a\": 1, \"b\": \"2\", \"c\": [3, 4.5], \"d\": \"not a number\"}'\ndf = task_func(json_example)\nprint(df)\n```\n\nThis function `task_func` takes a JSON string as input, processes it according to the specifications, and returns a Pandas DataFrame. It handles various types of values within the JSON, including integers, floats, lists of numbers, and strings that can be interpreted as numbers. It does not process nested dictionaries or complex data structures beyond the specified types."}
{"task_id": "BigCodeBench/461", "solution": "Here's the complete Python function that meets the specified requirements:\n\n```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n\n    # Start the subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Initialize metrics\n    cpu_usage = 0\n    memory_usage = 0\n    start_time = time.time()\n\n    try:\n        # Monitor the process\n        while process.poll() is None:\n            # Check if the timeout has been reached\n            if time.time() - start_time > timeout:\n                process.terminate()\n                raise TimeoutError(\"The script execution exceeded the specified timeout.\")\n\n            # Get the process object for psutil monitoring\n            proc = psutil.Process(process.pid)\n\n            # Accumulate CPU and memory usage\n            cpu_usage += proc.cpu_percent(interval=0.1)\n            memory_usage += proc.memory_info().rss\n\n    except psutil.NoSuchProcess:\n        pass  # The process has already terminated\n\n    finally:\n        # Ensure the process is terminated\n        process.terminate()\n        process.wait()\n\n    # Return the metrics\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    result = task_func('/path/to/your/script.sh', timeout=20)\n    print(result)\n```\n\n### Explanation:\n1. **Path Check**: The function first checks if the provided script path exists.\n2. **Subprocess Execution**: It then executes the script using `subprocess.Popen`.\n3. **Monitoring Loop**: A loop runs to monitor the process using `psutil`. It checks CPU and memory usage every 0.1 seconds.\n4. **Timeout Handling**: If the script runs longer than the specified timeout, it raises a `TimeoutError` and terminates the process.\n5. **Exception Handling**: It handles cases where the process might not be found by `psutil` (e.g., if it has already terminated).\n6. **Termination**: It ensures the process is terminated properly in the `finally` block.\n7. **Result**: The function returns a dictionary with accumulated CPU and memory usage.\n\nThis function should be run in an environment where `psutil` is installed, as it is not a standard library module and needs to be installed separately (e.g., via `pip install psutil`)."}
{"task_id": "BigCodeBench/477", "solution": "Here's the self-contained code for the function `task_func` that generates a DataFrame with random values and categories, and then plots a scatter plot of \"x\" vs \"y\" colored by \"category\":\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)  # Set seed for reproducibility\n    \n    # Generate random values for x and y\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Determine the number of categories to use\n    num_categories = len(CATEGORIES)\n    if N >= num_categories:\n        # Ensure each category appears at least once\n        category = np.random.choice(CATEGORIES, N)\n    else:\n        # Randomly sample categories without replacement\n        category = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': category\n    })\n    \n    # Plot scatter plot\n    fig, ax = plt.subplots()\n    for cat in CATEGORIES:\n        subset = df[df['category'] == cat]\n        ax.scatter(subset['x'], subset['y'], label=cat)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    plt.show()\n    \n    return (df, ax)\n\n# Example usage\ndf, ax = task_func(N=10, CATEGORIES=[\"A\", \"B\", \"C\"], seed=42)\n```\n\nThis function sets a random seed for reproducibility, generates random values for `x` and `y`, and assigns categories to each row. It then creates a DataFrame and plots a scatter plot with points colored by their category. The function returns the DataFrame and the Axes object of the scatter plot."}
{"task_id": "BigCodeBench/486", "solution": "Here's a self-contained Python function that generates a time series based on the provided start time, end time, step, trend, and optionally a seed for reproducibility. The function uses `pandas` for time series manipulation and `matplotlib` for plotting.\n\n```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Convert start_time and end_time to datetime objects if they are not already\n    if not isinstance(start_time, datetime):\n        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    if not isinstance(end_time, datetime):\n        end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Generate the time series index\n    time_index = pd.date_range(start=start_time, end=end_time, freq=step)\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate values from a normal distribution\n    values = np.random.normal(size=len(time_index))\n    \n    # Add a linear trend\n    trend_values = np.arange(len(time_index)) * trend\n    values += trend_values\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data=values, index=time_index, columns=['Value'])\n    \n    # Plot the time series\n    fig, ax = plt.subplots()\n    df.plot(ax=ax)\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    \n    return ax\n\n# Example usage:\n# start_time = '2023-01-01 00:00:00'\n# end_time = '2023-01-02 00:00:00'\n# step = 'H'  # hourly frequency\n# trend = 0.1  # a positive trend of 0.1 per step\n# ax = task_func(start_time, end_time, step, trend)\n# plt.show()\n```\n\nThis function takes a start time and end time (which can be strings in the format 'YYYY-MM-DD HH:MM:SS' or `datetime` objects), a step (e.g., 'H' for hourly, 'D' for daily), a trend value (which is the slope of the linear trend to be added to the normally distributed values), and an optional seed for the random number generator. It generates a time series, adds a linear trend, and plots it with appropriate labels on the axes."}
{"task_id": "BigCodeBench/492", "solution": "Here's a self-contained Python function that generates sales data for five products from a given epoch time up to the current time, ensuring input validity and outputting a pandas DataFrame with the specified columns:\n\n```python\nimport pandas as pd\nfrom datetime import datetime\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Validate input\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"random_seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"products must be a list with exactly 5 elements\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"all elements in products must be strings\")\n\n    # Set random seed\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Get current date\n    end_date = datetime.now()\n\n    # Initialize data\n    data = []\n\n    # Generate sales data for each day between start_date and end_date\n    for date in pd.date_range(start=start_date, end=end_date):\n        for product in products:\n            # Generate random sales data\n            sales = random.randint(10, 50)\n            # Append data to list\n            data.append([product, date, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['Product', 'Date', 'Sales'])\n\n    return df\n\n# Example usage:\n# epoch_time = 1609459200000  # Example epoch time for January 1, 2021\n# sales_data = task_func(epoch_time)\n# print(sales_data)\n```\n\nThis function starts by validating the input parameters to ensure they meet the specified conditions. It then sets the random seed for reproducibility, converts the epoch time to a datetime object, and generates sales data for each day between the start date and the current date for each product. The sales quantity is randomly sampled from the range [10, 50]. Finally, it constructs a pandas DataFrame with the generated data and returns it."}
{"task_id": "BigCodeBench/501", "solution": "Here's a Python function that meets the specified requirements:\n\n```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert JSON strings to an Excel file, including handling empty JSON arrays.\n\n    Parameters:\n    json_str (str): The JSON string to be converted.\n    filename (str): The name of the Excel file to be created.\n    sheet_name (str, optional): The name of the sheet in the Excel file. Defaults to \"sheet1\".\n\n    Returns:\n    str: The absolute path of the created Excel file.\n\n    Raises:\n    ValueError: If `json_str` is not valid JSON.\n    TypeError: If `json_str` is not a string, bytes, or bytearray.\n    Exception: For other general errors related to file writing.\n    \"\"\"\n    # Validate input type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Try to parse JSON\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string\")\n    \n    # Convert JSON to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create Excel file\n    try:\n        # Ensure the filename has the correct extension\n        if not filename.endswith('.xls'):\n            filename += '.xls'\n        \n        # Save DataFrame to Excel\n        df.to_excel(filename, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(f\"Error writing to file: {e}\")\n    \n    # Return the absolute path of the created file\n    return os.path.abspath(filename)\n\n# Example usage:\n# json_data = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Anna\", \"age\": 22}]'\n# print(task_func(json_data, \"output\"))\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input `json_str` is of the correct type (string, bytes, or bytearray). If not, it raises a `TypeError`.\n2. **JSON Parsing**: It attempts to parse the JSON string. If parsing fails, it raises a `ValueError`.\n3. **DataFrame Conversion**: The parsed JSON data is converted into a pandas DataFrame.\n4. **Excel File Creation**: The DataFrame is saved to an Excel file using `pandas.DataFrame.to_excel`. The function ensures the filename ends with `.xls` and handles any exceptions that occur during file writing.\n5. **Return Path**: Finally, the function returns the absolute path of the created Excel file.\n\nThis function should handle the conversion of JSON strings to Excel files, including cases where the JSON represents an empty array, and provide appropriate error handling for invalid inputs and file writing errors."}
{"task_id": "BigCodeBench/502", "solution": "Here's a self-contained function that generates the required DataFrame and plots the data using Seaborn:\n\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates\n    end_date = datetime.today()\n    start_date = end_date - timedelta(days=days_in_past)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Generate data\n    data = []\n    for date in date_range:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity')\n    ax.set_title('Daily Activity Durations')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax, df\n\n# Example usage\nax, df = task_func(days_in_past=7, random_seed=0)\nplt.show()\n```\n\nThis function `task_func` generates a DataFrame with randomly generated activity durations for each day in the specified number of days in the past. It then plots this data using a Seaborn lineplot, with the date on the x-axis, duration on the y-axis, and activity as the hue. The function returns a tuple containing the Axes object (`ax`) and the DataFrame (`df`)."}
{"task_id": "BigCodeBench/503", "solution": "Here's the self-contained code for creating a DataFrame of random stock prices for a specified number of days in the past:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Creates a DataFrame of stock prices for a specified number of days in the past using random data.\n\n    Args:\n        days_in_past (int): The number of days in the past to generate stock prices for. Defaults to 7.\n        stock_names (list): A list of stock names. Defaults to [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"].\n        random_seed (int): The seed for the random number generator. Defaults to 0.\n\n    Returns:\n        DataFrame: A pandas DataFrame containing random stock prices for the specified number of days.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Generate dates for the specified number of days in the past\n    end_date = datetime.now()\n    dates = [(end_date - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)]\n\n    # Generate random stock prices\n    stock_prices = np.random.rand(days_in_past, len(stock_names))\n\n    # Create DataFrame\n    df = pd.DataFrame(stock_prices, columns=stock_names, index=dates)\n\n    return df\n\n# Example usage\ndf = task_func(days_in_past=10)\nprint(df)\n```\n\nThis function `task_func` generates a DataFrame with random stock prices for the specified number of days in the past. The prices are floats in the range [0.0, 1.0). The DataFrame's index is the dates for the specified number of days in the past, and the columns are the stock names provided. The random seed ensures reproducibility of the random data."}
{"task_id": "BigCodeBench/509", "solution": "Here's a self-contained Python function that compares two CSV files line by line and generates a difference report in the form of a pandas DataFrame. The function handles various exceptions as specified and uses the `difflib.ndiff` function to find differences between the two files.\n\n```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the contents of the files\n        with open(file_path1, newline='', encoding='utf-8') as f1, open(file_path2, newline='', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            \n            # Convert readers to lists of lines\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n            \n            # Check if files are empty\n            if not lines1:\n                raise ValueError(f\"{file_path1} is empty\")\n            if not lines2:\n                raise ValueError(f\"{file_path2} is empty\")\n            \n            # Generate the diff\n            diff = list(ndiff(lines1, lines2))\n            \n            # Prepare the DataFrame\n            data = []\n            line_number = 1\n            for line in diff:\n                status = line[0]\n                content = line[2:].strip()\n                if status != ' ':\n                    data.append([line_number, status, content])\n                line_number += 1\n            \n            df = pd.DataFrame(data, columns=['Line Number', 'Status', 'Content'])\n            return df\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e}\")\n    except ValueError as e:\n        raise ValueError(f\"Value error: {e}\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n\n# Example usage:\n# df = task_func('file1.csv', 'file2.csv')\n# print(df)\n```\n\nThis function reads two CSV files, compares them line by line using `difflib.ndiff`, and constructs a pandas DataFrame that shows the differences. The DataFrame includes the line number, a status indicator (' ', '-', or '+'), and the content of the line where the difference occurs. The function handles exceptions for file not found, empty files, and other IO-related errors."}
{"task_id": "BigCodeBench/511", "solution": "Here's a self-contained Python function that fulfills the requirements you've described. This function takes a column name and a list of dictionaries (representing employee data) as input, calculates the required statistics, and visualizes the data with a pie chart using the 'Age' column as labels.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        stats = {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n        # Create an empty plot\n        fig, ax = plt.subplots()\n        ax.pie([], labels=[])\n    else:\n        # Calculate statistics\n        stats = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max()\n        }\n        # Create a pie chart using the 'Age' column as labels\n        fig, ax = plt.subplots()\n        ax.pie(df[column], labels=df['Age'], autopct='%1.1f%%')\n    \n    plt.show()\n    return (stats, ax)\n\n# Example usage:\ndata = [\n    {'Age': '25', 'Salary': 50000},\n    {'Age': '30', 'Salary': 60000},\n    {'Age': '35', 'Salary': 70000}\n]\ncolumn = 'Salary'\nresult = task_func(column, data)\nprint(result[0])  # Print the statistics\n```\n\nThis function does the following:\n1. Converts the input list of dictionaries into a Pandas DataFrame.\n2. Checks if the DataFrame is empty and calculates the statistics accordingly.\n3. If the DataFrame is not empty, it calculates the sum, mean, minimum, and maximum of the specified column.\n4. It then creates a pie chart using the 'Age' column as labels for the pie chart segments.\n5. Finally, it displays the pie chart and returns a tuple containing the statistics dictionary and the Axes object of the pie chart."}
{"task_id": "BigCodeBench/513", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Validate that the data list is not empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n    \n    # Convert the data list to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Validate that the specified column exists\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' does not exist in the data\")\n    \n    # Validate that steps, calories burned, and distance walked are non-negative\n    for col in ['steps', 'calories burned', 'distance walked']:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in column '{col}'\")\n    \n    # Calculate the sum, mean, min, max of the specified column\n    col_sum = df[column].sum()\n    col_mean = df[column].mean()\n    col_min = df[column].min()\n    col_max = df[column].max()\n    \n    # Prepare the output dictionary\n    result_dict = {\n        'sum': col_sum,\n        'mean': col_mean,\n        'min': col_min,\n        'max': col_max\n    }\n    \n    # Plot the line chart\n    ax = df.plot(x='Date', y=column, title=f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    \n    # Return the result tuple\n    return (result_dict, ax)\n\n# Example usage:\n# data = [\n#     {'Date': '2023-01-01', 'steps': 1000, 'calories burned': 200, 'distance walked': 1.5},\n#     {'Date': '2023-01-02', 'steps': 1500, 'calories burned': 250, 'distance walked': 2.0},\n#     # Add more data entries as needed\n# ]\n# result, chart = task_func('steps', data)\n# print(result)\n# plt.show()\n```\n\nThis function starts by validating the input data and column name. It then calculates the required statistics and plots a line chart using `matplotlib`. The function returns a tuple containing a dictionary with the calculated statistics and the `Axes` object of the plotted line chart."}
{"task_id": "BigCodeBench/526", "solution": "Here's a self-contained Python function that reads a JSON file, calculates the mean and median for each key while ignoring non-numeric or missing values, and converts the results into a Pandas DataFrame:\n\n```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize defaultdict to store sums and counts for each key\n    sums = defaultdict(float)\n    counts = defaultdict(int)\n    medians = defaultdict(list)\n    \n    # Iterate over the list of dictionaries\n    for dictionary in data:\n        for key, value in dictionary.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):\n                # Update sum and count for the key\n                sums[key] += value\n                counts[key] += 1\n                # Store the value for median calculation\n                medians[key].append(value)\n    \n    # Calculate means and medians\n    means = {key: sums[key] / counts[key] for key in sums}\n    medians = {key: np.median(medians[key]) for key in medians}\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'mean': means, 'median': medians})\n    \n    # Sort the DataFrame by index (variable names)\n    df = df.sort_index()\n    \n    return df\n\n# Example usage:\n# df = task_func('path_to_your_json_file.json')\n# print(df)\n```\n\nThis function does the following:\n1. Reads the JSON file into a Python list of dictionaries.\n2. Iterates over each dictionary, collecting numeric values for each key into separate lists for means and medians.\n3. Calculates the mean and median for each key using `numpy`.\n4. Constructs a Pandas DataFrame with the calculated means and medians, indexed by the key names.\n5. Sorts the DataFrame by the index (key names) and returns it.\n\nYou can call this function with the path to your JSON file to get the desired DataFrame."}
{"task_id": "BigCodeBench/528", "solution": "Here's a self-contained Python function that meets the specified requirements. This function reads a CSV file, identifies duplicate rows, converts the duplicated rows into a pandas DataFrame, and then plots the frequency of these duplicates using matplotlib.\n\n```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check if the file is a CSV file\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be a CSV file\")\n\n    # Read the CSV file and find duplicates\n    with open(file_path, mode='r', newline='') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n        duplicate_counter = Counter(tuple(row) for row in rows)\n\n    # Filter out unique rows and prepare data for DataFrame\n    duplicates = {row: count for row, count in duplicate_counter.items() if count > 1}\n    df_data = {'Row': list(duplicates.keys()), 'Count': list(duplicates.values())}\n    df = pd.DataFrame(df_data)\n\n    # Plotting the data\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='Row', y='Count', ax=ax, legend=False)\n    ax.set_title('Duplicate Rows Frequency')\n    ax.set_xlabel('Rows')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n\n    # Output the dictionary and the Axes object\n    return duplicates, ax\n\n# Example usage:\n# duplicates, ax = task_func('path_to_your_file.csv')\n# plt.show()\n```\n\n### Explanation:\n1. **File Check**: The function first checks if the provided file path ends with `.csv`. If not, it raises a `ValueError`.\n2. **Reading CSV**: It reads the CSV file using Python's built-in `csv` module. Each row is converted to a tuple to be used as a dictionary key.\n3. **Finding Duplicates**: It uses `collections.Counter` to count occurrences of each row. Rows with a count greater than 1 are considered duplicates.\n4. **Data Preparation**: Duplicates are filtered and prepared for a pandas DataFrame.\n5. **Plotting**: The DataFrame is plotted using matplotlib. The plot is a bar chart showing the frequency of each duplicate row.\n6. **Output**: The function returns a dictionary of duplicates and their counts, along with the matplotlib `Axes` object for further customization or display.\n\nThis function assumes that the CSV file is properly formatted and that each row can be uniquely identified by its contents. Adjustments might be needed for CSVs with headers or specific formatting requirements."}
{"task_id": "BigCodeBench/530", "solution": "Here is the complete implementation of the `task_func` function as described:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check for negative ages and round down to nearest integer\n    if 'age' in df.columns:\n        df['age'] = df['age'].apply(lambda x: np.floor(x) if x >= 0 else raise_negative_age_error())\n    \n    # Identify duplicate names\n    duplicate_names = df[df['name'].duplicated(keep=False)]\n    \n    # If no duplicates, return empty Counter and None for plot\n    if duplicate_names.empty:\n        return Counter(), None\n    \n    # Record age distribution among duplicate names\n    age_distribution = Counter(duplicate_names['age'].astype(int))\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    sns.histplot(duplicate_names['age'], bins=range(int(duplicate_names['age'].min() - 0.5), int(duplicate_names['age'].max() + 1.5)), ax=ax)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution of Duplicate Names')\n    \n    return age_distribution, ax\n\ndef raise_negative_age_error():\n    raise ValueError(\"Age must not be negative\")\n\n# Example usage:\n# df = pd.DataFrame({\n#     'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n#     'age': [25.2, 30.7, 25.8, 22.0, 30.3]\n# })\n# age_dist, plot = task_func(df)\n# print(age_dist)\n# plot.show()\n```\n\nThis function performs the following steps:\n1. Checks if the DataFrame is empty and raises a `ValueError` if it is.\n2. Checks for negative ages and rounds down ages to the nearest integer, raising a `ValueError` if any age is negative.\n3. Identifies duplicate names in the DataFrame.\n4. If there are no duplicates, it returns an empty `Counter` and `None` for the plot.\n5. Records the age distribution among the duplicate names using a `Counter`.\n6. Creates a histogram plot of the age distribution for the duplicate names, with bins adjusted by 0.5 to ensure integer ages fall squarely within bins.\n7. Returns the age distribution `Counter` and the histogram plot `Axes` object."}
{"task_id": "BigCodeBench/532", "solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    \n    # Prepare data for plotting\n    values = df['value']\n    mean = np.mean(values)\n    std = np.std(values)\n    \n    # Create histogram plot\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(values, bins=bins, color='green', alpha=0.6, density=True, label='Data')\n    \n    # Overlay normal distribution curve\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mean, std)\n    ax.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n    \n    # Set plot details\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    \n    # Return the results\n    return (value_counts, ax)\n\n# Example usage:\n# Assuming 'df' is a DataFrame with a 'value' column\n# result = task_func(df, bins=10)\n# plt.show()\n```\n\nThis function takes a DataFrame `df` and an optional `bins` parameter for the number of bins in the histogram. It counts the duplicates in the 'value' column, plots a histogram of the values, overlays a normal distribution curve, and returns a tuple containing the `Counter` object and the `Axes` object. The histogram bars are green with 60% opacity, and the normal distribution curve is black with a linewidth of 2. The plot is titled \"Distribution\", with \"Value\" as the x-axis label and \"Frequency\" as the y-axis label."}
{"task_id": "BigCodeBench/553", "solution": "Here's a self-contained Python function that creates a pandas DataFrame with random values based on the lists `a` and `b`, and then plots it as a bar chart using matplotlib:\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Ensure the length of 'b' does not exceed the number of predefined columns\n    if len(b) > len(COLUMNS):\n        raise ValueError(\"Length of 'b' exceeds the number of predefined columns.\")\n    \n    # Create a DataFrame with random values\n    data = np.random.rand(len(a), len(b))\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:len(b)])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 7))\n    plt.title('Random Data Bar Chart')\n    plt.ylabel('Values')\n    plt.xlabel('Indices')\n    plt.xticks(rotation=45)\n    plt.legend(title='Columns')\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\n\n# Example usage:\n# a = ['Row1', 'Row2', 'Row3']\n# b = [1, 2, 3]\n# task_func(a, b)\n```\n\nThis function does the following:\n1. Checks if the length of list `b` exceeds the number of predefined columns.\n2. Creates a DataFrame with random values, using list `a` as the row indices and the first `len(b)` elements of `COLUMNS` as the column names.\n3. Plots the DataFrame as a bar chart using matplotlib.\n4. Returns the `Axes` object of the plotted bar chart.\n\nYou can uncomment the example usage at the end to test the function with sample data."}
{"task_id": "BigCodeBench/560", "solution": "Here's a self-contained function that meets your requirements. This function takes a DataFrame `data` with columns 'month' and 'value', and plots a bar chart for a single year. The 'month' column should contain datetime objects or strings that can be converted to datetime objects.\n\n```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Ensure data is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame\")\n\n    # Check if data has the required columns\n    required_columns = ['month', 'value']\n    if not all(col in data.columns for col in required_columns):\n        raise ValueError(\"Input data must have 'month' and 'value' columns\")\n\n    # Ensure 'month' is in datetime format\n    data['month'] = pd.to_datetime(data['month'])\n    \n    # Extract year for the title\n    year = data['month'].dt.year.unique()[0]\n    \n    # Sort data by month\n    data = data.sort_values('month')\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.bar(data['month'].dt.strftime('%b'), data['value'])\n    \n    # Set plot labels and title\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Show plot\n    plt.show()\n    \n    return ax\n```\n\nThis function does the following:\n1. Checks if the input `data` is a pandas DataFrame and if it contains the required columns ('month' and 'value').\n2. Converts the 'month' column to datetime format to ensure proper handling of date data.\n3. Extracts the unique year from the 'month' column to use in the plot title.\n4. Sorts the data by month to ensure the bars are in chronological order.\n5. Creates a bar chart with abbreviated month names on the x-axis and values on the y-axis.\n6. Sets the title, x-label, and y-label of the plot.\n7. Returns the `Axes` object representing the plot."}
{"task_id": "BigCodeBench/567", "solution": "Here is the self-contained function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the string data to a numeric pandas Series\n    numeric_data = pd.to_numeric(data)\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(numeric_data.min(), numeric_data.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(numeric_data, bins=bins, align='mid', edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Return the Axes object\n    return ax\n\n# Example usage:\n# data_example = ['1', '2', '2', '3', '3', '3', '4', '4', '5']\n# ax = task_func(data_example)\n# plt.show()\n```\n\nThis function takes a list of string values `data`, converts them to numeric values, calculates the bins for the histogram, and then creates and displays the histogram with the appropriate labels and title. The function returns the `Axes` object of the created histogram. You can test this function by calling it with a sample dataset as shown in the example usage comment."}
{"task_id": "BigCodeBench/574", "solution": "Here is the self-contained code that generates a noisy sine wave of a specified length, fits a curve to it using `curve_fit` from `scipy.optimize`, and plots the results:\n\n```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate a sine wave\n    y_true = np.sin(x)\n    \n    # Add noise to the sine wave\n    y_noisy = y_true + noise_level * np.random.randn(array_length)\n    \n    # Define the function to fit\n    def sine_func(x, a, b, c, d):\n        return a * np.sin(b * x + c) + d\n    \n    # Initial guess for the parameters\n    initial_guess = [1, 1, 0, 0]\n    \n    # Fit the curve\n    params, _ = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n    \n    # Generate the fitted curve\n    y_fit = sine_func(x, *params)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(x, y_noisy, 'bo', label='Noisy data')\n    ax.plot(x, y_fit, 'r-', label='Fitted curve')\n    ax.legend()\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    \n    return ax\n\n# Example usage\nax = task_func()\nplt.show()\n```\n\nThis function `task_func` generates a sine wave with added noise, fits a sine function to the noisy data, and plots both the noisy data and the fitted curve. The `curve_fit` function from `scipy.optimize` is used to perform the fitting, and `matplotlib.pyplot` is used for plotting. The function returns the `Axes` object of the plot, allowing for further customization if needed."}
{"task_id": "BigCodeBench/579", "solution": "```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        # Read the CSV file and normalize text to ASCII\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = []\n            for row in reader:\n                for cell in row:\n                    normalized_text = unicodedata.normalize('NFKD', cell).encode('ascii', 'ignore').decode('ascii')\n                    text_data.append(normalized_text)\n        \n        # Join all text data into a single string and split into words\n        all_text = ' '.join(text_data)\n        words = all_text.split()\n        \n        # Convert words to lowercase and count the frequency of each word\n        words = [word.lower() for word in words]\n        word_counts = Counter(words)\n        \n        # Get the 10 most common words\n        most_common_words = word_counts.most_common(10)\n        \n        # Create a bar plot for the most common words\n        words, frequencies = zip(*most_common_words)\n        fig, ax = plt.subplots()\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        \n        # Return the plot and the list of tuples\n        return (ax, most_common_words)\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file {csv_file}.\")\n\n# Example usage:\n# result = task_func('path_to_your_csv_file.csv')\n# plt.show()  # To display the plot\n```\n\nThis function reads a CSV file, normalizes the text to ASCII, counts the words (converted to lowercase), and returns the 10 most common words along with their frequencies as a matplotlib bar plot and a list of tuples. It handles exceptions for file not found and I/O errors. The function returns a tuple containing the `matplotlib.axes.Axes` object for the bar plot and a list of the 10 most common words with their frequencies."}
{"task_id": "BigCodeBench/582", "solution": "Here's a self-contained Python function that generates a list of normally distributed random numbers, plots their histogram, and overlays the probability density function (PDF) using `matplotlib` and `scipy`:\n\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and an axis\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n    \n    # Calculate the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, 0, 1)\n    \n    # Plot the PDF\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    # Set labels and title\n    ax.set_title('Histogram and PDF of Normally Distributed Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    # Return the figure object\n    return fig\n\n# Example usage:\n# fig = task_func(size=1000)\n# plt.show()\n```\n\nThis function `task_func` generates a specified number of normally distributed random numbers (default is 1000), plots their histogram, and overlays the theoretical PDF of a normal distribution with mean 0 and standard deviation 1. The function returns a `matplotlib.figure.Figure` object, which can be displayed using `plt.show()` or saved to a file."}
{"task_id": "BigCodeBench/583", "solution": "Here's a Python function that meets the specified requirements:\n\n```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate random password and nonce\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(12)\n\n    # Encrypt private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    encrypted_private_key, tag = cipher.encrypt_and_digest(private_key.save_pkcs1())\n\n    # Save encrypted private key to file\n    with open(filename, \"wb\") as f:\n        f.write(encrypted_private_key + tag + nonce)\n\n    # Return public key, filename, password, and nonce\n    return public_key, filename, password, nonce\n\n# Example usage\nif __name__ == \"__main__\":\n    public_key, filename, password, nonce = task_func()\n    print(f\"RSA Public Key: {public_key}\")\n    print(f\"Filename: {filename}\")\n    print(f\"Password: {password}\")\n    print(f\"Nonce: {nonce}\")\n```\n\nThis function performs the following steps:\n1. Generates an RSA public and private key pair.\n2. Creates a random filename for the encrypted private key file.\n3. Generates a random password and nonce for AES encryption.\n4. Encrypts the private key using AES in GCM mode.\n5. Saves the encrypted private key, along with the tag and nonce, to a file.\n6. Returns the RSA public key, the filename of the encrypted private key, the AES encryption password, and the nonce for testing purposes."}
{"task_id": "BigCodeBench/587", "solution": "Here's a self-contained Python function that performs the described hybrid encryption approach. This function generates RSA keys, encrypts a file using AES, and then encrypts the AES key with the RSA public key. The encrypted file and the encrypted AES key are saved in separate new files.\n\n```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Read the file to be encrypted\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n    \n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256-bit key\n    \n    # Pad the data to be encrypted with AES\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n    \n    # Encrypt the data with AES\n    iv = os.urandom(16)  # Initialization vector\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n    \n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as file:\n        file.write(iv + encrypted_data)\n    \n    # Save the encrypted AES key\n    encrypted_aes_key_path = file_path + '.key.enc'\n    with open(encrypted_aes_key_path, 'wb') as file:\n        file.write(encrypted_aes_key)\n    \n    # Return the RSA public key, and the filenames of the encrypted file and the encrypted AES key\n    return public_key, encrypted_file_path, encrypted_aes_key_path\n\n# Example usage:\n# public_key, encrypted_file_path, encrypted_aes_key_path = task_func('path_to_your_file')\n```\n\nThis function does the following:\n1. Generates RSA public and private keys.\n2. Reads the file to be encrypted.\n3. Generates a random AES key.\n4. Pads the file data to be a multiple of the AES block size.\n5. Encrypts the padded data using AES in CBC mode.\n6. Encrypts the AES key using the RSA public key.\n7. Saves the encrypted file and the encrypted AES key to new files.\n8. Returns the RSA public key, the filename of the encrypted file, and the filename of the file containing the encrypted AES key."}
{"task_id": "BigCodeBench/590", "solution": "Here's a self-contained function that fulfills the requirements you've described:\n\n```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    # Validate the URL\n    if not url:\n        raise ValueError(\"The provided URL is empty.\")\n    \n    try:\n        # Fetch the HTML content from the URL\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except ValueError:\n        raise ValueError(\"The provided URL is invalid.\")\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Network connectivity or server issue: {e.reason}\")\n    \n    # Parse the HTML content using PyQuery\n    d = pq(html_content)\n    \n    # Extract the text and href attributes from all anchor tags\n    anchors = d('a')\n    data = [{'text': anchor.text, 'href': anchor.attrib.get('href'), 'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')} for anchor in anchors]\n    \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame(data)\n    \n    return df\n```\n\nThis function starts by validating the URL to ensure it's not empty. It then attempts to fetch the HTML content from the provided URL using `urllib.request.urlopen`. If the URL is invalid or if there's a network connectivity issue, it raises the appropriate exception.\n\nOnce the HTML content is fetched, it uses PyQuery to parse the HTML and extract the text and href attributes from all anchor tags. It collects this data along with the current timestamp in a list of dictionaries. Finally, it converts this list into a pandas DataFrame and returns it."}
{"task_id": "BigCodeBench/592", "solution": "Here's a self-contained Python function that generates sensor data for the specified number of hours and saves it in a CSV file with columns 'Time', 'Temperature', 'Humidity', and 'Pressure':\n\n```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import uniform\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the CSV file path\n    csv_file_path = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Start time for the data\n    start_time = datetime.now()\n\n    # Open the CSV file for writing\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Time'] + SENSORS)\n\n        # Generate data for each hour\n        for hour in range(hours):\n            current_time = start_time + timedelta(hours=hour)\n            temperature = uniform(20.0, 30.0)  # Random temperature between 20.0 and 30.0\n            humidity = uniform(30.0, 70.0)      # Random humidity between 30.0 and 70.0\n            pressure = uniform(980.0, 1030.0)   # Random pressure between 980.0 and 1030.0\n            writer.writerow([current_time.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n\n    print(f\"Sensor data for {hours} hours has been saved to {csv_file_path}\")\n\n# Example usage\ntask_func(24)\n```\n\nThis function does the following:\n1. Checks if the output directory exists and creates it if it doesn't.\n2. Defines the path for the CSV file.\n3. Opens the CSV file for writing and writes the header row.\n4. Generates sensor data for each hour, including a timestamp, temperature, humidity, and pressure.\n5. Saves the generated data to the CSV file.\n\nYou can call `task_func(hours)` with the desired number of hours to generate and save the sensor data."}
{"task_id": "BigCodeBench/593", "solution": "Here's a self-contained Python function that generates traffic data for different vehicle types over a specified number of hours, saves the data to a CSV file, and plots the data in a line chart:\n\n```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Generate data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n        for vehicle in VEHICLE_TYPES:\n            row.append(randint(0, 100))  # Random count between 0 and 100\n        data.append(row)\n        current_time += timedelta(hours=1)\n    \n    # Define CSV file path\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    \n    # Write data to CSV\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n    \n    # Read data for plotting\n    df = pd.read_csv(csv_file_path)\n    \n    # Plot data\n    fig, ax = plt.subplots()\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n    \n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Save plot to file (optional)\n    plot_file_path = os.path.join(output_dir, 'traffic_plot.png')\n    plt.savefig(plot_file_path)\n    \n    return csv_file_path, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    csv_path, axes_obj = task_func(24)\n    print(f\"CSV saved to: {csv_path}\")\n    plt.show()  # Display the plot\n```\n\nThis function `task_func` takes the number of hours as input and optionally an output directory. It generates random traffic data for each vehicle type over the specified number of hours, saves this data to a CSV file, and plots the data using `matplotlib`. The function returns a tuple containing the path to the CSV file and the `matplotlib` axes object of the line plot."}
{"task_id": "BigCodeBench/594", "solution": "```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Ensure backup directory exists\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Generate weather data\n    data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        data.append([current_time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(data)\n\n    # Backup the CSV file\n    backup_file_path = os.path.join(backup_dir, 'weather_data.csv')\n    shutil.copy(csv_file_path, backup_file_path)\n\n    return csv_file_path\n\n# Example usage\nif __name__ == \"__main__\":\n    print(task_func(24))\n```\n\nThis code defines a function `task_func` that generates weather data for a specified number of hours, saves it in a CSV file with columns 'Time' and 'Condition', and backs up the file to a backup directory. The function returns the path of the generated CSV file. The example usage at the end demonstrates how to call the function."}
{"task_id": "BigCodeBench/618", "solution": "Here's a self-contained Python function that generates a Pandas DataFrame of football match results, including random goals and penalties for multiple teams. It also visualizes the data using Seaborn plots for goals and penalty costs.\n\n```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generate a DataFrame with random goals and penalties\n    df = pd.DataFrame({\n        'Team': [team for team in TEAMS for _ in range(goals)],\n        'Goals': [randint(0, 5) for _ in range(goals * len(TEAMS))],\n        'Penalty Cost': [randint(0, penalties) * PENALTY_COST for _ in range(goals * len(TEAMS))]\n    })\n\n    # Calculate the total goals and penalty costs for each team\n    df_total = df.groupby('Team').sum().reset_index()\n\n    # Create a seaborn plot for goals\n    plt.figure(figsize=(10, 6))\n    ax1 = sns.barplot(x='Team', y='Goals', data=df_total)\n    ax1.set_title('Total Goals per Team')\n    ax1.set_xlabel('Team')\n    ax1.set_ylabel('Goals')\n\n    # Create a seaborn plot for penalty costs\n    plt.figure(figsize=(10, 6))\n    ax2 = sns.barplot(x='Team', y='Penalty Cost', data=df_total)\n    ax2.set_title('Total Penalty Costs per Team')\n    ax2.set_xlabel('Team')\n    ax2.set_ylabel('Penalty Cost (USD)')\n\n    # Return the DataFrame and the plot objects\n    return df, [ax1, ax2]\n\n# Example usage\ndf, plots = task_func(10, 5)\nprint(df)\nplt.show()\n```\n\nThis function `task_func` takes two arguments: `goals` (the number of matches to simulate per team) and `penalties` (the maximum number of penalties a team can have in a match). It generates random goals and penalties for each team, converts penalties into fines based on the `PENALTY_COST`, and then creates a DataFrame with these results. It also generates two Seaborn bar plots to visualize the total goals and penalty costs per team. The function returns the DataFrame and the plot objects."}
{"task_id": "BigCodeBench/636", "solution": "Here's a self-contained function that generates a Pandas DataFrame with random integer values between 0 and 9, counts the non-zero values in each column, and visualizes this information using a bar plot:\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.apply(lambda col: (col != 0).sum())\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Values in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    \n    # Show plot\n    plt.show()\n    \n    return df, ax\n\n# Example usage:\n# df, ax = task_func(10)\n```\n\nThis function `task_func` takes the number of rows as input, generates a DataFrame with random integers, counts the non-zero values in each column, and plots this information using a bar plot. The function returns a tuple containing the DataFrame and the matplotlib Axes object."}
{"task_id": "BigCodeBench/637", "solution": "Here's a self-contained Python function that generates a Pandas DataFrame with random grades for a specified number of students across multiple courses, calculates the required statistics, and visualizes the data using a bar plot:\n\n```python\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define courses\n    courses = ['Math', 'Science', 'History', 'English']\n    \n    # Generate random student IDs\n    student_ids = sample(range(1000, 9999), num_students)\n    \n    # Create DataFrame with random grades\n    data = {course: np.random.randint(0, 101, size=num_students) for course in courses}\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate average grade in each course\n    average_grades = df.mean()\n    \n    # Calculate number of students with passing grade (>= 60)\n    passing_counts = (df >= 60).sum()\n    \n    # Prepare data for plotting\n    plot_data = pd.DataFrame({'Average Grade': average_grades, 'Passing Count': passing_counts})\n    \n    # Create bar plot\n    ax = plot_data.plot(kind='bar', figsize=(10, 6), rot=0)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Counts / Grades')\n    ax.legend(loc='upper left')\n    \n    return df, ax\n\n# Example usage:\n# df, ax = task_func(30)\n# plt.show()\n```\n\nThis function `task_func` takes the number of students as input, generates random grades for each course, calculates the average grade and the number of students passing each course, and then plots this information in a bar chart. The function returns a tuple containing the DataFrame and the Axes object of the plot."}
{"task_id": "BigCodeBench/654", "solution": "Here's a self-contained Python function that fits an exponential decay function to the indices in the array where the first column matches the target value. The function returns a tuple containing the optimized parameters of the fitting function (`popt`) and the `matplotlib` Axes object.\n\n```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Convert the array to a numpy array for easier manipulation\n    array = np.array(array)\n    \n    # Filter the array to get the rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n    \n    # Extract the indices and the corresponding values to fit\n    indices = np.arange(len(filtered_array))\n    values = filtered_array[:, 1]\n    \n    # Define the exponential decay function to fit\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Initial guess for the parameters\n    p0 = [1, 1, 1]\n    \n    # Fit the exponential decay function to the data\n    popt, _ = optimize.curve_fit(exp_decay, indices, values, p0=p0)\n    \n    # Create a plot to visualize the fit\n    fig, ax = plt.subplots()\n    ax.scatter(indices, values, label='Data')\n    ax.plot(indices, exp_decay(indices, *popt), label='Fitted function', color='red')\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Exponential Decay Fit')\n    \n    # Return the optimized parameters and the Axes object\n    return (popt, ax)\n\n# Example usage:\n# array = np.array([[1, 2], [1, 3], [2, 4], [1, 5], [2, 6]])\n# target_value = 1\n# result = task_func(array, target_value)\n# print(result[0])  # Optimized parameters\n# result[1].show()  # Show the plot\n```\n\nThis function first filters the array to get the rows where the first column matches the target value. It then extracts the indices and corresponding values to fit an exponential decay function. The `curve_fit` function from `scipy.optimize` is used to fit the function, and a plot is created using `matplotlib` to visualize the fit. The function returns a tuple containing the optimized parameters (`popt`) and the `matplotlib` Axes object."}
{"task_id": "BigCodeBench/655", "solution": "Here is the self-contained Python function that performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF). The function includes preprocessing steps, TF-IDF vectorization, and NMF application to extract topics.\n\n```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(texts, num_topics):\n    # Preprocess the input texts\n    preprocessed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert all characters to lowercase\n        text = text.lower()\n        # Remove stopwords\n        text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n        preprocessed_texts.append(text)\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_vectors = vectorizer.fit_transform(preprocessed_texts)\n\n    # Apply NMF to extract the specified number of topics\n    nmf = NMF(n_components=num_topics, random_state=0)\n    nmf.fit(tfidf_vectors)\n\n    # Extract the most significant words for each topic\n    topics = []\n    feature_names = vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(nmf.components_):\n        topic_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(topic_words)\n\n    return topics\n\n# Example usage:\n# texts = [\"Example text one\", \"Another example text\", \"Yet another text example\"]\n# num_topics = 2\n# print(task_func(texts, num_topics))\n```\n\nThis function takes a list of text documents (`texts`) and the number of topics to extract (`num_topics`). It preprocesses the texts by removing non-alphanumeric characters, converting to lowercase, and removing stopwords. Then, it vectorizes the processed texts using TF-IDF and applies NMF to extract the specified number of topics. Each topic is represented as a list of its most significant words based on the NMF component weights. The exact output may vary depending on the TF-IDF vectorization and NMF initialization."}
{"task_id": "BigCodeBench/657", "solution": "Here's a self-contained Python function that processes the texts as specified, trains a Word2Vec model, and returns the trained model:\n\n```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # Download stopwords if not provided\n    if stopwords is None:\n        nltk.download('stopwords')\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n    \n    # Function to clean and tokenize text\n    def clean_and_tokenize(text):\n        # Remove non-alphanumeric characters except space, and lowercase\n        cleaned_text = ALPHANUMERIC.sub(' ', text).lower()\n        # Tokenize and remove stopwords\n        tokens = nltk.word_tokenize(cleaned_text)\n        return [word for word in tokens if word not in stopwords]\n    \n    # Process all texts\n    processed_texts = [clean_and_tokenize(text) for text in texts]\n    \n    # Train Word2Vec model\n    word2vec_model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return word2vec_model\n\n# Example usage:\n# texts = [\"This is a sample sentence.\", \"Another example here!\", \"And one more to go.\"]\n# model = task_func(texts)\n# print(model)\n```\n\nThis function includes:\n1. **Text Cleaning**: Removing non-alphanumeric characters and converting text to lowercase.\n2. **Tokenization and Stopword Removal**: Using NLTK's `word_tokenize` and a set of stopwords.\n3. **Training a Word2Vec Model**: Using the processed texts to train a `Word2Vec` model from the `gensim` library.\n\nYou can call this function with a list of texts to get a trained Word2Vec model. Adjust the parameters of the `Word2Vec` model (like `vector_size`, `window`, `min_count`, etc.) as needed based on your specific requirements."}
{"task_id": "BigCodeBench/678", "solution": "Here's a self-contained function that processes JSON files in a directory, reads them into a DataFrame, adds a \"Source\" column with the filename, and moves the processed files to a \"processed\" subdirectory:\n\n```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Ensure the path exists\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The directory {path} does not exist.\")\n\n    # Create the processed directory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    # Initialize an empty DataFrame to store all data\n    df = pd.DataFrame()\n\n    # List all JSON files in the directory and sort them alphabetically\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    json_files.sort()\n\n    for file_name in json_files:\n        file_path = os.path.join(path, file_name)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        \n        # Convert the JSON data to a DataFrame\n        temp_df = pd.DataFrame(data)\n        \n        # Add the \"Source\" column\n        temp_df['Source'] = file_name\n        \n        # Append the DataFrame to the main DataFrame\n        df = pd.concat([df, temp_df], ignore_index=True)\n        \n        # Move the processed file to the processed directory\n        shutil.move(file_path, os.path.join(processed_path, file_name))\n\n    return df\n```\n\nThis function does the following:\n1. Checks if the specified directory exists and raises an error if it doesn't.\n2. Creates a \"processed\" subdirectory if it doesn't already exist.\n3. Initializes an empty DataFrame to hold the combined data from all JSON files.\n4. Lists and sorts all JSON files in the directory alphabetically.\n5. Reads each JSON file, converts its contents to a DataFrame, adds a \"Source\" column with the filename, and appends it to the main DataFrame.\n6. Moves each processed file to the \"processed\" subdirectory.\n7. Returns the combined DataFrame containing data from all processed files."}
{"task_id": "BigCodeBench/720", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the directory and file path\n    dir_path = \"task_func_data\"\n    file_path = os.path.join(dir_path, \"Output.txt\")\n    \n    # Create the directory if it doesn't exist\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    \n    # Generate random sensor data\n    temperature = round(random.uniform(20.0, 30.0), 2)  # Random temperature between 20.0 and 30.0\n    humidity = round(random.uniform(30.0, 70.0), 2)     # Random humidity between 30.0 and 70.0\n    \n    # Prepare the data to write\n    data = {\n        'datetime': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n        'temperature': temperature,                                # Formatted temperature\n        'humidity': humidity                                       # Formatted humidity\n    }\n    \n    # Write data to the CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['datetime', 'temperature', 'humidity'])\n        if file.tell() == 0:  # Check if the file is empty\n            writer.writeheader()\n        writer.writerow(data)\n    \n    # Return the file path before deletion\n    return_path = file_path\n    \n    # Delete the file after use\n    if os.path.exists(file_path):\n        os.remove(file_path)\n    \n    return return_path\n\n# Example usage\nif __name__ == \"__main__\":\n    print(task_func())\n```\n\nThis function performs the following steps:\n1. Checks if the directory `task_func_data` exists and creates it if it doesn't.\n2. Generates random temperature and humidity values.\n3. Writes the current timestamp along with the temperature and humidity values to a CSV file named `Output.txt` in append mode.\n4. Returns the path to the CSV file.\n5. Deletes the CSV file after returning its path."}
{"task_id": "BigCodeBench/723", "solution": "Here's a self-contained Python function that scrapes data from a given URL and saves the scraped data to a CSV file. This function is designed to extract text from paragraph tags on the webpage and save them to a CSV file.\n\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Send a GET request to the URL\n    response = urllib.request.urlopen(url)\n    # If the GET request is successful, the status code will be 200\n    if response.getcode() == 200:\n        # Get the content of the response\n        page_content = response.read()\n        # Create a BeautifulSoup object and specify the parser\n        soup = BeautifulSoup(page_content, 'html.parser')\n        # Find all the paragraph tags on the webpage\n        paragraphs = soup.find_all('p')\n        # Create a list to store the scraped data\n        scraped_data = []\n        # Iterate over each paragraph tag\n        for paragraph in paragraphs:\n            # Get the text of the paragraph\n            text = paragraph.get_text()\n            # Add the text to the scraped data list\n            scraped_data.append([text])\n        # Check if the CSV file already exists\n        if os.path.exists(CSV_FILE_PATH):\n            # If the file exists, append the scraped data to it\n            with open(CSV_FILE_PATH, 'a', newline='') as csvfile:\n                writer = csv.writer(csvfile)\n                writer.writerows(scraped_data)\n        else:\n            # If the file does not exist, create it and write the scraped data to it\n            with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n                writer = csv.writer(csvfile)\n                writer.writerows(scraped_data)\n        # Return the path of the CSV file\n        return CSV_FILE_PATH\n    else:\n        # If the GET request is not successful, return None\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    url = 'http://example.com'  # Replace with the actual URL\n    csv_path = task_func(url)\n    print(f'Data saved to {csv_path}')\n```\n\n### Notes:\n1. **Customization Required**: The function currently extracts text from all paragraph tags (`<p>`). If you need to extract data from different HTML elements or with specific classes/IDs, you will need to adjust the `find_all` method accordingly.\n2. **Error Handling**: This script includes basic error handling for HTTP response codes. For a more robust solution, consider adding additional error handling for network issues, parsing errors, or changes in the webpage structure.\n3. **Ethics and Legality**: Ensure that your scraping activities comply with the website's `robots.txt` file and terms of service. Respect the website's resources by not overloading it with requests."}
{"task_id": "BigCodeBench/752", "solution": "Here's the self-contained code for the function `task_func` that meets the specified requirements:\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    \"\"\"\n    This function trains a linear regression model and returns the model score of the test set.\n    \n    Parameters:\n    data (pd.DataFrame): The DataFrame used as training data.\n    target_column (str): The column used as target in training the model.\n    test_size (float, optional): The proportion of the data to include in the test set. Defaults to 0.2.\n    random_state (int, optional): The seed used to shuffle the data before splitting. Defaults to 0.\n    \n    Returns:\n    float: The model's score.\n    \"\"\"\n    \n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    \n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(\"Target column is not a column of data.\")\n    \n    # Check if data contains non-numeric values\n    if not all(data.dtypes == np.number):\n        raise ValueError(\"Data contains non-numeric values.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state is not an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"Test size is not between 0 and 1.\")\n    \n    # Split the data into a training set and a test set\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Train a linear regression model on the training set\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    return model.score(X_test, y_test)\n```\n\nThis function performs the following steps:\n1. Validates the input data and parameters.\n2. Splits the data into training and test sets.\n3. Trains a linear regression model.\n4. Returns the model's score on the test set.\n\nYou can use this function to train a linear regression model and get the score on the test set, while ensuring that all specified exceptions are raised for invalid inputs."}
{"task_id": "BigCodeBench/760", "solution": "Here's the complete implementation of the `task_func` function as described:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    # Set the random seed for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Combine the lists of names\n    names = latin_names + other_names\n    \n    # Initialize lists to store the generated data\n    ids = []\n    names_list = []\n    dates_of_birth = []\n    emails = []\n    \n    # Generate 100 records\n    for i in range(100):\n        # Randomly select a name\n        name = np.random.choice(names)\n        \n        # Correct any improper encoding of Latin characters\n        name = codecs.decode(name, 'utf-8')\n        \n        # Generate a random date of birth\n        year = np.random.randint(start_year, end_year + 1)\n        month = np.random.randint(1, 13)\n        day = np.random.randint(1, 29)  # Simplified to avoid dealing with month lengths\n        date_of_birth = datetime.datetime(year, month, day)\n        \n        # Construct the email\n        email = f\"{name.lower()}{year}@{email_domain}\"\n        \n        # Add the generated data to the lists\n        ids.append(i + 1)\n        names_list.append(name)\n        dates_of_birth.append(date_of_birth)\n        emails.append(email)\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names_list,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n    \n    return df\n\n# Example usage\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\nprint(df)\n```\n\nThis function generates a DataFrame with 100 records, each containing an ID, Name, Date of Birth, and Email. The names are randomly selected from the provided lists, and the Date of Birth is randomly generated between the specified years. The email addresses are constructed using the name, year of birth, and provided email domain, with special characters in Latin names being properly encoded."}
{"task_id": "BigCodeBench/763", "solution": "Here's a self-contained Python function that reads from a JSON file, calculates the mean and median for each key, and writes the results to a CSV file:\n\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a dictionary to store the results\n    results = defaultdict(dict)\n\n    # Calculate the mean and median for each key\n    for key in data[0].keys():\n        values = [d[key] for d in data if isinstance(d[key], (int, float))]\n        results[key]['mean'] = np.mean(values)\n        results[key]['median'] = np.median(values)\n\n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['key', 'mean', 'median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for key, value in results.items():\n            writer.writerow({'key': key, 'mean': value['mean'], 'median': value['median']})\n\n    return results\n```\n\n### Explanation:\n1. **Reading the JSON File**: The function starts by reading the JSON file specified by `input_file`.\n2. **Calculating Statistics**: It initializes a `defaultdict` to store the results. For each key in the first dictionary of the JSON data, it collects all values associated with that key, ensuring they are numbers (either int or float).\n3. **Computing Mean and Median**: It computes the mean and median for each key using `numpy`.\n4. **Writing to CSV**: The results are written to a CSV file specified by `output_file`. The CSV file includes columns for the key, mean, and median.\n5. **Returning the Results**: The function returns a dictionary containing the mean and median for each key.\n\nThis function assumes that the JSON file contains a list of dictionaries where each dictionary has the same keys and the values are either integers or floats."}
{"task_id": "BigCodeBench/765", "solution": "Here's a self-contained Python function that processes files from a dictionary, checks if they exist and have content, and then copies them to a target directory:\n\n```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists, \n    and if it has content, then copies it to a target directory.\n\n    Args:\n    kwargs (dict): A dictionary containing file paths as keys and their contents as values.\n    target_dir (str): The target directory where files will be copied. Defaults to \"non_none_files\".\n\n    Returns:\n    dict: A dictionary with a key \"copied_files\" containing a list of full file paths that were copied.\n    \"\"\"\n    # Initialize an empty list to store the paths of copied files\n    copied_files = []\n\n    # Create the target directory if it doesn't exist\n    target_dir_path = Path(target_dir)\n    target_dir_path.mkdir(parents=True, exist_ok=True)\n\n    # Iterate over the dictionary items\n    for file_path, file_content in kwargs.items():\n        # Check if the file exists and has content\n        if os.path.exists(file_path) and file_content:\n            # Copy the file to the target directory\n            target_file_path = target_dir_path / Path(file_path).name\n            shutil.copy2(file_path, target_file_path)\n            # Add the full path of the copied file to the list\n            copied_files.append(str(target_file_path))\n\n    return {\"copied_files\": copied_files}\n\n# Example usage:\n# file_dict = {\n#     \"/path/to/file1.txt\": \"Hello, world!\",\n#     \"/path/to/file2.txt\": \"\",\n#     \"/path/to/file3.txt\": \"This is a test file.\"\n# }\n# copied_files = task_func(file_dict)\n# print(copied_files)\n```\n\nThis function does the following:\n1. Ensures the target directory exists.\n2. Iterates over the items in the provided dictionary.\n3. Checks if each file exists and has content.\n4. Copies the file to the target directory if it meets the criteria.\n5. Collects the paths of the copied files in a list and returns them in a dictionary with the key \"copied_files\"."}
{"task_id": "BigCodeBench/771", "solution": "Here's a self-contained Python function that processes CSV files in a specified directory based on a given pattern, creates new files with altered names while preserving the content, and returns a list of the new filenames:\n\n```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    This function processes CSV files in a directory based on a specified pattern and creates new files with altered names while preserving the content.\n\n    Args:\n        directory (str): The directory where the CSV files are located.\n        pattern (str): A regular expression pattern to match the CSV file names. Defaults to r'^(.*?)-\\d+\\.csv$'.\n\n    Returns:\n        list: A list of strings, where each string is the filename of a new CSV file created by the function.\n    \"\"\"\n    new_files = []\n    directory_path = Path(directory)\n    \n    # Ensure the directory exists\n    if not directory_path.is_dir():\n        raise ValueError(f\"The directory {directory} does not exist.\")\n    \n    # Compile the regex pattern\n    regex = re.compile(pattern)\n    \n    # Iterate over all files in the directory\n    for file_path in directory_path.iterdir():\n        if file_path.is_file() and file_path.suffix == '.csv':\n            match = regex.match(file_path.name)\n            if match:\n                # Extract the base name from the pattern\n                base_name = match.group(1)\n                # Create a new file name\n                new_file_name = f\"{base_name}.csv\"\n                new_file_path = directory_path / new_file_name\n                \n                # Copy the content to the new file\n                with open(file_path, 'r', newline='') as old_file, open(new_file_path, 'w', newline='') as new_file:\n                    reader = csv.reader(old_file)\n                    writer = csv.writer(new_file)\n                    for row in reader:\n                        writer.writerow(row)\n                \n                new_files.append(str(new_file_path))\n    \n    return new_files\n\n# Example usage:\n# new_files = task_func('/path/to/directory')\n# print(new_files)\n```\n\n### Explanation:\n1. **Imports**: The necessary modules are imported.\n2. **Function Definition**: The function `task_func` is defined with parameters `directory` and `pattern`.\n3. **Directory Check**: It checks if the provided directory exists.\n4. **Regex Compilation**: The provided pattern is compiled into a regex object.\n5. **File Iteration**: It iterates over all files in the directory, checking if they are CSV files and if they match the pattern.\n6. **New Filename Creation**: For matching files, it extracts the base name and creates a new filename.\n7. **Content Copying**: It copies the content of the old CSV file to the new CSV file.\n8. **Return**: It returns a list of the new filenames.\n\nThis function ensures that the content of the CSV files is preserved while renaming them according to the specified pattern."}
{"task_id": "BigCodeBench/777", "solution": "Here's a Python function that accomplishes the task described. This function will search for zip files in the specified directory that match the given pattern, extract them into directories named by the prefix of the filename, and return a list of directories where the files were extracted.\n\n```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzip all zip files in a directory whose name matches a certain pattern.\n\n    Parameters:\n    directory (str): The directory where the zip files are located.\n    pattern (str): A regular expression pattern to match the zip file names.\n\n    Returns:\n    list: A list of directories where the files were extracted.\n    \"\"\"\n    # Compile the pattern\n    regex = re.compile(pattern)\n\n    # Initialize an empty list to store the directories where files were extracted\n    extracted_dirs = []\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file name matches the pattern\n        if regex.match(filename):\n            # Extract the prefix part of the filename\n            prefix = filename.rsplit('-', 1)[0]\n\n            # Create the directory to extract the files if it does not exist\n            extract_dir = os.path.join(directory, prefix)\n            if not os.path.exists(extract_dir):\n                os.makedirs(extract_dir)\n\n            # Extract the zip file\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n\n            # Add the directory to the list of extracted directories\n            extracted_dirs.append(extract_dir)\n\n    return extracted_dirs\n\n# Example usage:\n# extracted_dirs = task_func('/path/to/directory')\n# print(extracted_dirs)\n```\n\nThis function takes two parameters: `directory` and `pattern`. The `directory` parameter is the path to the directory where the zip files are located, and the `pattern` parameter is a regular expression pattern to match the zip file names. The default pattern is `r'^(.*?)-\\d+\\.zip$'`, which matches any file name that starts with any characters (captured in group 1), followed by a hyphen, followed by one or more digits, and ends with \".zip\".\n\nThe function compiles the pattern using the `re.compile` function, then iterates over all files in the specified directory using the `os.listdir` function. For each file, it checks if the file name matches the pattern using the `match` method of the compiled pattern. If the file name matches the pattern, it extracts the prefix part of the filename by splitting the filename at the last occurrence of \"-\" using the `rsplit` method, and creates the directory to extract the files if it does not exist using the `os.makedirs` function.\n\nFinally, it extracts the zip file using the `ZipFile` class from the `zipfile` module, and adds the directory to the list of extracted directories. The function returns the list of extracted directories."}
{"task_id": "BigCodeBench/785", "solution": "Here's a self-contained Python function that archives all files matching a given pattern into a single ZIP file and then deletes the original files. The function returns the path to the created archive file.\n\n```python\nimport subprocess\nimport os\nimport glob\nimport zipfile\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    if not files_to_archive:\n        raise FileNotFoundError(f\"No files found matching the pattern: {pattern}\")\n\n    # Create a unique archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, f\"archive_{hash(pattern)}.zip\")\n\n    # Archive the files\n    with zipfile.ZipFile(archive_file, 'w') as zf:\n        for file in files_to_archive:\n            zf.write(file, os.path.basename(file))\n            os.remove(file)  # Delete the original file after adding to the archive\n\n    return archive_file\n\n# Example usage:\n# archive_path = task_func('*.txt')\n# print(f\"Archive created at: {archive_path}\")\n```\n\n### Explanation:\n1. **Directory Check and Creation**: The function first checks if the archive directory exists and creates it if it doesn't.\n2. **File Matching**: It uses `glob.glob(pattern)` to find all files matching the specified pattern.\n3. **Archive File Naming**: A unique archive file name is generated using a hash of the pattern to avoid conflicts.\n4. **Zipping and Deletion**: The function creates a ZIP file and adds each matching file to it, then deletes the original file.\n5. **Return Value**: The path to the created archive file is returned.\n\nThis function assumes that the pattern provided is valid and that the files are not currently in use or protected. Handling more complex scenarios (like file locks or permissions) would require additional error handling and checks."}
{"task_id": "BigCodeBench/800", "solution": "Here's the complete code for the function `task_func` which reads a CSV file, counts the total number of goals and penalties, updates these counts with the given goals and penalties, and returns a `Counter` object with the updated counts:\n\n```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter object to store the total counts\n    count = Counter({'goals': 0, 'penalties': 0})\n\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            # Increment the counts for each row\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n\n    # Update the counts with the given goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    return count\n\n# Example usage:\nif __name__ == \"__main__\":\n    setup_csv()\n    result = task_func(2, 3, 'test_data/test_case_2.csv')\n    print(result)  # Output: Counter({'goals': 8, 'penalties': 6})\n```\n\nThis code defines a function `task_func` that performs the following steps:\n1. Initializes a `Counter` object to keep track of the counts of goals and penalties.\n2. Reads the CSV file specified by `csv_file_path` and updates the `Counter` object with the counts of goals and penalties from the file.\n3. Adds the given goals and penalties to the `Counter` object.\n4. Returns the updated `Counter` object.\n\nThe `setup_csv` function is provided to create a test CSV file with sample data. The `create_test_csv` function is a helper to write the CSV content. The example usage at the end demonstrates how to call `task_func` with additional goals and penalties and print the result."}
{"task_id": "BigCodeBench/826", "solution": "Here's a self-contained Python function that moves files from a source directory to a target directory based on a specified pattern. The function uses regular expressions to match file names and the `shutil` module to move the files. It returns the count of files that were successfully moved.\n\n```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Compile the regular expression pattern\n    pattern = re.compile(file_pattern)\n    \n    # Initialize a counter for moved files\n    moved_files_count = 0\n    \n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the file name matches the pattern\n        if pattern.match(filename):\n            # Construct full file path\n            source_file_path = os.path.join(source_dir, filename)\n            target_file_path = os.path.join(target_dir, filename)\n            \n            # Move the file to the target directory\n            shutil.move(source_file_path, target_file_path)\n            \n            # Increment the counter\n            moved_files_count += 1\n    \n    # Return the number of files that were moved\n    return moved_files_count\n\n# Example usage:\n# moved_count = task_func('/source/directory', '/target/directory')\n# print(f\"Moved {moved_count} files.\")\n```\n\nThis function starts by compiling the regular expression pattern provided by `file_pattern`. It then iterates through each file in the `source_dir`, checks if the file name matches the pattern, and if so, moves the file to the `target_dir`. The function finally returns the count of files that were successfully moved."}
{"task_id": "BigCodeBench/845", "solution": "Here's a self-contained Python function that calculates both the cosine similarity and the Levenshtein ratio between two texts after cleaning them as specified:\n\n```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    # Clean texts\n    clean_text1 = re.sub(ALPHANUMERIC, ' ', text1).lower()\n    clean_text2 = re.sub(ALPHANUMERIC, ' ', text2).lower()\n    \n    # Tokenize texts\n    words1 = clean_text1.split()\n    words2 = clean_text2.split()\n    \n    # Compute term frequencies\n    tf1 = Counter(words1)\n    tf2 = Counter(words2)\n    \n    # Get unique words from both texts\n    unique_words = set(words1).union(set(words2))\n    \n    # Create vectors\n    vector1 = np.array([tf1.get(word, 0) for word in unique_words])\n    vector2 = np.array([tf2.get(word, 0) for word in unique_words])\n    \n    # Compute cosine similarity\n    cosine_sim = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n    \n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(clean_text1, clean_text2)\n    \n    return (cosine_sim, levenshtein_ratio)\n\n# Example usage\ntext1 = \"Hello, world!\"\ntext2 = \"Hello, everyone!\"\nprint(task_func(text1, text2))\n```\n\nThis function performs the following steps:\n1. Cleans the input texts by removing all non-alphanumeric characters except spaces and converting them to lowercase.\n2. Tokenizes the cleaned texts into words.\n3. Computes the term frequency (TF) for each word in both texts.\n4. Creates vectors for each text based on the term frequencies of the unique words in both texts.\n5. Calculates the cosine similarity between these vectors.\n6. Computes the Levenshtein ratio using the `ratio` function from the `python-Levenshtein` library.\n7. Returns a tuple containing the cosine similarity and the Levenshtein ratio."}
{"task_id": "BigCodeBench/854", "solution": "Here's the complete implementation of the `task_func` function as described:\n\n```python\nfrom functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Check if numbers is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all numbers are non-negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative\")\n    \n    # Handle the case of an empty list\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations of the list\n    all_permutations = list(permutations(numbers))\n    \n    # Calculate the sum of the factorials of each number in each permutation\n    factorial_sums = [reduce(lambda x, y: x + math.factorial(y), perm, 0) for perm in all_permutations]\n    \n    return factorial_sums, all_permutations\n\n# Example usage:\nfac, perm = task_func([0, 4])\nprint(fac)  # Output: [25, 25]\nprint(perm)  # Output: [(0, 4), (4, 0)]\n```\n\nThis function first checks if the input is a list of integers and that all numbers are non-negative. It then handles the case of an empty list by returning two empty lists. It generates all permutations of the input list and calculates the sum of the factorials for each permutation using `reduce`. Finally, it returns the list of factorial sums and the list of all permutations."}
{"task_id": "BigCodeBench/857", "solution": "Here's a self-contained Python function that transfers files from one directory to another based on specified file extensions and issues warnings for files that could not be transferred:\n\n```python\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    \n    # Ensure the destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    # Convert extensions to a list if it's not already\n    if isinstance(EXTENSIONS, str):\n        EXTENSIONS = [EXTENSIONS]\n    \n    # Iterate over each extension\n    for ext in EXTENSIONS:\n        # Find all files with the current extension in the source directory\n        files = glob.glob(os.path.join(SOURCE_DIR, f'*.{ext}'))\n        \n        # Transfer each file\n        for file_path in files:\n            try:\n                # Construct the destination path\n                dest_path = os.path.join(DEST_DIR, os.path.basename(file_path))\n                \n                # Copy the file to the destination directory\n                shutil.copy2(file_path, dest_path)\n                \n                # Record the successfully transferred file\n                transferred_files.append(os.path.basename(file_path))\n            except Exception as e:\n                # Issue a warning if the file could not be transferred\n                warnings.warn(f\"Could not transfer {file_path}: {e}\")\n    \n    return transferred_files\n\n# Example usage:\n# transferred = task_func('/source/directory', '/destination/directory', ['txt', 'jpg'])\n# print(transferred)\n```\n\nThis function does the following:\n1. Ensures the destination directory exists.\n2. Handles both single and multiple file extensions.\n3. Uses `glob` to find files with the specified extensions in the source directory.\n4. Attempts to copy each file to the destination directory using `shutil.copy2`.\n5. Records successfully transferred files in a list.\n6. Issues warnings for any files that could not be transferred due to exceptions.\n\nYou can call this function with the appropriate source directory, destination directory, and a list of file extensions to transfer the files as required."}
{"task_id": "BigCodeBench/865", "solution": "Here is the complete implementation of the function `task_func` that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item[0] for item in data]\n    counts = np.array([item[1] for item in data]).reshape(-1, 1)\n    weights = np.array([item[2] for item in data]).reshape(-1, 1)\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts, axis=0)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights)\n    \n    # Create a DataFrame with the items, normalized counts, and normalized weights\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts.flatten(),\n        'Normalized Weight': normalized_weights.flatten()\n    })\n    \n    return df\n\n# Example usage:\n# data = [('item1', 10, 0.5), ('item2', 20, 0.7), ('item3', 30, 0.9)]\n# result = task_func(data)\n# print(result)\n```\n\nThis function first extracts the items, counts, and weights from the input list of tuples. It then normalizes the counts using z-score normalization and the weights using min-max scaling. Finally, it constructs a pandas DataFrame with the items, normalized counts, and normalized weights, and returns this DataFrame."}
{"task_id": "BigCodeBench/870", "solution": "Here's the self-contained code for the function `task_func` that calculates the mean of numerical values in each position across tuples in a list and returns a DataFrame with the mean values:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Transpose the list of tuples\n    transposed_data = list(zip(*data_list))\n\n    # Initialize an empty list to store the mean values\n    mean_values = []\n\n    # Iterate over each position in the transposed data\n    for position in transposed_data:\n        # Filter out non-numeric values\n        numeric_values = [value for value in position if isinstance(value, (int, float))]\n\n        # Calculate the mean of the numeric values\n        mean_value = np.mean(numeric_values) if numeric_values else np.nan\n\n        # Append the mean value to the list\n        mean_values.append(mean_value)\n\n    # Create a DataFrame with the mean values\n    df = pd.DataFrame({'Mean Value': mean_values}, index=[f'Position {i}' for i in range(len(mean_values))])\n\n    return df\n\n# Example usage\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)\nprint(df)\n```\n\nThis code will output a DataFrame with the mean values of the numerical data at each position, with the index formatted as 'Position i'. Non-numeric values are ignored in the calculation. If an empty list is passed, it will return an empty DataFrame."}
{"task_id": "BigCodeBench/879", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    Perform a chi-square test of independence of variables in a contingency table.\n\n    Parameters:\n    data (DataFrame): A DataFrame containing categorical data.\n    col1 (str): The name of the first column.\n    col2 (str): The name of the second column.\n\n    Returns:\n    float: The p-value of the chi-square test of independence.\n\n    Raises:\n    ValueError: If 'data' is empty, if 'col1' or 'col2' are not in 'data', \n                 if one or both of the columns do not have multiple categories, \n                 or if some categories have less than 5 observations.\n    TypeError: If one or both of the columns contain non-categorical data.\n    \"\"\"\n    \n    # Check if 'data' is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame 'data' is empty.\")\n    \n    # Check if 'col1' and 'col2' are in 'data'\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns 'col1' and 'col2' are not in 'data'.\")\n    \n    # Check if 'col1' and 'col2' are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both of the columns 'col1' and 'col2' contain non-categorical data.\")\n    \n    # Check if 'col1' and 'col2' have multiple categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns 'col1' and 'col2' do not have multiple categories.\")\n    \n    # Check if some categories have less than 5 observations\n    if (data[col1].value_counts() < 5).any() or (data[col2].value_counts() < 5).any():\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Construct the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Return the p-value\n    return p\n\n# Example usage:\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})\ndata['a'] = data['a'].astype('category')\ndata['b'] = data['b'].astype('category')\n\nprint(task_func(data, 'a', 'b'))\n```\n\nThis function performs a chi-square test of independence on a contingency table constructed from two categorical columns in a DataFrame. It includes error handling for the specified conditions and returns the p-value of the test. The example usage at the end demonstrates how to use the function with a sample DataFrame."}
{"task_id": "BigCodeBench/897", "solution": "Here's a self-contained Python function that simulates a number of dice rolls, calculates the frequency of each result, and returns both the frequency array and a histogram of the results:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    # Set seed for reproducibility\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(6, dtype=int)\n    for result in results:\n        frequency[result - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(1, 8) - 0.5, rwidth=0.8, color='blue', edgecolor='black')\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    \n    # Return the frequency array and the Axes object\n    return (frequency, ax)\n\n# Example usage:\n# freq, ax = task_func(1000, seed=42)\n# plt.show()\n```\n\nThis function `task_func` takes two arguments: `rolls` (the number of dice rolls to simulate) and an optional `seed` for the random number generator to ensure reproducibility. It simulates the dice rolls, calculates the frequency of each outcome, and plots a histogram. The function returns a tuple containing a numpy array with the frequency of each outcome and a matplotlib `Axes` object representing the histogram."}
{"task_id": "BigCodeBench/906", "solution": "Here's a self-contained Python function that accomplishes the task of archiving all files with the '_processed' suffix from a source directory to a target directory:\n\n```python\nimport zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from a source directory to a target directory.\n    \n    The function identifies processed files by the '_processed' suffix in the filename.\n    \n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        archive_name (str, optional): The name of the archive file. Defaults to 'archive.zip'.\n    \n    Returns:\n        str: The path to the created archive.\n    \"\"\"\n    \n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Define the pattern to match processed files\n    processed_pattern = re.compile(r'.*_processed\\..+')\n\n    # Create a list to hold paths of processed files\n    processed_files = []\n\n    # Walk through the source directory to find processed files\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if processed_pattern.match(file):\n                file_path = os.path.join(root, file)\n                processed_files.append(file_path)\n\n    # Define the full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create the zip archive\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in processed_files:\n            zipf.write(file, os.path.relpath(file, source_dir))\n\n    return archive_path\n\n# Example usage:\n# archive_path = task_func('/source/directory', '/target/directory', 'processed_files.zip')\n# print(f'Archive created at: {archive_path}')\n```\n\nThis function:\n1. Ensures the target directory exists.\n2. Uses a regular expression to identify files with the '_processed' suffix.\n3. Collects the paths of all such files in the source directory.\n4. Creates a zip archive in the target directory containing all the identified files.\n5. Returns the path to the created archive."}
{"task_id": "BigCodeBench/914", "solution": "Here's a self-contained Python function that uses simple linear regression to predict stock closing prices for the next 7 days and plots the data:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure the DataFrame has a 'Close' column and a datetime index\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must have a 'Close' column\")\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame must have a datetime index\")\n\n    # Prepare the data for linear regression\n    X = np.array(df.index.astype(np.int64) // 10**9).reshape(-1, 1)  # Convert datetime to Unix timestamp\n    y = df['Close'].values\n\n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Generate future timestamps for the next 7 days\n    last_timestamp = X[-1][0]\n    future_timestamps = np.array([last_timestamp + i * 24 * 60 * 60 for i in range(1, 8)]).reshape(-1, 1)\n\n    # Predict the closing prices for the next 7 days\n    future_prices = model.predict(future_timestamps)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df.index, y, label='Historical Prices')\n    future_dates = pd.to_datetime([pd.Timestamp(ts * 10**9) for ts in future_timestamps[:, 0]])\n    ax.plot(future_dates, future_prices, label='Predicted Prices', linestyle='--')\n    ax.legend()\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Prices Prediction')\n\n    return (future_prices.tolist(), ax)\n\n# Example usage:\n# df = pd.read_csv('stock_data.csv', index_col='Date', parse_dates=True)\n# predicted_prices, plot_axes = task_func(df)\n# plt.show()\n```\n\nThis function assumes that the input DataFrame `df` has a column named 'Close' containing the stock closing prices and that the index is a datetime index. It converts the datetime index to Unix timestamps for the linear regression model, predicts the future prices, and plots both the historical and predicted prices. The function returns a tuple containing the list of predicted prices and the matplotlib Axes object with the plot."}
{"task_id": "BigCodeBench/915", "solution": "Here is the self-contained Python function that identifies and plots outliers in the 'closing_price' column of a given DataFrame using the Z-Score method. The function allows customization of the Z-Score threshold for identifying outliers and returns a tuple containing the DataFrame of outliers and the plot object:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\n\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[np.abs(z_scores) > z_threshold]\n    \n    # Plotting the data\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df.index, df['closing_price'], label='Closing Price', color='blue')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outlier')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    ax.grid(True)\n    \n    return outliers, ax\n\n# Example usage\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\nprint(outliers2)\nplt.show()\n```\n\nThis function performs the following steps:\n1. Calculates the Z-Scores for the 'closing_price' column using `scipy.stats.zscore`.\n2. Identifies outliers by selecting rows where the absolute Z-Score exceeds the specified threshold.\n3. Plots the closing prices with outliers highlighted in red.\n4. Returns a tuple containing the DataFrame of outliers and the plot object.\n\nYou can customize the `z_threshold` parameter to adjust the sensitivity for identifying outliers. The example usage at the end demonstrates how to call the function and display the results."}
{"task_id": "BigCodeBench/916", "solution": "Here's a self-contained Python function that creates a figure with both a box plot and a histogram for stock closing prices using `matplotlib` and `seaborn`:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Parameters:\n    df (pd.DataFrame): A pandas DataFrame containing the stock closing prices.\n    \n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    \n    # Ensure the DataFrame has a column named 'Close'\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a column named 'Close'\")\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Plot the box plot\n    sns.boxplot(x=df['Close'], ax=axs[0])\n    axs[0].set_title('Box Plot of Closing Prices')\n\n    # Plot the histogram\n    sns.histplot(df['Close'], kde=True, ax=axs[1])\n    axs[1].set_title('Histogram of Closing Prices')\n\n    # Adjust layout for better visualization\n    plt.tight_layout()\n\n    # Return the axes objects\n    return axs\n\n# Example usage:\n# df = pd.DataFrame({'Close': [100, 102, 101, 105, 103, 104, 107, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120]})\n# axes = task_func(df)\n# plt.show()\n```\n\nThis function takes a DataFrame as input, where the DataFrame must contain a column named 'Close'. It then creates a figure with two subplots: one for the box plot and one for the histogram. The function returns a tuple containing the two `Axes` objects. The titles for the box plot and histogram are set as specified."}
{"task_id": "BigCodeBench/917", "solution": "Here's a self-contained function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has a datetime index\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    \n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5,1,0))  # Example order, you might need to tune this\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecast_values = forecast.tolist()\n    \n    # Plot the forecast\n    fig, ax = plt.subplots()\n    df['Close'].plot(ax=ax, label='Historical Prices')\n    forecast.plot(ax=ax, label='Forecasted Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n    \n    return forecast_values, ax\n\n# Example usage:\n# df = pd.read_csv('stock_prices.csv', index_col='Date', parse_dates=True)\n# forecasted_prices, plot_axes = task_func(df)\n# plt.show()\n```\n\nThis function assumes that the DataFrame `df` has a column named 'Close' containing the closing prices and that the index is a datetime index. The ARIMA model's order is set to (5,1,0) as an example; you may need to adjust this based on the characteristics of your specific dataset. The function returns a tuple containing the forecasted prices for the next 7 days and a matplotlib Axes object with the plot."}
{"task_id": "BigCodeBench/928", "solution": "Here's the self-contained code that accomplishes the task described:\n\n```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    This function creates a dictionary containing all possible two-letter combinations \n    of the lowercase English alphabets. The dictionary values represent the frequency \n    of these two-letter combinations in the given word. If a combination does not \n    appear in the word, its value will be 0.\n\n    Args:\n        word (str): The input word.\n\n    Returns:\n        dict: A dictionary with keys as two-letter alphabet combinations and values \n        as their counts in the word.\n    \"\"\"\n    \n    # Generate all possible two-letter combinations of the lowercase English alphabets\n    all_combinations = [''.join(p) for p in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Convert the word to lowercase\n    word = word.lower()\n    \n    # Initialize a dictionary with all combinations as keys and 0 as values\n    combination_counts = {combination: 0 for combination in all_combinations}\n    \n    # Count the frequency of each combination in the word\n    for i in range(len(word) - 1):\n        combination = word[i:i+2]\n        if combination in combination_counts:\n            combination_counts[combination] += 1\n    \n    return combination_counts\n\n# Example usage:\n# result = task_func(\"hello\")\n# print(result)\n```\n\nThis function `task_func` takes a string `word` as input and returns a dictionary where the keys are all possible two-letter combinations of the lowercase English alphabets, and the values are their counts in the given word. If a combination does not appear in the word, its value in the dictionary will be 0."}
{"task_id": "BigCodeBench/942", "solution": "Here's a self-contained Python function that generates a sales report for different categories over a specified period of time and visualizes this data using Matplotlib:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Initialize DataFrame\n    data = {\n        'Date': np.repeat(dates, len(categories)),\n        'Category': categories * periods\n    }\n    \n    # Generate random sales data\n    np.random.seed(0)  # For reproducibility\n    data['Sales'] = np.random.randint(100, 1000, size=len(data['Date']))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n    \n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func()\nplt.show()\n```\n\nThis function does the following:\n1. Generates a date range starting from `start_date` with `periods` number of dates, using the specified frequency `freq`.\n2. Creates a DataFrame with columns 'Date', 'Category', and 'Sales'. The 'Date' and 'Category' columns are populated based on the generated dates and the list of categories. The 'Sales' column is populated with random integer values between 100 and 1000.\n3. Plots the sales data for each category over time using Matplotlib, with each category represented by a different line.\n4. Returns the DataFrame and the Matplotlib Axes object for further manipulation or inspection."}
{"task_id": "BigCodeBench/945", "solution": "Here's a self-contained Python function that generates a time series of sales data starting from a specified date, uses linear regression to forecast future sales, and outputs a numpy array containing the forecasted future sales for the same number of periods as the input data.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate sales data if not provided\n    if sales_data is None:\n        np.random.seed(0)\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the sales data\n    df = pd.DataFrame({'date': date_range, 'sales': sales_data})\n    \n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Set the date column as the index\n    df.set_index('date', inplace=True)\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Prepare the data for the model\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['sales'].values\n    \n    # Fit the model\n    model.fit(X, y)\n    \n    # Forecast future sales\n    future_X = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_X)\n    \n    # Return the forecasted future sales\n    return future_sales\n\n# Example usage\nforecasted_sales = task_func(start_date='2022-01-01', periods=13, freq='WOM-2FRI')\nprint(forecasted_sales)\n```\n\nThis function does the following:\n1. If `sales_data` is not provided, it generates random sales data.\n2. It creates a date range using the specified start date, number of periods, and frequency.\n3. It creates a DataFrame with the date range and corresponding sales data.\n4. It converts the date column to datetime and sets it as the index.\n5. It fits a linear regression model using the time index as the predictor and sales as the response variable.\n6. It forecasts future sales for the same number of periods as the input data.\n7. It returns the forecasted future sales as a numpy array."}
{"task_id": "BigCodeBench/952", "solution": "Here is the self-contained code for the `task_func` function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_task_list = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Randomly select n_tasks from the sanitized task list\n    selected_tasks = random.sample(sanitized_task_list, n_tasks)\n\n    # Assign each task to a random employee\n    assignments = [\n        {\n            \"Task Name\": task,\n            \"Assigned To\": random.choice(employees),\n            \"Due Date\": datetime.today().strftime('%Y-%m-%d')\n        }\n        for task in selected_tasks\n    ]\n\n    # Create a DataFrame from the assignments\n    df = pd.DataFrame(assignments)\n\n    return df\n\n# Example usage:\n# task_list = [\"Task One\", \"Task Two\", \"Task Three\", \"Task Four\"]\n# n_tasks = 2\n# print(task_func(task_list, n_tasks))\n```\n\nThis function takes a list of task names, the number of tasks to assign, a list of employees, and an optional seed for random number generation. It sanitizes the task names, randomly selects tasks, assigns them to employees, sets the due date to the current date, and returns a DataFrame with the assignments. If `n_tasks` is negative, it raises a `ValueError`."}
{"task_id": "BigCodeBench/955", "solution": "Here's a self-contained Python function that meets the specified requirements. This function takes a list of strings and a text input, replaces spaces with underscores in each word, counts the frequency of each unique word (case-insensitive), and plots the frequency using matplotlib. It also handles the specified exception for an empty input text.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Replace spaces with underscores and convert to lowercase\n    modified_words = [word.replace(' ', '_').lower() for word in text.split()]\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(modified_words)\n    \n    # Prepare data for plotting\n    words, frequencies = zip(*word_counts.items())\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to make room for rotated labels\n    \n    return ax\n\n# Example usage:\n# ax = task_func([\"Hello world\", \"This is a test\"], \"Hello world this is a test hello world\")\n# plt.show()\n```\n\nThis function performs the following steps:\n1. Checks if the input text is empty and raises a `ValueError` if it is.\n2. Splits the text into words, replaces spaces with underscores, and converts them to lowercase.\n3. Counts the frequency of each unique word using `Counter`.\n4. Prepares the data for plotting by separating words and their frequencies.\n5. Creates a bar plot using matplotlib, setting appropriate labels and title.\n6. Returns the `Axes` object of the plot.\n\nYou can uncomment the example usage at the end to test the function with sample data."}
{"task_id": "BigCodeBench/963", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from a source directory and save it as a zip file saved to a target directory.\n\n    Args:\n        source_directory (str): The path to the source directory.\n        target_directory (str): The path to the target directory.\n        zip_name (str): The name of the zip file.\n\n    Returns:\n        str: The full path to the created zip file.\n\n    Raises:\n        OSError: If the source_directory does not exist.\n    \"\"\"\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n\n    # Define the valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Create the target directory if it does not exist\n    target_dir_path = Path(target_directory)\n    target_dir_path.mkdir(parents=True, exist_ok=True)\n\n    # Create the zip file\n    zip_file_path = target_dir_path / f\"{zip_name}.zip\"\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        # Iterate over the files in the source directory\n        for file_path in glob.glob(os.path.join(source_directory, '*')):\n            # Check if the file has a valid extension\n            file_ext = os.path.splitext(file_path)[1]\n            if file_ext in valid_extensions:\n                # Add the file to the zip file\n                zip_file.write(file_path, os.path.basename(file_path))\n\n    # Return the full path to the created zip file\n    return str(zip_file_path)\n\n# Example usage:\n# source_dir = \"/path/to/source\"\n# target_dir = \"/path/to/target\"\n# zip_filename = \"archive\"\n# print(task_func(source_dir, target_dir, zip_filename))\n```\n\nThis function performs the following steps:\n1. Checks if the `source_directory` exists and raises an `OSError` if it does not.\n2. Ensures the `target_directory` exists by creating it if necessary.\n3. Constructs the full path for the zip file in the `target_directory`.\n4. Collects all files with the specified extensions from the `source_directory`.\n5. Creates a zip file containing these files, with their original filenames inside the zip.\n6. Returns the full path to the created zip file."}
{"task_id": "BigCodeBench/964", "solution": "Here is the self-contained Python function that meets the specified requirements:\n\n```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory.\n    \n    Args:\n        source_directory (str): The path to the source directory.\n        target_directory (str): The path to the target directory.\n    \n    Returns:\n        int: The number of files successfully converted to CSV.\n    \n    Raises:\n        FileNotFoundError: If the source directory does not exist.\n    \"\"\"\n    \n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory {source_directory} does not exist.\")\n    \n    # Create the target directory if it does not exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # List of supported file extensions\n    supported_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n    \n    # Counter for successfully converted files\n    converted_count = 0\n    \n    # Iterate over all files in the source directory\n    for file_path in Path(source_directory).iterdir():\n        if file_path.suffix in supported_extensions:\n            # Read the content based on the file type\n            if file_path.suffix == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            elif file_path.suffix == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([para.text for para in doc.paragraphs])\n            elif file_path.suffix == '.xlsx':\n                df = pd.read_excel(file_path)\n                content = df.to_csv(index=False)\n            elif file_path.suffix == '.csv':\n                df = pd.read_csv(file_path)\n                content = df.to_csv(index=False)\n            \n            # Create a DataFrame with the content\n            df = pd.DataFrame({'Text': [content]})\n            \n            # Save the DataFrame to a CSV file in the target directory\n            target_file_path = Path(target_directory) / (file_path.stem + '.csv')\n            df.to_csv(target_file_path, index=False, quoting=csv.QUOTE_ALL)\n            \n            # Increment the counter\n            converted_count += 1\n    \n    return converted_count\n```\n\nThis function performs the following steps:\n1. Checks if the source directory exists and raises a `FileNotFoundError` if it does not.\n2. Creates the target directory if it does not exist.\n3. Iterates over all files in the source directory with supported extensions.\n4. Reads the content of each file based on its type and converts it to a string.\n5. Stores the content in a DataFrame with a single 'Text' column.\n6. Saves the DataFrame to a CSV file in the target directory.\n7. Returns the number of files successfully converted to CSV."}
{"task_id": "BigCodeBench/969", "solution": "Here's a self-contained function that meets the specified requirements:\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n\n    Returns:\n        pd.DataFrame: A DataFrame where each column contains the normalized cumulative sum of the\n            respective column in the input DataFrame, retaining the original column names.\n\n    Raises:\n        TypeError: If the DataFrame contains non-numeric data types.\n        ValueError: If the DataFrame is empty or contains NaN values.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if the DataFrame contains non-numeric data types\n    if not df.applymap(np.isreal).all().all():\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n\n    # Check if the DataFrame contains NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply the MinMaxScaler to each column\n    normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n    # Compute the cumulative sum for each column\n    cumulative_sum_df = normalized_df.cumsum()\n\n    return cumulative_sum_df\n```\n\nThis function performs the following steps:\n1. Checks if the DataFrame is empty and raises a `ValueError` if it is.\n2. Checks if the DataFrame contains non-numeric data types by applying `np.isreal` to each element and raises a `TypeError` if any non-numeric data is found.\n3. Checks for NaN values in the DataFrame and raises a `ValueError` if any are found.\n4. Initializes a `MinMaxScaler` object.\n5. Normalizes each column of the DataFrame using the `MinMaxScaler`.\n6. Computes the cumulative sum for each normalized column.\n7. Returns the DataFrame containing the normalized cumulative sums with the original column names."}
{"task_id": "BigCodeBench/971", "solution": "Here is the complete implementation of the `task_func` function that meets the specified requirements:\n\n```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size, \n    creation time, and last modification time without recursing into subdirectories.\n\n    Args:\n        directory_path (str): The path to the directory to be analyzed.\n\n    Returns:\n        list: A list of tuples, each containing the file name, file size in bytes, \n              creation time in ISO format, and modification time in ISO format.\n\n    Raises:\n        ValueError: If the provided directory does not exist.\n    \"\"\"\n    # Check if the directory exists\n    if not Path(directory_path).exists():\n        raise ValueError(\"The provided directory does not exist.\")\n\n    # Initialize an empty list to store the file information\n    file_info = []\n\n    # Iterate over each file in the directory\n    for file in os.listdir(directory_path):\n        # Construct the full path to the file\n        file_path = os.path.join(directory_path, file)\n\n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(file_path):\n            # Get the file size in bytes\n            file_size = os.path.getsize(file_path)\n\n            # Get the creation time and modification time in seconds since the epoch\n            creation_time = os.path.getctime(file_path)\n            modification_time = os.path.getmtime(file_path)\n\n            # Convert the times to datetime objects in UTC\n            creation_time = datetime.fromtimestamp(creation_time, timezone.utc)\n            modification_time = datetime.fromtimestamp(modification_time, timezone.utc)\n\n            # Convert the datetime objects to ISO format strings\n            creation_time = creation_time.isoformat()\n            modification_time = modification_time.isoformat()\n\n            # Append the file information to the list\n            file_info.append((file, file_size, creation_time, modification_time))\n\n    # Return the list of file information\n    return file_info\n\n# Example usage:\n# result = task_func('/path/to/empty_directory')\n# print(result)\n```\n\nThis function checks if the provided directory exists and iterates over its contents, collecting information about each file. It formats the creation and modification times in ISO format in UTC and returns a list of tuples containing the file name, size, creation time, and modification time. If the directory does not exist, it raises a `ValueError`."}
{"task_id": "BigCodeBench/985", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Validate and parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n    \n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n    \n    # Prepare DataFrame\n    df_data = []\n    for item in data:\n        country = item.get(\"Country\")\n        population = item.get(\"Population\")\n        \n        if not isinstance(country, str):\n            raise ValueError(\"Country names must be strings.\")\n        \n        if not isinstance(population, (int, float)):\n            raise ValueError(\"Population must be numeric.\")\n        \n        if population < 0:\n            raise ValueError(\"Population cannot be negative.\")\n        \n        df_data.append({\"Country\": country, \"Population\": math.floor(population)})\n    \n    # Create DataFrame\n    df = pd.DataFrame(df_data, columns=[\"Country\", \"Population\"])\n    \n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate file path\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"The file cannot be written to the specified directory.\")\n    \n    return file_path, df\n```\n\n### Explanation:\n1. **JSON Parsing and Validation**: The function first attempts to parse the JSON data. If the JSON is malformed, it raises a `ValueError`.\n2. **Data Validation**: It checks if the JSON data is empty, if country names are strings, and if populations are numeric and non-negative.\n3. **DataFrame Creation**: It constructs a DataFrame from the validated data, ensuring populations are rounded down to the nearest integer.\n4. **File Path and Directory Handling**: It ensures the output directory exists and constructs the full file path.\n5. **CSV Writing**: It writes the DataFrame to a CSV file. If there's an issue writing the file, it raises an `IOError`.\n\nThis function should handle the specified requirements and edge cases effectively."}
{"task_id": "BigCodeBench/988", "solution": "Here's a Python function that meets the specified requirements:\n\n```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n\n    Args:\n        dir_path (str): The path to the directory to evaluate.\n        predicates (list): A list of functions that take a file/directory name as input and return a boolean.\n\n    Returns:\n        dict: A dictionary with directory items as keys and the results of condition checks as values.\n\n    Raises:\n        ValueError: If no valid predicates are provided.\n        FileNotFoundError: If the specified directory does not exist or is not a directory.\n    \"\"\"\n    # Check if the directory exists and is a directory\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The directory '{dir_path}' does not exist or is not a directory.\")\n\n    # Check if predicates are provided\n    if not predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Deduplicate predicates\n    predicates = list(set(predicates))\n\n    # Initialize the result dictionary\n    result = {}\n\n    # Iterate over the directory items\n    for item in os.listdir(dir_path):\n        # Evaluate the item against each predicate\n        results = [predicate(item) for predicate in predicates]\n        # Store the results in the dictionary\n        result[item] = all(results)\n\n    return result\n\n# Example predicates\ndef contains_digit(name: str) -> bool:\n    return any(char.isdigit() for char in name)\n\ndef contains_uppercase(name: str) -> bool:\n    return any(char.isupper() for char in name)\n\n# Example usage\nif __name__ == \"__main__\":\n    predicates = [contains_digit, contains_uppercase]\n    try:\n        result = task_func(\"/path/to/directory\", predicates)\n        print(result)\n    except (FileNotFoundError, ValueError) as e:\n        print(e)\n```\n\n### Explanation:\n1. **Directory Check**: The function first checks if the provided `dir_path` exists and is a directory using `os.path.exists()` and `os.path.isdir()`. If not, it raises a `FileNotFoundError`.\n2. **Predicate Check**: It checks if any predicates are provided. If not, it raises a `ValueError`.\n3. **Deduplicate Predicates**: It deduplicates the predicates using `list(set(predicates))`.\n4. **Result Dictionary**: It initializes an empty dictionary to store the results.\n5. **Iterate Over Items**: It iterates over each item in the directory using `os.listdir(dir_path)`.\n6. **Evaluate Predicates**: For each item, it evaluates whether all predicates return `True`. The result is stored in the dictionary with the item name as the key and the evaluation result as the value.\n7. **Return Result**: Finally, it returns the result dictionary.\n\n### Example Predicates:\n- `contains_digit`: Checks if the name contains any digit.\n- `contains_uppercase`: Checks if the name contains any uppercase letter.\n\n### Example Usage:\n- The example usage demonstrates how to call the function with a directory path and a list of predicates. It also handles potential exceptions."}
{"task_id": "BigCodeBench/990", "solution": "Here's the self-contained code that performs the required conversions:\n\n```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Decode the hexadecimal string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n    \n    # Convert bytes to a UTF-8 string\n    utf8_string = byte_data.decode('utf-8')\n    \n    # Encode the UTF-8 string into different formats\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(byte_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_string if all(ord(c) < 128 for c in utf8_string) else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n    \n    return result\n\n# Example usage\nprint(task_func(\"68656c6c6f\"))\n```\n\nThis function takes a hexadecimal string, decodes it to bytes, converts those bytes to a UTF-8 string, and then encodes that string into various formats as specified. The `ASCII` key checks if the string can be represented in ASCII and returns 'Not representable in ASCII' if it cannot. The `utf-16` and `utf-32` keys are handled by encoding the string in those formats and then decoding it back to a string to ensure compatibility with the expected output format."}
{"task_id": "BigCodeBench/998", "solution": "Here is the self-contained code for the function `task_func` that meets the specified requirements:\n\n```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_md5 = md5_hash.hexdigest()\n        \n        # Validate the MD5 checksum\n        if calculated_md5 == EXPECTED_MD5_CHECKSUM:\n            # Extract the contents of the tar.gz file\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall()\n            os.remove(TARGET_TAR_FILE)  # Remove the tar.gz file after extraction\n            return True\n        else:\n            # Delete the downloaded file if the checksum does not match\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        # Handle any exceptions that occur during the process\n        print(f\"An error occurred: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False\n\n# Example usage:\n# result = task_func(\"http://example.com/path/to/file.tar.gz\")\n# print(result)\n```\n\nThis function performs the following steps:\n1. Downloads the file from the specified URL and saves it as `TARGET_TAR_FILE`.\n2. Calculates the MD5 checksum of the downloaded file.\n3. Compares the calculated checksum with the `EXPECTED_MD5_CHECKSUM`.\n4. If the checksums match, it extracts the contents of the tar.gz file and then deletes the tar.gz file.\n5. If the checksums do not match, it deletes the downloaded file.\n6. Returns `True` if the file is successfully downloaded, its checksum matches, and it is extracted. Returns `False` otherwise.\n7. Handles any exceptions that occur during the process and cleans up the downloaded file if necessary."}
{"task_id": "BigCodeBench/999", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise Exception(f\"Failed to download CSV file from {url}: {e}\")\n\n    try:\n        # Read the CSV file and count occurrences of each value in the specified column\n        value_counts = collections.defaultdict(int)\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            for row in reader:\n                value = row[column_name]\n                value_counts[value] += 1\n    except Exception as e:\n        raise e\n    finally:\n        # Delete the downloaded CSV file\n        os.remove(csv_file_path)\n\n    return dict(value_counts)\n\n# Example usage:\n# result = task_func('http://example.com/data.csv', 'ColumnName', 'downloaded_data.csv')\n# print(result)\n```\n\n### Explanation:\n1. **Downloading the CSV File**: The function uses `urllib.request.urlretrieve` to download the CSV file from the provided URL and save it to the specified path.\n2. **Reading and Counting Values**: It reads the CSV file using `csv.DictReader`, which allows accessing columns by name. It checks if the specified column exists. If not, it raises a `ValueError`.\n3. **Counting Occurrences**: It uses a `defaultdict` to count occurrences of each value in the specified column.\n4. **File Deletion**: Regardless of whether the operation succeeds or fails, the function ensures that the downloaded CSV file is deleted using `os.remove`.\n5. **Error Handling**: The function handles various errors, including issues with downloading the file and missing columns, and raises appropriate exceptions.\n\nThis function is designed to be robust and handle the specified scenarios effectively."}
{"task_id": "BigCodeBench/1003", "solution": "Here is the self-contained Python function that meets the specified requirements:\n\n```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        response = urllib.request.urlopen(url)\n        xml_data = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(\"Invalid URL or unable to fetch XML file\") from e\n\n    try:\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(\"Invalid XML syntax\") from e\n\n    # Check if the XML structure matches the expected format\n    if root.tag != 'root' or len(root) == 0:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Extract the 'item' elements and their child elements\n    items = []\n    for item in root:\n        item_dict = {}\n        for child in item:\n            item_dict[child.tag] = child.text\n        items.append(item_dict)\n\n    # Convert the list of dictionaries to a Pandas DataFrame\n    df = pd.DataFrame(items)\n\n    return df\n\n# Example usage:\n# df = task_func('http://example.com/valid_structure.xml')\n# print(df)\n```\n\n### Explanation:\n1. **Fetching the XML File**: The function attempts to fetch the XML file from the provided URL using `urllib.request.urlopen`. If this fails due to an invalid URL or inability to fetch the file, it raises a `ValueError`.\n2. **Parsing the XML Data**: The XML data is parsed using `etree.fromstring` from the `lxml` library. If the XML syntax is invalid, it raises a `ValueError`.\n3. **Validating the XML Structure**: The function checks if the root tag of the XML is 'root' and if it has child elements. If it doesn't match the expected structure, it raises a `ValueError`.\n4. **Extracting Data**: It iterates over the `item` elements in the XML, extracting child elements and their text values into a list of dictionaries.\n5. **Creating the DataFrame**: The list of dictionaries is converted into a Pandas DataFrame. Each row in the DataFrame corresponds to an 'item' element in the XML, with child elements of 'item' becoming columns in the DataFrame.\n\nThis function should handle the specified error scenarios and correctly convert a valid XML file into a Pandas DataFrame."}
{"task_id": "BigCodeBench/1004", "solution": "Here's a self-contained Python function that fulfills the specified requirements:\n\n```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n    \n    # Use a regular expression to find all words (case-sensitive)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    most_common_words = word_counts.most_common(10)\n    \n    # Extract words and their counts for plotting\n    words, counts = zip(*most_common_words)\n    \n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Frequent Words')\n    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to make room for rotated labels\n    \n    # Return the Counter object and the Axes object\n    return (word_counts, ax)\n\n# Example usage:\n# result = task_func('http://example.com/textfile.txt')\n# word_counts, ax = result\n# plt.show()  # If you want to display the plot immediately\n```\n\nThis function does the following:\n1. Downloads the text file from the specified URL.\n2. Processes the text to find all words using a regular expression.\n3. Counts the frequency of each word using `Counter` from the `collections` module.\n4. Identifies the ten most frequently occurring words.\n5. Plots a bar chart showing these ten most common words using `matplotlib`.\n6. Returns a tuple containing the `Counter` object with word frequencies and the `Axes` object representing the plotted bar chart.\n\nNote: The function assumes that the URL points to a plain text file and that the environment has internet access to download the file. The function also assumes that the environment has `matplotlib` installed for plotting."}
{"task_id": "BigCodeBench/1006", "solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download path exists\n        os.makedirs(download_path, exist_ok=True)\n        \n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        \n        # Check if the content type is 'application/zip'\n        content_type = response.headers.get('content-type')\n        if content_type != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Save the file to the download path\n        file_path = os.path.join(download_path, 'downloaded_file.zip')\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Extract the ZIP file\n        try:\n            with ZipFile(file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Return the path to the directory containing the extracted contents\n        return download_path\n    \n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    \n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage:\n# result = task_func('http://example.com/somefile.zip')\n# print(result)\n```\n\nThis function performs the following steps:\n1. Ensures the download directory exists.\n2. Attempts to download the file from the provided URL.\n3. Checks if the content type of the downloaded file is `application/zip`.\n4. Saves the downloaded file to the specified path.\n5. Attempts to extract the ZIP file.\n6. Handles specific exceptions for network issues, incorrect file types, and corrupt ZIP files.\n7. Catches and reports any other exceptions that might occur during the process."}
{"task_id": "BigCodeBench/1008", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for unsuccessful HTTP requests\n    except requests.exceptions.HTTPError as http_err:\n        raise http_err\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', id=table_id)\n\n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    # Convert the table to a DataFrame\n    df = pd.read_html(StringIO(table.prettify()))[0]\n\n    return df\n\n# Example usage:\n# df = task_func('https://example.com/data.html', 'table_id')\n# print(df)\n```\n\nThis function performs the following steps:\n1. Sends an HTTP GET request to the specified URL and raises an `HTTPError` for unsuccessful HTTP requests.\n2. Parses the HTML content of the response using BeautifulSoup.\n3. Searches for a table with the specified `table_id`.\n4. If the table is not found, it raises a `ValueError`.\n5. Converts the table to a Pandas DataFrame using `pd.read_html`.\n6. Returns the DataFrame. If the table has no rows, an empty DataFrame is returned.\n\nThis function handles HTTP errors by raising `requests.exceptions.HTTPError` for unsuccessful HTTP requests and `ValueError` if the specified table ID is not found in the HTML content."}
{"task_id": "BigCodeBench/1012", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure directories exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n\n    # Path to the zip file\n    zip_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the zip file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        with open(zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n    except requests.RequestException as e:\n        return (\"Error: Network-related exception - \" + str(e), [])\n    except IOError as e:\n        return (\"Error: File-related exception during download - \" + str(e), [])\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            filenames = [Path(name).name for name in zip_ref.namelist()]\n        return (\"Success\", filenames)\n    except zipfile.BadZipFile as e:\n        return (\"Error: Bad zip file - \" + str(e), [])\n    except IOError as e:\n        return (\"Error: File-related exception during extraction - \" + str(e), [])\n\n# Example usage\nif __name__ == \"__main__\":\n    result = task_func(\"http://example.com/somefile.zip\", \"downloaded_file.zip\")\n    print(result)\n```\n\nThis function `task_func` takes a URL and a filename as arguments. It ensures that the directories for downloads and unzipped files exist. It then attempts to download the zip file from the given URL and save it locally. If the download fails due to network issues or file handling problems, it catches the exceptions and returns an error message with an empty list.\n\nAfter successfully downloading the file, it attempts to extract the contents of the zip file into the specified directory. If the extraction fails due to the zip file being corrupted or other file-related issues, it catches these exceptions and returns an error message with an empty list. If the extraction is successful, it returns a success message along with a list of filenames extracted from the zip file."}
{"task_id": "BigCodeBench/1013", "solution": "Here's a self-contained function that scrapes a webpage for all hyperlinks, converts them to absolute URLs, saves them to a CSV file, and returns the number of unique absolute links scraped:\n\n```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Fetch the webpage\n    response = requests.get(url)\n    response.raise_for_status()\n\n    # Parse the webpage\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all hyperlinks\n    links = set()\n    for a_tag in soup.find_all('a', href=True):\n        absolute_url = urljoin(base_url, a_tag['href'])\n        links.add(absolute_url)\n\n    # Save the links to a CSV file\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['URL'])\n        for link in links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(links)\n\n# Example usage:\n# num_links = task_func(\"https://www.example.com\")\n# print(f\"Number of unique links: {num_links}\")\n```\n\nThis function does the following:\n1. Fetches the webpage using `requests`.\n2. Parses the webpage content using `BeautifulSoup`.\n3. Finds all `<a>` tags with an `href` attribute to extract hyperlinks.\n4. Converts each relative URL to an absolute URL using `urljoin`.\n5. Stores the unique absolute URLs in a set to ensure uniqueness.\n6. Writes the unique URLs to a CSV file.\n7. Returns the number of unique absolute URLs scraped."}
{"task_id": "BigCodeBench/1015", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the webpage content\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Failed to fetch the webpage: {e}\")\n\n    try:\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n        tables = tree.xpath('//table')\n        \n        if not tables or not tables[0].findall('tr'):\n            return 0  # No table found or table is empty in the HTML content\n        \n        # Convert the first table to a DataFrame\n        df = pd.read_html(response.content)[0]\n        \n        if df.empty:\n            return 0  # The table is empty\n        \n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n        \n        # Drop the existing table if it exists\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        \n        # Create a new table with the DataFrame's columns\n        df.to_sql('my_table', conn, index=False)\n        \n        # Commit the transaction and close the connection\n        conn.commit()\n        conn.close()\n        \n        return len(df)  # Return the number of rows in the table\n    \n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database error: {e}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {e}\")\n```\n\n### Explanation:\n1. **Fetching Webpage Content**: The function uses `requests.get` to fetch the content of the specified URL. It raises an exception if there's a network issue.\n2. **Parsing HTML**: The function uses `lxml` to parse the HTML content and locate tables using XPath.\n3. **Handling No Table or Empty Table**: If no tables are found or the table is empty, the function returns 0.\n4. **Data Conversion**: The function converts the first HTML table found into a pandas DataFrame.\n5. **Database Operations**: The function connects to an SQLite database, drops the existing `my_table` if it exists, and creates a new table with the DataFrame's data.\n6. **Exception Handling**: The function handles exceptions related to network issues (`requests.RequestException`) and database errors (`sqlite3.DatabaseError`). It also catches and raises any other unexpected exceptions.\n7. **Return Value**: The function returns the number of rows in the parsed HTML table."}
{"task_id": "BigCodeBench/1019", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nfrom PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Extract text using OCR\n            ocr_text = pytesseract.image_to_string(img)\n            # Convert the extracted text to the target encoding\n            converted_text = ocr_text.encode(from_encoding).decode(to_encoding)\n            return converted_text\n    except (UnicodeDecodeError, LookupError) as e:\n        # Fallback to image comment processing\n        try:\n            comment = img.info.get(\"comment\", \"\")\n            if comment:\n                converted_comment = comment.decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                return converted_comment\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(\"UnicodeDecodeError or LookupError occurred during conversion\")\n    except Exception as e:\n        # If both OCR extraction and comment processing fail, return an empty string\n        return \"\"\n\n# Example usage\nif __name__ == \"__main__\":\n    print(task_func())\n```\n\n### Explanation:\n1. **Image Opening**: The function opens the image file using `PIL.Image.open`.\n2. **OCR Text Extraction**: It attempts to extract text from the image using `pytesseract.image_to_string`.\n3. **Text Encoding Conversion**: The extracted text is converted from the specified `from_encoding` to `to_encoding`.\n4. **Exception Handling**: If a `UnicodeDecodeError` or `LookupError` occurs during the text conversion, it falls back to processing the image comment.\n5. **Image Comment Processing**: If the image has a comment, it attempts to convert the comment text from `from_encoding` to `to_encoding`.\n6. **Error Handling**: If any errors occur during the conversion of the comment, it raises a `ValueError`.\n7. **Fallback Mechanism**: If both OCR extraction and comment processing fail, the function returns an empty string.\n\nThis function ensures that all specified error conditions are handled and the output is correctly formatted as per the requirements."}
{"task_id": "BigCodeBench/1020", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Initiate HTTP GET request with a timeout of 5 seconds\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Retrieve the response content in raw bytes\n        raw_content = response.content\n\n        # Handle empty content\n        if not raw_content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding is None:\n            detected_encoding = chardet.detect(raw_content)\n            from_encoding = detected_encoding.get('encoding')\n\n            # Raise an exception if encoding cannot be detected\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the raw content using the detected or provided encoding\n        decoded_content = raw_content.decode(from_encoding)\n\n        # Re-encode the content to the desired encoding\n        re_encoded_content = decoded_content.encode(to_encoding)\n\n        # Parse the JSON data\n        json_data = json.loads(re_encoded_content.decode(to_encoding))\n\n        return json_data\n\n    except requests.RequestException as e:\n        # Handle any request exceptions (e.g., connection error, timeout)\n        print(f\"Request failed: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        print(f\"JSON decoding failed: {e}\")\n        return {}\n    except ValueError as e:\n        # Handle value errors (e.g., encoding detection failure)\n        print(f\"Value error: {e}\")\n        raise\n```\n\nThis function performs the following steps:\n1. Sends an HTTP GET request to the specified URL with a timeout of 5 seconds.\n2. Checks for empty content and returns an empty dictionary if the content is empty.\n3. Detects the encoding of the response content if not provided.\n4. Raises a `ValueError` if the encoding cannot be detected for non-empty content.\n5. Decodes the raw content using the detected or provided encoding.\n6. Re-encodes the content to the desired encoding.\n7. Parses the JSON data and returns it as a dictionary.\n8. Handles various exceptions, including HTTP request errors and JSON parsing errors, by printing the error message and returning an empty dictionary."}
{"task_id": "BigCodeBench/1022", "solution": "Here is the self-contained Python function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads a CSV file and processes its date-related data.\n\n    Parameters:\n    csv_file_path (str): The path to the CSV file.\n    column_name (str): The name of the column containing date values.\n    date_format (str): The format of the date values in the CSV file. Defaults to \"%Y-%m-%d\".\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing the filtered and sorted data.\n\n    Raises:\n    FileNotFoundError: If the specified CSV file is not found at the given path.\n    ValueError: If the specified column is not present in the CSV file.\n    \"\"\"\n\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Try to read the CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # If the file is empty, return an empty DataFrame\n        return pd.DataFrame()\n\n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n\n    # Convert the date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date <= current_date]\n\n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n\n    return df\n```\n\nThis function performs the following steps:\n1. Checks if the specified CSV file exists and raises a `FileNotFoundError` if it does not.\n2. Attempts to read the CSV file using pandas. If the file is empty, it catches the `EmptyDataError` and returns an empty DataFrame.\n3. Validates the presence of the specified column and raises a `ValueError` if the column is not found.\n4. Converts the specified date column to datetime objects using the provided date format. Any invalid dates are coerced to `NaT`.\n5. Filters the rows to include only those with dates on or before the current date.\n6. Sorts the resulting DataFrame by the date column in ascending order.\n7. Returns the processed DataFrame."}
{"task_id": "BigCodeBench/1039", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Secure the client socket using SSL/TLS\n        ssl_socket = ssl.wrap_socket(client_socket, server_side=True, certfile=cert_file, keyfile=key_file)\n        \n        # Receive the file path request from the client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the file exists\n        if not os.path.exists(file_path):\n            ssl_socket.sendall('File not found'.encode('utf-8'))\n            return 'File not found'\n        \n        # Open the file and compute the SHA256 hash\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        \n        # Convert the hash to a hexadecimal string\n        hash_hex = sha256_hash.hexdigest()\n        \n        # Send the hash back to the client\n        ssl_socket.sendall(hash_hex.encode('utf-8'))\n        \n        return hash_hex\n    \n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n\n### Explanation:\n1. **SSL/TLS Wrap**: The `ssl.wrap_socket` function is used to secure the client socket with SSL/TLS using the provided certificate and key files.\n2. **Receive File Path**: The function receives the file path from the client using the secured socket.\n3. **File Existence Check**: It checks if the file exists using `os.path.exists`. If the file does not exist, it sends 'File not found' back to the client and returns the same.\n4. **SHA256 Hash Calculation**: If the file exists, it opens the file in binary mode and computes the SHA256 hash using `hashlib.sha256`.\n5. **Send Hash Back**: The computed hash is sent back to the client in hexadecimal format.\n6. **Error Handling**: If any exception occurs during the process, an error message is returned.\n\nThis function assumes that the client socket is already created and connected, and it does not handle the closing of the socket. The error handling is basic and can be expanded based on specific requirements."}
{"task_id": "BigCodeBench/1040", "solution": "Here's a self-contained Python script for a non-blocking echo server that appends the server's current time to received data and sends it back to the client. The server handles exceptional conditions for each socket and outputs a status message indicating its operation and run duration.\n\n```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Run a non-blocking echo server that appends the server's current time to received data and sends it back to the client.\n    \n    Parameters:\n    server_address (str): The address of the server. Defaults to \"localhost\".\n    server_port (int): The port of the server. Defaults to 12345.\n    buffer_size (int): The size of the buffer for receiving and sending data. Defaults to 1024.\n    run_duration (int): The duration of the server's operation in seconds. Defaults to 5.\n    \n    Returns:\n    str: A status message indicating the server's operation and run duration.\n    \"\"\"\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n    server.bind((server_address, server_port))\n    server.listen(5)\n\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n\n    timeout = run_duration * 60  # Convert run_duration to seconds\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=timeout)\n\n    while inputs and datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n        for s in readable:\n            if s is server:\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    response = f\"{data.decode()} (Server Time: {current_time})\"\n                    message_queues[s].put(response)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                s.send(next_msg.encode())\n\n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    for s in inputs:\n        s.close()\n\n    run_time = datetime.now() - start_time\n    status_message = f\"Server operated for {run_time.total_seconds()} seconds.\"\n    return status_message\n\nif __name__ == \"__main__\":\n    print(task_func())\n```\n\nThis script sets up a non-blocking TCP server that listens for incoming connections. It uses `select` to handle multiple sockets asynchronously. When data is received, it appends the server's current time to the data and sends it back to the client. The server runs for the specified duration and then shuts down, printing the total operation time."}
{"task_id": "BigCodeBench/1042", "solution": "Here's a self-contained Python function that receives a message from a client socket and sends it as an email via an SMTP server. This function includes the necessary imports and configurations for the SMTP server, as well as handling the client socket communication and email sending process.\n\n```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Receive data from the client socket\n    data = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Prompt for email details\n    sender_email = input(\"Enter your email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    password = getpass.getpass(\"Enter your email password: \")\n    \n    # Create the email message\n    msg = EmailMessage()\n    msg.set_content(data)\n    msg['Subject'] = 'Message from client socket'\n    msg['From'] = sender_email\n    msg['To'] = recipient_email\n    \n    # Send the email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(sender_email, password)\n        server.send_message(msg)\n        server.quit()\n        print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Failed to send email: {e}\")\n\n# Example usage:\n# Assuming you have a client socket connected and ready to send data\n# client_socket = ... (this would be your actual client socket object)\n# task_func(client_socket)\n```\n\nThis function does the following:\n1. Receives data from a client socket.\n2. Prompts the user for their email details, including the sender's email, recipient's email, and the sender's email password.\n3. Constructs an email message using the received data.\n4. Connects to an SMTP server (in this case, Gmail's SMTP server), logs in with the provided credentials, and sends the email.\n5. Handles any exceptions that might occur during the email sending process.\n\nNote: This example uses Gmail's SMTP server. If you are using a different email provider, you will need to adjust the `SMTP_SERVER` and possibly the `SMTP_PORT` accordingly. Also, be aware that using less secure apps on Gmail might be restricted; you may need to enable this in your Google account settings or use an App Password if two-factor authentication is enabled."}
{"task_id": "BigCodeBench/1053", "solution": "Here's a self-contained Python function that processes a CSV file containing text data and generates a histogram of the ten most common words, excluding predefined stopwords. The function uses `pandas` for data manipulation, `sklearn`'s `CountVectorizer` for text vectorization, and `matplotlib` for plotting.\n\n```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Check if the file exists\n        try:\n            with open(file_path):\n                pass\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"The file at path {file_path} was not found.\")\n\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n\n        # Ensure the DataFrame has at least one column\n        if df.shape[1] == 0:\n            raise Exception(\"The CSV file does not contain any columns.\")\n\n        # Use the first column as text data if no header is present\n        text_column = df.columns[0] if 'Text' not in df.columns else 'Text'\n        text_data = df[text_column]\n\n        # Initialize CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        X = vectorizer.fit_transform(text_data)\n\n        # Get word frequencies\n        word_frequencies = X.toarray().sum(axis=0)\n        word_freq_dict = dict(zip(vectorizer.get_feature_names_out(), word_frequencies))\n\n        # Sort words by frequency and get top 10\n        top_words = sorted(word_freq_dict.items(), key=lambda item: item[1], reverse=True)[:10]\n\n        # Plotting\n        words, frequencies = zip(*top_words)\n        plt.figure(figsize=(10, 5))\n        plt.bar(words, frequencies, color='blue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n\n        # Save or display the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.show()\n            return plt.gca()\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage:\n# task_func('path_to_csv_file.csv', 'path_to_save_histogram.png')\n```\n\nThis function handles the reading of the CSV file, processing the text data, and generating a histogram of the top ten most common words, excluding the specified stopwords. It also handles exceptions for file not found errors and other potential errors during execution. The plot can be saved to a file or displayed directly, depending on whether a `save_path` is provided."}
{"task_id": "BigCodeBench/1057", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Define default lists for animals and foods\n    default_animals = ['Lion', 'Tiger', 'Bear']\n    default_foods = ['Meat', 'Fish', 'Vegetables']\n    \n    # Handle special cases\n    if animals is None or not animals:\n        animals = default_animals\n    if foods is None or not foods:\n        foods = default_foods\n    \n    # If both lists are empty, return an empty DataFrame\n    if not animals or not foods:\n        return pd.DataFrame()\n    \n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations randomly\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame from the shuffled combinations\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    \n    # Format the cells as 'animal:food'\n    df['Animal:Food'] = df['Animal'] + ':' + df['Food']\n    \n    # Pivot the DataFrame to have animals as rows and foods as columns\n    pivot_df = df.pivot(index='Animal', columns='Food', values='Animal:Food')\n    \n    return pivot_df\n\n# Example usage\n# df = task_func(['Dog', 'Cat'], ['Bone', 'Milk'])\n# print(df)\n```\n\nThis function handles the special cases as specified:\n- If either `animals` or `foods` is `None` or empty, it uses the predefined list for that parameter.\n- If both `animals` and `foods` are empty or not provided, it returns an empty DataFrame.\n\nThe function generates all possible combinations of the provided animals and foods using `itertools.product`, shuffles these combinations randomly, and constructs a DataFrame. The DataFrame is then pivoted to ensure that each row represents a unique animal and each column represents a food item, with each cell containing a string in the format 'animal:food'."}
{"task_id": "BigCodeBench/1077", "solution": "Here's the implementation of the `task_func` function as described:\n\n```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Convert each timestamp to the specified timezone\n    tz = pytz.timezone(timezone)\n    time_stamps_tz = [datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n    \n    # Calculate the absolute time difference in seconds between each consecutive pair of timestamps\n    time_diffs = [(b - a).total_seconds() for a, b in zip(time_stamps_tz[:-1], time_stamps_tz[1:])]\n    \n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n    \n    # Calculate the mean (average) time difference in seconds\n    mean_time_diff = np.mean(time_diffs)\n    \n    return mean_time_diff\n\n# Example usage:\n# time_strings = [\"2023-01-01 12:00:00\", \"2023-01-01 13:00:00\", \"2023-01-01 14:00:00\"]\n# timezone = \"America/New_York\"\n# print(task_func(time_strings, timezone))\n```\n\nThis function first checks if there are less than two timestamps and returns 0.0 if true. It then converts each timestamp to the specified timezone and calculates the absolute time difference in seconds between each consecutive pair of timestamps. Finally, it calculates the mean of these differences and returns it."}
{"task_id": "BigCodeBench/1085", "solution": "Here's a self-contained function that performs the required tasks:\n\n```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation and split into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Extract words and counts for plotting\n    words, counts = zip(*top_10_words)\n    \n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return top_10_words, ax\n\n# Example usage:\n# text = \"This is a test. This test is only a test. If this were a real text, it would be more interesting.\"\n# top_10_words, ax = task_func(text)\n# print(top_10_words)\n```\n\nThis function takes a text string as input, processes it to lowercase and remove punctuation, counts the frequency of each word, identifies the top 10 most common words, and plots them in a bar chart using `matplotlib`. The function returns a list of tuples containing the top 10 most common words and their counts, as well as the `Axes` object of the bar chart."}
{"task_id": "BigCodeBench/1124", "solution": "Here's a self-contained Python function that meets the specified requirements:\n\n```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to match URLs\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    \n    # Find the first URL in the string\n    url_match = re.search(url_pattern, myString)\n    \n    if url_match is None:\n        return \"No valid URL found in the provided string.\"\n    \n    url = url_match.group()\n    \n    # Check if the URL is valid\n    try:\n        result = urlparse(url)\n        if not all([result.scheme, result.netloc]):\n            return \"No valid URL found in the provided string.\"\n    except ValueError:\n        return \"No valid URL found in the provided string.\"\n    \n    # Fetch the content of the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n    except requests.exceptions.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the HTML content\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find the title tag\n    title = soup.find('title')\n    \n    if title is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title.text.strip()\n\n# Example usage\n# print(task_func(\"Check out this website: https://www.example.com\"))\n```\n\nThis function performs the following steps:\n1. Uses a regular expression to find the first URL in the provided string.\n2. Validates the URL using `urlparse`.\n3. Attempts to fetch the content of the URL using `requests`.\n4. Parses the HTML content using `BeautifulSoup` to find the `<title>` tag.\n5. Returns the title text if found, or an appropriate error message if any step fails."}
{"task_id": "BigCodeBench/1129", "solution": "Here's a self-contained Python function that accomplishes the task described:\n\n```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Find the URL associated with the specified key\n    url = data.get(unknown_key)\n    if not url:\n        raise KeyError(f\"Key '{unknown_key}' not found in JSON data\")\n    \n    # Get the current timestamp in the specified format\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create the filename with the timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory, default to current working directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    \n    # Ensure the save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Construct the full path for the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an error for bad status codes\n    \n    # Save the file to the specified path\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the downloaded file\n    return os.path.abspath(file_path)\n\n# Example usage:\n# json_string = '{\"key1\": \"http://example.com/file.txt\", \"key2\": \"http://example.com/another_file.txt\"}'\n# print(task_func(json_string, 'key1'))\n```\n\nThis function:\n1. Parses the JSON string.\n2. Retrieves the URL associated with the specified key.\n3. Generates a timestamped filename.\n4. Determines the save directory, defaulting to the current working directory if none is specified.\n5. Ensures the save directory exists.\n6. Downloads the file from the URL and saves it to the specified path.\n7. Returns the absolute path of the downloaded file."}
{"task_id": "BigCodeBench/1137", "solution": "Here's a Python function that extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format:\n\n```python\nimport bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n\n    Args:\n        url (str): The URL of the webpage or local file to extract phone numbers from.\n        output_path (str): The path to the file where the extracted phone numbers will be saved in JSON format.\n\n    Returns:\n        list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Check if the URL is a local file\n    if url.startswith('file://'):\n        # If it's a local file, open it and read the contents\n        with open(url.replace('file://', ''), 'r') as file:\n            html = file.read()\n    else:\n        # If it's a URL, send a GET request and get the HTML response\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure we got the content successfully\n        html = response.text\n\n    # Parse the HTML using BeautifulSoup\n    soup = bs4.BeautifulSoup(html, 'html.parser')\n\n    # Use regular expressions to find phone numbers in the HTML\n    phone_numbers = re.findall(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', soup.get_text())\n\n    # Save the phone numbers to a JSON file\n    with open(output_path, 'w') as file:\n        json.dump(phone_numbers, file)\n\n    return phone_numbers\n```\n\nThis function does the following:\n1. Checks if the provided URL is a local file or a URL. If it's a local file, it opens the file and reads the contents. If it's a URL, it sends a GET request and gets the HTML response.\n2. Parses the HTML using BeautifulSoup to extract all text.\n3. Uses a regular expression to find all phone numbers in the text. The regex pattern used here matches common US phone number formats (e.g., `123-456-7890`, `123.456.7890`, `123 456 7890`, `1234567890`).\n4. Saves the found phone numbers to the specified file in JSON format.\n5. Returns a list of the found phone numbers.\n\nYou can use this function by passing a URL or local file path and an output file path to it. For example:\n\n```python\nphone_numbers = task_func('http://example.com', 'phone_numbers.json')\nprint(phone_numbers)\n```\n\nOr for a local file:\n\n```python\nphone_numbers = task_func('file:///path/to/local/file.html', 'phone_numbers.json')\nprint(phone_numbers)\n```\n\nThis will print the list of phone numbers found on the page or in the local file and save them to `phone_numbers.json`."}
