{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    attempted_files = []\n    \n    try:\n        # Connect to FTP server\n        with ftplib.FTP(ftp_server) as ftp:\n            try:\n                # Login to FTP server\n                ftp.login(user=ftp_user, passwd=ftp_password)\n            except ftplib.error_perm as e:\n                raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n            \n            try:\n                # Change to specified directory\n                ftp.cwd(ftp_dir)\n            except ftplib.error_perm as e:\n                raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n            \n            # List files in the directory\n            files = ftp.nlst()\n            \n    except ftplib.all_errors as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    # Download files using wget\n    for file in files:\n        ftp_url = f'ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{file}'\n        try:\n            subprocess.run(['wget', '-q', ftp_url], check=True)\n            attempted_files.append(file)\n        except subprocess.CalledProcessError:\n            print(f\"Failed to download file: {file}\")\n    \n    return attempted_files"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    # Read commands from the CSV file\n    with open(commands_file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader):\n            if row:  # Skip empty rows\n                command = row[0]\n                output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n                output_files.append(output_file_path)\n\n                try:\n                    # Execute the command\n                    result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n                    \n                    # Write the output to the file\n                    with open(output_file_path, 'w') as outfile:\n                        outfile.write(result.stdout)\n                        if result.stderr:\n                            outfile.write(\"\\nSTDERR:\\n\")\n                            outfile.write(result.stderr)\n\n                except subprocess.CalledProcessError as e:\n                    # If the command fails, write the error message and exit code\n                    with open(output_file_path, 'w') as outfile:\n                        outfile.write(f\"Command execution failed with exit code {e.returncode}\\n\")\n                        outfile.write(f\"STDERR:\\n{e.stderr}\")\n\n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    process_found = False\n    for proc in psutil.process_iter(['name']):\n        if proc.info['name'] == process_name:\n            process_found = True\n            break\n    \n    if not process_found:\n        # Process not found, start it\n        try:\n            subprocess.Popen(process_name)\n            return f\"Process not found. Starting {process_name}.\"\n        except Exception as e:\n            return f\"Error starting {process_name}: {str(e)}\"\n    else:\n        # Process found, terminate and restart it\n        try:\n            for proc in psutil.process_iter(['name']):\n                if proc.info['name'] == process_name:\n                    proc.terminate()\n                    proc.wait(timeout=5)  # Wait up to 5 seconds for the process to terminate\n            \n            time.sleep(1)  # Short delay to ensure the process has fully terminated\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n        except Exception as e:\n            return f\"Error restarting {process_name}: {str(e)}\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The specified directory '{directory}' does not exist\")\n    \n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n    \n    # If there are no files, return None\n    if not files:\n        return None\n    \n    # Create the zip file path\n    zip_path = os.path.join(directory, 'files.zip')\n    \n    # Create the zip file and add all files to it\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text_without_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n    \n    # Remove extra whitespace and split into words\n    words = text_without_urls.strip().split()\n    \n    # Check if there are any words left after removing URLs\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Join the words back into a single string\n    clean_text = ' '.join(words)\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=10).generate(clean_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Train RandomForestClassifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Get feature importances\n    importances = pd.DataFrame({\n        'feature': X.columns,\n        'importance': rf_classifier.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    # Create bar plot\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='importance', y='feature', data=importances)\n    \n    # Set labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n    \n    # Adjust layout and return\n    plt.tight_layout()\n    return rf_classifier, ax"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\nusers = {\n    1: User(1, 'user1', 'password123'),\n    2: User(2, 'user2', 'password456')\n}\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return users.get(int(user_id))\n\n    @app.route('/')\n    def index():\n        return redirect(url_for('login'))\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        if current_user.is_authenticated:\n            return redirect(url_for('protected'))\n        \n        form = LoginForm()\n        if form.validate_on_submit():\n            user = next((u for u in users.values() if u.username == form.username.data), None)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                flash('Invalid username or password')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', username=current_user.username)\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(data, column, outlier_z_score):\n    # Ensure data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n    \n    # Extract the specified column\n    col_data = data[column].values.reshape(-1, 1)\n    \n    # Standardize the column\n    scaler = StandardScaler()\n    col_data_standardized = scaler.fit_transform(col_data)\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(col_data_standardized))\n    \n    # Identify outliers\n    outlier_mask = z_scores > outlier_z_score\n    outlier_indices = np.where(outlier_mask)[0]\n    \n    # Remove outliers\n    data_without_outliers = data[~outlier_mask.flatten()].reset_index(drop=True)\n    \n    # Visualize data before and after outlier removal\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Plot original data\n    ax1.scatter(data.index, data[column], color='blue', alpha=0.6)\n    ax1.set_title('Data with Outliers')\n    ax1.set_xlabel('Index')\n    ax1.set_ylabel(column)\n    \n    # Plot data without outliers\n    ax2.scatter(data_without_outliers.index, data_without_outliers[column], color='green', alpha=0.6)\n    ax2.set_title('Data without Outliers')\n    ax2.set_xlabel('Index')\n    ax2.set_ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_without_outliers, outlier_indices)"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pd.DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Generate scatter plot\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Plot data points\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n    \n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n    \n    # Customize plot\n    ax.set_title('K-means Clustering Results')\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    \n    # Add legend for clusters\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    \n    # Add legend for centroids\n    ax.legend(loc='upper right')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return np.array(cluster_labels), ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Create DataFrame with transformed data\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    df_transformed = pd.DataFrame(data=transformed_data, columns=columns)\n\n    # Generate scatter plot\n    fig, ax = plt.subplots(figsize=(10, 8))\n    if n_components >= 2:\n        scatter = ax.scatter(df_transformed['PC1'], df_transformed['PC2'], alpha=0.7)\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n    else:\n        scatter = ax.scatter(range(len(df_transformed)), df_transformed['PC1'], alpha=0.7)\n        ax.set_xlabel('Index')\n        ax.set_ylabel('Principal Component 1')\n\n    ax.set_title('PCA Transformed Data')\n    plt.colorbar(scatter)\n\n    return df_transformed, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n    # Set the global font to Arial for better readability and visual appeal\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate a pair plot from the iris dataset\n    pair_plot = sns.pairplot(iris_df, hue='species', height=2.5)\n\n    # Set the title of the plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16, y=1.02)\n\n    # Update the labels for each feature on the axes\n    for i, feature in enumerate(iris.feature_names):\n        for ax in pair_plot.axes[i, :]:\n            ax.set_xlabel(feature, fontsize=10)\n        for ax in pair_plot.axes[:, i]:\n            ax.set_ylabel(feature, fontsize=10)\n\n    # Adjust the layout to prevent overlapping\n    plt.tight_layout()\n\n    return pair_plot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\ndef task_func(seed=42):\n    try:\n        # Set seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        end_date = datetime.now().date()\n        start_date = end_date - timedelta(days=29)\n        dates = pd.date_range(start=start_date, end=end_date, freq='D')\n        \n        # Generate random values\n        values = [random.uniform(0, 100) for _ in range(len(dates))]\n        \n        # Create DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Set plot style\n        plt.style.use('default')\n        plt.rcParams['font.family'] = 'Arial'\n        \n        # Create plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(df['Date'], df['Value'], marker='o', linestyle='-', linewidth=2, markersize=6)\n        \n        # Set labels and title\n        ax.set_xlabel('Date', fontsize=12)\n        ax.set_ylabel('Value', fontsize=12)\n        ax.set_title('Random Time Series Data', fontsize=14, fontweight='bold')\n        \n        # Format x-axis\n        ax.tick_params(axis='x', rotation=45)\n        ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n        \n        # Add grid\n        ax.grid(True, linestyle='--', alpha=0.7)\n        \n        # Tight layout\n        plt.tight_layout()\n        \n        return ax\n    \n    except Exception as e:\n        raise ValueError(f\"Error generating data or plot: {str(e)}\")"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_boston\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Set random seed for reproducibility\n        np.random.seed(seed)\n\n        # Load the Boston Housing dataset\n        boston = load_boston()\n        data = pd.DataFrame(boston.data, columns=boston.feature_names)\n        data['MEDV'] = boston.target\n\n        # Calculate the correlation matrix\n        corr_matrix = data.corr()\n\n        # Set up the matplotlib figure\n        plt.figure(figsize=(12, 10))\n\n        # Generate the heatmap\n        ax = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', \n                         square=True, linewidths=0.5, cbar_kws={\"shrink\": .75})\n\n        plt.title(\"Boston Housing Dataset Correlation Heatmap\", fontsize=16)\n        plt.tight_layout()\n\n        # Save the plot if save_path is provided\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            print(f\"Heatmap saved to {save_path}\")\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"Error in generating or saving the plot: {str(e)}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data.\")\n    \n    # Validate frequency\n    valid_freqs = ['D', 'W', 'M', 'Q', 'Y', 'H', 'T', 'S']\n    if freq not in valid_freqs:\n        raise ValueError(f\"Invalid frequency. 'freq' must be one of {', '.join(valid_freqs)}.\")\n    \n    # Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n    \n    # Ensure the index is datetime\n    if not isinstance(df.index, pd.DatetimeIndex):\n        try:\n            df.index = pd.to_datetime(df.index)\n        except:\n            raise ValueError(\"DataFrame index must be convertible to DatetimeIndex.\")\n    \n    # Perform seasonal decomposition\n    result = seasonal_decompose(df['value'], model=decomposition_model, period=None, extrapolate_trend='freq')\n    \n    # Create plot\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n    \n    result.observed.plot(ax=ax1)\n    ax1.set_ylabel('Observed')\n    result.trend.plot(ax=ax2)\n    ax2.set_ylabel('Trend')\n    result.seasonal.plot(ax=ax3)\n    ax3.set_ylabel('Seasonal')\n    result.resid.plot(ax=ax4)\n    ax4.set_ylabel('Residual')\n    \n    plt.tight_layout()\n    \n    return result, (ax1, ax2, ax3, ax4)"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range.\n\n    Args:\n        start_date (datetime): The start date of the range (inclusive). Default is January 1, 2020.\n        end_date (datetime): The end date of the range (inclusive). Default is December 31, 2020.\n        seed (int): Seed for the random number generator. Default is 42.\n\n    Returns:\n        pandas.Series: A Series containing random dates within the specified range.\n        Each date is a datetime.datetime object. The series length matches the number\n        of days in the specified range.\n\n    Raises:\n        ValueError: If start_date or end_date is not a datetime.datetime instance,\n                    or if start_date is later than end_date.\n    \"\"\"\n    # Validate input types\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both start_date and end_date must be datetime.datetime instances.\")\n\n    # Validate date range\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date.\")\n\n    # Set the random seed\n    random_seed(seed)\n\n    # Calculate the number of days in the range (inclusive)\n    days_range = (end_date - start_date).days + 1\n\n    # Generate random dates\n    random_dates = [start_date + timedelta(days=randint(0, days_range - 1)) for _ in range(days_range)]\n\n    # Create and return the pandas Series\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list\")\n\n    # Add the element '12' to the list\n    my_list.append(12)\n\n    # Calculate the number of files to concatenate\n    num_files = sum(my_list)\n\n    # Get the list of CSV files in the directory\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    csv_files = glob.glob(file_pattern)\n\n    # Check if any files are found\n    if not csv_files:\n        raise FileNotFoundError(f\"No {file_ext} files found in the specified directory: {file_dir}\")\n\n    # Ensure we don't try to concatenate more files than available\n    num_files = min(num_files, len(csv_files))\n\n    # Read and concatenate the CSV files\n    dfs = [pd.read_csv(file) for file in csv_files[:num_files]]\n    concatenated_df = pd.concat(dfs, ignore_index=True)\n\n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Validate input\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric elements (int or float)\")\n\n    # Append 12 to my_list\n    my_list.append(12)\n\n    # Calculate sum of elements and determine size of random list\n    sum_elements = sum(my_list)\n    random_list_size = min(sum_elements, size)\n\n    # Set random seed\n    random_seed(seed)\n\n    # Generate random integers and measure time\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(random_list_size)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), align='left', rwidth=0.8)\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Random Numbers')\n    ax.set_xlim(0.5, 100.5)\n    ax.set_xticks(range(0, 101, 10))\n\n    return time_taken, ax"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the specified URL.\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP request failed: {e}\")\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n\n    if not table:\n        raise ValueError(\"No table found on the page.\")\n\n    headers = []\n    header_row = table.find('tr')\n    if header_row:\n        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n        if headers:\n            table_rows = table.find_all('tr')[1:]\n        else:\n            table_rows = table.find_all('tr')\n    else:\n        table_rows = table.find_all('tr')\n\n    data = []\n    for row in table_rows:\n        row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n        if row_data:\n            data.append(row_data)\n\n    if not data:\n        raise ValueError(\"No data found in the table.\")\n\n    df = pd.DataFrame(data, columns=headers if headers else None)\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n\n    Raises:\n        ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    # Check if input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n\n    # Get numeric columns\n    numeric_columns = df.select_dtypes(include=[np.number]).columns\n\n    # Check if there are any numeric columns\n    if len(numeric_columns) == 0:\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n\n    # Create histograms for each numeric column\n    axes = []\n    for column in numeric_columns:\n        fig, ax = plt.subplots()\n        df[column].hist(ax=ax)\n        ax.set_title(column)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n\n    return axes"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread, Lock\ndef task_func(ip_range, port):\n    def check_port(ip, port, results, lock):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        sock.close()\n        \n        with lock:\n            results[str(ip)] = (result == 0)\n\n    results = {}\n    lock = Lock()\n    threads = []\n\n    for ip in IPv4Network(ip_range):\n        thread = Thread(target=check_port, args=(ip, port, results, lock))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Regular expression pattern to match log entries\n    pattern = r'^(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)$'\n    \n    log_entries = []\n    \n    # Read and process the log file\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(pattern, line.strip())\n            if match:\n                msg_type, timestamp, message = match.groups()\n                try:\n                    # Validate timestamp\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    log_entries.append({\n                        'Type': msg_type,\n                        'Timestamp': timestamp,\n                        'Message': message\n                    })\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp in log entry: {line.strip()}\")\n    \n    # Check if any valid log entries were found\n    if not log_entries:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create DataFrame from log entries\n    df = pd.DataFrame(log_entries)\n    \n    # Generate output CSV file name\n    csv_file = log_file.rsplit('.', 1)[0] + '_structured.csv'\n    \n    # Save DataFrame to CSV\n    df.to_csv(csv_file, index=False)\n    \n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Extract words using regex, considering only alphabetic characters\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    \n    # If no words are found, return an empty subplot\n    if not words:\n        fig, ax = plt.subplots()\n        ax.set_title('Word Length Distribution (No words found)')\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        return ax\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Determine the range for bins\n    max_length = max(word_lengths)\n    bins = range(1, max_length + 2)  # +2 to include the maximum length\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, align='left', edgecolor='black')\n    \n    # Set labels and title\n    ax.set_title('Distribution of Word Lengths')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    \n    # Set x-axis ticks to show all integer values\n    ax.set_xticks(range(1, max_length + 1))\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\nimport pandas as pd\ndef task_func(df):\n    # Check if DataFrame is empty or missing required columns\n    if df.empty or not {'Title', 'Content'}.issubset(df.columns):\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n\n    # Filter articles with titles containing \"like\" or \"what\" (case-insensitive)\n    pattern = r'\\b(like|what)\\b'\n    filtered_df = df[df['Title'].str.contains(pattern, case=False, regex=True)]\n\n    # Combine all content into a single string\n    all_content = ' '.join(filtered_df['Content'].fillna(''))\n\n    # Remove punctuation and convert to lowercase\n    cleaned_content = re.sub(f'[{re.escape(punctuation)}]', '', all_content.lower())\n\n    # Tokenize the content\n    words = nltk.word_tokenize(cleaned_content)\n\n    # Count word frequencies\n    word_freq = Counter(words)\n\n    return dict(word_freq)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    def preprocess_text(text):\n        # Remove numbers and punctuation\n        text = re.sub(r'[^\\w\\s]', '', re.sub(r'\\d+', '', text))\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        return ' '.join([word for word in text.split() if word not in STOPWORDS])\n\n    # Apply preprocessing to the specified column\n    dataframe['processed_text'] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorize the preprocessed text\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe['processed_text'])\n\n    # Create DataFrame with word counts\n    word_count_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return word_count_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Check if 'Lon' and 'Lat' keys are present in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"'Lon' or 'Lat' keys are missing in the dictionary.\")\n    \n    # Check if the values of 'Lon' and 'Lat' are tuples\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values of 'Lon' and 'Lat' must be tuples.\")\n    \n    # Generate random coordinates within the specified ranges\n    lons = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    lats = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n    \n    # Create a list of Point objects for the coordinates\n    geometries = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n    \n    # Create the GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': geometries}, geometry='Coordinates')\n    \n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Input validation\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    \n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    \n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    \n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    \n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n\n    # Set random seed for reproducibility\n    set_seed(seed)\n\n    # Ensure UTC datetime is timezone-aware\n    if utc_datetime.tzinfo is None:\n        utc_datetime = utc_datetime.replace(tzinfo=pytz.UTC)\n\n    # Generate weather report\n    weather_data = []\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone not provided for city: {city}\")\n        \n        try:\n            local_tz = pytz.timezone(timezones[city])\n        except pytz.exceptions.UnknownTimeZoneError:\n            raise ValueError(f\"Invalid timezone provided for city: {city}\")\n        \n        local_time = utc_datetime.astimezone(local_tz)\n        local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        weather_data.append({\n            'City': city,\n            'Local Time': local_time_str,\n            'Weather Condition': weather\n        })\n\n    # Create DataFrame\n    df = pd.DataFrame(weather_data)\n    return df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    # Input validation\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Calculate cumulative sum for the random walk\n    walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = pd.Series(walk).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    \n    # Create dictionary of descriptive statistics\n    stats_dict = {\n        'count': int(stats['count']),\n        'mean': stats['mean'],\n        'std': stats['std'],\n        'min': stats['min'],\n        '5th percentile': stats['5%'],\n        '25th percentile': stats['25%'],\n        'median': stats['50%'],\n        '75th percentile': stats['75%'],\n        '95th percentile': stats['95%'],\n        'max': stats['max']\n    }\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(walk, color='blue', alpha=0.8)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    return stats_dict, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers, stream=True)\n    response.raise_for_status()\n\n    # Get the filename from the URL or Content-Disposition header\n    filename = url.split('/')[-1]\n    if 'Content-Disposition' in response.headers:\n        content_disposition = response.headers['Content-Disposition']\n        if 'filename=' in content_disposition:\n            filename = content_disposition.split('filename=')[1].strip('\"')\n\n    zip_path = os.path.join(destination_directory, filename)\n\n    # Save the zip file\n    with open(zip_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    # Extract the contents of the zip file\n    extracted_files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n\n    # Return the list of extracted files\n    return [os.path.basename(file) for file in extracted_files]"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and view it.\n\n    Args:\n        seed (int): Seed for random number generation.\n        image_size (tuple): Size of the image (height, width, channels).\n        range_low (int): Lower bound for pixel values.\n        range_high (int): Upper bound for pixel values.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axes object of the plot.\n        image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Remove axis ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Show the plot\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file '{audio_file}' does not exist.\")\n\n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n\n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create the MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10**(spl/20))\n\n    # Generate the spectrogram from the normalized matrix\n    S = librosa.stft(normalized_matrix.flatten())\n    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n\n    # Create the figure and plot the spectrogram\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(S_db, sr=sample_rate, x_axis='time', y_axis='hz', ax=ax)\n    ax.set_yscale('log')\n    ax.set_title('Spectrogram')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency (Hz)')\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n\n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([item for tup in original for item in tup if isinstance(item, (int, float))])\n\n    # Compute basic statistics\n    stats_dict = {\n        \"mean\": np.mean(numeric_values),\n        \"standard_deviation\": np.std(numeric_values),\n        \"minimum\": np.min(numeric_values),\n        \"maximum\": np.max(numeric_values)\n    }\n\n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto', label='Histogram')\n    \n    # Calculate and plot the PDF\n    x = np.linspace(stats_dict[\"minimum\"], stats_dict[\"maximum\"], 100)\n    pdf = stats.norm.pdf(x, loc=stats_dict[\"mean\"], scale=stats_dict[\"standard_deviation\"])\n    ax.plot(x, pdf, 'r-', label='PDF')\n\n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with Overlaid PDF')\n    ax.legend()\n\n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n\n    # Normalize the array\n    normalized_array = preprocessing.normalize([original_array])[0]\n\n    # Create a figure and axes\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the original and normalized arrays\n    ax.plot(original_array, label='Original', marker='o')\n    ax.plot(normalized_array, label='Normalized', marker='s')\n\n    # Customize the plot\n    ax.set_title('Original vs Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    # Add value annotations\n    for i, (orig, norm) in enumerate(zip(original_array, normalized_array)):\n        ax.annotate(f'{orig:.2f}', (i, orig), textcoords=\"offset points\", xytext=(0,10), ha='center')\n        ax.annotate(f'{norm:.2f}', (i, norm), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n\n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()\n\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # 1. Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    signal = np.array(list(data.values()))\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n\n    # 4. Plot the FFT of the signal\n    frequencies = fftpack.fftfreq(len(signal), d=1/sample_rate)\n    magnitude = np.abs(fft_result)\n\n    fig, ax = plt.subplots()\n    ax.plot(frequencies, magnitude)\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('FFT of the Signal')\n\n    # Improve plot readability\n    ax.grid(True)\n    ax.set_xlim(0, sample_rate/2)  # Show only positive frequencies up to Nyquist frequency\n\n    # Return the FFT of the signal and the plot axes\n    return fft_result, ax"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\ndef task_func():\n    class DataHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check Content-Type header\n            if self.headers.get('Content-Type') != 'application/json':\n                self.send_error_response(400, \"Content-Type header is not application/json\")\n                return\n\n            # Read and parse JSON data\n            content_length = int(self.headers.get('Content-Length', 0))\n            post_data = self.rfile.read(content_length)\n            \n            try:\n                data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_error_response(400, \"Invalid JSON\")\n                return\n\n            # Check for 'data' key in JSON\n            if 'data' not in data:\n                self.send_error_response(400, \"No data key in request\")\n                return\n\n            # If all checks pass, send success response\n            self.send_success_response()\n\n        def send_success_response(self):\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            response = json.dumps(SUCCESS_RESPONSE).encode('utf-8')\n            self.send_header('Content-Length', len(response))\n            self.end_headers()\n            self.wfile.write(response)\n\n        def send_error_response(self, status_code, message):\n            self.send_response(status_code)\n            self.send_header('Content-Type', 'application/json')\n            error_response = json.dumps({'status': 'error', 'message': message}).encode('utf-8')\n            self.send_header('Content-Length', len(error_response))\n            self.end_headers()\n            self.wfile.write(error_response)\n\n    return DataHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Read and parse JSON data\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length).decode('utf-8')\n                email_data = json.loads(post_data)\n\n                # Validate required keys\n                required_keys = ['subject', 'message', 'to']\n                if not all(key in email_data for key in required_keys):\n                    raise ValueError(\"Missing required keys in email data\")\n\n                # Prepare email\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n\n                # Send email\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.send_message(msg)\n\n                # Send success response\n                self.send_response(200)\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', '0')\n                self.end_headers()\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON data\")\n            except ValueError as e:\n                self.send_error(400, str(e))\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication failed\")\n            except Exception as e:\n                self.send_error(500, f\"Internal server error: {str(e)}\")\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counts = {}\n    total_words = 0\n\n    # Iterate through all files in the directory\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            file_path = os.path.join(directory, file)\n            with open(file_path, 'r', encoding='utf-8') as f:\n                # Read the content and split into words\n                words = f.read().split()\n                # Count words for this file\n                file_word_count = len(words)\n                # Add to the total word count\n                total_words += file_word_count\n                # Store the count for this file\n                word_counts[file] = file_word_count\n\n    # Export word counts to JSON file\n    with open(filename, 'w', encoding='utf-8') as json_file:\n        json.dump(word_counts, json_file, indent=4)\n\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    # Validate input DataFrame\n    if df.empty or not set(COLUMNS).issubset(df.columns):\n        raise ValueError(\"DataFrame is empty or missing required columns.\")\n    \n    if not all(isinstance(val, list) for val in df['Value']):\n        raise ValueError(\"'Value' column must contain lists.\")\n\n    # Split lists in 'Value' column into separate columns\n    expanded_df = pd.DataFrame(df['Value'].tolist(), index=df['Date'])\n    \n    # Calculate Pearson correlation coefficients\n    corr_matrix = expanded_df.corr(method='pearson')\n    \n    if plot:\n        # Create heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n        plt.title(\"Correlation Heatmap\")\n        plt.tight_layout()\n        \n        return corr_matrix, ax\n    \n    return corr_matrix"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {student: {field: random.randint(0, 100) for field in all_fields} for student in STUDENTS}\n    \n    # Calculate average grade for each student\n    for student in data:\n        data[student]['Average Grade'] = mean(data[student].values())\n    \n    # Create DataFrame\n    df = pd.DataFrame.from_dict(data, orient='index')\n    \n    # Calculate average grade for each subject\n    subject_averages = df.mean()\n    df.loc['Average'] = subject_averages\n    \n    # Ensure 'Average Grade' column is at the end\n    cols = df.columns.tolist()\n    cols.remove('Average Grade')\n    cols.append('Average Grade')\n    df = df[cols]\n    \n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    \"\"\"\n    Generates a CSV file containing simulated data for 100 people, including name, age, height, and weight.\n    It also calculates and appends the average age, height, and weight at the end of the file.\n\n    Args:\n        filename (str): The name of the CSV file to be created.\n\n    Returns:\n        str: The path of the created CSV file.\n    \"\"\"\n    data = []\n    \n    # Generate random data for 100 people\n    for i in range(PEOPLE_COUNT):\n        name = f\"Person_{i+1}\"\n        age = random.randint(18, 80)\n        height = round(random.uniform(150, 200), 1)  # Height in cm\n        weight = round(random.uniform(45, 120), 1)  # Weight in kg\n        data.append([name, age, height, weight])\n    \n    # Calculate averages\n    avg_age = round(mean(row[1] for row in data), 1)\n    avg_height = round(mean(row[2] for row in data), 1)\n    avg_weight = round(mean(row[3] for row in data), 1)\n    \n    # Append averages to the data\n    data.append(['Average', avg_age, avg_height, avg_weight])\n    \n    # Write data to CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n    \n    # Return the absolute path of the created CSV file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    organized_files = {}\n    \n    # Regex pattern to match text not in square brackets at the start of the filename\n    pattern = re.compile(r'^([^[\\]]+)')\n    \n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n        \n        # Find the first text not in square brackets\n        match = pattern.match(filename)\n        \n        if match:\n            subdirectory_name = match.group(1).strip()\n            \n            # Create subdirectory if it doesn't exist\n            subdirectory_path = os.path.join(directory, subdirectory_name)\n            if not os.path.exists(subdirectory_path):\n                os.makedirs(subdirectory_path)\n            \n            # Move the file to the subdirectory\n            new_file_path = os.path.join(subdirectory_path, filename)\n            shutil.move(file_path, new_file_path)\n            \n            # Update the organized_files dictionary\n            if subdirectory_name not in organized_files:\n                organized_files[subdirectory_name] = []\n            organized_files[subdirectory_name].append(filename)\n    \n    return directory, organized_files"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    exit_codes = [None] * len(file_list)\n    lock = threading.Lock()\n\n    def run_subprocess(index, file):\n        try:\n            result = subprocess.run(['python', file], capture_output=True, text=True)\n            with lock:\n                exit_codes[index] = result.returncode\n        except Exception as e:\n            print(f\"Error running {file}: {str(e)}\")\n            with lock:\n                exit_codes[index] = -1\n\n    threads = []\n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(i, file))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    results = []\n    \n    # Ensure the directory path is absolute\n    directory_path = os.path.abspath(directory_path)\n    \n    # Find all .bat files in the given directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Run the .bat file and capture the exit code\n            process = subprocess.run([bat_file], capture_output=True, text=True, check=False)\n            exit_code = process.returncode\n        except Exception as e:\n            # If an exception occurs, set the exit code to None\n            print(f\"Error executing {file_name}: {str(e)}\", file=sys.stderr)\n            exit_code = None\n        \n        # Append the result tuple to the list\n        results.append((file_name, exit_code))\n    \n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be DataFrame\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty\")\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column '{col}'\")\n\n    # Create figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))\n\n    # Plot histogram with kernel density estimate\n    sns.histplot(data=df, x=col, kde=True, ax=ax1)\n    ax1.set_title(f'Histogram of {col}')\n    ax1.set_xlabel(col)\n    ax1.set_ylabel('Count')\n\n    # Plot box plot\n    sns.boxplot(data=df, y=col, ax=ax2)\n    ax2.set_title(f'Box Plot of {col}')\n    ax2.set_ylabel(col)\n\n    # Adjust layout and return figure\n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n\n    Args:\n        script_path (str): Path to the Python script to run.\n        wait (bool): Whether to wait for the process to complete. Defaults to True.\n        *args: Additional arguments to pass to the script.\n\n    Returns:\n        int: The return code of the subprocess if wait is True, None otherwise.\n\n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script '{script_path}' does not exist.\")\n\n    cmd = [sys.executable, script_path] + list(args)\n\n    try:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        if wait:\n            stdout, stderr = process.communicate()\n            if process.returncode != 0:\n                raise subprocess.CalledProcessError(process.returncode, cmd, stdout, stderr)\n            return process.returncode\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        raise e"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The Excel file does not exist at the specified path: {file_location}\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError:\n        raise ValueError(f\"The specified sheet '{sheet_name}' does not exist in the workbook.\")\n    \n    # Calculate mean and standard deviation for each column\n    stats = {\n        column: {\n            'mean': df[column].mean(),\n            'std': df[column].std()\n        } for column in df.columns\n    }\n    \n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    x = np.arange(len(df.columns))\n    width = 0.35\n    \n    means = [stats[col]['mean'] for col in df.columns]\n    stds = [stats[col]['std'] for col in df.columns]\n    \n    ax.bar(x - width/2, means, width, label='Mean', color='skyblue')\n    ax.bar(x + width/2, stds, width, label='Standard Deviation', color='lightgreen')\n    \n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df.columns, rotation=45, ha='right')\n    ax.legend()\n    \n    plt.tight_layout()\n    \n    return stats, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Validate input\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Count activities per day\n    day_counts = defaultdict(int)\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    for activity in activities:\n        day_counts[days[activity.weekday()]] += 1\n\n    # Prepare data for plotting\n    x = days\n    y = [day_counts[day] for day in days]\n\n    # Create bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(x, y)\n\n    # Customize chart\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Adjust layout and return\n    plt.tight_layout()\n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n\n    Args:\n        src_dir (str): Path to the source directory.\n        dest_dir (str): Path to the destination directory.\n        seed (int): Seed for random number generator. Default is 100.\n\n    Returns:\n        str: The name of the file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n\n    Raises:\n        FileNotFoundError: If the source directory doesn't exist or is empty.\n        OSError: If there's an error creating the destination directory or moving the file.\n    \"\"\"\n    random.seed(seed)\n\n    # Check if source directory exists and is not empty\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    \n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        raise FileNotFoundError(f\"Source directory '{src_dir}' is empty.\")\n\n    # Select a random file\n    file_to_move = random.choice(files)\n\n    # Ensure destination directory exists\n    os.makedirs(dest_dir, exist_ok=True)\n\n    # Move the file\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n    \n    try:\n        shutil.move(src_path, dest_path)\n    except OSError as e:\n        raise OSError(f\"Error moving file: {e}\")\n\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    processed_files = 0\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    for file_path in xlsx_files:\n        wb = load_workbook(file_path)\n        modified = False\n\n        for sheet in wb.worksheets:\n            for row in sheet.iter_rows():\n                for cell in row:\n                    if isinstance(cell.value, str):\n                        new_value = re.sub(r'\"', r'\\\\\"', cell.value)\n                        if new_value != cell.value:\n                            cell.value = new_value\n                            modified = True\n\n        if modified:\n            wb.save(file_path)\n        wb.close()\n        processed_files += 1\n\n    return processed_files"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Create a diagram of sine and cosine waves with a given frequency.\n\n    Args:\n        frequency (float): The frequency of the waves.\n        sample_size (int, optional): The number of samples. Defaults to 10000.\n\n    Returns:\n        tuple: (matplotlib.figure.Figure, matplotlib.axes.Axes)\n\n    Raises:\n        ValueError: If the frequency is negative or if the sample size is non-positive.\n    \"\"\"\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Generate time array\n    t = np.linspace(0, 2*np.pi, sample_size)\n\n    # Calculate sine and cosine waves\n    sine_wave = np.sin(frequency * t)\n    cosine_wave = np.cos(frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n\n    # Customize the plot\n    ax.set_title(f'Sine and Cosine Waves (Frequency: {frequency} Hz)')\n    ax.set_xlabel('Time (radians)')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n\n    # Set y-axis limits to show full wave\n    ax.set_ylim(-1.1, 1.1)\n\n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    # Create Flask app\n    app = Flask(app_name)\n\n    # Configure mail settings from environment variables or use defaults\n    app.config['MAIL_SERVER'] = os.environ.get('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.environ.get('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.environ.get('MAIL_USE_TLS', '').lower() in ('true', 'yes', '1', 'on')\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME')\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD')\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Create a dictionary of mail configurations\n    mail_config = {\n        'MAIL_SERVER': app.config['MAIL_SERVER'],\n        'MAIL_PORT': app.config['MAIL_PORT'],\n        'MAIL_USE_TLS': app.config['MAIL_USE_TLS'],\n        'MAIL_USERNAME': app.config['MAIL_USERNAME'],\n        'MAIL_PASSWORD': app.config['MAIL_PASSWORD']\n    }\n\n    return mail, mail_config"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of data from a specific column in an Excel file.\n\n    Args:\n    excel_file_path (str): Path to the directory containing the Excel file.\n    file_name (str): Name of the Excel file.\n    column_name (str): Name of the column to analyze.\n\n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation.\n\n    Raises:\n    FileNotFoundError: If the Excel file does not exist at the specified path.\n    ValueError: If the specified column is not found in the Excel file.\n    \"\"\"\n    # Construct full file path\n    full_path = os.path.join(excel_file_path, file_name)\n\n    # Check if file exists\n    if not os.path.isfile(full_path):\n        raise FileNotFoundError(f\"The Excel file '{file_name}' does not exist at the specified path: {excel_file_path}\")\n\n    # Read the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise ValueError(f\"Error reading the Excel file: {str(e)}\")\n\n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not found in the Excel file.\")\n\n    # Calculate statistics\n    column_data = df[column_name]\n    mean = np.mean(column_data)\n    median = np.median(column_data)\n    std_dev = np.std(column_data)\n\n    # Return results as a dictionary\n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"standard_deviation\": std_dev\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training (75%) and test (25%) sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct the Sequential model\n    model = Sequential([\n        Dense(16, activation='sigmoid', input_dim=2),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    # Fit the model to the training data, evaluating on the test set\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0, \n                        validation_data=(X_test, Y_test))\n\n    # Plot the model's training and validation loss\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    return model, ax"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer using a sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(units=64, activation='sigmoid', input_shape=(X.shape[1],)),\n        keras.layers.Dense(units=1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer specifying a learning rate\n    optimizer = keras.optimizers.SGD(learning_rate=0.01)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_pred = model.predict(X_test)\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.set_title('ROC curve')\n    ax.legend(loc=\"lower right\")\n\n    return model, ax"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(f\"'n_clusters' must be a positive integer. Got {n_clusters}\")\n    \n    # Read the image in RGB format\n    original_image = cv2.imread(image_path)\n    if original_image is None:\n        raise FileNotFoundError(f\"Unable to read the image file: {image_path}\")\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n    \n    # Reshape the image for clustering\n    pixels = original_image.reshape((-1, 3))\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n    \n    # Create the segmented image\n    segmented_pixels = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_image = segmented_pixels.reshape(original_image.shape).astype(np.uint8)\n    \n    # Save each region as a separate image\n    for i in range(n_clusters):\n        mask = (kmeans.labels_ == i).reshape(original_image.shape[:2])\n        region = np.zeros_like(original_image)\n        region[mask] = segmented_image[mask]\n        cv2.imwrite(f'region_{i}.png', cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n    \n    return original_image, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Calculate the product of matrix P and tensor T\n    product = np.tensordot(P, T, axes=([1], [0]))\n    \n    # Flatten the result\n    flattened = product.reshape(-1, 1)\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened)\n    \n    # Visualize the clustering\n    fig, ax = plt.subplots(figsize=(10, 6))\n    scatter = ax.scatter(range(len(flattened)), flattened, c=cluster_result, cmap='viridis')\n    \n    # Customize the plot\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Add a color bar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    \n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Input validation\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input 'points' must be a numpy array.\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input 'points' must be a 2D array with shape (n, 2).\")\n    if len(points) < 3:\n        raise ValueError(\"At least 3 points are required to compute a Voronoi diagram.\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Apply jittering to avoid duplicate points\n    jitter = np.random.normal(0, 1e-10, points.shape)\n    jittered_points = points + jitter\n\n    # Calculate Voronoi diagram\n    vor = Voronoi(jittered_points)\n\n    # Plot Voronoi diagram\n    fig, ax = plt.subplots(figsize=(8, 6))\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='black', line_width=2, line_alpha=0.6, point_size=2)\n    \n    # Customize plot\n    ax.set_title(\"Voronoi Diagram\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_aspect('equal')\n    \n    # Adjust plot limits\n    margin = 0.1\n    ax.set_xlim(vor.min_bound[0] - margin, vor.max_bound[0] + margin)\n    ax.set_ylim(vor.min_bound[1] - margin, vor.max_bound[1] + margin)\n\n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n\n    Args:\n        src_dir (str): Path to the source directory.\n        dest_dir (str): Path to the destination directory.\n        ext (str): File extension to search for (e.g., '.txt', '.pdf').\n\n    Returns:\n        list: Full paths of successfully moved files.\n\n    Raises:\n        FileNotFoundError: If either the source or destination directory does not exist.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist\")\n\n    moved_files = []\n\n    # Ensure the extension starts with a dot\n    if not ext.startswith('.'):\n        ext = '.' + ext\n\n    # Find all files with the specified extension in the source directory\n    for file_path in glob.glob(os.path.join(src_dir, f'*{ext}')):\n        file_name = os.path.basename(file_path)\n        dest_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_path):\n            # Move the file\n            shutil.move(file_path, dest_path)\n            moved_files.append(dest_path)\n\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    if not json_str:\n        return pd.DataFrame()\n\n    try:\n        # Load JSON string into a dictionary\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return float(value * 2)\n        elif isinstance(value, list):\n            return [normalize_value(item) for item in value]\n        elif isinstance(value, str):\n            # Extract numerical values from strings using regex\n            match = re.search(r'^-?\\d+(?:\\.\\d+)?$', value)\n            if match:\n                return float(float(match.group()) * 2)\n        return value\n\n    # Normalize the dictionary by doubling the numerical values\n    normalized_dict = {key: normalize_value(value) for key, value in data_dict.items()}\n\n    # Create a Pandas DataFrame from the dictionary\n    df = pd.DataFrame([normalized_dict])\n\n    return df"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.isfile(script_path):\n        raise FileNotFoundError(f\"The script '{script_path}' does not exist.\")\n\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    cpu_usage = 0.0\n    memory_usage = 0\n    start_time = time.time()\n\n    try:\n        proc = psutil.Process(process.pid)\n        while process.poll() is None:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                process.wait(timeout=1)\n                if process.poll() is None:\n                    process.kill()\n                raise TimeoutError(f\"Script execution exceeded the {timeout} second timeout.\")\n\n            try:\n                cpu_usage += proc.cpu_percent(interval=0.1)\n                memory_info = proc.memory_info()\n                memory_usage += memory_info.rss\n            except (psutil.NoSuchProcess, psutil.ZombieProcess):\n                break\n\n    except psutil.NoSuchProcess:\n        print(\"Process ended unexpectedly.\")\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n    finally:\n        if process.poll() is None:\n            process.terminate()\n            try:\n                process.wait(timeout=1)\n            except subprocess.TimeoutExpired:\n                process.kill()\n\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random values for x and y\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Generate categories\n    num_categories = len(CATEGORIES)\n    if N >= num_categories:\n        categories = np.random.choice(CATEGORIES, size=N, replace=True)\n        # Ensure each category appears at least once\n        categories[:num_categories] = np.random.permutation(CATEGORIES)\n    else:\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(10, 8))\n    for category in CATEGORIES:\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category, alpha=0.7)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter plot of x vs y, colored by category')\n    ax.legend(title='Category')\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Convert start_time and end_time to datetime objects if they're strings\n    if isinstance(start_time, str):\n        start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    if isinstance(end_time, str):\n        end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Generate time series\n    time_range = pd.date_range(start=start_time, end=end_time, freq=step)\n    \n    # Generate values from normal distribution\n    values = np.random.normal(size=len(time_range))\n    \n    # Add linear trend\n    trend_values = np.arange(len(time_range)) * trend\n    values += trend_values\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_range, 'Value': values})\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'])\n    \n    # Set labels and title\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Input validation\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"random_seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"products must be a list of 5 strings\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    end_date = datetime.now()\n\n    # Generate sales data\n    sales_data = []\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({\n                'Product': product,\n                'Date': current_date,\n                'Sales': sales\n            })\n        current_date += timedelta(days=1)\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data)\n\n    # Ensure correct column types\n    df['Product'] = df['Product'].astype(str)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Sales'] = df['Sales'].astype(int)\n\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is of the correct type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    try:\n        # Parse the JSON string\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string\")\n\n    # Ensure data is a list (array)\n    if not isinstance(data, list):\n        raise ValueError(\"JSON must represent an array\")\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(data)\n\n    try:\n        # Ensure the filename has the .xlsx extension\n        if not filename.endswith('.xlsx'):\n            filename += '.xlsx'\n\n        # Get the absolute path for the file\n        abs_path = os.path.abspath(filename)\n\n        # Write the DataFrame to an Excel file\n        with pd.ExcelWriter(abs_path, engine='xlwt') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n\n        return abs_path\n\n    except Exception as e:\n        raise Exception(f\"Error writing to file: {str(e)}\")"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past-1)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Generate data\n    data = []\n    for date in date_range:\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date.strftime('%Y-%m-%d'), activity, duration])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Create plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Customize plot\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate date range\n    end_date = datetime.now().date()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices\n    prices = np.random.random((days_in_past, len(stock_names)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(prices, columns=stock_names, index=date_range)\n    \n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files and create a difference report.\n\n    Args:\n        file_path1 (str): Path to the first CSV file.\n        file_path2 (str): Path to the second CSV file.\n        delimiter (str): Delimiter used in the CSV files (default: ',').\n        quotechar (str): Quote character used in the CSV files (default: '\"').\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the differences between the files.\n\n    Raises:\n        FileNotFoundError: If either of the files cannot be found.\n        ValueError: If either of the files is empty.\n        Exception: For other IO related errors.\n    \"\"\"\n    try:\n        # Read the contents of both files\n        with open(file_path1, 'r', newline='') as f1, open(file_path2, 'r', newline='') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            \n            lines1 = list(reader1)\n            lines2 = list(reader2)\n\n        # Check if either file is empty\n        if not lines1:\n            raise ValueError(f\"The file {file_path1} is empty.\")\n        if not lines2:\n            raise ValueError(f\"The file {file_path2} is empty.\")\n\n        # Convert lines to strings for comparison\n        str_lines1 = [delimiter.join(line) for line in lines1]\n        str_lines2 = [delimiter.join(line) for line in lines2]\n\n        # Generate diff\n        diff = list(ndiff(str_lines1, str_lines2))\n\n        # Process diff and create DataFrame\n        result = []\n        for line_num, line in enumerate(diff, start=1):\n            if line.startswith('  '):\n                result.append({'Line Number': line_num, 'Status': ' ', 'Content': line[2:]})\n            elif line.startswith('- '):\n                result.append({'Line Number': line_num, 'Status': '-', 'Content': line[2:]})\n            elif line.startswith('+ '):\n                result.append({'Line Number': line_num, 'Status': '+', 'Content': line[2:]})\n\n        return pd.DataFrame(result)\n\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\")\n    except ValueError as e:\n        raise ValueError(str(e))\n    except Exception as e:\n        raise Exception(f\"An IO related error occurred: {str(e)}\")"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Initialize statistics dictionary\n    stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n    \n    # Calculate statistics if data is not empty and column exists\n    if not df.empty and column in df.columns:\n        stats['sum'] = df[column].sum()\n        stats['mean'] = df[column].mean()\n        stats['min'] = df[column].min()\n        stats['max'] = df[column].max()\n    \n    # Create pie chart\n    fig, ax = plt.subplots()\n    if 'Age' in df.columns and not df.empty:\n        age_counts = df['Age'].value_counts()\n        ax.pie(age_counts.values, labels=age_counts.index, autopct='%1.1f%%', startangle=90)\n        ax.set_title(f'Distribution of Ages')\n    else:\n        ax.pie([1], labels=['No Data'], autopct='%1.1f%%')\n        ax.set_title('No Data Available')\n    \n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n    \n    return (stats, ax)"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if data is empty\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if column exists\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n    \n    # Check for non-negative values\n    numeric_columns = ['steps', 'calories burned', 'distance walked']\n    for col in numeric_columns:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in '{col}' column.\")\n    \n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Create line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df[column], marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    ax.set_title(f'Line Chart of {column}')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize defaultdict to store numeric values for each key\n    values = defaultdict(list)\n\n    # Collect numeric values for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, (int, float)) and not pd.isna(value):\n                values[key].append(value)\n\n    # Calculate mean and median for each key\n    results = {}\n    for key, nums in values.items():\n        if nums:\n            results[key] = {\n                'mean': np.mean(nums),\n                'median': np.median(nums)\n            }\n\n    # Create and sort DataFrame\n    df = pd.DataFrame.from_dict(results, orient='index')\n    df.sort_index(inplace=True)\n\n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.lower().endswith('.csv'):\n        raise ValueError(\"File must have a .csv extension\")\n\n    # Read the CSV file and count duplicate rows\n    with open(file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        row_counter = Counter(tuple(row) for row in reader)\n\n    # Filter out non-duplicate rows\n    duplicate_rows = {row: count for row, count in row_counter.items() if count > 1}\n\n    # Convert duplicate rows to pandas DataFrame\n    df = pd.DataFrame(list(duplicate_rows.items()), columns=['Row', 'Count'])\n\n    # Plot the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(range(len(df)), df['Count'], align='center')\n    ax.set_xticks(range(len(df)))\n    ax.set_xticklabels([' | '.join(row) for row in df['Row']], rotation=45, ha='right')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_title('Duplicate Rows in CSV File')\n    plt.tight_layout()\n\n    return duplicate_rows, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check for negative ages and round down to nearest integer\n    df['age'] = df['age'].apply(lambda x: np.floor(x))\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative\")\n    df['age'] = df['age'].astype(int)\n\n    # Identify duplicate names\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n\n    # If no duplicates, return empty Counter and None for plot\n    if duplicates.empty:\n        return Counter(), None\n\n    # Record age distribution among duplicate names\n    age_distribution = Counter(duplicates['age'])\n\n    # Create histogram plot\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)\n\n    fig, ax = plt.subplots()\n    sns.histplot(data=duplicates, x='age', bins=bins, ax=ax)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution Among Duplicate Names')\n\n    return age_distribution, ax"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Identify and count duplicate values\n    value_counts = df['value'].value_counts()\n    duplicates = value_counts[value_counts > 1]\n    duplicate_counter = Counter(dict(duplicates))\n\n    # Extract values for plotting\n    values = df['value']\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=bins, density=True, alpha=0.6, color='green', edgecolor='black')\n\n    # Fit normal distribution\n    mu, std = norm.fit(values)\n\n    # Generate points for normal distribution curve\n    x = np.linspace(min(values), max(values), 100)\n    p = norm.pdf(x, mu, std)\n\n    # Plot normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set plot attributes\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    return duplicate_counter, ax"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    # Determine the number of columns based on the length of b\n    num_cols = min(len(b), len(COLUMNS))\n    \n    # Create a DataFrame with random values\n    data = np.random.rand(len(a), num_cols)\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_cols])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    \n    # Customize the plot\n    plt.title('Random Data Bar Chart')\n    plt.xlabel('Row Indices')\n    plt.ylabel('Values')\n    plt.legend(title='Columns')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    return ax\na = ['Row1', 'Row2', 'Row3', 'Row4']\nb = [1, 2, 3]"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    # Ensure 'month' is in the correct format\n    if data['month'].dtype == 'int64':\n        data['month'] = pd.to_datetime(data['month'], format='%m').dt.strftime('%B')\n    elif data['month'].dtype == 'object':\n        data['month'] = pd.to_datetime(data['month'], format='%B')\n    \n    # Sort the data by month\n    month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n                   'July', 'August', 'September', 'October', 'November', 'December']\n    data['month'] = pd.Categorical(data['month'], categories=month_order, ordered=True)\n    data = data.sort_values('month')\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot the bar chart\n    ax.bar(data['month'], data['value'])\n    \n    # Set the title and labels\n    year = datetime.now().year  # Assuming current year if not provided\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Adjust layout to prevent clipping of tick-labels\n    plt.tight_layout()\n    \n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the input string to a pandas Series of numeric values\n    values = pd.to_numeric(pd.Series(data.split()), errors='coerce')\n    \n    # Remove any non-numeric values\n    values = values.dropna()\n    \n    # Calculate the bins\n    bins = np.arange(values.min(), values.max() + 2) - 0.5\n    \n    # Create the figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot the histogram\n    ax.hist(values, bins=bins, edgecolor='black')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate a sine wave with noise\n    y_true = np.sin(x)\n    y_noisy = y_true + noise_level * np.random.randn(array_length)\n    \n    # Define the function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Fit the curve\n    initial_guess = [1, 1, 0, 0]\n    params, _ = curve_fit(sine_func, x, y_noisy, p0=initial_guess)\n    \n    # Generate the fitted curve\n    y_fit = sine_func(x, *params)\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(x, y_noisy, 'b.', alpha=0.5, label='Noisy data')\n    ax.plot(x, y_fit, 'r-', label='Fitted curve')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(csv_file):\n    try:\n        # Read and process the CSV file\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join([' '.join(row) for row in reader])\n        \n        # Normalize text to ASCII\n        normalized_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n        \n        # Count words\n        words = normalized_text.lower().split()\n        word_counts = Counter(words)\n        \n        # Get 10 most common words\n        top_10 = word_counts.most_common(10)\n        \n        # Create bar plot\n        words, counts = zip(*top_10)\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.bar(words, counts)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        ax.set_title('10 Most Common Words')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        \n        return ax, top_10\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    except IOError as e:\n        raise IOError(f\"An error occurred while reading the file {csv_file}: {str(e)}\")"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot histogram\n    n, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n    \n    # Calculate the PDF\n    x = np.linspace(data.min(), data.max(), 100)\n    pdf = stats.norm.pdf(x, loc=data.mean(), scale=data.std())\n    \n    # Plot the PDF\n    ax.plot(x, pdf, 'r-', lw=2, label='Normal PDF')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    \n    # Add legend\n    ax.legend()\n    \n    # Improve layout\n    plt.tight_layout()\n    \n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate random 8 bytes for filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n\n    # Generate random password and nonce for AES encryption\n    password = get_random_bytes(32)  # 256-bit key\n    nonce = get_random_bytes(16)\n\n    # Encrypt private key with AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    encrypted_private_key, tag = cipher.encrypt_and_digest(private_key.save_pkcs1())\n\n    # Save encrypted private key to file\n    with open(filename, \"wb\") as f:\n        f.write(nonce + tag + encrypted_private_key)\n\n    return public_key, filename, password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n\n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256-bit key\n\n    # Read the file content\n    with open(file_path, 'rb') as file:\n        file_content = file.read()\n\n    # Pad the file content\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_content) + padder.finalize()\n\n    # Generate a random IV\n    iv = os.urandom(16)\n\n    # Create AES cipher\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Encrypt the file content\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Save the encrypted file\n    encrypted_file_path = file_path + '.encrypted'\n    with open(encrypted_file_path, 'wb') as encrypted_file:\n        encrypted_file.write(iv + encrypted_data)\n\n    # Save the encrypted AES key\n    encrypted_key_path = file_path + '.key'\n    with open(encrypted_key_path, 'wb') as key_file:\n        key_file.write(encrypted_aes_key)\n\n    return public_key, encrypted_file_path, encrypted_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    # Validate URL\n    if not url or not isinstance(url, str):\n        raise ValueError(\"The provided URL is invalid or empty.\")\n\n    try:\n        # Fetch HTML content\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Network connectivity issue or server error: {e.reason}\")\n\n    # Parse HTML content\n    doc = pq(html_content)\n\n    # Extract anchor tags\n    anchors = doc('a')\n\n    # Prepare data for DataFrame\n    data = []\n    for anchor in anchors.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        data.append({'text': text, 'href': href})\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Add fetch time\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df['fetch_time'] = fetch_time\n\n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint, uniform\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define output file path\n    output_file = os.path.join(output_dir, 'sensor_data.csv')\n    \n    # Generate data\n    start_time = datetime.now()\n    data = []\n    \n    for i in range(hours * 60):  # Generate data every minute\n        current_time = start_time + timedelta(minutes=i)\n        temperature = round(uniform(20, 30), 1)  # Temperature between 20\u00b0C and 30\u00b0C\n        humidity = randint(30, 70)  # Humidity between 30% and 70%\n        pressure = randint(980, 1020)  # Pressure between 980 hPa and 1020 hPa\n        \n        data.append([\n            current_time.strftime('%Y-%m-%d %H:%M:%S'),\n            temperature,\n            humidity,\n            pressure\n        ])\n    \n    # Write data to CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time'] + SENSORS)  # Write header\n        writer.writerows(data)\n    \n    print(f\"Sensor data for {hours} hours has been saved to {output_file}\")\n    return hours"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate traffic data\n    data = []\n    start_time = datetime.now()\n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour)\n        row = {\n            'Time': current_time.strftime('%Y-%m-%d %H:%M:%S'),\n            **{vehicle: randint(0, 100) for vehicle in VEHICLE_TYPES}\n        }\n        data.append(row)\n\n    # Save data to CSV file\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Time'] + VEHICLE_TYPES\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n\n    # Read CSV file and create plot\n    df = pd.read_csv(csv_file_path)\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title(f'Traffic Data Over {hours} Hours')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Save plot as image\n    plot_file_path = os.path.join(output_dir, 'traffic_plot.png')\n    plt.savefig(plot_file_path)\n\n    return csv_file_path, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(BACKUP_DIR, exist_ok=True)\n\n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    for _ in range(hours):\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append([current_time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n        current_time += timedelta(hours=1)\n\n    # Create CSV file path\n    csv_filename = f'weather_data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n    csv_filepath = os.path.join(output_dir, csv_filename)\n\n    # Write data to CSV file\n    with open(csv_filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Backup the CSV file\n    backup_filepath = os.path.join(BACKUP_DIR, csv_filename)\n    shutil.copy2(csv_filepath, backup_filepath)\n\n    return csv_filepath"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    # Generate random match data\n    matches = []\n    for _ in range(goals):\n        for team in TEAMS:\n            match = {\n                'Team': team,\n                'Goals': randint(0, 5),\n                'Penalties': randint(0, penalties)\n            }\n            matches.append(match)\n    \n    # Create DataFrame\n    df = pd.DataFrame(matches)\n    df['Penalty Cost'] = df['Penalties'] * PENALTY_COST\n    \n    # Aggregate data by team\n    df_agg = df.groupby('Team').agg({\n        'Goals': 'sum',\n        'Penalty Cost': 'sum'\n    }).reset_index()\n    \n    # Create plots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Goals plot\n    sns.barplot(x='Team', y='Goals', data=df_agg, ax=ax1)\n    ax1.set_title('Total Goals by Team')\n    ax1.set_ylabel('Goals')\n    \n    # Penalty Cost plot\n    sns.barplot(x='Team', y='Penalty Cost', data=df_agg, ax=ax2)\n    ax2.set_title('Total Penalty Costs by Team')\n    ax2.set_ylabel('Penalty Cost ($)')\n    \n    plt.tight_layout()\n    \n    return df, [ax1, ax2]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    # Generate DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.astype(bool).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    non_zero_counts.plot(kind='bar', ax=ax)\n    \n    # Customize the plot\n    ax.set_title('Non-Zero Values in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    ax.tick_params(axis='x', rotation=0)\n    \n    # Add value labels on top of each bar\n    for i, v in enumerate(non_zero_counts):\n        ax.text(i, v, str(v), ha='center', va='bottom')\n    \n    # Adjust layout to prevent clipping of labels\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define courses\n    courses = ['Math', 'Science', 'History', 'English']\n    \n    # Generate random student IDs\n    student_ids = sample(range(1000, 9999), num_students)\n    \n    # Create DataFrame with random grades\n    data = {course: np.random.randint(0, 101, size=num_students) for course in courses}\n    df = pd.DataFrame(data, index=student_ids)\n    \n    # Calculate average grade in each course\n    average_grades = df.mean()\n    \n    # Calculate number of students with passing grade (>= 60)\n    passing_counts = (df >= 60).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    x = np.arange(len(courses))\n    width = 0.35\n    \n    ax.bar(x - width/2, average_grades, width, label='Average Grade', color='skyblue')\n    ax.bar(x + width/2, passing_counts, width, label='Passing Students', color='lightgreen')\n    \n    # Customize plot\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Counts / Grades')\n    ax.set_xticks(x)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    \n    # Add value labels on top of bars\n    for i, v in enumerate(average_grades):\n        ax.text(i - width/2, v, f'{v:.1f}', ha='center', va='bottom')\n    for i, v in enumerate(passing_counts):\n        ax.text(i + width/2, v, str(v), ha='center', va='bottom')\n    \n    # Adjust y-axis to accommodate labels\n    ax.set_ylim(0, max(max(average_grades), max(passing_counts)) * 1.1)\n    \n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    # Convert input to numpy array if it's not already\n    array = np.array(array)\n    \n    # Filter the array to get rows where the first column matches the target value\n    mask = array[:, 0] == target_value\n    filtered_array = array[mask]\n    \n    # Extract x and y data\n    x_data = np.arange(len(filtered_array))\n    y_data = filtered_array[:, 1]\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Perform curve fitting\n    popt, _ = optimize.curve_fit(exp_decay, x_data, y_data, p0=[max(y_data), 0.1, min(y_data)])\n    \n    # Create plot\n    fig, ax = plt.subplots()\n    \n    # Plot original data points\n    ax.scatter(x_data, y_data, color='blue', label='Data')\n    \n    # Plot fitted curve\n    x_fit = np.linspace(0, len(x_data) - 1, 100)\n    y_fit = exp_decay(x_fit, *popt)\n    ax.plot(x_fit, y_fit, color='red', label='Fitted Curve')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Exponential Decay Fit (Target Value: {target_value})')\n    ax.legend()\n    \n    return popt, ax"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    \"\"\"\n    Performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF).\n\n    Args:\n        texts (list of str): A list of text documents.\n        num_topics (int): The number of topics to extract.\n\n    Returns:\n        list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess the texts\n    def preprocess(text):\n        text = ALPHANUMERIC.sub(' ', text)  # Remove non-alphanumeric characters\n        text = text.lower()  # Convert to lowercase\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)  # Remove stopwords\n\n    processed_texts = [preprocess(text) for text in texts]\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer(max_features=5000)\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract topics\n    nmf_model = NMF(n_components=num_topics, random_state=42, init='nndsvd')\n    nmf_output = nmf_model.fit_transform(tfidf_matrix)\n\n    # Get the feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract the top words for each topic\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        top_words_idx = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n        top_words = [feature_names[i] for i in top_words_idx]\n        topics.append(top_words)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    # Download stopwords if not provided\n    if stopwords is None:\n        nltk.download('stopwords', quiet=True)\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n\n    def preprocess_text(text):\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize and remove stopwords\n        return [word for word in text.split() if word not in stopwords]\n\n    # Preprocess all texts\n    processed_texts = [preprocess_text(text) for text in texts]\n\n    # Train Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Create the directory if it doesn't exist\n    os.makedirs(path, exist_ok=True)\n    \n    # Create the processed subdirectory\n    processed_path = os.path.join(path, \"processed\")\n    os.makedirs(processed_path, exist_ok=True)\n    \n    # Initialize an empty list to store DataFrames\n    dfs = []\n    \n    # Get all JSON files in the directory and sort them alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n    \n    for filename in json_files:\n        file_path = os.path.join(path, filename)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        \n        # Convert to DataFrame\n        df = pd.DataFrame(data)\n        \n        # Add Source column\n        df['Source'] = filename\n        \n        # Append to the list of DataFrames\n        dfs.append(df)\n        \n        # Move the file to the processed subdirectory\n        shutil.move(file_path, os.path.join(processed_path, filename))\n    \n    # Combine all DataFrames\n    if dfs:\n        final_df = pd.concat(dfs, ignore_index=True)\n    else:\n        final_df = pd.DataFrame()\n    \n    return final_df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the directory and file path\n    dir_path = \"task_func_data\"\n    file_path = os.path.join(dir_path, \"Output.txt\")\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(dir_path, exist_ok=True)\n    \n    # Generate random sensor data\n    temperature = round(random.uniform(20.0, 30.0), 1)\n    humidity = round(random.uniform(40.0, 60.0), 1)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Write data to CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        # Write header if the file is empty\n        if os.stat(file_path).st_size == 0:\n            writer.writerow([\"Timestamp\", \"Temperature (\u00b0C)\", \"Humidity (%)\"])\n        writer.writerow([timestamp, temperature, humidity])\n    \n    # Store the file path before deletion\n    return_path = file_path\n    \n    # Delete the file after use\n    os.remove(file_path)\n    \n    return return_path"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a GET request to the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            html = response.read()\n    except urllib.error.URLError as e:\n        print(f\"Error fetching URL: {e}\")\n        return None\n\n    # Parse the HTML content\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find all tables in the HTML\n    tables = soup.find_all('table')\n\n    if not tables:\n        print(\"No tables found on the page.\")\n        return None\n\n    # Use the first table found\n    table = tables[0]\n\n    # Extract headers\n    headers = [th.text.strip() for th in table.find_all('th')]\n    if not headers:\n        headers = [td.text.strip() for td in table.find('tr').find_all('td')]\n\n    # Extract rows\n    rows = []\n    for tr in table.find_all('tr')[1:]:  # Skip the header row\n        row = [td.text.strip() for td in tr.find_all('td')]\n        if row:\n            rows.append(row)\n\n    # Write data to CSV file\n    try:\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(headers)\n            writer.writerows(rows)\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return None\n\n    print(f\"Data successfully scraped and saved to {CSV_FILE_PATH}\")\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validate input parameters\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a DataFrame.\")\n    \n    if data.empty:\n        raise ValueError(\"Data cannot be empty.\")\n    \n    if target_column not in data.columns:\n        raise ValueError(\"Target column must be a column of data.\")\n    \n    if not data.select_dtypes(include=[np.number]).columns.tolist() == data.columns.tolist():\n        raise ValueError(\"Data must contain only numeric values.\")\n    \n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be an integer.\")\n    \n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size must be between 0 and 1.\")\n\n    # Split the data into features (X) and target (y)\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Initialize and train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Calculate and return the model's score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \n    # Set random seed for reproducibility\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Combine and correct Latin characters in names\n    all_names = latin_names + other_names\n    corrected_names = [re.sub(r'[^\\x00-\\x7F]+', lambda x: codecs.decode(x.group(), 'utf-8'), name) for name in all_names]\n    \n    # Generate random data\n    ids = np.arange(1, 101)\n    names = np.random.choice(corrected_names, size=100)\n    \n    # Generate random dates of birth\n    date_range = pd.date_range(start=f\"{start_year}-01-01\", end=f\"{end_year}-12-31\")\n    dates_of_birth = np.random.choice(date_range, size=100)\n    \n    # Generate emails\n    emails = [f\"{name.lower()}{date.year}@{email_domain}\" for name, date in zip(names, dates_of_birth)]\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n    \n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Calculate mean and median for each key\n    results = defaultdict(lambda: {\"mean\": 0, \"median\": 0})\n    for key in data[0].keys():\n        values = [item[key] for item in data if isinstance(item.get(key), (int, float))]\n        if values:\n            results[key][\"mean\"] = np.mean(values)\n            results[key][\"median\"] = np.median(values)\n    \n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Field\", \"Mean\", \"Median\"])\n        for key, value in results.items():\n            writer.writerow([key, value[\"mean\"], value[\"median\"]])\n\n    return dict(results)"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    \n    # Ensure the target directory exists\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    for file_path in kwargs.values():\n        file_path = Path(file_path)\n        \n        # Check if the file exists and is not empty\n        if file_path.exists() and file_path.stat().st_size > 0:\n            # Copy the file to the target directory\n            dest_path = target_path / file_path.name\n            shutil.copy2(file_path, dest_path)\n            copied_files.append(str(dest_path))\n    \n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    dir_path = Path(directory)\n    \n    if not dir_path.is_dir():\n        raise ValueError(f\"The specified directory '{directory}' does not exist.\")\n    \n    for file_path in dir_path.glob('*.csv'):\n        match = re.match(pattern, file_path.name)\n        if match:\n            base_name = match.group(1)\n            new_filename = f\"{base_name}.csv\"\n            new_file_path = dir_path / new_filename\n            \n            with file_path.open('r', newline='') as source, new_file_path.open('w', newline='') as target:\n                reader = csv.reader(source)\n                writer = csv.writer(target)\n                for row in reader:\n                    writer.writerow(row)\n            \n            new_files.append(new_filename)\n    \n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    \n    for filename in os.listdir(directory):\n        match = re.match(pattern, filename)\n        if match:\n            # Extract the prefix part of the filename\n            prefix = match.group(1)\n            \n            # Create the full path for the zip file\n            zip_path = os.path.join(directory, filename)\n            \n            # Create the extraction directory\n            extract_dir = os.path.join(directory, prefix)\n            os.makedirs(extract_dir, exist_ok=True)\n            \n            # Extract the zip file\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n            \n            # Add the extraction directory to the list\n            extracted_dirs.append(extract_dir)\n    \n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport time\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n\n    if not files_to_archive:\n        raise FileNotFoundError(f\"No files found matching the pattern: {pattern}\")\n\n    # Create a unique archive filename using timestamp\n    timestamp = int(time.time())\n    archive_name = f\"archive_{timestamp}.zip\"\n    archive_file = os.path.join(ARCHIVE_DIR, archive_name)\n\n    # Archive the files\n    try:\n        subprocess.run(['zip', '-j', archive_file] + files_to_archive, check=True, capture_output=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to create archive: {e.stderr.decode()}\")\n\n    # Delete the original files\n    for file in files_to_archive:\n        try:\n            os.remove(file)\n        except OSError as e:\n            print(f\"Warning: Failed to delete {file}: {e}\")\n\n    return archive_file"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    count = Counter()\n\n    try:\n        with open(csv_file_path, 'r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                count['goals'] += int(row['goals'])\n                count['penalties'] += int(row['penalties'])\n    except FileNotFoundError:\n        print(f\"Error: File '{csv_file_path}' not found.\")\n        return count\n    except csv.Error as e:\n        print(f\"Error reading CSV file: {e}\")\n        return count\n\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Compile the regex pattern for efficiency\n    pattern = re.compile(file_pattern)\n    \n    # Initialize the count of moved files\n    moved_files_count = 0\n    \n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the file name matches the pattern\n        if pattern.match(filename):\n            source_path = os.path.join(source_dir, filename)\n            target_path = os.path.join(target_dir, filename)\n            \n            # Only move if it's a file (not a directory)\n            if os.path.isfile(source_path):\n                try:\n                    # Move the file to the target directory\n                    shutil.move(source_path, target_path)\n                    moved_files_count += 1\n                except (IOError, OSError) as e:\n                    print(f\"Error moving file {filename}: {e}\")\n    \n    # Return the count of moved files\n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    # Clean texts\n    clean_text1 = ALPHANUMERIC.sub(' ', text1).lower().strip()\n    clean_text2 = ALPHANUMERIC.sub(' ', text2).lower().strip()\n    \n    # Compute term frequencies\n    tf1 = Counter(clean_text1.split())\n    tf2 = Counter(clean_text2.split())\n    \n    # Get all unique terms\n    all_terms = set(tf1.keys()) | set(tf2.keys())\n    \n    # Create term frequency vectors\n    vec1 = np.array([tf1.get(term, 0) for term in all_terms])\n    vec2 = np.array([tf2.get(term, 0) for term in all_terms])\n    \n    # Compute cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n    cosine_similarity = dot_product / (norm1 * norm2) if norm1 * norm2 != 0 else 0.0\n    \n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(clean_text1, clean_text2)\n    \n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Type checking\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list\")\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Value checking\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"Input numbers must be non-negative\")\n    \n    # Handle empty list case\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations\n    all_permutations = list(permutations(numbers))\n    \n    # Calculate sum of factorials for each permutation\n    factorial_sums = [sum(math.factorial(num) for num in perm) for perm in all_permutations]\n    \n    return factorial_sums, all_permutations"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure the destination directory exists\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Convert EXTENSIONS to a list if it's a string\n    if isinstance(EXTENSIONS, str):\n        EXTENSIONS = [EXTENSIONS]\n\n    # Create a pattern to match files with the specified extensions\n    pattern = os.path.join(SOURCE_DIR, f\"*.{{{','.join(EXTENSIONS)}}}\")\n\n    # Find all files matching the pattern\n    for file_path in glob.glob(pattern):\n        file_name = os.path.basename(file_path)\n        dest_path = os.path.join(DEST_DIR, file_name)\n\n        try:\n            # Attempt to move the file\n            shutil.move(file_path, dest_path)\n            transferred_files.append(file_name)\n        except Exception as e:\n            warnings.warn(f\"Could not transfer '{file_name}': {str(e)}\")\n\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Convert input data to numpy arrays\n    items, counts, weights = zip(*data)\n    counts = np.array(counts).reshape(-1, 1)\n    weights = np.array(weights).reshape(-1, 1)\n\n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts, axis=0)\n\n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights)\n\n    # Create DataFrame with normalized data\n    df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts.flatten(),\n        'Normalized Weight': normalized_weights.flatten()\n    })\n\n    return df"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Determine the maximum length of tuples in the list\n    max_length = max(len(t) for t in data_list)\n\n    # Initialize lists to store numeric values for each position\n    numeric_values = [[] for _ in range(max_length)]\n\n    # Collect numeric values for each position\n    for tuple_item in data_list:\n        for i, value in enumerate(tuple_item):\n            if isinstance(value, (int, float)):\n                numeric_values[i].append(value)\n\n    # Calculate means for each position\n    means = [np.mean(values) if values else np.nan for values in numeric_values]\n\n    # Create DataFrame with mean values\n    df = pd.DataFrame({'Mean Value': means}, index=[f'Position {i}' for i in range(max_length)])\n\n    return df"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Columns '{col1}' and/or '{col2}' not found in the DataFrame.\")\n    \n    # Check if columns are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not data[col1].dtype == 'object':\n        raise TypeError(f\"Column '{col1}' is not categorical.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not data[col2].dtype == 'object':\n        raise TypeError(f\"Column '{col2}' is not categorical.\")\n    \n    # Check if columns have multiple categories\n    if data[col1].nunique() < 2:\n        raise ValueError(f\"Column '{col1}' does not have multiple categories.\")\n    if data[col2].nunique() < 2:\n        raise ValueError(f\"Column '{col2}' does not have multiple categories.\")\n    \n    # Construct the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if any category has less than 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating chi-square test assumptions.\")\n    \n    # Perform chi-square test\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n    \n    return p_value"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate dice rolls, calculate result frequencies, and create a histogram.\n\n    Args:\n    rolls (int): Number of dice rolls to simulate.\n    seed (int, optional): Random seed for reproducibility.\n\n    Returns:\n    tuple: (np.array of frequencies, matplotlib.Axes of histogram)\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.array([results.count(i) for i in NUMBERS])\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(0.5, 7.5, 1), align='mid', rwidth=0.8)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    ax.set_xlim(0.5, 6.5)\n    \n    return frequency, ax"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Full path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n    \n    # Create a new ZIP file\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                if file.endswith('_processed'):\n                    file_path = os.path.join(root, file)\n                    # Add file to ZIP, preserving relative path\n                    arcname = os.path.relpath(file_path, source_dir)\n                    zip_file.write(file_path, arcname)\n    \n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure 'timestamp' is in datetime format\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    \n    # Convert timestamp to numeric (seconds since epoch)\n    df['timestamp_numeric'] = df['timestamp'].astype(int) // 10**9\n    \n    # Prepare data for linear regression\n    X = df[['timestamp_numeric']]\n    y = df['close']\n    \n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    last_timestamp = df['timestamp_numeric'].max()\n    future_timestamps = np.array([last_timestamp + i * 24*60*60 for i in range(1, 8)])\n    future_X = future_timestamps.reshape(-1, 1)\n    \n    # Predict future prices\n    predicted_prices = model.predict(future_X)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    ax.plot(df['timestamp'], df['close'], label='Historical Prices', color='blue')\n    \n    # Plot predicted prices\n    future_dates = pd.to_datetime(future_timestamps, unit='s')\n    ax.plot(future_dates, predicted_prices, label='Predicted Prices', color='red', linestyle='--', marker='o')\n    \n    # Customize plot\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Price Prediction for Next 7 Days')\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    # Format x-axis dates\n    plt.gcf().autofmt_xdate()\n    \n    return predicted_prices.tolist(), ax"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    # Calculate Z-scores for the 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-score threshold\n    outliers = df[np.abs(z_scores) > z_threshold]\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot all data points\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Closing Prices')\n    \n    # Highlight outliers\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    # Add legend\n    ax.legend()\n    \n    # Return the outliers DataFrame and the plot object\n    return outliers, ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Args:\n    df (pd.DataFrame): DataFrame containing stock data with a 'Close' column for closing prices.\n    \n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot\n           and the second for the histogram.\n    \"\"\"\n    # Ensure 'Close' column exists\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column\")\n    \n    # Create a figure with two subplots side by side\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Create box plot\n    sns.boxplot(y=df['Close'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n    ax1.set_ylabel('Closing Price')\n    \n    # Create histogram\n    sns.histplot(df['Close'], kde=True, ax=ax2)\n    ax2.set_title('Histogram of Closing Prices')\n    ax2.set_xlabel('Closing Price')\n    ax2.set_ylabel('Frequency')\n    \n    # Adjust layout to prevent overlapping\n    plt.tight_layout()\n    \n    return (ax1, ax2)"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame has a datetime index\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    \n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(1,1,1))  # You can adjust the order as needed\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecast_values = forecast.tolist()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    # Plot historical data\n    ax.plot(df.index, df['Close'], label='Historical Prices', color='blue')\n    \n    # Plot forecast\n    forecast_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7)\n    ax.plot(forecast_dates, forecast_values, label='Forecasted Prices', color='red')\n    \n    # Fill the area between the last historical point and the forecast\n    ax.fill_between(forecast_dates, df['Close'].iloc[-1], forecast_values, color='red', alpha=0.3)\n    \n    # Customize the plot\n    ax.set_title('Stock Price Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    # Format x-axis to show dates nicely\n    plt.gcf().autofmt_xdate()\n    \n    return forecast_values, ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations\n    all_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    result = {combo: 0 for combo in all_combinations}\n    \n    # Count the occurrences of each two-letter combination in the word\n    word = word.lower()\n    for i in range(len(word) - 1):\n        combo = word[i:i+2]\n        if combo in result:\n            result[combo] += 1\n    \n    return result"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate sales data\n    np.random.seed(42)  # For reproducibility\n    sales_data = np.random.randint(100, 1000, size=(len(dates), len(categories)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=categories, index=dates)\n    df = df.reset_index().melt(id_vars='index', var_name='Category', value_name='Sales')\n    df.columns = ['Date', 'Category', 'Sales']\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], marker='o', label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate or use provided sales data\n    if sales_data is None:\n        sales_data = np.random.randint(100, 1000, size=periods)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'date': dates, 'sales': sales_data})\n    \n    # Add numeric time index for regression\n    df['time_index'] = np.arange(len(df))\n    \n    # Prepare data for linear regression\n    X = df['time_index'].values.reshape(-1, 1)\n    y = df['sales'].values\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future time indices\n    future_indices = np.arange(len(df), len(df) * 2).reshape(-1, 1)\n    \n    # Forecast future sales\n    forecasted_sales = model.predict(future_indices)\n    \n    return forecasted_sales"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    if n_tasks > len(sanitized_tasks):\n        n_tasks = len(sanitized_tasks)\n\n    selected_tasks = random.sample(sanitized_tasks, n_tasks)\n    assigned_employees = random.choices(employees, k=n_tasks)\n    due_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    assignments = pd.DataFrame({\n        'Task Name': selected_tasks,\n        'Assigned To': assigned_employees,\n        'Due Date': [due_date] * n_tasks\n    })\n\n    return assignments"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text or text.isspace():\n        raise ValueError(\"Input text is empty\")\n\n    # Convert to lowercase and replace spaces with underscores\n    modified_text = re.sub(r'\\s+', '_', text.lower())\n\n    # Split the text into words\n    words = modified_text.split('_')\n\n    # Count word frequencies while preserving order\n    word_freq = Counter()\n    unique_words = []\n    for word in words:\n        if word not in word_freq:\n            unique_words.append(word)\n        word_freq[word] += 1\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(unique_words, [word_freq[word] for word in unique_words])\n\n    # Customize the plot\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n\n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n\n    # Define the full path for the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n\n    # Create a zip file\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Valid file extensions\n        valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n        # Iterate through files in the source directory\n        for ext in valid_extensions:\n            for file_path in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                # Add file to the zip archive\n                zipf.write(file_path, os.path.basename(file_path))\n\n    # Return the full path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport csv\ndef task_func(source_directory: str, target_directory: str) -> int:\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    converted_count = 0\n    \n    for file_path in Path(source_directory).rglob('*'):\n        if file_path.is_file() and file_path.suffix.lower() in ['.txt', '.docx', '.xlsx', '.csv']:\n            try:\n                if file_path.suffix.lower() == '.txt':\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                elif file_path.suffix.lower() == '.docx':\n                    doc = docx.Document(file_path)\n                    content = '\\n'.join([para.text for para in doc.paragraphs])\n                elif file_path.suffix.lower() == '.xlsx':\n                    df = pd.read_excel(file_path)\n                    content = df.to_csv(index=False)\n                else:  # .csv\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                \n                df = pd.DataFrame({'Text': [content]})\n                output_path = Path(target_directory) / f\"{file_path.stem}.csv\"\n                df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)\n                converted_count += 1\n            except Exception as e:\n                print(f\"Error processing {file_path}: {str(e)}\")\n    \n    return converted_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check for NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    \n    # Check for non-numeric data types\n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    \n    # Compute cumulative sum\n    cumsum_df = df.cumsum()\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Normalize cumulative sum\n    normalized_cumsum = scaler.fit_transform(cumsum_df)\n    \n    # Create DataFrame with normalized cumulative sum\n    result_df = pd.DataFrame(normalized_cumsum, columns=df.columns, index=df.index)\n    \n    return result_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n\n    Args:\n        directory_path (str): Path to the directory to analyze.\n\n    Returns:\n        list of tuples: Each tuple contains (file name, file size in bytes,\n        creation time in ISO format, modification time in ISO format).\n\n    Raises:\n        ValueError: If the provided directory does not exist.\n\n    Notes:\n        - Only processes files in the given directory, not subdirectories.\n        - Times are reported in UTC.\n        - Creation time might be replaced by last metadata change time on some systems.\n    \"\"\"\n    directory = Path(directory_path)\n    \n    if not directory.is_dir():\n        raise ValueError(f\"The provided directory does not exist: {directory_path}\")\n    \n    result = []\n    \n    for entry in directory.iterdir():\n        if entry.is_file():\n            stats = entry.stat()\n            \n            # Get file size\n            file_size = stats.st_size\n            \n            # Get creation time (or metadata change time)\n            ctime = datetime.fromtimestamp(stats.st_ctime, tz=timezone.utc)\n            creation_time = ctime.isoformat()\n            \n            # Get modification time\n            mtime = datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc)\n            modification_time = mtime.isoformat()\n            \n            result.append((entry.name, file_size, creation_time, modification_time))\n    \n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"The JSON data is malformed.\")\n\n    # Check if data is empty\n    if not data:\n        raise ValueError(\"The JSON data is empty.\")\n\n    # Validate and process data\n    processed_data = []\n    for item in data:\n        if not isinstance(item, dict) or \"Country\" not in item or \"Population\" not in item:\n            raise ValueError(\"Invalid data structure in JSON.\")\n        \n        country = item[\"Country\"]\n        population = item[\"Population\"]\n        \n        if not isinstance(country, str):\n            raise ValueError(\"Country names must be strings.\")\n        \n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Population must be a non-negative number.\")\n        \n        processed_data.append({\n            \"Country\": country,\n            \"Population\": math.floor(population)\n        })\n\n    # Create DataFrame\n    df = pd.DataFrame(processed_data)\n\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate full file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(\"Unable to write the file to the specified directory.\")\n\n    return file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if the directory exists and is a directory\n    if not Path(dir_path).is_dir():\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n\n    # Check if valid predicates are provided\n    if not predicates:\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n\n    result = {}\n    for item in os.listdir(dir_path):\n        item_results = []\n        for predicate in unique_predicates:\n            if callable(predicate):\n                item_results.append(predicate(item))\n            elif isinstance(predicate, str):\n                item_results.append(bool(re.search(predicate, item)))\n            else:\n                raise TypeError(f\"Invalid predicate type: {type(predicate)}. Must be callable or string (regex).\")\n        result[item] = item_results\n\n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    \"\"\"\n    Convert a hexadecimal string to various encodings.\n\n    Args:\n        hex_string (str): A hexadecimal string to be converted.\n\n    Returns:\n        dict: A dictionary containing the input string encoded in various formats.\n    \"\"\"\n    try:\n        # Decode hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to UTF-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Prepare the result dictionary\n        result = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n            'URL': urllib.parse.quote(utf8_string),\n            'ROT13': codecs.encode(utf8_string, 'rot_13')\n        }\n        \n        # Handle ASCII encoding\n        try:\n            result['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n        except UnicodeEncodeError:\n            result['ASCII'] = 'Not representable in ASCII'\n        \n        return result\n    except Exception as e:\n        return {'error': str(e)}"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the file from the specified URL\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        calculated_checksum = md5_hash.hexdigest()\n        \n        # Validate the checksum\n        if calculated_checksum != EXPECTED_MD5_CHECKSUM:\n            os.remove(TARGET_TAR_FILE)\n            return False\n        \n        # Extract the contents of the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        \n        return True\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    Downloads a CSV file from a given URL, saves it to a specified path,\n    and counts the occurrences of each value in a particular column.\n\n    Args:\n        url (str): The URL of the CSV file to download.\n        column_name (str): The name of the column to count values from.\n        csv_file_path (str): The path to save the downloaded CSV file.\n\n    Returns:\n        dict: A dictionary mapping the values from the specified column to their\n              corresponding occurrence counts.\n\n    Raises:\n        ValueError: If the specified column_name does not exist in the CSV file.\n        Exception: If there's an error downloading the CSV file.\n    \"\"\"\n    try:\n        # Download the CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise Exception(f\"Error downloading CSV file: {e}\")\n\n    try:\n        # Count occurrences of values in the specified column\n        value_counts = collections.defaultdict(int)\n        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            for row in reader:\n                value_counts[row[column_name]] += 1\n        return dict(value_counts)\n    except ValueError as ve:\n        raise ve\n    except Exception as e:\n        raise Exception(f\"Error processing CSV file: {e}\")\n    finally:\n        # Delete the downloaded CSV file\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Invalid URL or XML file cannot be fetched: {e}\")\n\n    try:\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f\"Invalid XML syntax: {e}\")\n\n    # Check if the XML structure conforms to the expected format\n    if root.tag != 'root' or not root.findall('item'):\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Extract data from 'item' elements\n    data = []\n    for item in root.findall('item'):\n        item_data = {child.tag: child.text for child in item}\n        data.append(item_data)\n\n    # Convert the extracted data to a Pandas DataFrame\n    df = pd.DataFrame(data)\n    return df"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the specified URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            text = response.read().decode('utf-8')\n    except Exception as e:\n        raise Exception(f\"Failed to download or decode the file: {e}\")\n\n    # Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_counts = Counter(words)\n\n    # Get the ten most frequently occurring words\n    top_ten = word_counts.most_common(10)\n\n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    words, counts = zip(*top_ten)\n    ax.bar(words, counts)\n\n    # Customize the chart\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Ten Most Frequently Occurring Words')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n\n        # Check if the content type is 'application/zip'\n        content_type = response.headers.get('Content-Type', '').lower()\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Save the file\n        file_name = os.path.join(download_path, 'downloaded.zip')\n        with open(file_name, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Extract the ZIP file\n        try:\n            with ZipFile(file_name, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            os.remove(file_name)\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Clean up the downloaded zip file\n        os.remove(file_name)\n\n        return download_path\n\n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        # Send GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raises HTTPError for bad responses\n\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find the table with the specified ID\n        table = soup.find('table', id=table_id)\n        if not table:\n            raise ValueError(\"Table with the specified ID not found.\")\n\n        # Check if the table has any rows\n        rows = table.find_all('tr')\n        if not rows:\n            return pd.DataFrame()  # Return empty DataFrame if no rows\n\n        # Extract table headers\n        headers = [th.text.strip() for th in rows[0].find_all(['th', 'td'])]\n\n        # Extract table data\n        data = []\n        for row in rows[1:]:\n            data.append([td.text.strip() for td in row.find_all('td')])\n\n        # Create DataFrame\n        df = pd.DataFrame(data, columns=headers)\n        return df\n\n    except requests.exceptions.HTTPError as http_err:\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {req_err}\")"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Ensure directories exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n    zip_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the zip file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        with open(zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n    except requests.RequestException as e:\n        return f\"Error: Network-related exception - {str(e)}\", []\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            extracted_files = [f.name for f in ZIP_DIR.iterdir() if f.is_file()]\n        return \"Success\", extracted_files\n    except (zipfile.BadZipFile, OSError) as e:\n        return f\"Error: File-related exception - {str(e)}\", []"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Fetch the webpage content\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all hyperlinks and convert to absolute URLs\n    links = set()\n    for a_tag in soup.find_all('a', href=True):\n        href = a_tag['href']\n        absolute_url = urljoin(base_url, href)\n        links.add(absolute_url)\n\n    # Save unique absolute URLs to CSV file\n    try:\n        with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['Absolute URL'])  # Header\n            for link in links:\n                writer.writerow([link])\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return 0\n\n    # Return the number of unique absolute links\n    return len(links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the webpage content\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Network error: {e}\")\n\n    # Parse the HTML content\n    tree = html.fromstring(response.content)\n    \n    # Find all tables in the HTML\n    tables = tree.xpath('//table')\n    \n    if not tables:\n        return 0  # No tables found\n    \n    # Convert the first table to a pandas DataFrame\n    try:\n        df = pd.read_html(html.tostring(tables[0]))[0]\n    except ValueError:\n        return 0  # Table is empty or couldn't be parsed\n    \n    if df.empty:\n        return 0  # DataFrame is empty\n    \n    try:\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        \n        # Replace the existing table with new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Commit changes and close the connection\n        conn.commit()\n        conn.close()\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database error: {e}\")\n    \n    # Return the number of rows in the parsed HTML table\n    return len(df)"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    def convert_encoding(text):\n        try:\n            return text.encode(from_encoding).decode(to_encoding)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(f\"Error during text encoding conversion: {str(e)}\")\n\n    try:\n        with Image.open(filename) as img:\n            # Try OCR extraction\n            ocr_text = pytesseract.image_to_string(img)\n            if ocr_text.strip():\n                return convert_encoding(ocr_text)\n\n            # Fallback to image comment\n            comment = img.info.get('comment', '')\n            if comment:\n                if isinstance(comment, bytes):\n                    comment = comment.decode('utf-8', errors='ignore')\n                return convert_encoding(comment)\n\n    except IOError as e:\n        raise ValueError(f\"Error opening or processing the image file: {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Unexpected error during image processing: {str(e)}\")\n\n    # If both OCR and comment extraction fail, return an empty string\n    return \"\""}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Send HTTP GET request with 5-second timeout\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n\n        # Retrieve response content in raw bytes\n        content = response.content\n\n        # Handle empty content\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding is None:\n            detected = chardet.detect(content)\n            from_encoding = detected['encoding']\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode and re-encode content\n        decoded_content = content.decode(from_encoding)\n        re_encoded_content = decoded_content.encode(to_encoding)\n\n        # Parse JSON\n        json_data = json.loads(re_encoded_content.decode(to_encoding))\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request error: {e}\")\n        return {}\n    except json.JSONDecodeError as e:\n        print(f\"JSON decoding error: {e}\")\n        return {}\n    except ValueError as e:\n        print(f\"Value error: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return {}"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Check if the specified column exists\n        if column_name not in df.columns:\n            raise ValueError(f\"The column '{column_name}' is not present in the CSV file.\")\n\n        # Convert the date column to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n        # Filter rows based on the current date\n        current_date = datetime.now().date()\n        df = df[df[column_name].dt.date <= current_date]\n\n        # Sort the resulting data by the date column\n        df = df.sort_values(by=column_name)\n\n        return df\n\n    except EmptyDataError:\n        # Return an empty DataFrame if the CSV file is empty\n        return pd.DataFrame()\n\n    except pd.errors.ParserError:\n        raise ValueError(\"The CSV file is not properly formatted.\")"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Create SSL context and wrap the client socket\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8').strip()\n\n        # Check if the file exists\n        if not os.path.isfile(file_path):\n            ssl_socket.sendall(b'File not found')\n            return 'File not found'\n\n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            for chunk in iter(lambda: file.read(4096), b''):\n                sha256_hash.update(chunk)\n        \n        file_hash = sha256_hash.hexdigest()\n\n        # Send the hash back to the client\n        ssl_socket.sendall(file_hash.encode('utf-8'))\n\n        return file_hash\n\n    except ssl.SSLError as ssl_err:\n        error_msg = f\"SSL Error: {str(ssl_err)}\"\n        return error_msg\n    except IOError as io_err:\n        error_msg = f\"I/O Error: {str(io_err)}\"\n        return error_msg\n    except Exception as e:\n        error_msg = f\"An unexpected error occurred: {str(e)}\"\n        return error_msg"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n    server.bind((server_address, server_port))\n    server.listen(5)\n\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    print(f\"Server started on {server_address}:{server_port}\")\n\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n        for s in readable:\n            if s is server:\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n                print(f\"New connection from {client_address}\")\n            else:\n                try:\n                    data = s.recv(buffer_size)\n                    if data:\n                        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                        response = f\"{data.decode().strip()} (Server time: {current_time})\"\n                        message_queues[s].put(response)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n                        print(f\"Connection closed by client\")\n                except ConnectionResetError:\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n                    print(f\"Connection reset by client\")\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                try:\n                    s.send(next_msg.encode())\n                except ConnectionResetError:\n                    print(f\"Failed to send message, connection reset by client\")\n                    outputs.remove(s)\n                    if s in inputs:\n                        inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in exceptional:\n            print(f\"Handling exceptional condition for {s.getpeername()}\")\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    server.close()\n    run_duration = (datetime.now() - start_time).total_seconds()\n    return f\"Server operated for {run_duration:.2f} seconds on {server_address}:{server_port}\""}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    try:\n        # Receive message from client socket\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        \n        # Get sender's email, recipient's email, and sender's email password\n        sender_email = input(\"Enter sender's email: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        sender_password = getpass.getpass(\"Enter sender's email password: \")\n        \n        # Create email message\n        email = EmailMessage()\n        email.set_content(message)\n        email['Subject'] = 'Message from Client Socket'\n        email['From'] = sender_email\n        email['To'] = recipient_email\n        \n        # Send email via SMTP server\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()\n            server.login(sender_email, sender_password)\n            server.send_message(email)\n        \n        print(\"Email sent successfully.\")\n    except smtplib.SMTPAuthenticationError:\n        print(\"SMTP authentication failed. Please check your email and password.\")\n    except smtplib.SMTPException as e:\n        print(f\"An error occurred while sending the email: {str(e)}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n    finally:\n        client_socket.close()"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path, header=None)\n        \n        # Check if the CSV has a header 'Text', if not assume the first column is the text data\n        if df.columns[0] != 'Text':\n            df.columns = ['Text']\n        \n        # Initialize CountVectorizer with stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the text data\n        word_count_matrix = vectorizer.fit_transform(df['Text'])\n        \n        # Get word frequencies\n        word_freq = word_count_matrix.sum(axis=0).A1\n        \n        # Get words corresponding to frequencies\n        words = vectorizer.get_feature_names_out()\n        \n        # Create a dictionary of word frequencies\n        word_freq_dict = dict(zip(words, word_freq))\n        \n        # Sort words by frequency and get top 10\n        top_10_words = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n        \n        # Separate words and frequencies for plotting\n        words, frequencies = zip(*top_10_words)\n        \n        # Create the histogram\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path)\n            plt.close(fig)\n            return None\n        else:\n            return ax\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path '{file_path}' was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Predefined lists\n    default_animals = ['dog', 'cat', 'rabbit', 'hamster']\n    default_foods = ['kibble', 'carrot', 'apple', 'lettuce']\n\n    # Handle special cases\n    if animals is None or len(animals) == 0:\n        animals = default_animals\n    if foods is None or len(foods) == 0:\n        foods = default_foods\n\n    # If both lists are empty after handling, return an empty DataFrame\n    if len(animals) == 0 and len(foods) == 0:\n        return pd.DataFrame()\n\n    # Generate all possible combinations\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle the combinations randomly\n    np.random.shuffle(combinations)\n\n    # Create a DataFrame\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    \n    # Create the 'animal:food' format\n    df['Combination'] = df['Animal'] + ':' + df['Food']\n\n    # Pivot the DataFrame to have animals as rows and foods as columns\n    pivot_df = df.pivot(index='Animal', columns='Food', values='Combination')\n\n    # Fill NaN values with empty strings for better readability\n    pivot_df = pivot_df.fillna('')\n\n    return pivot_df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n\n    tz = pytz.timezone(timezone)\n    \n    # Convert timestamps to datetime objects in the specified timezone\n    timestamps = [datetime.fromisoformat(ts).astimezone(tz) for ts in time_strings]\n    \n    # Calculate absolute time differences in seconds between consecutive pairs\n    time_diffs = [abs((t2 - t1).total_seconds()) for t1, t2 in zip(timestamps[:-1], timestamps[1:])]\n    \n    # If there are no time differences, return 0.0\n    if not time_diffs:\n        return 0.0\n    \n    # Calculate and return the mean time difference\n    return np.mean(time_diffs)"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text and remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split into words\n    words = text.split()\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10 = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10)\n    \n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts)\n    \n    # Customize the plot\n    ax.set_xlabel('Words', fontsize=12)\n    ax.set_ylabel('Frequency', fontsize=12)\n    ax.set_title('Top 10 Most Common Words', fontsize=14)\n    ax.tick_params(axis='x', rotation=45)\n    \n    # Adjust layout to prevent cutting off labels\n    plt.tight_layout()\n    \n    return top_10, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs\n    url_pattern = re.compile(r'https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n    \n    # Find the first URL in the string\n    match = url_pattern.search(myString)\n    \n    if not match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = match.group(0)\n    \n    try:\n        # Attempt to fetch the webpage\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Find the title tag\n        title_tag = soup.find('title')\n        \n        if title_tag:\n            return title_tag.string.strip()\n        else:\n            return \"No title tag found in the webpage.\"\n    \n    except requests.RequestException as e:\n        return f\"Unable to fetch the content of the URL: {url}\""}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string provided\")\n\n    # Find the URL associated with the specified key\n    url = data.get(unknown_key)\n    if not url:\n        raise KeyError(f\"Key '{unknown_key}' not found in the JSON data\")\n\n    # Download the file from the URL\n    try:\n        response = requests.get(url, timeout=30)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise RuntimeError(f\"Failed to download file from {url}: {str(e)}\")\n\n    # Generate timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f'{unknown_key}_{timestamp}.txt'\n\n    # Determine save directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    \n    # Ensure the save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Construct full file path\n    file_path = os.path.join(save_dir, filename)\n\n    # Save the file\n    try:\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n    except IOError as e:\n        raise IOError(f\"Failed to save file: {str(e)}\")\n\n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern for phone numbers\n    phone_pattern = re.compile(r'\\+?1?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}')\n    \n    # Fetch content from URL or local file\n    if url.startswith(('http://', 'https://')):\n        response = requests.get(url)\n        content = response.text\n    else:\n        with open(url, 'r', encoding='utf-8') as file:\n            content = file.read()\n    \n    # Parse HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    \n    # Extract text from HTML\n    text = soup.get_text()\n    \n    # Find all phone numbers in the text\n    phone_numbers = phone_pattern.findall(text)\n    \n    # Clean and format phone numbers\n    cleaned_numbers = []\n    for number in phone_numbers:\n        cleaned = re.sub(r'[^\\d+]', '', number)\n        if len(cleaned) >= 10:\n            cleaned_numbers.append(cleaned)\n    \n    # Remove duplicates while preserving order\n    unique_numbers = list(dict.fromkeys(cleaned_numbers))\n    \n    # Save phone numbers to JSON file\n    with open(output_path, 'w', encoding='utf-8') as json_file:\n        json.dump(unique_numbers, json_file, indent=2)\n    \n    return unique_numbers"}
