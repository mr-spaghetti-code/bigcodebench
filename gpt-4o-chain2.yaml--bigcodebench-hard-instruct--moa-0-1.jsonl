{"task_id": "BigCodeBench/13", "solution": "```python\nimport subprocess\nimport ftplib\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    # List to store filenames that were attempted to be downloaded\n    filenames = []\n\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    try:\n        # List files in the specified directory\n        filenames = ftp.nlst()\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # Close the FTP connection\n    ftp.quit()\n\n    # Download each file using wget in a subprocess\n    for filename in filenames:\n        file_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        try:\n            subprocess.run(['wget', file_url], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {filename} from {ftp_server}: {str(e)}\")\n\n    return filenames\n```"}
{"task_id": "BigCodeBench/15", "solution": "```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Ensure the output directory exists\n    os.makedirs(output_dir_path, exist_ok=True)\n    \n    output_files = []\n    \n    # Read commands from CSV file\n    with open(commands_file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        \n        for index, row in enumerate(reader):\n            if not row:\n                continue  # Skip empty rows\n            \n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n            output_files.append(output_file_path)\n            \n            try:\n                # Execute the command\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                output = result.stdout.decode('utf-8')\n                \n            except subprocess.CalledProcessError as e:\n                # Capture the error message and exit code\n                output = f\"Command failed with exit code {e.returncode}\\nError message: {e.stderr.decode('utf-8')}\"\n            \n            # Write the output to the respective file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(output)\n    \n    return output_files\n\n# Example usage:\n# output_files = task_func('commands.csv', 'output_directory')\n# print(output_files)\n```"}
{"task_id": "BigCodeBench/17", "solution": "```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    try:\n        # Check if the process is running\n        process_running = False\n        for proc in psutil.process_iter(['pid', 'name']):\n            if proc.info['name'] == process_name:\n                process_running = True\n                proc_pid = proc.info['pid']\n                break\n        \n        if process_running:\n            # Process is found, terminate it\n            process = psutil.Process(proc_pid)\n            process.terminate()\n            process.wait(timeout=5)  # Wait for the process to terminate\n            # Start the process again\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n        else:\n            # Process not found, start it\n            subprocess.Popen([process_name])\n            return f\"Process not found. Starting {process_name}.\"\n    except psutil.NoSuchProcess:\n        return f\"Process {process_name} was not running and could not be terminated.\"\n    except (psutil.AccessDenied, psutil.TimeoutExpired) as e:\n        return f\"Could not terminate {process_name}: {e}\"\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n\n# Example usage\nif __name__ == \"__main__\":\n    process_name = \"example_process\"\n    result = task_func(process_name)\n    print(result)\n```"}
{"task_id": "BigCodeBench/19", "solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n    \n    # Get the list of all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, '*')) if os.path.isfile(f)]\n    \n    # Check if there are any files to zip\n    if not files:\n        return None\n    \n    # Define the path for the zip file\n    zip_path = os.path.join(directory, 'files.zip')\n    \n    # Create the zip file and add files to it\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    return zip_path\n```"}
{"task_id": "BigCodeBench/34", "solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    cleaned_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove extra whitespace\n    cleaned_text = cleaned_text.strip()\n    \n    # Check if there are any words left after cleaning\n    if not cleaned_text:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud\n\n# Example usage:\n# text = \"Check out this link: https://example.com and also visit http://anotherlink.com for more info.\"\n# wordcloud = task_func(text)\n```"}
{"task_id": "BigCodeBench/37", "solution": "```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Train Random Forest Classifier\n    rf = RandomForestClassifier()\n    rf.fit(X, y)\n\n    # Get feature importances\n    importances = rf.feature_importances_\n    feature_names = X.columns\n\n    # Create a DataFrame for feature importances\n    feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\n    # Plot feature importances\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(x='Importance', y='Feature', data=feature_importances)\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n\n    plt.show()\n\n    return rf, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example DataFrame\n    data = {\n        'feature1': [1, 2, 3, 4, 5],\n        'feature2': [5, 4, 3, 2, 1],\n        'feature3': [2, 3, 4, 5, 6],\n        'target': [0, 1, 0, 1, 0]\n    }\n    df = pd.DataFrame(data)\n\n    # Call the function\n    rf_model, ax = task_func(df, 'target')\n```"}
{"task_id": "BigCodeBench/82", "solution": "```python\nfrom flask import Flask, render_template, redirect, url_for, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\nclass User(UserMixin):\n    def __init__(self, id, username, password_hash):\n        self.id = id\n        self.username = username\n        self.password_hash = password_hash\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\nusers = {\n    1: User(1, \"testuser\", generate_password_hash(\"password123\")),\n}\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return users.get(int(user_id))\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n    \n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            user = next((u for u in users.values() if u.username == username), None)\n            if user and user.check_password(password):\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                flash('Invalid username or password', 'danger')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello {current_user.username}! This is a protected page.'\n\n    return app\n\nif __name__ == \"__main__\":\n    secret_key = 'your-secret-key'\n    template_folder = 'templates'\n    app = task_func(secret_key, template_folder)\n    app.run(debug=True)\n```"}
{"task_id": "BigCodeBench/89", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    \n    Parameters:\n    data (np.ndarray): The dataset as a numpy array.\n    column (int): The index of the column to analyze for outliers.\n    outlier_z_score (float): The Z-score threshold for identifying outliers.\n    \n    Returns:\n    tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    \n    # Extract the specified column\n    column_data = data[:, column].reshape(-1, 1)\n    \n    # Standardize the column\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data)\n    \n    # Calculate Z-scores directly on the column data\n    z_scores = np.abs(stats.zscore(column_data))\n    \n    # Identify outliers\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers\n    data_without_outliers = np.delete(data, outlier_indices, axis=0)\n    \n    # Visualize the data\n    plt.figure(figsize=(12, 6))\n    \n    # Scatter plot of original data\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(data)), data[:, column], color='blue', label='Original Data')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    # Scatter plot of data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(np.delete(np.arange(len(data)), outlier_indices), data_without_outliers[:, column], color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_without_outliers, outlier_indices\n\n# Example usage:\n# Assuming `data` is a numpy array with your dataset and you want to check the first column for outliers with a Z-score threshold of 3\n# original_data, cleaned_data, outliers = task_func(data, 0, 3)\n```"}
{"task_id": "BigCodeBench/92", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef task_func(data, n_clusters=3):\n    # Validate input types and values\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input 'data' must be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Input 'n_clusters' must be an integer greater than 1.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', marker='o')\n    \n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100, label='Centroids')\n    \n    # Add legend and title\n    ax.legend()\n    ax.set_title('K-means Clustering')\n    \n    # Show plot\n    plt.show()\n    \n    return (np.array(cluster_labels), ax)\n\n# Example usage (assuming 'data' is a pandas DataFrame with the dataset):\n# data = pd.DataFrame({\n#     'x': [1, 2, 3, 4, 5, 6],\n#     'y': [1, 2, 3, 4, 5, 6]\n# })\n\n# cluster_labels, ax = task_func(data, n_clusters=2)\n# print(cluster_labels)\n```"}
{"task_id": "BigCodeBench/93", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform PCA on a dataset and return the transformed data and scatter plot.\n\n    Parameters:\n    - data (np.ndarray or pd.DataFrame): The input data to be transformed.\n    - n_components (int): Number of principal components to compute.\n\n    Returns:\n    - transformed_df (pd.DataFrame): The DataFrame containing the principal components.\n    - ax (matplotlib.axes.Axes): The Axes object containing the scatter plot.\n\n    Raises:\n    - ValueError: If n_components is not a positive integer.\n    \"\"\"\n    \n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Ensure input data is in the correct format (either a NumPy array or a pandas DataFrame)\n    if not isinstance(data, (np.ndarray, pd.DataFrame)):\n        raise ValueError(\"Input data must be a NumPy array or pandas DataFrame\")\n    \n    # Convert DataFrame to NumPy array if necessary\n    if isinstance(data, pd.DataFrame):\n        data = data.values\n    \n    # Check if data is empty\n    if data.size == 0:\n        raise ValueError(\"Input data cannot be empty\")\n\n    # Check if n_components exceeds the number of features\n    n_features = data.shape[1]\n    if n_components > n_features:\n        raise ValueError(f\"n_components cannot exceed the number of features ({n_features})\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert the transformed data into a DataFrame\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    transformed_df = pd.DataFrame(transformed_data, columns=columns)\n    \n    # Generate a scatter plot\n    fig, ax = plt.subplots()\n    if n_components == 1:\n        ax.scatter(transformed_df.iloc[:, 0], np.zeros_like(transformed_df.iloc[:, 0]))\n        ax.set_xlabel('PC1')\n    else:\n        ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n    \n    ax.set_title('PCA Transformed Data')\n    plt.grid(True)\n    \n    # Return the DataFrame and Axes object\n    return transformed_df, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some example data\n    np.random.seed(0)\n    example_data = np.random.randn(100, 5)\n    \n    try:\n        df, ax = task_func(example_data, n_components=2)\n        print(df.head())\n        plt.show()\n    except ValueError as e:\n        print(e)\n```"}
{"task_id": "BigCodeBench/99", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    Generates a seaborn pair plot of the iris dataset using Arial font.\n    \n    Returns:\n    plt.Figure: A matplotlib Figure object containing the seaborn pair plot.\n    \"\"\"\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n    \n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n    \n    # Create the pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species')\n    \n    # Set title and labels\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16)\n    pair_plot.fig.subplots_adjust(top=0.95)  # Adjust the title position\n    \n    # Set axis labels for each subplot\n    for ax in pair_plot.axes.flatten():\n        ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n        ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n    \n    # Return the figure\n    return pair_plot.fig\n\n# When running the function, the plot will be displayed\nfig = task_func()\nplt.show()\n```"}
{"task_id": "BigCodeBench/100", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days with reproducibility controlled by an optional seed parameter.\n\n    Parameters:\n    seed (int): Random seed for reproducibility. Default is 42.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object containing a line plot of the time series data.\n    The plot will have 'Date' as the x-axis label, 'Value' as the y-axis label,\n    and 'Random Time Series Data' as the title.\n    \n    Raises:\n    ValueError: If there is an issue generating the data or plot.\n    \"\"\"\n    try:\n        # Setting the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=29)\n        dates = pd.date_range(start=start_date, end=end_date)\n\n        # Generate random time series data\n        data = [random.random() for _ in range(len(dates))]\n\n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': data})\n\n        # Plotting the data\n        plt.figure(figsize=(10, 5))\n        plt.plot(df['Date'], df['Value'], marker='o', linestyle='-')\n\n        # Styling the plot\n        plt.xlabel('Date', fontsize=12, fontname='Arial')\n        plt.ylabel('Value', fontsize=12, fontname='Arial')\n        plt.title('Random Time Series Data', fontsize=14, fontname='Arial')\n        plt.xticks(fontsize=10, fontname='Arial', rotation=45)\n        plt.yticks(fontsize=10, fontname='Arial')\n\n        # Ensuring the plot layout is tight\n        plt.tight_layout()\n\n        # Getting the Axes object\n        ax = plt.gca()\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the data or plot: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    ax = task_func()\n    plt.show()\n```"}
{"task_id": "BigCodeBench/101", "solution": "Here is the complete self-contained code that meets your requirements:\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load the dataset\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        column_names = [\n            'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'\n        ]\n        df = pd.DataFrame(data, columns=column_names)\n\n        # Compute the correlation matrix\n        corr = df.corr()\n\n        # Create a heatmap\n        plt.figure(figsize=(12, 8))\n        ax = sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n        plt.title('Correlation Heatmap of Boston Housing Dataset')\n\n        # Save the heatmap to a file if a path is provided\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n\n        plt.show()\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred in generating or saving the plot: {e}\")\n\n# Example usage:\n# ax = task_func(save_path=\"boston_correlation_heatmap.png\")\n```\n\nThis function includes the following steps:\n1. Load the Boston Housing dataset.\n2. Combine the data from multiple lines into a single DataFrame.\n3. Compute the correlation matrix.\n4. Plot the heatmap using Seaborn.\n5. Save the heatmap to a specified file if a path is provided.\n6. Raise a `ValueError` if any error occurs during the process.\n\nYou can call this function with the `save_path` parameter to save the heatmap. For example:\n```python\nax = task_func(save_path=\"boston_correlation_heatmap.png\")\n```"}
{"task_id": "BigCodeBench/108", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Check if 'value' column exists in DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Check if 'value' column contains valid data types\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must contain numeric data types.\")\n    \n    # Validate frequency string\n    valid_freqs = ['B', 'C', 'D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S', 'L', 'U', 'N']\n    if freq not in valid_freqs:\n        raise ValueError(\"Invalid frequency string. Must be one of: {}\".format(\", \".join(valid_freqs)))\n    \n    # Validate decomposition_model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"decomposition_model must be either 'additive' or 'multiplicative'.\")\n    \n    # Ensure the index is a datetime index\n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        raise ValueError(\"DataFrame index must be a datetime index.\")\n    \n    # Decompose the time series\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.Timedelta(freq).days)\n    \n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\n    decomposition_result.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    decomposition_result.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    decomposition_result.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    decomposition_result.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residual')\n    \n    plt.tight_layout()\n    \n    return decomposition_result, axes\n\n# Example usage:\n# df = pd.DataFrame({'value': [your_time_series_data]}, index=pd.date_range(start='YYYY-MM-DD', periods=len(your_time_series_data), freq='D'))\n# result, axes = task_func(df, freq='D', decomposition_model='multiplicative')\n# plt.show()\n```"}
{"task_id": "BigCodeBench/120", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate input date types\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances\")\n    \n    # Validate date range\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date\")\n    \n    # Set the seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days in the date range\n    num_days = (end_date - start_date).days + 1\n    \n    # Generate a random date for each day in the range\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n    \n    # Create a pandas Series from the list of random dates\n    date_series = pd.Series(random_dates)\n    \n    return date_series\n\n# Example usage:\nif __name__ == \"__main__\":\n    start_date = datetime(2020, 1, 1)\n    end_date = datetime(2020, 12, 31)\n    random_dates_series = task_func(start_date, end_date)\n    print(random_dates_series)\n```"}
{"task_id": "BigCodeBench/123", "solution": "```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if 'my_list' is indeed a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The 'my_list' parameter must be a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append(12)\n    \n    # Calculate the sum of numbers in the list\n    total_files = sum(my_list)\n    \n    # Get all files in the directory with the specified extension\n    file_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(file_pattern)\n    \n    # Check if any files are found in the directory\n    if not files:\n        raise FileNotFoundError(f\"No files found in the directory: {file_dir}\")\n    \n    # Select the number of files equal to the sum of the list elements\n    selected_files = files[:total_files]\n    \n    # Read and concatenate files into a single DataFrame\n    df_list = [pd.read_csv(file) for file in selected_files]\n    concatenated_df = pd.concat(df_list, ignore_index=True)\n    \n    return concatenated_df\n\n# Example usage:\n# Assuming you have some CSV files in './data_files/' directory\n# result_df = task_func([1, 2, 3])\n# print(result_df)\n```"}
{"task_id": "BigCodeBench/124", "solution": "```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Check if 'my_list' is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    \n    # Check if all elements in 'my_list' are numeric (int or float)\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in my_list must be numeric (int or float).\")\n    \n    # Append 12 to 'my_list'\n    my_list.append(12)\n    \n    # Calculate the sum of elements in 'my_list'\n    sum_elements = sum(my_list)\n    \n    # Determine the size of the random numbers list\n    list_size = min(sum_elements, size)\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Start time measurement\n    start_time = time.time()\n    \n    # Generate the list of random integers\n    random_numbers = [randint(1, 100) for _ in range(list_size)]\n    \n    # Stop time measurement\n    end_time = time.time()\n    \n    # Calculate the time taken\n    time_taken = end_time - start_time\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Random Numbers')\n    \n    return (time_taken, ax)\n\n# Example usage\nif __name__ == \"__main__\":\n    my_list = [5, 7, 3]\n    time_taken, ax = task_func(my_list)\n    print(f\"Time taken: {time_taken} seconds\")\n    plt.show()\n```"}
{"task_id": "BigCodeBench/129", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrape the first table from a web page and extract data into a Pandas DataFrame.\n\n    This function scrapes the first table found on the specified web page URL and extracts the data\n    into a DataFrame, where each row in the DataFrame corresponds to a table row (<tr>) from the web page,\n    and each column represents the data contained within table data elements (<td>) of that row.\n    The DataFrame's columns are named after the table's header row (<th> elements), if present.\n    If the table lacks headers, the DataFrame's columns remain unnamed.\n\n    Parameters:\n    url (str): The web page URL to scrape. Default is 'http://example.com'.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the scraped table data, with rows corresponding to table rows and\n                  columns named after the table headers, if available.\n\n    Raises:\n    ConnectionError: If there is an issue connecting to the URL.\n    requests.HTTPError: If the HTTP request to the URL fails.\n    ValueError: If no table data is found on the page or if the page content cannot be parsed.\n    \"\"\"\n    try:\n        # Send an HTTP request to the specified URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n    except requests.ConnectionError:\n        raise ConnectionError(f\"Failed to connect to the URL: {url}\")\n    except requests.HTTPError as http_err:\n        raise requests.HTTPError(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        raise Exception(f\"An error occurred: {err}\")\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the first table in the HTML content\n    table = soup.find('table')\n    if table is None:\n        raise ValueError(\"No table found on the page.\")\n\n    # Extract the table's header, if present\n    headers = []\n    header_row = table.find('tr')\n    if header_row:\n        for th in header_row.find_all('th'):\n            headers.append(th.get_text(strip=True))\n\n    # Extract the table's rows\n    rows = []\n    for tr in table.find_all('tr')[1:]:\n        cells = tr.find_all('td')\n        if cells:\n            row = [cell.get_text(strip=True) for cell in cells]\n            rows.append(row)\n\n    if not rows:\n        raise ValueError(\"No table data found on the page.\")\n\n    # Create the DataFrame\n    if headers:\n        df = pd.DataFrame(rows, columns=headers)\n    else:\n        df = pd.DataFrame(rows)\n\n    return df\n```"}
{"task_id": "BigCodeBench/139", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame containing numeric columns.\n\n    Raises:\n    ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n\n    Returns:\n    list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \"\"\"\n    # Validate the input DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input is not a non-empty DataFrame\")\n    \n    # Select numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"There are no numeric columns in the DataFrame\")\n    \n    # Create a list to store the Axes objects\n    axes_list = []\n    \n    # Plot histograms for each numeric column\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col].dropna(), bins=30, edgecolor='black')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n    \n    # Return the list of Axes objects\n    return axes_list\n```"}
{"task_id": "BigCodeBench/147", "solution": "```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread, Lock\n\ndef check_port(ip, port, results, lock, timeout=1):\n    \"\"\"Check if a specific port is open on a given IP address.\"\"\"\n    try:\n        with socket.create_connection((ip, port), timeout=timeout) as sock:\n            with lock:\n                results[ip] = True\n    except (socket.timeout, ConnectionRefusedError, OSError):\n        with lock:\n            results[ip] = False\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Scan a specified IP address range and check if a specified port is open on each IP.\n\n    Args:\n        ip_range (str): IP address range in CIDR notation (e.g., \"192.168.1.0/24\").\n        port (int): Port number to check.\n\n    Returns:\n        dict: A dictionary mapping IP addresses to their port status (True if open).\n    \"\"\"\n    network = IPv4Network(ip_range)\n    results = {}\n    threads = []\n    lock = Lock()\n\n    for ip in network:\n        thread = Thread(target=check_port, args=(str(ip), port, results, lock))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results\n\n# Example usage:\nif __name__ == \"__main__\":\n    ip_range = \"192.168.1.0/24\"\n    port = 80\n    open_ports = task_func(ip_range, port)\n    print(open_ports)\n```"}
{"task_id": "BigCodeBench/161", "solution": "```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, and the message itself from a log file\n    and stores the data in a CSV format.\n\n    Parameters:\n    log_file (str): The path to the log file to be processed.\n\n    Returns:\n    str: The file path to the newly created CSV file which contains the structured log data.\n\n    Raises:\n    ValueError: If the timestamp in any log entry is invalid or if no valid log entries are found.\n    \"\"\"\n    log_pattern = re.compile(r'^(INFO|WARNING|ERROR): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)$')\n    log_entries = []\n\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line.strip())\n            if match:\n                message_type, timestamp, message = match.groups()\n                try:\n                    # Validating the timestamp\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                    log_entries.append((message_type, timestamp, message))\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp in log entry: {line.strip()}\")\n\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Creating DataFrame\n    df = pd.DataFrame(log_entries, columns=['Type', 'Timestamp', 'Message'])\n\n    # Saving to CSV\n    csv_file_path = 'structured_logs.csv'\n    df.to_csv(csv_file_path, index=False)\n\n    return csv_file_path\n```"}
{"task_id": "BigCodeBench/162", "solution": "Sure, here is the self-contained code that analyzes and visualizes the distribution of word lengths in a text and generates a histogram subplot:\n\n```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Use regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the lengths of all words\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram plot\n    fig, ax = plt.subplots()\n\n    if word_lengths:\n        ax.hist(word_lengths, bins=range(1, max(word_lengths) + 2), rwidth=rwidth, edgecolor='black')\n\n    # Set labels and title for the histogram\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax\n\n# Example usage:\n# ax = task_func(\"This is an example sentence to demonstrate the histogram of word lengths.\")\n# plt.show()\n```\n\nThis function does the following:\n1. Uses a regular expression to extract all words from the input text.\n2. Calculates the length of each word.\n3. Creates a histogram of the word lengths if there are any words.\n4. Labels the axes and provides a title for the histogram.\n5. Returns the `Axes` object containing the histogram."}
{"task_id": "BigCodeBench/177", "solution": "Certainly! Here is the self-contained code that meets your requirements:\n\n```python\nimport re\nimport pandas as pd\nfrom collections import Counter\nfrom string import punctuation\n\ndef task_func(df):\n    # Check if the DataFrame is empty or does not contain the necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'\")\n    \n    # Define the keywords to filter the titles\n    keywords = ['like', 'what']\n    pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n    \n    # Filter the DataFrame to include only rows with titles containing the keywords\n    filtered_df = df[df['Title'].str.contains(pattern, na=False)]\n    \n    # If no articles match the filter, return an empty dictionary\n    if filtered_df.empty:\n        return {}\n    \n    # Combine the content of the filtered articles into a single string\n    all_content = ' '.join(filtered_df['Content'].tolist())\n    \n    # Remove punctuation from the combined content\n    all_content = all_content.translate(str.maketrans('', '', punctuation))\n    \n    # Tokenize the content into words\n    words = all_content.split()\n    \n    # Create a frequency dictionary\n    word_freq = Counter(words)\n    \n    return dict(word_freq)\n\n# Example usage\nif __name__ == \"__main__\":\n    data = {\n        'Title': ['What is Python?', 'I like Java', 'Introduction to programming', 'Learn JavaScript'],\n        'Content': ['Python is a programming language.', 'Java is also popular.', 'Programming is fun.', 'JavaScript is widely used.']\n    }\n    df = pd.DataFrame(data)\n    result = task_func(df)\n    print(result)\n```\n\n### Summary:\n1. **Error Handling**: The function raises a `ValueError` if the DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\n2. **Keyword Filtering**: It uses a case-insensitive regex pattern to filter titles containing the keywords \"like\" or \"what\".\n3. **Content Combination**: The content of the filtered articles is combined into a single string.\n4. **Punctuation Removal**: Punctuation is removed from the combined content.\n5. **Word Frequency Calculation**: The function uses `Counter` from the `collections` module to create a frequency dictionary of the words.\n6. **Output**: The function returns a dictionary with words as keys and their corresponding frequencies as values."}
{"task_id": "BigCodeBench/184", "solution": "```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef preprocess_text(text):\n    # Remove punctuation and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n    return text\n\ndef task_func(dataframe, text_column):\n    # Apply preprocessing to the specified text column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the text data\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert the result to a DataFrame\n    df_vectorized = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return df_vectorized\n\n# Example usage\nif __name__ == \"__main__\":\n    data = {\n        'text': [\"This is a sample text for processing.\",\n                 \"Another sample text goes here!\",\n                 \"Text data preprocessing is essential for analysis.\"]\n    }\n    df = pd.DataFrame(data)\n    print(task_func(df, 'text'))\n```"}
{"task_id": "BigCodeBench/187", "solution": "```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate input dictionary\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values must be tuples.\")\n    if len(dic['Lon']) != 2 or len(dic['Lat']) != 2:\n        raise ValueError(\"'Lon' and 'Lat' tuples must have exactly two elements.\")\n    if not all(isinstance(i, (int, float)) for i in dic['Lon']) or not all(isinstance(i, (int, float)) for i in dic['Lat']):\n        raise ValueError(\"The elements of 'Lon' and 'Lat' tuples must be numbers.\")\n    \n    # Generate random coordinates within the specified ranges\n    lon_range = dic['Lon']\n    lat_range = dic['Lat']\n    coordinates = [\n        Point(np.random.uniform(lon_range[0], lon_range[1]), np.random.uniform(lat_range[0], lat_range[1]))\n        for _ in cities\n    ]\n    \n    # Create a GeoDataFrame\n    data = {'City': cities, 'Coordinates': coordinates}\n    gdf = gpd.GeoDataFrame(data, geometry='Coordinates')\n    \n    return gdf\n\n# Example usage\nif __name__ == \"__main__\":\n    gdf = task_func()\n    print(gdf)\n```"}
{"task_id": "BigCodeBench/199", "solution": "Here's the self-contained code to generate a weather report for specified cities at a given UTC datetime:\n\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    \n    # Validate cities is a list of strings\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    \n    # Validate weather_conditions is a list of strings\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    \n    # Validate timezones is a dictionary with string keys and values\n    if not isinstance(timezones, dict) or not all(isinstance(key, str) and isinstance(value, str) for key, value in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    \n    # Set the seed for reproducibility\n    set_seed(seed)\n    \n    report_data = []\n\n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone for city '{city}' is not provided in the timezones dictionary\")\n        \n        # Get the timezone for the city\n        city_tz = pytz.timezone(timezones[city])\n        \n        # Convert UTC datetime to local time\n        local_time = utc_datetime.astimezone(city_tz)\n        local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Append the data to the report list\n        report_data.append({\n            'City': city,\n            'Local Time': local_time_str,\n            'Weather Condition': weather_condition\n        })\n    \n    # Create a DataFrame from the report data\n    weather_report_df = pd.DataFrame(report_data)\n    \n    return weather_report_df\n\n# Example usage:\n# utc_time = datetime.utcnow().replace(tzinfo=pytz.utc)\n# df = task_func(utc_time)\n# print(df)\n```"}
{"task_id": "BigCodeBench/208", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(elements, seed=0):\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"The 'elements' parameter must be a positive integer.\")\n    \n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    random_walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    descriptive_stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'std': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th_percentile': np.percentile(random_walk, 5),\n        '25th_percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th_percentile': np.percentile(random_walk, 75),\n        '95th_percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk)\n    }\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title(\"Random Walk\")\n    ax.set_xlabel(\"Steps\")\n    ax.set_ylabel(\"Position\")\n    plt.grid(True)\n    \n    return descriptive_stats, ax\n\n# Example usage:\n# stats, ax = task_func(1000)\n# print(stats)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/211", "solution": "Certainly! Here is the self-contained function that downloads a zip file from a given URL, keeps the zip file, extracts its contents to a specified directory, and returns the list of extracted files.\n\n```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    zip_path = os.path.join(destination_directory, 'downloaded.zip')\n    \n    with open(zip_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the zip file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n    \n    return extracted_files\n\n# Example usage:\n# url = 'https://example.com/path/to/your/zipfile.zip'\n# destination_directory = '/path/to/destination/directory'\n# headers = {'Authorization': 'Bearer YOUR_TOKEN'} # If needed\n# extracted_files = task_func(url, destination_directory, headers)\n# print(extracted_files)\n```"}
{"task_id": "BigCodeBench/214", "solution": "```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high + 1, size=image_size, dtype=np.uint8)\n    \n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Hide the axes\n    \n    plt.show()\n    \n    # Return the Axes object and the numpy array of the generated image\n    return ax, image\n\n# Example usage\nax, image = task_func()\n```"}
{"task_id": "BigCodeBench/227", "solution": "```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from a list L, normalizes it based on the sound pressure level (SPL) \n    of a specified audio file, and generates a spectrogram from the matrix.\n\n    Args:\n        L (list): A list of numeric values to be reshaped into an MxN matrix.\n        M (int): The number of rows for the matrix.\n        N (int): The number of columns for the matrix.\n        audio_file (str): Path to the audio file used to calculate the SPL.\n\n    Returns:\n        numpy.ndarray: The normalized MxN matrix.\n        matplotlib.figure.Figure: The figure object for the generated spectrogram.\n\n    Raises:\n        FileNotFoundError: If the specified audio file does not exist.\n        ValueError: If the length of list L does not match MxN or if the audio file contains no data.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n    \n    # Read the audio file\n    data, sample_rate = sf.read(audio_file)\n    \n    # Ensure the audio data is not empty\n    if len(data) == 0:\n        raise ValueError(\"Audio file contains no data.\")\n    \n    # Handle multi-channel audio by averaging the channels\n    if len(data.shape) > 1:\n        data = np.mean(data, axis=1)\n    \n    # Calculate the Sound Pressure Level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n    \n    # Create the MxN matrix from the list L\n    if len(L) != M * N:\n        raise ValueError(\"The length of list L does not match the specified dimensions MxN.\")\n    \n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / spl\n    \n    # Generate the spectrogram from the normalized matrix\n    fig, ax = plt.subplots()\n    S = librosa.feature.melspectrogram(S=normalized_matrix, sr=sample_rate, power=1.0)\n    S_dB = librosa.power_to_db(S, ref=np.max)\n    \n    img = librosa.display.specshow(S_dB, sr=sample_rate, x_axis='time', y_axis='log', ax=ax)\n    ax.set(title='Log-frequency power spectrogram')\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    \n    return normalized_matrix, fig\n\n# Example usage:\n# L = np.random.rand(300).tolist()\n# M, N = 10, 30\n# audio_file = 'path_to_audio_file.wav'\n# normalized_matrix, fig = task_func(L, M, N, audio_file)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/239", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    \"\"\"\n    Extract numeric values from a list of tuples, compute basic statistics, and generate a histogram\n    with an overlaid probability density function (PDF).\n\n    Parameters:\n    original (list of tuples): Input list containing tuples with potential numeric values.\n\n    Returns:\n    np.array: A numpy array of the extracted numeric values.\n    dict: Basic statistics for the array including mean, standard deviation, minimum, and maximum.\n    Axes: A matplotlib Axes object showing the histogram with overlaid PDF.\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    numeric_values = [item for sublist in original for item in sublist if isinstance(item, (int, float))]\n    \n    if not numeric_values:\n        raise ValueError(\"No numeric values found in the input list of tuples.\")\n    \n    # Convert the list to a numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_array),\n        'std_dev': np.std(numeric_array),\n        'min': np.min(numeric_array),\n        'max': np.max(numeric_array)\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram\n    count, bins, ignored = ax.hist(numeric_array, bins='auto', density=True, alpha=0.6, color='g')\n    \n    # Plot the PDF\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, statistics['mean'], statistics['std_dev'])\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Add labels\n    ax.set_title('Histogram with PDF')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    # Return the results\n    return numeric_array, statistics, ax\n\n# Example usage\n# original = [(1, 2.5), (3.5, 'a'), (2, 4), (5, 6.1), ('b', 7)]\n# numeric_array, statistics, ax = task_func(original)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/241", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    \"\"\"\n    Converts a list to a numpy array, normalizes the array, and plots both original and normalized arrays.\n    \n    Parameters:\n    original (list): A list of numeric values.\n    \n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: Normalized array.\n    matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Input validation\n    if not original or not all(isinstance(x, (int, float)) for x in original):\n        raise ValueError(\"Input must be a non-empty list of numeric values.\")\n    \n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using sklearn's MinMaxScaler\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Plot the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original', marker='o')\n    ax.plot(normalized_array, label='Normalized', marker='x')\n    ax.set_title('Original and Normalized Data')\n    ax.legend()\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.grid(True)\n    \n    # Display the plot\n    plt.show()\n    \n    return original_array, normalized_array, ax\n\n# Example usage:\noriginal = [10, 20, 15, 30, 25]\noriginal_array, normalized_array, ax = task_func(original)\n```"}
{"task_id": "BigCodeBench/267", "solution": "```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Perform FFT on the signal generated from the dictionary values and plot the result.\n\n    Parameters:\n    data (dict): Input dictionary with numerical values.\n    sample_rate (int): Sample rate for the signal, default is 8000.\n\n    Returns:\n    tuple: A tuple containing:\n        - ndarray: The FFT of the signal.\n        - Axes: The plot of the FFT.\n    \"\"\"\n    # Validate data types of dictionary values\n    for key, value in data.items():\n        if not isinstance(value, (int, float)):\n            raise ValueError(f\"Value for key '{key}' is not numerical: {value}\")\n\n    # 1. Adds a new key \"a\" with the value 1 to the dictionary.\n    data['a'] = 1\n\n    # 2. Generates a signal based on the values in \"data\".\n    signal = np.array(list(data.values()))\n\n    if len(signal) <= 1:\n        raise ValueError(\"Signal length must be greater than 1 to perform FFT\")\n\n    # 3. Runs a Fast Fourier Transform (FFT) on the signal.\n    fft_signal = fftpack.fft(signal)\n\n    # 4. Plots and returns the FFT of the signal.\n    freqs = fftpack.fftfreq(len(signal), 1/sample_rate)\n    \n    fig, ax = plt.subplots()\n    ax.plot(freqs[:len(freqs)//2], np.abs(fft_signal)[:len(freqs)//2])  # Plot only positive frequencies\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Magnitude')\n    ax.set_title('FFT of the Signal')\n\n    # Display the plot\n    plt.show()\n\n    return fft_signal, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_data = {'x': 2, 'y': 3, 'z': 4}\n    fft_result, fft_plot = task_func(sample_data)\n    print(\"FFT Result:\", fft_result)\n```"}
{"task_id": "BigCodeBench/273", "solution": "```python\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nclass PostHandler(http.server.BaseHTTPRequestHandler):\n    def _set_headers(self, status_code, response_body):\n        self.send_response(status_code)\n        self.send_header('Content-Type', 'application/json')\n        self.send_header('Content-Length', str(len(response_body)))\n        self.end_headers()\n\n    def do_POST(self):\n        content_type = self.headers.get('Content-Type')\n        if content_type != 'application/json':\n            response_body = json.dumps({'status': 'error', 'message': 'Content-Type header is not application/json'})\n            self._set_headers(400, response_body)\n            self.wfile.write(response_body.encode('utf-8'))\n            return\n\n        content_length = int(self.headers.get('Content-Length'))\n        post_data = self.rfile.read(content_length)\n\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            response_body = json.dumps({'status': 'error', 'message': 'Invalid JSON'})\n            self._set_headers(400, response_body)\n            self.wfile.write(response_body.encode('utf-8'))\n            return\n\n        if 'data' not in data:\n            response_body = json.dumps({'status': 'error', 'message': 'No data key in request'})\n            self._set_headers(400, response_body)\n            self.wfile.write(response_body.encode('utf-8'))\n            return\n\n        response_body = json.dumps(SUCCESS_RESPONSE)\n        self._set_headers(200, response_body)\n        self.wfile.write(response_body.encode('utf-8'))\n\ndef task_func():\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, PostHandler)\n    print('Starting server on port 8000...')\n    httpd.serve_forever()\n\nif __name__ == '__main__':\n    task_func()\n```"}
{"task_id": "BigCodeBench/274", "solution": "```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        \n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n\n            try:\n                email_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400, \"Bad Request\")\n                self.send_header('Content-Type', 'text/plain')\n                response_message = 'Invalid JSON'\n                self.send_header('Content-Length', str(len(response_message)))\n                self.end_headers()\n                self.wfile.write(response_message.encode('utf-8'))\n                return\n\n            try:\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n            except KeyError:\n                self.send_response(400, \"Bad Request\")\n                self.send_header('Content-Type', 'text/plain')\n                response_message = 'Missing required email fields'\n                self.send_header('Content-Length', str(len(response_message)))\n                self.end_headers()\n                self.wfile.write(response_message.encode('utf-8'))\n                return\n\n            msg = MIMEText(message)\n            msg['Subject'] = subject\n            msg['From'] = smtp_username\n            msg['To'] = to\n\n            try:\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, [to], msg.as_string())\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535, \"Authentication Failed\")\n                self.send_header('Content-Type', 'text/plain')\n                response_message = 'SMTP Authentication Error'\n                self.send_header('Content-Length', str(len(response_message)))\n                self.end_headers()\n                self.wfile.write(response_message.encode('utf-8'))\n                return\n\n            self.send_response(200, \"OK\")\n            self.send_header('Content-Type', 'text/plain')\n            response_message = 'Email sent successfully'\n            self.send_header('Content-Length', str(len(response_message)))\n            self.end_headers()\n            self.wfile.write(response_message.encode('utf-8'))\n\n    return EmailHandler\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     smtp_server = 'smtp.example.com'\n#     smtp_port = 587\n#     smtp_username = 'your_username'\n#     smtp_password = 'your_password'\n#     handler_class = task_func(smtp_server, smtp_port, smtp_username, smtp_password)\n#     server = http.server.HTTPServer(('localhost', 8080), handler_class)\n#     print('Starting server at http://localhost:8080')\n#     server.serve_forever()\n```"}
{"task_id": "BigCodeBench/287", "solution": "```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    # Ensure the directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    word_count = Counter()\n    \n    # Iterate through all files in the specified directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                \n                # Open and read the content of the file\n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                        words = content.split()\n                        word_count[file] = len(words)\n                except Exception as e:\n                    print(f\"Error reading file {file}: {e}\")\n\n    # Calculate the total number of words\n    total_words = sum(word_count.values())\n\n    # Export the word counts to a JSON file\n    try:\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(word_count, f, ensure_ascii=False, indent=4)\n    except Exception as e:\n        print(f\"Error writing to JSON file {filename}: {e}\")\n        raise\n    \n    return total_words\n\n# Example usage:\n# total_words = task_func('output.json', '/path/to/directory')\n# print(total_words)\n```"}
{"task_id": "BigCodeBench/302", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Args:\n    df (pd.DataFrame): Input DataFrame containing 'Date' and 'Value' columns.\n    plot (bool): If True, displays a heatmap of the correlation matrix.\n\n    Returns:\n    tuple: (correlation_matrix, ax)\n        - correlation_matrix (pd.DataFrame): A pandas DataFrame containing the correlation coefficients.\n        - ax (matplotlib.axes.Axes or None): A matplotlib Axes object containing the heatmap plot, or None if plot is False.\n\n    Raises:\n    ValueError: If the DataFrame is empty or 'Value' column is invalid.\n    \"\"\"\n    \n    # Check if DataFrame is empty or if 'Value' column is invalid\n    if df.empty or 'Value' not in df.columns or not all(isinstance(i, list) for i in df['Value']):\n        raise ValueError(\"DataFrame is empty or 'Value' column is invalid.\")\n    \n    # Ensure all lists in 'Value' column are of the same length\n    list_lengths = [len(lst) for lst in df['Value']]\n    if len(set(list_lengths)) != 1:\n        raise ValueError(\"'Value' column contains lists of varying lengths.\")\n    \n    # Expand the 'Value' lists into separate columns\n    value_df = pd.DataFrame(df['Value'].tolist())\n    \n    # Calculate the Pearson correlation coefficient\n    correlation_matrix = value_df.corr(method='pearson')\n    \n    # Plot the heatmap if requested\n    ax = None\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar=True)\n        ax.set_title(\"Correlation Heatmap\")\n        plt.show()\n    \n    return correlation_matrix, ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n#     'Value': [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# })\n# correlation_matrix, ax = task_func(df, plot=True)\n```"}
{"task_id": "BigCodeBench/308", "solution": "```python\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n    all_fields = FIELDS + additional_fields\n    data = {field: [random.randint(0, 100) for _ in range(len(STUDENTS))] for field in all_fields}\n    df = pd.DataFrame(data, index=STUDENTS)\n\n    # Calculate average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n\n    # Calculate average grade for each subject\n    avg_grades = {field: mean(df[field]) for field in all_fields}\n    avg_grades['Average Grade'] = mean(df['Average Grade'])\n    df.loc['Average'] = avg_grades\n\n    return df\n\n# Example usage\ndf = task_func(['Art', 'Music'])\nprint(df)\n```"}
{"task_id": "BigCodeBench/310", "solution": "```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef generate_random_name():\n    first_names = ['John', 'Jane', 'Alex', 'Emily', 'Chris', 'Katie', 'Mike', 'Sarah', 'David', 'Laura']\n    last_names = ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor']\n    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n\ndef task_func(filename):\n    # Generate simulated data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = generate_random_name()\n        age = random.randint(18, 80)\n        height = round(random.uniform(150, 200), 2)  # height in cm\n        weight = round(random.uniform(50, 100), 2)  # weight in kg\n        data.append([name, age, height, weight])\n    \n    # Calculate averages\n    avg_age = mean([person[1] for person in data])\n    avg_height = mean([person[2] for person in data])\n    avg_weight = mean([person[3] for person in data])\n\n    # Write data to CSV\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n        writer.writerow(['Average', avg_age, avg_height, avg_weight])\n    \n    return os.path.abspath(filename)\n\n# Example usage\ncsv_file_path = task_func('simulated_data.csv')\nprint(f\"CSV file created at: {csv_file_path}\")\n```"}
{"task_id": "BigCodeBench/313", "solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(directory):\n    \"\"\"\n    Organize files in a directory based on the first text that is not enclosed in square brackets.\n    Move the files to subdirectories named after this text. If no matching text is found, the file is not moved.\n    \n    Args:\n    directory (str): The directory path to organize.\n\n    Returns:\n    tuple: \n        str: The directory path with organized files.\n        dict: A dictionary where keys are the created subdirectories and values are lists of files moved to them.\n    \"\"\"\n    \n    # Validate the provided directory\n    if not os.path.isdir(directory):\n        raise NotADirectoryError(f\"Provided path '{directory}' is not a valid directory.\")\n    \n    # Dictionary to keep track of created subdirectories and moved files\n    moved_files_dict = {}\n\n    # Pattern to match text outside square brackets\n    pattern = re.compile(r'^[^\\[\\]]+')\n\n    # List all files in the given directory\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n\n        # Check if it is a file (ignore directories)\n        if os.path.isfile(file_path):\n            match = pattern.match(filename)\n            if match:\n                # Extract the matched text and strip any extra whitespace\n                subdir_name = match.group(0).strip()\n                \n                # Create the subdirectory path\n                subdir_path = os.path.join(directory, subdir_name)\n                \n                # Create the subdirectory if it does not exist\n                if not os.path.exists(subdir_path):\n                    os.makedirs(subdir_path)\n                    # Initialize the list in the dictionary\n                    moved_files_dict[subdir_name] = []\n                \n                # Move the file to the subdirectory\n                new_file_path = os.path.join(subdir_path, filename)\n                try:\n                    shutil.move(file_path, new_file_path)\n                    # Add the file to the dictionary\n                    moved_files_dict[subdir_name].append(filename)\n                except Exception as e:\n                    print(f\"Error moving file '{filename}': {e}\")\n\n    return (directory, moved_files_dict)\n\n# Example usage\n# directory_path = \"/path/to/your/directory\"\n# result = task_func(directory_path)\n# print(result)\n```"}
{"task_id": "BigCodeBench/324", "solution": "Here's the self-contained code that meets your requirements:\n\n```python\nimport subprocess\nimport threading\n\ndef task_func(file_list):\n    \"\"\"\n    Run files from a list as subprocesses concurrently and return their exit codes.\n    \n    Args:\n    file_list (list): List of file paths to be executed.\n    \n    Returns:\n    list: List of exit codes from the subprocesses.\n    \"\"\"\n    def run_subprocess(file, results, index):\n        try:\n            result = subprocess.run(['python', file], check=True)\n            results[index] = result.returncode\n        except subprocess.CalledProcessError as e:\n            results[index] = e.returncode\n        except Exception as e:\n            # Catch any other exceptions and set a distinct error code, e.g., -1\n            results[index] = -1\n            print(f\"Error running {file}: {e}\")\n\n    if not isinstance(file_list, list):\n        raise ValueError(\"file_list should be a list of file paths.\")\n    \n    threads = []\n    results = [None] * len(file_list)\n    \n    for i, file in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(file, results, i))\n        threads.append(thread)\n        thread.start()\n    \n    for thread in threads:\n        thread.join()\n    \n    return results\n\n# Example usage:\nif __name__ == \"__main__\":\n    file_list = ['script1.py', 'script2.py', 'script3.py']\n    print(task_func(file_list))\n```\n\n### Explanation:\n1. **Imports**:\n   - `subprocess`: For running the subprocesses.\n   - `threading`: For running subprocesses concurrently.\n\n2. **Function Definition**:\n   - `task_func(file_list)`: This function takes a list of file paths to be executed.\n\n3. **Running Subprocesses**:\n   - `run_subprocess(file, results, index)`: This nested function runs a subprocess and stores the exit code in the `results` list. It handles exceptions and stores the return code in case of a failure.\n\n4. **Argument Validation**:\n   - The function checks if `file_list` is a list and raises a `ValueError` if it is not.\n\n5. **Creating Threads**:\n   - A thread is created for each file in `file_list` and started. Each thread runs the `run_subprocess` function.\n\n6. **Joining Threads**:\n   - The function waits for all threads to finish using `thread.join()`.\n\n7. **Returning Results**:\n   - The function returns the list of exit codes.\n\n8. **Example Usage**:\n   - The `__main__` block demonstrates how to call `task_func` with a sample list of script files. Replace the example `file_list` with the actual list of files you want to run.\n\nThis code ensures that the subprocesses are run concurrently and their exit codes are collected and returned as a list."}
{"task_id": "BigCodeBench/326", "solution": "Here's the self-contained Python code that finds and runs all `.bat` files in a given directory, returning their file names and exit codes:\n\n```python\nimport subprocess\nimport os\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Find and run all .bat files in the given directory, returning their file names and exit codes.\n    The exit code is None if the file could not be executed.\n\n    Args:\n        directory_path (str): The path to the directory containing .bat files.\n\n    Returns:\n        list of tuples: A list where each tuple contains the file name and its exit code.\n    \"\"\"\n    \n    # Ensure directory_path is a valid directory\n    if not os.path.isdir(directory_path):\n        raise ValueError(f\"Provided path '{directory_path}' is not a valid directory.\")\n    \n    # Find all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    results = []\n\n    for bat_file in bat_files:\n        try:\n            # Run the .bat file\n            result = subprocess.run([bat_file], shell=True, capture_output=True)\n            # Append the file name and exit code to the results list\n            results.append((bat_file, result.returncode))\n        except Exception as e:\n            # Log the exception and set the exit code to None\n            print(f\"Failed to execute {bat_file}: {e}\")\n            results.append((bat_file, None))\n\n    return results\n\n# Example usage:\n# directory_path = 'path_to_your_directory'\n# output = task_func(directory_path)\n# print(output)\n```\n\nThis code ensures:\n1. **Directory Validation**: It checks if the provided path is a valid directory.\n2. **Finding `.bat` Files**: Uses `glob` to find all `.bat` files in the specified directory.\n3. **Running `.bat` Files**: Utilizes `subprocess.run` to execute each `.bat` file, capturing the exit code.\n4. **Exception Handling**: Catches any exceptions during the execution of the `.bat` files and logs the error, setting the exit code to `None`.\n5. **Returning Results**: Provides a list of tuples, each containing the file name and its exit code."}
{"task_id": "BigCodeBench/341", "solution": "```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    \n    # Check if DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The input DataFrame must not be empty.\")\n    \n    # Check if specified column is in the DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The DataFrame does not contain the specified column: {col}\")\n    \n    # Create the figure and subplots\n    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n    \n    # Plot histogram with kernel density estimate if numerical data\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.histplot(df[col], kde=False, ax=axes[0])\n    \n    axes[0].set_title(f'Histogram of {col}')\n    \n    # Plot box plot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box Plot of {col}')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return fig\n\n# Example usage:\n# df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n# fig = task_func(df, 'A')\n# plt.show()\n```"}
{"task_id": "BigCodeBench/346", "solution": "```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a subprocess with predefined arguments.\n    \n    Parameters:\n    script_path (str): Path to the Python script to run.\n    wait (bool): Whether to wait for the process to complete. Defaults to True.\n    *args: Additional arguments to pass to the script.\n    \n    Returns:\n    int: The return code of the subprocess if 'wait' is True.\n    None: If 'wait' is False.\n    \n    Raises:\n    ValueError: If the script does not exist.\n    subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script {script_path} does not exist.\")\n    \n    # Build the command with the script path and additional arguments\n    cmd = [sys.executable, script_path] + list(args)\n    \n    try:\n        # Start the subprocess\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        if wait:\n            # Wait for the process to complete\n            stdout, stderr = process.communicate()\n            return_code = process.returncode\n            \n            if return_code != 0:\n                raise subprocess.CalledProcessError(return_code, cmd, output=stdout, stderr=stderr)\n            \n            return return_code\n        else:\n            # If not waiting, return None\n            return None\n\n    except subprocess.CalledProcessError as e:\n        # Handle the case where the subprocess raises an exception\n        print(f\"Error: {e.stderr.decode()}\", file=sys.stderr)\n        raise\n\n    except Exception as e:\n        # Handle other exceptions\n        raise e\n\n# Example usage\nif __name__ == \"__main__\":\n    script_path = \"example_script.py\"\n    try:\n        return_code = task_func(script_path, wait=True, \"arg1\", \"arg2\")\n        print(f\"Script completed with return code: {return_code}\")\n    except ValueError as ve:\n        print(f\"ValueError: {ve}\")\n    except subprocess.CalledProcessError as cpe:\n        print(f\"CalledProcessError: {cpe}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n```"}
{"task_id": "BigCodeBench/360", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    try:\n        # Load the Excel file\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\") from e\n    \n    # Filter only numeric columns\n    numeric_df = df.select_dtypes(include=[np.number])\n    \n    # Calculate the mean and standard deviation for each numeric column\n    stats = {}\n    for column in numeric_df.columns:\n        stats[column] = {\n            'mean': numeric_df[column].mean(),\n            'std': numeric_df[column].std()\n        }\n    \n    # Prepare data for the bar chart\n    columns = list(stats.keys())\n    means = [stats[col]['mean'] for col in columns]\n    stds = [stats[col]['std'] for col in columns]\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    bar_width = 0.35\n    index = np.arange(len(columns))\n    \n    bar1 = ax.bar(index, means, bar_width, label='Mean')\n    bar2 = ax.bar(index + bar_width, stds, bar_width, label='Standard Deviation')\n    \n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(columns)\n    ax.legend()\n    \n    plt.tight_layout()\n    \n    # Return the stats dictionary and the figure object\n    return stats, fig\n\n# Example usage:\n# stats, fig = task_func('data.xlsx', 'Sheet1')\n# fig.show()\n```"}
{"task_id": "BigCodeBench/367", "solution": "```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    \"\"\"\n    Generate a bar chart of the number of activities performed on each day of the week.\n\n    Parameters:\n    activities (list): A list of datetime objects representing activity timestamps.\n\n    Raises:\n    TypeError: If the activities list contains non-datetime objects.\n\n    Returns:\n    matplotlib.axes.Axes: Axes object representing the bar chart.\n    \"\"\"\n\n    # Check if all items in activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n\n    # Dictionary to count the number of activities for each day of the week\n    day_counts = defaultdict(int)\n\n    # Count activities for each day\n    for activity in activities:\n        day_of_week = activity.strftime('%A')  # Get the day of the week (Monday, Tuesday, etc.)\n        day_counts[day_of_week] += 1\n\n    # Prepare data for plotting\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [day_counts[day] for day in days]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n\n    # Labeling the chart\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n\n    # Return the Axes object\n    return ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    activities = [\n        datetime(2023, 10, 2, 10, 0),  # Monday\n        datetime(2023, 10, 3, 14, 0),  # Tuesday\n        datetime(2023, 10, 4, 18, 0),  # Wednesday\n        datetime(2023, 10, 5, 9, 0),   # Thursday\n        datetime(2023, 10, 6, 20, 0),  # Friday\n        datetime(2023, 10, 7, 8, 0),   # Saturday\n        datetime(2023, 10, 8, 22, 0),  # Sunday\n        datetime(2023, 10, 2, 11, 0)   # Another Monday\n    ]\n\n    ax = task_func(activities)\n    plt.show()\n```"}
{"task_id": "BigCodeBench/368", "solution": "```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n\n    Parameters:\n    src_dir (str): The source directory containing the files.\n    dest_dir (str): The destination directory where the file will be moved.\n    seed (int, optional): The seed for the random number generator for reproducibility. Default is 100.\n\n    Returns:\n    str: The name of the file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n\n    Raises:\n    FileNotFoundError: If no files are found in the source directory.\n    ValueError: If the source or destination directories are invalid.\n    \"\"\"\n    # Input validation\n    if not src_dir or not dest_dir:\n        raise ValueError(\"Source and destination directories must be specified.\")\n\n    # Set the random seed for reproducibility\n    random.seed(seed)\n    \n    # List all files in the source directory\n    try:\n        files = os.listdir(src_dir)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    except PermissionError:\n        raise PermissionError(f\"Permission denied to access source directory '{src_dir}'.\")\n\n    # Filter out directories, only keep files\n    files = [f for f in files if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files to move\n    if not files:\n        raise FileNotFoundError(\"No files found in the source directory.\")\n    \n    # Randomly select a file from the list\n    file_to_move = random.choice(files)\n    \n    # Define the full source and destination paths\n    src_file_path = os.path.join(src_dir, file_to_move)\n    dest_file_path = os.path.join(dest_dir, file_to_move)\n    \n    # Ensure the destination directory exists\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # Move the file\n    try:\n        shutil.move(src_file_path, dest_file_path)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to move file '{file_to_move}': {e}\")\n    \n    # Return the name of the moved file\n    return file_to_move\n```"}
{"task_id": "BigCodeBench/374", "solution": "Here is the self-contained code that meets the specified requirements:\n\n```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Count of processed Excel files\n    file_count = 0\n    \n    # Get all Excel files in the specified directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    for file_path in excel_files:\n        try:\n            # Load the workbook\n            workbook = load_workbook(filename=file_path)\n            \n            # Iterate through all sheets\n            for sheet in workbook.worksheets:\n                # Iterate through all cells in the sheet\n                for row in sheet.iter_rows():\n                    for cell in row:\n                        if cell.value and isinstance(cell.value, str):\n                            # Prefix all double quotes with double backslashes\n                            cell.value = re.sub(r'\"', r'\\\\\"', cell.value)\n            \n            # Save the changes to the workbook\n            workbook.save(file_path)\n            \n            # Increment the processed file count\n            file_count += 1\n        except Exception as e:\n            print(f\"Error processing file {file_path}: {e}\")\n    \n    return file_count\n\n# Example usage\nif __name__ == \"__main__\":\n    directory = './your_directory_path/'  # specify your directory path here\n    processed_files = task_func(directory)\n    print(f'Number of Excel files processed: {processed_files}')\n```\n\n### Explanation:\n1. **Imports**:\n    - `regex` for regular expressions.\n    - `glob` for finding all Excel files in the specified directory.\n    - `os` for handling file paths.\n    - `load_workbook` from `openpyxl` for reading and writing Excel files.\n    \n2. **Function Definition (`task_func`)**:\n    - Takes `directory_path` as an argument with a default value of `'./xlsx_files/'`.\n    - Initializes `file_count` to keep track of processed files.\n    - Uses `glob` to find all `.xlsx` files in the given directory.\n\n3. **Processing Each File**:\n    - Loads each Excel file using `load_workbook`.\n    - Iterates through all sheets and cells in each sheet.\n    - If a cell contains a string with double quotes, replaces each double quote `\"` with `\\\\\"` using `re.sub`.\n    - Saves the modified workbook.\n    - Increments the `file_count`.\n    - Handles any exceptions that might occur during processing and prints an error message.\n\n4. **Return**:\n    - Returns the total number of processed files.\n\n5. **Example Usage**:\n    - Demonstrates how to call the function and print the number of processed files.\n\nReplace `'./your_directory_path/'` with the actual path to your directory containing the Excel files."}
{"task_id": "BigCodeBench/399", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Generate and plot sine and cosine waves for a given frequency and sample size.\n\n    Parameters:\n    frequency (float): Frequency of the sine and cosine waves. Must be non-negative.\n    sample_size (int): Number of samples. Must be positive. Default is 10000.\n\n    Returns:\n    matplotlib.figure.Figure: The figure object containing the plot.\n    matplotlib.axes.Axes: The axes object of the plot.\n\n    Raises:\n    ValueError: If the frequency is negative.\n    ValueError: If the sample size is non-positive.\n    \"\"\"\n    # Raise exceptions for invalid inputs\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    # Generate time values\n    t = np.linspace(0, 1, sample_size)\n    \n    # Generate sine and cosine values\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine wave')\n    ax.plot(t, cosine_wave, label='Cosine wave')\n    \n    # Customize the plot\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency} Hz')\n    ax.set_xlabel('Time [s]')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    ax.grid(True)\n    \n    # Show the plot\n    plt.show()\n    \n    return fig, ax\n\n# Example usage\nfig, ax = task_func(frequency=5)\n```"}
{"task_id": "BigCodeBench/401", "solution": "```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    app = Flask(app_name)\n\n    # Retrieve email server details from environment variables, or use defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n\n    # Configure the app for Flask-Mail\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Return tuple with Flask-Mail instance and app's mail configurations\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']\n\n# Example usage\nif __name__ == '__main__':\n    mail_instance, mail_server, mail_port, mail_use_tls, mail_username, mail_password = task_func('my_flask_app')\n    print(f\"Mail Instance: {mail_instance}\")\n    print(f\"Mail Server: {mail_server}\")\n    print(f\"Mail Port: {mail_port}\")\n    print(f\"Mail Use TLS: {mail_use_tls}\")\n    print(f\"Mail Username: {mail_username}\")\n    print(f\"Mail Password: {mail_password}\")\n```"}
{"task_id": "BigCodeBench/409", "solution": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculate the mean, median, and standard deviation of the data from a specific column in an Excel file.\n\n    Parameters:\n    excel_file_path (str): The directory path where the Excel file is located.\n    file_name (str): The name of the Excel file.\n    column_name (str): The name of the column to perform calculations on.\n\n    Returns:\n    dict: A dictionary with the mean, median, and standard deviation.\n\n    Raises:\n    FileNotFoundError: If the Excel file does not exist at the specified path.\n    ValueError: If the specified column is not found in the Excel file.\n    \"\"\"\n    # Combine file path and file name\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    # Read the Excel file\n    try:\n        df = pd.read_excel(file_path)\n    except Exception as e:\n        raise ValueError(f\"Error reading the Excel file: {e}\")\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not found in the Excel file.\")\n    \n    # Check if the column data is numeric\n    if not np.issubdtype(df[column_name].dtype, np.number):\n        raise ValueError(f\"The data in column '{column_name}' is not numeric.\")\n    \n    # Calculate mean, median, and standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'std_dev': std_dev\n    }\n\n# Example usage:\nif __name__ == \"__main__\":\n    result = task_func('path/to/excel', 'file.xlsx', 'column_name')\n    print(result)\n```"}
{"task_id": "BigCodeBench/417", "solution": "```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y, learning_rate=0.01, epochs=100, batch_size=32):\n    # Ensure the input dimension of X is always 2\n    assert X.shape[1] == 2, \"Input dimension of X must be 2\"\n    \n    # Splitting the data into training (75%) and test (25%) sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Constructing the Sequential model\n    model = Sequential()\n    model.add(Dense(10, input_dim=2, activation='sigmoid'))  # Hidden layer\n    model.add(Dense(1, activation='sigmoid'))  # Output layer\n    \n    # Compiling the model\n    sgd = SGD(learning_rate=learning_rate)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    \n    # Fitting the model to the training data\n    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=0)\n    \n    # Plotting the training and validation loss over epochs\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n\n    return model, ax\n\n# Example usage:\n# import numpy as np\n# X = np.random.rand(100, 2)\n# Y = np.random.randint(2, size=(100, 1))\n# model, ax = task_func(X, Y)\n# plt.show()\n```\n\nThis code:\n- Ensures the input dimension of `X` is always 2.\n- Splits the data into training (75%) and testing (25%) sets.\n- Constructs a Sequential model with one hidden layer using a sigmoid activation function.\n- Compiles the model with binary cross-entropy loss and the SGD optimizer.\n- Fits the model to the training data and evaluates it on the test set.\n- Plots the training and validation loss over epochs and returns the plot's Axes object for further customization."}
{"task_id": "BigCodeBench/418", "solution": "```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create the Keras Sequential model\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(10, input_dim=X.shape[1], activation='sigmoid'))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    # Compile the model\n    sgd = keras.optimizers.SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    \n    # Fit the model to the training data\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n    \n    # Predict probabilities for the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n    \n    # Compute ROC curve and AUC score\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.legend(loc=\"lower right\")\n    plt.show()\n    \n    return model, ax\n\n# Example usage:\n# X, Y = some_data_loading_function()\n# model, ax = task_func(X, Y)\n```"}
{"task_id": "BigCodeBench/424", "solution": "```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(f\"'n_clusters' must be a positive integer, got {n_clusters}\")\n\n    # Read the image\n    original_image = cv2.imread(image_path)\n    if original_image is None:\n        raise FileNotFoundError(f\"The image file could not be read: {image_path}\")\n\n    # Convert the image from BGR to RGB\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n\n    # Handle the case when n_clusters == 1\n    if n_clusters == 1:\n        segmented_image = np.copy(original_image)\n    else:\n        # Reshape the image to a 2D array of pixels\n        pixels = original_image.reshape(-1, 3)\n\n        # Apply K-means clustering\n        kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n        kmeans.fit(pixels)\n        labels = kmeans.labels_\n        centroids = kmeans.cluster_centers_\n\n        # Create segmented image\n        segmented_image = centroids[labels].reshape(original_image.shape).astype(np.uint8)\n\n        # Save each region as a separate image\n        for i in range(n_clusters):\n            mask = labels.reshape(original_image.shape[:2]) == i\n            region = np.zeros_like(original_image)\n            region[mask] = original_image[mask]\n            output_path = f'region_{i}.jpg'\n            cv2.imwrite(output_path, cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n\n    # Return the original and segmented images\n    return original_image, segmented_image\n\n# Example Usage\n# original, segmented = task_func('path_to_image.jpg', n_clusters=3)\n```"}
{"task_id": "BigCodeBench/443", "solution": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Ensure P and T have compatible shapes for multiplication\n    assert P.shape[1] == T.shape[0], \"Incompatible shapes for matrix multiplication\"\n    \n    # Compute the product of matrix P and tensor T\n    product = np.tensordot(P, T, axes=(1, 0))\n    \n    # Flatten the result\n    flattened_product = product.flatten().reshape(-1, 1)\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_product)\n    \n    # Visualize the clustering result\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(np.arange(flattened_product.shape[0]), flattened_product, c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    \n    # Return the cluster result and the visualization axis\n    return cluster_result, ax\n\n# Example usage (uncomment to use):\n# P = np.random.rand(4, 3)\n# T = np.random.rand(3, 5, 2)\n# cluster_result, ax = task_func(P, T)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/445", "solution": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n    Jittering is applied prior to plotting.\n\n    Parameters:\n        points (array-like): The input points for which to compute the Voronoi diagram.\n        seed (int): The seed for the random number generator to ensure reproducibility.\n        \n    Returns:\n        tuple: A tuple containing the Voronoi object and the axes of the plot.\n    \"\"\"\n    \n    # Validate input points\n    points = np.array(points)\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points must be a 2D array with shape (n_points, 2).\")\n    \n    # Apply jitter for better visualization\n    np.random.seed(seed)\n    jitter = np.random.normal(scale=1e-9, size=points.shape)\n    jittered_points = points + jitter\n    \n    # Calculate Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=10)\n    \n    # Show plot\n    plt.show()\n    \n    return vor, ax\n\n# Example usage:\n# points = np.random.rand(10, 2)  # Example random points\n# vor, ax = task_func(points)\n```"}
{"task_id": "BigCodeBench/454", "solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n\n    Args:\n        src_dir (str): The source directory.\n        dest_dir (str): The destination directory.\n        ext (str): The file extension to search for.\n\n    Returns:\n        list: A list of the full paths of files that were successfully moved.\n              If a file was not moved because it already exists in the destination directory,\n              it will not be included in this list.\n\n    Raises:\n        FileNotFoundError: If either the source or destination directory does not exist.\n    \"\"\"\n    \n    # Check if the source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n    \n    # Ensure the source and destination directories are not the same\n    if os.path.abspath(src_dir) == os.path.abspath(dest_dir):\n        raise ValueError(\"Source and destination directories must be different.\")\n\n    # Ensure the extension starts with a period\n    if not ext.startswith('.'):\n        ext = '.' + ext\n    \n    # Find all files in the source directory with the specified extension\n    search_pattern = os.path.join(src_dir, f'*{ext}')\n    files_to_move = glob.glob(search_pattern)\n    \n    # List to keep track of successfully moved files\n    moved_files = []\n\n    for file_path in files_to_move:\n        file_name = os.path.basename(file_path)\n        dest_path = os.path.join(dest_dir, file_name)\n        \n        # Move the file if it does not already exist in the destination directory\n        if not os.path.exists(dest_path):\n            shutil.move(file_path, dest_path)\n            moved_files.append(dest_path)\n    \n    return moved_files\n\n# Example usage:\n# moved_files = task_func('/path/to/source', '/path/to/destination', '.txt')\n# print(moved_files)\n```"}
{"task_id": "BigCodeBench/458", "solution": "```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    def normalize_value(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, str):\n            num_match = re.match(r'^-?\\d+(\\.\\d+)?$', value)\n            if num_match:\n                return float(value) * 2\n            else:\n                return value\n        elif isinstance(value, list):\n            return [normalize_value(item) for item in value]\n        else:\n            return value\n\n    if not json_str:\n        return pd.DataFrame()\n    \n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    if not isinstance(data_dict, dict):\n        return pd.DataFrame()\n\n    normalized_dict = {k: normalize_value(v) for k, v in data_dict.items()}\n    \n    return pd.DataFrame([normalized_dict])\n\n# Example usage\njson_str = '{\"a\": \"2\", \"b\": 3, \"c\": [4, \"5\"], \"d\": \"not a number\"}'\ndf = task_func(json_str)\nprint(df)\n```"}
{"task_id": "BigCodeBench/461", "solution": "```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path '{script_path}' does not exist.\")\n    \n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    pid = process.pid\n    \n    start_time = time.time()\n    cpu_usage = 0.0\n    memory_usage = 0\n    \n    try:\n        while True:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                raise TimeoutError(f\"The script exceeded the timeout of {timeout} seconds.\")\n            \n            try:\n                proc = psutil.Process(pid)\n                if proc.status() == psutil.STATUS_ZOMBIE:\n                    break  # Break if the process is a zombie\n                \n                cpu_usage += proc.cpu_percent(interval=0.1)  # Get CPU usage percentage over 0.1 second\n                memory_usage += proc.memory_info().rss       # Get RSS memory usage in bytes\n            except psutil.NoSuchProcess:\n                break\n            \n            if process.poll() is not None:\n                break  # Process has terminated, exit loop\n            \n    except Exception as e:\n        process.terminate()\n        raise e\n    finally:\n        if process.poll() is None:  # if the process is still running\n            process.terminate()\n    \n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }\n\n# Example usage:\n# result = task_func('/path/to/your/script.sh', timeout=10)\n# print(result)\n```"}
{"task_id": "BigCodeBench/477", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random values for 'x' and 'y'\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Ensure each category appears at least once if N >= number of categories\n    if N >= len(CATEGORIES):\n        category = np.random.choice(CATEGORIES, size=N - len(CATEGORIES), replace=True)\n        category = np.concatenate([category, CATEGORIES])\n        np.random.shuffle(category)\n    else:\n        category = np.random.choice(CATEGORIES, size=N, replace=False)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'x': x, 'y': y, 'category': category})\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=pd.factorize(df['category'])[0], cmap='viridis', alpha=0.6)\n    \n    # Create a legend\n    handles, labels = scatter.legend_elements()\n    legend_labels = {i: cat for i, cat in enumerate(CATEGORIES)}\n    ax.legend(handles, [legend_labels[int(label.split(\"{\")[1].split(\"}\")[0])] for label in labels], title=\"Category\")\n    \n    # Set labels\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter plot of x vs y colored by category')\n    \n    plt.show()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func(N=100)\n```"}
{"task_id": "BigCodeBench/486", "solution": "```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n\n    # Convert epoch times to datetime\n    start_datetime = datetime.fromtimestamp(start_time)\n    end_datetime = datetime.fromtimestamp(end_time)\n\n    # Generate a range of timestamps from start to end with the given step\n    timestamps = pd.date_range(start=start_datetime, end=end_datetime, freq=f'{step}S')\n\n    # Generate values from a normal distribution\n    values = np.random.normal(size=len(timestamps))\n\n    # Add a linear trend to the values\n    trend_values = trend * np.arange(len(timestamps))\n    values_with_trend = values + trend_values\n\n    # Create a DataFrame for easier plotting\n    df = pd.DataFrame({'Time': timestamps, 'Value': values_with_trend})\n\n    # Plot the time series\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    # Show the plot\n    plt.show()\n\n    return ax\n\n# Example usage:\nstart_epoch = 1633046400  # Corresponds to 2021-10-01 00:00:00\nend_epoch = 1633132800    # Corresponds to 2021-10-02 00:00:00\nstep = 3600               # Step in seconds (1 hour)\ntrend = 0.1               # Linear trend value\n\ntask_func(start_epoch, end_epoch, step, trend)\n```"}
{"task_id": "BigCodeBench/492", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for specified products from a given epoch time up to the current time.\n\n    Parameters:\n    epoch_milliseconds (int): The epoch time in milliseconds from which to start generating data.\n    random_seed (int): Seed for the random number generator to ensure reproducibility.\n    products (list of str): List of product names for which to generate sales data.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing sales data with columns 'Product' (string), 'Date' (datetime),\n                  and 'Sales' (integer). Sales quantity is randomly sampled from range [10, 50].\n    \"\"\"\n    \n    # Validate epoch_milliseconds\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch_milliseconds to a datetime object\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000)\n    end_date = datetime.utcnow()\n\n    # Check if start_date is in the future\n    if start_date > end_date:\n        raise ValueError(\"The epoch time provided is in the future\")\n\n    # Generate a list of dates from start_date to end_date\n    date_list = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n\n    # Prepare data storage\n    data = {'Product': [], 'Date': [], 'Sales': []}\n\n    # Generate random sales data for each product and each date\n    for date in date_list:\n        for product in products:\n            sales = random.randint(10, 50)  # Random sales quantity between 10 and 50\n            data['Product'].append(product)\n            data['Date'].append(date)\n            data['Sales'].append(sales)\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    return df\n\n# Example usage:\nepoch_milliseconds = 1609459200000  # January 1, 2021, 00:00:00 UTC in milliseconds\ndf = task_func(epoch_milliseconds)\nprint(df)\n```"}
{"task_id": "BigCodeBench/501", "solution": "```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Validate input type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray.\")\n    \n    # Validate JSON string\n    try:\n        json_data = json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string.\")\n    \n    # Determine the absolute path for the output file\n    abs_path = os.path.abspath(filename)\n    \n    try:\n        # Handle case where JSON data is an empty array\n        if isinstance(json_data, list) and len(json_data) == 0:\n            # Create an empty Excel file\n            workbook = xlwt.Workbook()\n            sheet = workbook.add_sheet(sheet_name)\n            workbook.save(abs_path)\n        else:\n            # Convert JSON data to DataFrame and write to Excel\n            df = pd.DataFrame(json_data)\n            df.to_excel(abs_path, sheet_name=sheet_name, index=False)\n    \n    except Exception as e:\n        raise Exception(f\"Error writing to Excel file: {e}\")\n    \n    return abs_path\n\n# Example usage:\njson_str = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\nfilename = \"output.xls\"\npath = task_func(json_str, filename)\nprint(f\"Excel file created at: {path}\")\n```"}
{"task_id": "BigCodeBench/502", "solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Create an empty list to store the data\n    data = []\n\n    # Generate data for the specified number of days\n    for i in range(days_in_past):\n        date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append([date, activity, duration])\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    # Create a line plot using seaborn\n    plt.figure(figsize=(14, 7))\n    ax = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\", marker=\"o\")\n    \n    # Rotate the x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the DataFrame and plot axes\n    return df, ax\n\n# Example usage\ndf, ax = task_func(days_in_past=7, random_seed=42)\n```"}
{"task_id": "BigCodeBench/503", "solution": "```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generate a DataFrame of random stock prices for a specified number of days in the past.\n\n    Parameters:\n        days_in_past (int): Number of days in the past to generate stock prices for.\n        stock_names (list): List of stock names.\n        random_seed (int): Seed for the random number generator to ensure reproducibility.\n\n    Returns:\n        pd.DataFrame: DataFrame containing random stock prices for the specified number of days.\n    \"\"\"\n    \n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate date range for the specified number of days in the past\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past - 1)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Generate random stock prices\n    data = np.random.rand(days_in_past, len(stock_names))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, index=date_range, columns=stock_names)\n    \n    return df\n\n# Example usage:\ndf = task_func(days_in_past=7)\nprint(df)\n```"}
{"task_id": "BigCodeBench/509", "solution": "Here's a self-contained Python function that compares two CSV files line by line and generates a detailed report of the differences in a pandas DataFrame. The function handles exceptions for file not found, empty files, and other I/O related errors.\n\n```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, 'r', newline='', encoding='utf-8') as file1, open(file_path2, 'r', newline='', encoding='utf-8') as file2:\n            reader1 = list(csv.reader(file1, delimiter=delimiter, quotechar=quotechar))\n            reader2 = list(csv.reader(file2, delimiter=delimiter, quotechar=quotechar))\n            \n            if not reader1:\n                raise ValueError(f\"The file {file_path1} is empty\")\n            if not reader2:\n                raise ValueError(f\"The file {file_path2} is empty\")\n                \n            diff = list(ndiff([''.join(row) for row in reader1], [''.join(row) for row in reader2]))\n\n            differences = []\n            line_number1 = 1\n            line_number2 = 1\n\n            for line in diff:\n                status = line[0]\n                content = line[2:]\n                \n                if status == ' ':\n                    differences.append({'Line Number': line_number1, 'Status': status, 'Content': content})\n                    line_number1 += 1\n                    line_number2 += 1\n                elif status == '-':\n                    differences.append({'Line Number': line_number1, 'Status': status, 'Content': content})\n                    line_number1 += 1\n                elif status == '+':\n                    differences.append({'Line Number': line_number2, 'Status': status, 'Content': content})\n                    line_number2 += 1\n\n            df = pd.DataFrame(differences)\n            return df\n\n    except FileNotFoundError as fnf_error:\n        raise FileNotFoundError(f\"File not found: {fnf_error.filename}\") from fnf_error\n    except ValueError as ve:\n        raise ve\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\") from e\n\n# Example usage:\n# df = task_func('file1.csv', 'file2.csv')\n# print(df)\n```\n\n### Explanation:\n1. **Importing Modules**:\n    - `pandas` for DataFrame creation.\n    - `csv` for reading CSV files.\n    - `ndiff` from `difflib` to compute differences between files.\n\n2. **Reading Files**:\n    - The files are opened using the `with` statement to ensure they are properly closed afterward.\n    - The CSV files are read into lists of rows.\n\n3. **Empty File Checking**:\n    - Raises `ValueError` if either file is empty.\n\n4. **Computing Differences**:\n    - `ndiff` is used to get a diff-like output where:\n        - `' '` indicates no change.\n        - `'-'` indicates a line present in the first file but not in the second.\n        - `'+'` indicates a line present in the second file but not in the first.\n\n5. **Building the DataFrame**:\n    - Differences are appended to a list with details and then converted to a pandas DataFrame.\n\n6. **Exception Handling**:\n    - `FileNotFoundError` for missing files.\n    - `ValueError` for empty files.\n    - General `Exception` for other I/O related errors.\n\nThis function reads two CSV files, compares them line by line, and provides a detailed difference report in the form of a pandas DataFrame."}
{"task_id": "BigCodeBench/511", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Convert data to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Initialize the statistics dictionary\n    stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n    \n    if not df.empty and column in df.columns:\n        # Check if the column contains numeric data\n        if pd.api.types.is_numeric_dtype(df[column]):\n            # Calculate statistics\n            stats['sum'] = df[column].sum()\n            stats['mean'] = df[column].mean()\n            stats['min'] = df[column].min()\n            stats['max'] = df[column].max()\n        else:\n            print(f\"The column '{column}' does not contain numeric data.\")\n            return stats, None\n    \n    # Create a pie chart using the Age column as labels\n    fig, ax = plt.subplots()\n    if not df.empty and 'Age' in df.columns and pd.api.types.is_numeric_dtype(df[column]):\n        ax.pie(df[column].dropna(), labels=df['Age'].dropna(), autopct='%1.1f%%')\n        ax.set_title(f'Pie Chart of {column}')\n    else:\n        ax.text(0.5, 0.5, 'No data available', horizontalalignment='center', verticalalignment='center')\n        ax.axis('off')\n    \n    return stats, ax\n\n# Example usage\ndata = [\n    {'Name': 'Alice', 'Age': 30, 'Salary': 70000},\n    {'Name': 'Bob', 'Age': 35, 'Salary': 80000},\n    {'Name': 'Charlie', 'Age': 40, 'Salary': 90000}\n]\n\ncolumn = 'Salary'\nstats, ax = task_func(column, data)\nprint(stats)\nplt.show()\n```"}
{"task_id": "BigCodeBench/513", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze fitness data, calculate statistics, and plot a line chart.\n\n    Parameters:\n    column (str): The column to analyze.\n    data (list): A list of dictionaries with fitness data.\n\n    Returns:\n    tuple: A tuple containing:\n        dict: A dictionary with the sum, mean, min, and max of the column.\n        matplotlib.axes.Axes: The Axes object of the plotted line chart.\n    \"\"\"\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    df = pd.DataFrame(data)\n    \n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n    \n    if 'Date' not in df.columns:\n        raise KeyError(\"The 'Date' column is missing from the data.\")\n    \n    # Ensure 'Date' is in datetime format\n    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n    if df['Date'].isnull().any():\n        raise ValueError(\"The 'Date' column contains invalid date formats.\")\n    \n    # Check for non-negative values in steps, calories burned, and distance walked\n    for col in ['steps', 'calories_burned', 'distance_walked']:\n        if col in df.columns and (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in '{col}'.\")\n    \n    # Calculate statistics\n    column_data = df[column]\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n    \n    # Plotting the line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], column_data, marker='o')\n    ax.set_title(f\"Line Chart of {column}\")\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(column)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return stats, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [\n        {'Date': '2023-01-01', 'steps': 10000, 'calories_burned': 300, 'distance_walked': 8},\n        {'Date': '2023-01-02', 'steps': 15000, 'calories_burned': 400, 'distance_walked': 10},\n        {'Date': '2023-01-03', 'steps': 8000, 'calories_burned': 250, 'distance_walked': 6},\n        # Add more data as needed\n    ]\n\n    # Call the function with column 'steps' and the data list\n    try:\n        stats, ax = task_func('steps', data)\n        print(stats)\n        plt.show()  # Show the plot\n    except (KeyError, ValueError) as e:\n        print(e)\n```"}
{"task_id": "BigCodeBench/526", "solution": "```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    Read a list of dictionaries from a JSON file, calculate the mean and median for each key \n    (ignoring non-numeric or missing values), and convert the results into a Pandas DataFrame.\n    \n    Parameters:\n    input_file (str): Path to the JSON file containing a list of dictionaries.\n\n    Returns:\n    pd.DataFrame: A DataFrame indexed and sorted by the variable names (keys) from the input data,\n                  containing columns 'mean' and 'median'.\n    \"\"\"\n    try:\n        with open(input_file, 'r') as file:\n            data = json.load(file)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        raise ValueError(\"Error reading the JSON file.\") from e\n    \n    values_dict = defaultdict(list)\n    \n    for entry in data:\n        if isinstance(entry, dict):\n            for key, value in entry.items():\n                if isinstance(value, (int, float)):\n                    values_dict[key].append(value)\n    \n    stats = {\n        'variable': [],\n        'mean': [],\n        'median': []\n    }\n    \n    for key, values in values_dict.items():\n        if values:  # Only calculate stats if there are numeric values\n            stats['variable'].append(key)\n            stats['mean'].append(np.mean(values))\n            stats['median'].append(np.median(values))\n    \n    df = pd.DataFrame(stats)\n    df.set_index('variable', inplace=True)\n    df.sort_index(inplace=True)\n    \n    return df\n```\n\nThis code implements the requested functionality, including error handling for reading the JSON file, ignoring non-numeric or missing values, and checking that the input contains dictionaries."}
{"task_id": "BigCodeBench/528", "solution": "```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check file extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file provided is not a CSV file.\")\n    \n    duplicate_counts = Counter()\n    rows = []\n\n    # Read the CSV file and identify duplicates\n    with open(file_path, mode='r', newline='') as file:\n        reader = csv.reader(file)\n        try:\n            header = next(reader)  # Assuming the first row is the header\n        except StopIteration:\n            # Handle empty file\n            raise ValueError(\"The CSV file is empty.\")\n        \n        for row in reader:\n            row_tuple = tuple(row)\n            rows.append(row_tuple)\n            duplicate_counts[row_tuple] += 1\n\n    # Filter out rows that are not duplicates\n    duplicates = {row: count for row, count in duplicate_counts.items() if count > 1}\n\n    if not duplicates:\n        raise ValueError(\"No duplicate rows found in the CSV file.\")\n\n    # Convert duplicates to a DataFrame\n    duplicate_rows = [list(row) for row in duplicates.keys()]\n    df_duplicates = pd.DataFrame(duplicate_rows, columns=header)\n\n    # Plot the duplicate rows\n    fig, ax = plt.subplots()\n    ax.bar(range(len(duplicates)), duplicates.values(), tick_label=[str(row) for row in duplicates.keys()])\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_title('Duplicate Rows Count')\n\n    # Improve plot readability if there are many duplicate rows\n    if len(duplicates) > 10:\n        plt.xticks(rotation=90, fontsize=8)\n    plt.tight_layout()\n    plt.show()\n\n    return duplicates, ax\n\n# Example usage\n# duplicates, ax = task_func('your_file_path.csv')\n```"}
{"task_id": "BigCodeBench/530", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes or None):\n    \"\"\"\n    Identify duplicate entries in a DataFrame and record the age distribution for the duplicate names.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame with 'name' and 'age' columns.\n\n    Returns:\n    Counter: Age distribution among duplicate names.\n    plt.Axes or None: Histogram plot displaying age distribution, or None if there are no duplicates.\n    \n    Raises:\n    ValueError: If the DataFrame is empty or if any age is negative.\n    \"\"\"\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure age is rounded down to the nearest integer and check for negative ages\n    df['age'] = df['age'].apply(lambda x: int(np.floor(x)))\n    \n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated('name', keep=False)]\n    \n    # If there are no duplicates, return an empty Counter and None\n    if duplicates.empty:\n        return Counter(), None\n    \n    # Create the age distribution Counter for duplicate names\n    age_distribution = Counter(duplicates['age'])\n    \n    # Plot histogram of the age distribution for duplicate names\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    \n    # Compute bin edges\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(duplicates['age'], bins=bins, kde=False)\n    ax.set(title='Age Distribution of Duplicate Names', xlabel='Age', ylabel='Count')\n    plt.show()\n    \n    return age_distribution, ax\n\n# Example usage:\n# df = pd.DataFrame({'name': ['Alice', 'Bob', 'Alice', 'Bob', 'Charlie'], 'age': [25.0, 30.5, 25.8, 32.1, 22]})\n# task_func(df)\n```"}
{"task_id": "BigCodeBench/532", "solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, bins=4):\n    # Validate input DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column\")\n    \n    # Count duplicate values in the 'value' column\n    value_counts = df['value'].value_counts()\n    duplicates = value_counts[value_counts > 1]\n    counter = Counter(duplicates.to_dict())\n\n    # Plot the histogram with the normal distribution curve\n    fig, ax = plt.subplots()\n    values = df['value'].dropna()  # Drop NaNs for plotting\n    n, bins, patches = ax.hist(values, bins=bins, color='green', alpha=0.6, density=True)\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(values)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Add titles and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    # Show the plot\n    plt.show()\n\n    return (counter, ax)\n\n# Example usage:\n# df = pd.DataFrame({'value': np.random.normal(0, 1, 1000)})\n# result = task_func(df)\n```"}
{"task_id": "BigCodeBench/553", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b',\n    and plots it as a bar chart.\n\n    Parameters:\n    a (list): List of row indices.\n    b (list): List that determines the number of columns.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    \n    # Validate inputs\n    if not a:\n        raise ValueError(\"List 'a' must be non-empty.\")\n    if not b:\n        raise ValueError(\"List 'b' must be non-empty.\")\n    if len(b) > len(COLUMNS):\n        raise ValueError(f\"List 'b' length must not exceed {len(COLUMNS)}.\")\n    \n    # Determine the number of columns based on the length of list 'b'\n    num_columns = len(b)\n    column_names = COLUMNS[:num_columns]\n    \n    # Create a DataFrame with random values\n    data = np.random.rand(len(a), num_columns)\n    df = pd.DataFrame(data, index=a, columns=column_names)\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    plt.title('Random Data Bar Chart')\n    plt.xlabel('Row Indices')\n    plt.ylabel('Values')\n    plt.show()\n    \n    # Return the Axes object\n    return ax\n\n# Example usage\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3]\ntask_func(a, b)\n```"}
{"task_id": "BigCodeBench/560", "solution": "```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n    \n    Parameters:\n    data (pd.DataFrame): DataFrame containing 'month' and 'value' columns. \n                         'month' should be in datetime format.\n    \n    Returns:\n    matplotlib.axes.Axes: Axes object representing the plot.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    \n    # Ensure the 'month' column is in datetime format\n    if not pd.api.types.is_datetime64_any_dtype(data['month']):\n        data['month'] = pd.to_datetime(data['month'])\n    \n    # Ensure all dates are in the same year\n    years = data['month'].dt.year.unique()\n    if len(years) != 1:\n        raise ValueError(\"All months must be in the same year.\")\n    \n    year = years[0]\n    \n    # Generate the plot\n    fig, ax = plt.subplots()\n    ax.bar(data['month'].dt.strftime('%B'), data['value'])\n    \n    # Set the title and labels\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Rotate x-tick labels for better readability\n    plt.xticks(rotation=45, ha='right')\n    \n    # Show the plot for interactive environments\n    plt.tight_layout()\n    plt.show()\n    \n    return ax\n\n# Sample usage\nif __name__ == \"__main__\":\n    # Sample data\n    sample_data = {\n        'month': pd.date_range(start='2023-01-01', periods=12, freq='M'),\n        'value': [10, 20, 15, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n    }\n    df = pd.DataFrame(sample_data)\n    \n    # Plot the data\n    task_func(df)\n```"}
{"task_id": "BigCodeBench/567", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the string of numeric values to a pandas Series\n    data_series = pd.Series(map(float, data.split()))\n\n    # Calculate the bins\n    bins = np.arange(data_series.min(), data_series.max() + 2) - 0.5\n\n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_series, bins=bins, edgecolor='black')\n\n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n\n    # Show the plot (optional if you want to display the plot immediately)\n    plt.show()\n\n    return ax\n\n# Example usage:\n# data_str = \"1 2 2 3 3 3 4 4 4 4\"\n# ax = task_func(data_str)\n```"}
{"task_id": "BigCodeBench/574", "solution": "```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    Generates a noisy sine wave and fits a curve to the data using curve_fit from scipy.optimize.\n    \n    Parameters:\n    array_length (int): The number of data points to generate.\n    noise_level (float): The standard deviation of the Gaussian noise added to the sine wave.\n    \n    Returns:\n    matplotlib.axes._subplots.AxesSubplot: The Axes object containing the plot.\n    \"\"\"\n    \n    # Define the true sine function\n    def true_sine(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n\n    # Generate x values\n    x_values = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate clean sine wave data\n    amplitude, frequency, phase, offset = 1, 1, 0, 0\n    clean_data = true_sine(x_values, amplitude, frequency, phase, offset)\n    \n    # Add noise to the data\n    noise = noise_level * np.random.normal(size=array_length)\n    noisy_data = clean_data + noise\n    \n    # Fit the noisy data to the sine function\n    try:\n        popt, _ = curve_fit(true_sine, x_values, noisy_data, p0=[1, 1, 0, 0])\n    except RuntimeError as e:\n        print(f\"An error occurred during curve fitting: {e}\")\n        return None\n    \n    # Generate the fitted curve data\n    fitted_data = true_sine(x_values, *popt)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(x_values, noisy_data, 'b.', label='Noisy data')\n    ax.plot(x_values, fitted_data, 'r-', label='Fitted curve')\n    ax.legend()\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\n\n# Example usage\ntask_func()\n```"}
{"task_id": "BigCodeBench/579", "solution": "```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    Reads a CSV file, normalizes the text in it to ASCII, counts the words, \n    and returns the 10 most common words along with their frequencies as a \n    matplotlib bar plot and a list of tuples.\n\n    Parameters:\n    csv_file (str): The path to the CSV file.\n\n    Returns:\n    tuple: A tuple containing matplotlib.axes.Axes object for the bar plot and \n           a list of the 10 most common words with their frequencies.\n    \n    Raises:\n    FileNotFoundError: If the CSV file cannot be found at the specified path.\n    IOError: If there is an error in reading the file.\n    \"\"\"\n    # Initialize an empty string to accumulate text\n    text_data = \"\"\n    \n    try:\n        # Open and read the CSV file\n        with open(csv_file, newline='', encoding='utf-8') as f:\n            reader = csv.reader(f)\n            for row in reader:\n                # Assuming that the text data is in the first column\n                text_data += \" \".join(row) + \" \"\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {csv_file} was not found.\")\n    \n    except IOError:\n        raise IOError(f\"An error occurred while reading the file {csv_file}.\")\n    \n    # Check if text_data is empty\n    if not text_data.strip():\n        raise ValueError(\"The CSV file is empty or contains no valid text data.\")\n    \n    # Normalize text to ASCII\n    normalized_text = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('ascii')\n    \n    # Split text into words and count the occurrences\n    words = normalized_text.split()\n    word_count = Counter(words)\n    \n    # Get the 10 most common words and their frequencies\n    common_words = word_count.most_common(10)\n    \n    # Handle the case where there are less than 10 unique words\n    if len(common_words) == 0:\n        raise ValueError(\"The text data contains no valid words to analyze.\")\n    \n    # Unzip the words and their frequencies for plotting\n    words, frequencies = zip(*common_words)\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return ax, common_words\n\n# Example usage:\n# ax, common_words = task_func('path_to_your_file.csv')\n# plt.show()  # This will display the plot\n# print(common_words)  # This will print the list of tuples with the 10 most common words and their frequencies\n```"}
{"task_id": "BigCodeBench/582", "solution": "```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    \"\"\"\n    Generate a normally distributed random sample, plot its histogram and overlay the PDF.\n\n    Parameters:\n    size (int): The number of samples to generate. Default is 1000.\n\n    Returns:\n    matplotlib.figure.Figure: A figure object containing the histogram and PDF plot.\n    \"\"\"\n    # Generate a normally distributed random sample\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a histogram of the data\n    fig, ax = plt.subplots()\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Calculate the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, loc=0, scale=1)\n    \n    # Plot the PDF\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Calculate and set the title with dynamic mean and std\n    mu, std = np.mean(data), np.std(data)\n    title = f\"Fit results: mu = {mu:.2f},  std = {std:.2f}\"\n    ax.set_title(title)\n    \n    return fig\n\n# Example usage\nfig = task_func()\nplt.show()\n```"}
{"task_id": "BigCodeBench/583", "solution": "```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair\n    pubkey, privkey = rsa.newkeys(2048)\n    \n    # Serialize the private key\n    privkey_bytes = privkey.save_pkcs1()\n    \n    # Generate a random encryption password and nonce for AES\n    password = get_random_bytes(32)  # AES-256 requires a 32 byte key\n    nonce = get_random_bytes(16)  # 16 bytes for the nonce\n    \n    # Encrypt the private key using AES-GCM\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    ciphertext, tag = cipher.encrypt_and_digest(privkey_bytes)\n    \n    # Generate a random 8 byte number for the filename\n    random_bytes = get_random_bytes(8)\n    hex_representation = random_bytes.hex()\n    filename = f\"private_key_{hex_representation}.txt\"\n    \n    # Save the encrypted private key and tag to a file\n    with open(filename, 'wb') as file:\n        file.write(ciphertext)\n        file.write(tag)  # Save the authentication tag at the end of the file\n    \n    # Return the required values\n    return pubkey, filename, password, nonce\n\n# Example usage\nif __name__ == \"__main__\":\n    pubkey, filename, password, nonce = task_func()\n    print(\"Public Key:\", pubkey)\n    print(\"Filename:\", filename)\n    print(\"Password (base64):\", b64encode(password).decode())\n    print(\"Nonce (base64):\", b64encode(nonce).decode())\n```"}
{"task_id": "BigCodeBench/587", "solution": "```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n\n    # Read the file to encrypt\n    with open(file_path, 'rb') as f:\n        file_data = f.read()\n\n    # Generate AES key\n    aes_key = os.urandom(32)  # 256-bit AES key\n\n    # AES encryption\n    iv = os.urandom(16)  # Initialization vector\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad the data to be AES block size (16 bytes)\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n\n    # Encrypt the data\n    encrypted_file_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(iv + encrypted_file_data)  # Prepend the IV for decryption\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Save the encrypted AES key\n    encrypted_aes_key_path = file_path + '.key.enc'\n    with open(encrypted_aes_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    return public_key, encrypted_file_path, encrypted_aes_key_path\n\n# Example usage\n# public_key, enc_file, enc_key = task_func('example.txt')\n# print(f\"Public Key: {public_key}\")\n# print(f\"Encrypted File: {enc_file}\")\n# print(f\"Encrypted AES Key File: {enc_key}\")\n```"}
{"task_id": "BigCodeBench/590", "solution": "```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nfrom urllib.error import URLError\nfrom urllib.parse import urljoin\nimport validators\n\ndef task_func(url):\n    # Validate the URL\n    if not url or not isinstance(url, str) or not validators.url(url):\n        raise ValueError(\"The provided URL is invalid or empty.\")\n    \n    try:\n        # Fetch the HTML content from the URL\n        response = urllib.request.urlopen(url, timeout=10)\n        html_content = response.read().decode('utf-8')\n    except URLError as e:\n        raise URLError(f\"There is an issue with network connectivity or the server: {e}\")\n    \n    # Parse the HTML content\n    document = pq(html_content)\n    \n    # Extract anchor tags\n    anchors = document('a')\n    \n    # Collect data\n    data = []\n    fetch_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    for anchor in anchors.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        if href:\n            href = urljoin(url, href)\n        data.append({'text': text, 'href': href, 'fetch_time': fetch_time})\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['text', 'href', 'fetch_time'])\n    \n    return df\n\n# Example usage (uncomment the following lines to test the function)\n# url = \"https://example.com\"\n# print(task_func(url))\n```"}
{"task_id": "BigCodeBench/592", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generate sensor data for the specified number of hours and save it to a CSV file.\n\n    Args:\n        hours (int): Number of hours to generate data for.\n        output_dir (str): Directory to save the CSV file. Defaults to './output'.\n    \"\"\"\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate file path\n    filename = os.path.join(output_dir, 'sensor_data.csv')\n    \n    # Calculate end time\n    end_time = datetime.now()\n    start_time = end_time - timedelta(hours=hours)\n    \n    # Generate data\n    data = []\n    current_time = start_time\n    while current_time <= end_time:\n        temperature = randint(-10, 40)  # Random temperature between -10 and 40 degrees Celsius\n        humidity = randint(0, 100)      # Random humidity between 0 and 100%\n        pressure = randint(950, 1050)   # Random pressure between 950 and 1050 hPa\n        data.append([current_time.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n        current_time += timedelta(hours=1)\n    \n    # Write data to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writerows(data)\n    \n    print(f\"Data for {hours} hours has been written to {filename}\")\n\n# Example Usage:\n# task_func(24)\n```"}
{"task_id": "BigCodeBench/593", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # File path for the CSV file\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    \n    # Initialize empty data list\n    data = []\n    \n    # Generate data for each hour\n    start_time = datetime.now()\n    for i in range(hours):\n        time = start_time + timedelta(hours=i)\n        row = {\n            'Time': time.strftime('%Y-%m-%d %H:%M:%S'),\n            'Car': randint(0, 100),\n            'Bus': randint(0, 20),\n            'Truck': randint(0, 50),\n            'Bike': randint(0, 200)\n        }\n        data.append(row)\n    \n    # Write data to CSV file\n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Time'] + VEHICLE_TYPES\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Load data into a pandas DataFrame\n    df = pd.read_csv(file_path, parse_dates=['Time'])\n    \n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    for vehicle_type in VEHICLE_TYPES:\n        plt.plot(df['Time'], df[vehicle_type], label=vehicle_type)\n    \n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data Over Time')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n    \n    # Save the plot\n    plot_file_path = os.path.join(output_dir, 'traffic_plot.png')\n    plt.savefig(plot_file_path)\n    \n    # Show the plot\n    ax = plt.gca()\n    plt.show()\n    \n    return file_path, ax\n\n# Example usage:\ncsv_path, plot_ax = task_func(24)\nprint(f\"CSV file saved at: {csv_path}\")\n```"}
{"task_id": "BigCodeBench/594", "solution": "```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(BACKUP_DIR, exist_ok=True)\n    \n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    for i in range(hours):\n        weather_data.append({\n            'Time': current_time.strftime('%Y-%m-%d %H:%M'),\n            'Condition': choice(WEATHER_CONDITIONS)\n        })\n        current_time += timedelta(hours=1)\n    \n    # Create CSV file path\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    \n    # Write data to CSV file\n    with open(csv_file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Time', 'Condition']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for data in weather_data:\n            writer.writerow(data)\n    \n    # Backup the file\n    shutil.copy(csv_file_path, BACKUP_DIR)\n    \n    return csv_file_path\n\n# Example usage:\n# csv_path = task_func(24)\n# print(f\"CSV file generated at: {csv_path}\")\n```"}
{"task_id": "BigCodeBench/618", "solution": "```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(num_matches=5, seed=None):\n    if seed is not None:\n        import random\n        random.seed(seed)\n    \n    # Generate random match results\n    data = {\n        'Team': [team for team in TEAMS for _ in range(num_matches)],\n        'Goals': [randint(0, 5) for _ in range(len(TEAMS) * num_matches)],\n        'Penalties': [randint(0, 3) for _ in range(len(TEAMS) * num_matches)]\n    }\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate Penalty Costs\n    df['Penalty Cost'] = df['Penalties'] * PENALTY_COST\n    \n    # Plot Goals\n    plt.figure(figsize=(14, 6))\n    ax1 = sns.barplot(x='Team', y='Goals', data=df, estimator=sum, ci=None)\n    ax1.set_title('Total Goals by Team')\n    ax1.set_ylabel('Total Goals')\n    ax1.set_xlabel('Team')\n    \n    # Plot Penalty Costs\n    plt.figure(figsize=(14, 6))\n    ax2 = sns.barplot(x='Team', y='Penalty Cost', data=df, estimator=sum, ci=None)\n    ax2.set_title('Total Penalty Costs by Team')\n    ax2.set_ylabel('Total Penalty Costs ($)')\n    ax2.set_xlabel('Team')\n    \n    # Show plots\n    plt.show()\n    \n    return df, [ax1, ax2]\n\n# Calling the function\ndf, axes = task_func(num_matches=5, seed=42)\nprint(df)\n```"}
{"task_id": "BigCodeBench/636", "solution": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Generate a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Count the non-zero values in each column and visualize this information using a bar plot.\n    \n    Args:\n        rows (int): The number of rows in the DataFrame.\n    \n    Returns:\n        tuple: A tuple containing the following elements:\n            DataFrame: The generated DataFrame with random integer values.\n            Axes: The matplotlib Axes object containing the bar plot.\n    \"\"\"\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"The 'rows' parameter must be a positive integer.\")\n    \n    # Generate a DataFrame with random integer values between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count the non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Values in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count of Non-Zero Values')\n    \n    # Return the DataFrame and Axes object\n    return df, ax\n\n# Example usage:\n# df, ax = task_func(100)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/637", "solution": "```python\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Optional: Set a random seed for reproducibility\n    # np.random.seed(42)\n\n    # Generate a list of student names\n    students = [f'Student_{i+1}' for i in range(num_students)]\n    \n    # Define the courses\n    courses = ['Math', 'Science', 'History', 'Art', 'Physical_Education']\n    \n    # Generate random grades for each student in each course\n    grades = {course: np.random.randint(50, 101, size=num_students) for course in courses}\n    \n    # Create DataFrame\n    df = pd.DataFrame(grades, index=students)\n    \n    # Calculate average grade for each course\n    average_grades = df.mean()\n    \n    # Calculate the number of students with a passing grade (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create a new DataFrame for the plot\n    summary_df = pd.DataFrame({\n        'Average Grade': average_grades,\n        'Passing Count': passing_counts\n    })\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    summary_df.plot(kind='bar', ax=ax)\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_ylabel('Counts / Average Grade')\n    ax.set_xlabel('Courses')\n    ax.legend(['Average Grade', 'Passing Count'])\n    \n    # Return the DataFrame and the Axes object\n    return df, ax\n\n# Example usage:\n# df, ax = task_func(10)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/654", "solution": "```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef exponential_decay(x, a, b, c):\n    \"\"\"Exponential decay function.\"\"\"\n    return a * np.exp(-b * x) + c\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n\n    Args:\n        array (np.ndarray): 2D array with values to fit.\n        target_value (int or float): Value to match in the first column.\n\n    Returns:\n        tuple: Containing the optimized parameters of the fitting function (popt) and the matplotlib Axes object.\n    \"\"\"\n    # Check if the input array is a numpy array\n    if not isinstance(array, np.ndarray):\n        raise TypeError(\"Input array must be a numpy array.\")\n    \n    # Check if the array has two columns\n    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have exactly two columns.\")\n    \n    # Extract the indices where the first column matches the target value\n    indices = np.where(array[:, 0] == target_value)[0]\n    if len(indices) == 0:\n        raise ValueError(f\"No matching values found in the first column for target value {target_value}.\")\n\n    # Check if there are sufficient data points for fitting\n    if len(indices) < 3:\n        raise ValueError(\"Not enough data points to perform fitting. At least 3 points are required.\")\n\n    # Define x data as the indices and y data as the values in the second column\n    x_data = indices\n    y_data = array[indices, 1]\n\n    # Initial guess for the parameters\n    initial_guess = (1.0, 1.0, 1.0)\n\n    try:\n        # Fit the exponential decay function to the data\n        popt, _ = optimize.curve_fit(exponential_decay, x_data, y_data, p0=initial_guess)\n    except RuntimeError as e:\n        raise RuntimeError(f\"An error occurred during fitting: {e}\")\n\n    # Plot the data and the fit\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Data')\n    ax.plot(x_data, exponential_decay(x_data, *popt), label='Fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt), color='red')\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title('Exponential Decay Fit')\n\n    return popt, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example array\n    example_array = np.array([\n        [1, 10],\n        [1, 8],\n        [1, 6],\n        [2, 5],\n        [1, 4],\n        [2, 3],\n        [1, 2],\n        [2, 1]\n    ])\n    target_value = 1\n    try:\n        popt, ax = task_func(example_array, target_value)\n        plt.show()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```"}
{"task_id": "BigCodeBench/655", "solution": "Here's the complete self-contained code that performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF):\n\n```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef preprocess_text(text):\n    # Remove non-alphanumeric characters (excluding spaces)\n    text = ALPHANUMERIC.sub(' ', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    words = text.split()\n    filtered_words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(filtered_words)\n\ndef task_func(texts, num_topics, num_words=10):\n    # Preprocess the input texts\n    processed_texts = [preprocess_text(text) for text in texts]\n    \n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=1)\n    nmf.fit(tfidf_matrix)\n    \n    # Get the words corresponding to the TF-IDF vocabulary\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Extract topics and their most significant words\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        # Get the indices of the most significant words for this topic\n        significant_word_indices = topic.argsort()[:-num_words - 1:-1]\n        significant_words = [feature_names[i] for i in significant_word_indices]\n        topics.append(significant_words)\n    \n    return topics\n\n# Example usage\nif __name__ == \"__main__\":\n    texts = [\n        \"Natural Language Processing and Machine Learning are amazing fields.\",\n        \"Artificial Intelligence and Deep Learning are subsets of Machine Learning.\",\n        \"NLP involves text processing, tokenization, stemming, and lemmatization.\",\n        \"Deep Learning uses neural networks for various applications.\",\n        \"Machine Learning models can be supervised or unsupervised.\"\n    ]\n\n    num_topics = 3\n    topics = task_func(texts, num_topics)\n    for i, topic in enumerate(topics):\n        print(f\"Topic {i + 1}: {', '.join(topic)}\")\n```\n\nThis code performs the following steps:\n1. **Preprocesses the input texts** by removing non-alphanumeric characters, converting to lowercase, and removing stopwords.\n2. **Vectorizes the processed texts** using TF-IDF.\n3. **Applies NMF** to extract the specified number of topics.\n4. **Returns a list of topics**, where each topic is represented by a list of its most significant words.\n\nThe example usage at the end demonstrates how to use the `task_func` function to extract topics from a sample collection of texts. The number of topics to extract is set to 3, and the output will be printed, showing the most significant words for each topic."}
{"task_id": "BigCodeBench/657", "solution": "Certainly! Here is the self-contained code that meets the requirements:\n\n```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords as nltk_stopwords\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text, stopwords):\n    # Remove all non-alphanumeric characters except space and lowercase the text\n    text = ALPHANUMERIC.sub(' ', text).lower()\n    # Tokenize the text\n    tokens = text.split()\n    # Remove stop words\n    tokens = [token for token in tokens if token not in stopwords]\n    return tokens\n\ndef task_func(texts, stopwords=None):\n    # If no stopwords list is provided, use NLTK's English stopwords\n    if stopwords is None:\n        nltk.download('stopwords')  # Ensure stopwords are downloaded\n        stopwords = set(nltk_stopwords.words('english'))\n    \n    # Clean and tokenize each text\n    cleaned_texts = [clean_text(text, stopwords) for text in texts]\n    \n    # Train Word2Vec model\n    model = Word2Vec(sentences=cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model\n\n# Example usage:\ntexts = [\n    \"This is the first text.\",\n    \"Here's the second text! It has more words.\",\n    \"And finally, the third text.\"\n]\n\nmodel = task_func(texts)\nprint(model)  # Outputs the trained Word2Vec model\n\n# To see the vocabulary of the model\nprint(model.wv.key_to_index)\n\n# To get the vector of a specific word\nword = 'text'\nif word in model.wv.key_to_index:\n    print(model.wv[word])\nelse:\n    print(f\"The word '{word}' is not in the vocabulary.\")\n```\n\nThis code snippet:\n1. Imports the necessary libraries.\n2. Defines a regular expression for cleaning text.\n3. Implements a `clean_text` function to preprocess the text.\n4. Implements the `task_func` function to clean the input texts, train a Word2Vec model, and return the model.\n5. Provides an example usage to demonstrate how to use the `task_func` and inspect the resulting Word2Vec model."}
{"task_id": "BigCodeBench/678", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a directory, reads each JSON file alphabetically into a DataFrame, \n    and inserts a \"Source\" column that specifies the filename. The processed files are then \n    moved to a \"processed\" subdirectory. If the path does not exist, it will be created.\n\n    Parameters:\n    path (str): The directory path containing the JSON files.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the data from all processed files.\n    \"\"\"\n    \n    # Ensure the directory exists\n    if not os.path.exists(path):\n        os.makedirs(path)\n    \n    if not os.path.isdir(path):\n        raise ValueError(f\"The provided path '{path}' is not a valid directory.\")\n    \n    # Ensure the \"processed\" subdirectory exists\n    processed_dir = os.path.join(path, \"processed\")\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n    \n    # List all JSON files in the directory\n    files = [f for f in os.listdir(path) if f.endswith('.json')]\n    files.sort()  # Sort files alphabetically\n    \n    # Initialize an empty DataFrame\n    df = pd.DataFrame()\n    \n    # Process each file\n    for file in files:\n        file_path = os.path.join(path, file)\n        try:\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n        except (json.JSONDecodeError, IOError) as e:\n            print(f\"Error reading {file}: {e}\")\n            continue\n        \n        # Convert JSON data to DataFrame\n        temp_df = pd.DataFrame(data)\n        \n        # Add the \"Source\" column\n        temp_df['Source'] = file\n        \n        # Append to the main DataFrame\n        df = pd.concat([df, temp_df], ignore_index=True)\n        \n        # Move the processed file to the \"processed\" directory\n        try:\n            shutil.move(file_path, os.path.join(processed_dir, file))\n        except IOError as e:\n            print(f\"Error moving {file}: {e}\")\n    \n    return df\n\n# Example usage:\n# path = \"your_directory_path_here\"\n# result_df = task_func(path)\n# print(result_df)\n```"}
{"task_id": "BigCodeBench/720", "solution": "```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define file path\n    directory = \"task_func_data\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    file_path = os.path.join(directory, \"Output.txt\")\n\n    # Generate random sensor data\n    data = []\n    for _ in range(10):  # Generate 10 rows of data\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        temperature = round(random.uniform(-10, 40), 2)  # Simulate temperature between -10 and 40 degrees Celsius\n        humidity = round(random.uniform(0, 100), 2)  # Simulate humidity between 0 and 100%\n        data.append([timestamp, temperature, humidity])\n\n    # Write data to CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Timestamp\", \"Temperature\", \"Humidity\"])\n        writer.writerows(data)\n\n    # Output the file path before deletion\n    print(f\"File created at: {file_path}\")\n\n    # Return the file path before deletion\n    file_path_before_deletion = file_path\n\n    # Delete the file\n    os.remove(file_path)\n\n    return file_path_before_deletion\n\n# Test the function\nprint(task_func())\n```"}
{"task_id": "BigCodeBench/723", "solution": "```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Fetches data from the given URL and saves it to a CSV file.\n\n    Args:\n    url (str): The URL of the webpage to scrape.\n\n    Returns:\n    str: The path of the CSV file where the scraped data is saved.\n    \"\"\"\n    try:\n        # Fetch the content from the URL\n        response = urllib.request.urlopen(url)\n        web_content = response.read()\n\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(web_content, 'html.parser')\n\n        # Example: Extracting all table rows\n        # This is a general example; you should modify the parsing logic based on the structure of the webpage\n        table_rows = soup.find_all('tr')\n\n        # Prepare data for CSV\n        data = []\n        for row in table_rows:\n            cols = row.find_all('td')\n            if cols:  # Ensure cols is not empty\n                cols = [col.text.strip() for col in cols]\n                data.append(cols)\n\n        # Write data to CSV file\n        with open(CSV_FILE_PATH, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerows(data)\n\n        print(f\"Data has been scraped and saved to {CSV_FILE_PATH}\")\n        return CSV_FILE_PATH\n\n    except urllib.error.URLError as e:\n        print(f\"URL Error: {e.reason}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    example_url = 'http://example.com'  # Replace with the actual URL to scrape\n    task_func(example_url)\n```"}
{"task_id": "BigCodeBench/752", "solution": "```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data is not a DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data is empty.\")\n    \n    # Check if target_column is in data\n    if target_column not in data.columns:\n        raise ValueError(f\"'{target_column}' is not a column in the data.\")\n    \n    # Check if all columns contain numeric values\n    if not all(pd.api.types.is_numeric_dtype(data[col]) for col in data.columns):\n        raise ValueError(\"Data contains values that are not numeric.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state is not an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"test_size is not between 0 and 1.\")\n    \n    # Split the data into training and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    score = model.score(X_test, y_test)\n    \n    return score\n\n# Example usage:\nif __name__ == \"__main__\":\n    rng = np.random.default_rng(seed=0)\n    data = pd.DataFrame({\n        'x1': rng.random(500),\n    })\n    data['y'] = data['x1'] * 2 + 1\n    \n    result = task_func(data, 'y', random_state=9, test_size=0.1)\n    print(result)  # Expected output is 1.0\n```"}
{"task_id": "BigCodeBench/760", "solution": "Here's the complete implementation of the `task_func` function as requested. This function will generate a DataFrame with 100 records, each containing an ID, Name, Date of Birth, and Email. The names are randomly selected from the provided lists, and dates of birth are generated within the specified range. Improperly encoded Latin characters in names are corrected.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n\n    # Validate year range\n    current_year = datetime.datetime.now().year\n    if start_year < 0 or end_year > current_year:\n        raise ValueError(\"Please provide a start_year and end_year within a reasonable range (e.g., 0 to current year).\")\n    if start_year > end_year:\n        raise ValueError(\"start_year cannot be greater than end_year.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Total number of records\n    num_records = 100\n    \n    # Generate IDs\n    ids = np.arange(1, num_records + 1)\n    \n    # Combine lists of names\n    all_names = latin_names + other_names\n    \n    # Correct improperly encoded Latin characters\n    def correct_encoding(name):\n        return codecs.decode(codecs.encode(name, 'latin1'), 'utf-8')\n    \n    corrected_names = [correct_encoding(name) for name in all_names]\n    \n    # Randomly select names for each record\n    names = np.random.choice(corrected_names, num_records)\n    \n    # Generate random dates of birth\n    start_date = datetime.datetime(start_year, 1, 1)\n    end_date = datetime.datetime(end_year, 12, 31)\n    date_range = (end_date - start_date).days\n    \n    dates_of_birth = [start_date + datetime.timedelta(days=np.random.randint(0, date_range)) for _ in range(num_records)]\n    \n    # Generate emails\n    def generate_email(name, birth_date):\n        year_of_birth = birth_date.year\n        sanitized_name = re.sub(r'[^a-zA-Z0-9]', '', name.lower())\n        return f\"{sanitized_name}{year_of_birth}@{email_domain}\"\n    \n    emails = [generate_email(name, dob) for name, dob in zip(names, dates_of_birth)]\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dates_of_birth,\n        'Email': emails\n    })\n    \n    return df\n\n# Example usage\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\nprint(df)\n```\n\nThis code will generate a DataFrame with the required structure and content, ensuring that names are properly encoded, dates of birth are within the specified range, and emails are correctly formatted."}
{"task_id": "BigCodeBench/763", "solution": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    try:\n        with open(input_file, 'r') as f:\n            data = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        print(f\"Error reading JSON file: {e}\")\n        return {}\n        \n    # Initialize dictionary to store values for each key\n    values_dict = defaultdict(list)\n    \n    # Collect values for each key\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):  # Ensure the value is numeric\n                values_dict[key].append(value)\n    \n    # Calculate mean and median for each key\n    stats_dict = {}\n    for key, values in values_dict.items():\n        mean_val = np.mean(values)\n        median_val = np.median(values)\n        stats_dict[key] = {'mean': mean_val, 'median': median_val}\n    \n    # Write results to CSV file\n    try:\n        with open(output_file, 'w', newline='') as csvfile:\n            fieldnames = ['field', 'mean', 'median']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            \n            writer.writeheader()\n            for key, stats in stats_dict.items():\n                writer.writerow({'field': key, 'mean': stats['mean'], 'median': stats['median']})\n    except IOError as e:\n        print(f\"Error writing CSV file: {e}\")\n        \n    return stats_dict\n\n# Example usage:\n# input_file = 'data.json'\n# output_file = 'stats.csv'\n# result = task_func(input_file, output_file)\n# print(result)\n```"}
{"task_id": "BigCodeBench/765", "solution": "```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    \n    # Ensure the target directory exists\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    for file_path in kwargs.values():\n        file_path = Path(file_path)\n        \n        # Check if the file exists and is not empty\n        if file_path.exists() and file_path.is_file() and file_path.stat().st_size > 0:\n            destination_path = target_path / file_path.name\n\n            # Copy the file to the target directory\n            shutil.copy(file_path, destination_path)\n            copied_files.append(str(destination_path))\n    \n    return copied_files\n\n# Example usage:\n# kwargs = {'file1': 'path/to/file1.txt', 'file2': 'path/to/file2.txt'}\n# copied_files = task_func(kwargs)\n# print(copied_files)\n```"}
{"task_id": "BigCodeBench/771", "solution": "Certainly! Below is the self-contained code starting with the specified import statements and function definition:\n\n```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Ensure the directory exists\n    if not os.path.isdir(directory):\n        raise NotADirectoryError(f\"The directory {directory} does not exist\")\n\n    # Compile the regex pattern for performance\n    regex = re.compile(pattern)\n    \n    # List to store the names of new files created\n    new_files = []\n\n    # Iterate over all files in the given directory\n    for file in os.listdir(directory):\n        file_path = os.path.join(directory, file)\n        \n        # Check if the file is a CSV file and matches the pattern\n        if os.path.isfile(file_path) and file.endswith('.csv'):\n            match = regex.match(file)\n            if match:\n                # Construct new filename\n                new_filename = match.group(1) + '.csv'\n                new_file_path = os.path.join(directory, new_filename)\n                \n                # Open original file and new file to copy the content\n                with open(file_path, mode='r', newline='', encoding='utf-8') as orig_file:\n                    reader = csv.reader(orig_file)\n                    with open(new_file_path, mode='w', newline='', encoding='utf-8') as new_file:\n                        writer = csv.writer(new_file)\n                        for row in reader:\n                            writer.writerow(row)\n                \n                # Append the new file's name to the list if it's not already there\n                if new_filename not in new_files:\n                    new_files.append(new_filename)\n    \n    return new_files\n\n# Example usage\nif __name__ == \"__main__\":\n    directory_path = \"path/to/your/csv/files\"\n    new_files_created = task_func(directory_path)\n    print(\"New files created:\", new_files_created)\n```\n\nThis function processes CSV files in a given directory by matching their filenames against a specified pattern. It creates new files with altered names while preserving the content, and returns a list of the new filenames created."}
{"task_id": "BigCodeBench/777", "solution": "```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    compiled_pattern = re.compile(pattern)\n    \n    for filename in os.listdir(directory):\n        if filename.endswith('.zip'):\n            match = compiled_pattern.match(filename)\n            if match:\n                prefix = match.group(1)\n                extract_dir = os.path.join(directory, prefix)\n                if not os.path.exists(extract_dir):\n                    os.makedirs(extract_dir)\n                with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n                extracted_dirs.append(extract_dir)\n    \n    return extracted_dirs\n\n# Example usage:\n# extracted_directories = task_func('/path/to/directory')\n# print(extracted_directories)\n```"}
{"task_id": "BigCodeBench/785", "solution": "```python\nimport subprocess\nimport os\nimport glob\nfrom datetime import datetime\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Create the archive directory if it does not exist\n    if not os.path.exists(ARCHAVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n    \n    # Get the list of files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    \n    if not files_to_archive:\n        raise FileNotFoundError(\"No files matched the given pattern.\")\n    \n    # Create a unique archive file name based on the current timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n    archive_file = os.path.join(ARCHIVE_DIR, f\"archive_{timestamp}.tar.gz\")\n    \n    # Archive the files\n    with subprocess.Popen(['tar', '-czf', archive_file] + files_to_archive, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as proc:\n        stdout, stderr = proc.communicate()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Error creating archive: {stderr.decode('utf-8')}\")\n    \n    # Delete the original files\n    for file_path in files_to_archive:\n        os.remove(file_path)\n    \n    return archive_file\n\n# Example usage\nif __name__ == \"__main__\":\n    pattern = \"/path/to/files/*.txt\"\n    archive_path = task_func(pattern)\n    print(f\"Archive created at: {archive_path}\")\n```"}
{"task_id": "BigCodeBench/800", "solution": "```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    \"\"\"Creates a CSV file with the given content.\"\"\"\n    directory = os.path.dirname(filename)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    \"\"\"Sets up a test CSV file with predefined data.\"\"\"\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Reads the CSV file, counts the total number of goals and penalties,\n    updates the counts with the given goals and penalties, and returns a Counter object.\n    \"\"\"\n    count = Counter({'goals': 0, 'penalties': 0})\n    \n    # Read the CSV file and count the total goals and penalties\n    if os.path.exists(csv_file_path):\n        with open(csv_file_path, mode='r', newline='') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                try:\n                    count['goals'] += int(row['goals'])\n                    count['penalties'] += int(row['penalties'])\n                except ValueError:\n                    print(f\"Skipping invalid row: {row}\")\n    \n    # Update the counts with the given goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n    \n    return count\n\n# Example usage:\nif __name__ == '__main__':\n    setup_csv()\n    updated_counts = task_func(4, 3, 'test_data/test_case_2.csv')\n    print(updated_counts)  # Output: Counter({'goals': 10, 'penalties': 6})\n```"}
{"task_id": "BigCodeBench/826", "solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'[A-Za-z0-9]+\\.(txt|doc|docx)'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \n    Parameters:\n    source_dir (str): The directory to move files from.\n    target_dir (str): The directory to move files to.\n    file_pattern (str): The regex pattern to match file names against.\n    \n    Returns:\n    int: The number of files that were successfully moved.\n    \"\"\"\n    # Compile the regular expression pattern\n    pattern = re.compile(file_pattern)\n    \n    # Ensure the source directory exists\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    \n    # Ensure the target directory exists, create it if it doesn't\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    moved_files_count = 0\n    \n    # Iterate over all files in the source directory\n    for filename in os.listdir(source_dir):\n        source_file = os.path.join(source_dir, filename)\n        \n        # Check if the current item is a file and if it matches the pattern\n        if os.path.isfile(source_file) and pattern.match(filename):\n            target_file = os.path.join(target_dir, filename)\n            \n            try:\n                # Move the file to the target directory\n                shutil.move(source_file, target_file)\n                moved_files_count += 1\n            except Exception as e:\n                print(f\"Error moving file {source_file} to {target_file}: {e}\")\n    \n    return moved_files_count\n\n# Example usage:\n# moved_files_count = task_func('/path/to/source', '/path/to/target')\n# print(f\"Moved {moved_files_count} files.\")\n```"}
{"task_id": "BigCodeBench/845", "solution": "```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text):\n    \"\"\"Remove non-alphanumeric characters except spaces and convert to lowercase.\"\"\"\n    return ALPHANUMERIC.sub(' ', text).lower()\n\ndef cosine_similarity(text1, text2):\n    \"\"\"Compute the cosine similarity between two cleaned texts.\"\"\"\n    # Tokenize the cleaned texts\n    tokens1 = text1.split()\n    tokens2 = text2.split()\n    \n    # Create frequency counters\n    freq1 = Counter(tokens1)\n    freq2 = Counter(tokens2)\n    \n    # Create a set of all unique words in both texts\n    all_words = set(freq1.keys()).union(set(freq2.keys()))\n    \n    # Create vectors for each text\n    vec1 = np.array([freq1[word] for word in all_words])\n    vec2 = np.array([freq2[word] for word in all_words])\n    \n    # Compute cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    \n    if norm_vec1 == 0 or norm_vec2 == 0:\n        return 0.0\n    \n    return dot_product / (norm_vec1 * norm_vec2)\n\ndef task_func(text1, text2):\n    \"\"\"Calculate the similarity values between two texts based on cosine similarity and Levenshtein ratio.\"\"\"\n    # Clean the texts\n    cleaned_text1 = clean_text(text1)\n    cleaned_text2 = clean_text(text2)\n    \n    # Calculate cosine similarity\n    cosine_sim = cosine_similarity(cleaned_text1, cleaned_text2)\n    \n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(cleaned_text1, cleaned_text2)\n    \n    return (cosine_sim, levenshtein_ratio)\n\n# Example usage:\ntext1 = \"Hello, World!\"\ntext2 = \"Hello world\"\nprint(task_func(text1, text2))\n```"}
{"task_id": "BigCodeBench/854", "solution": "Certainly! Below is the self-contained code that meets the specified requirements:\n\n```python\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    Generate all permutations of a given list of numbers and calculate the sum of the factorials of each number in each permutation.\n    \n    Parameters:\n    numbers (list of int): The list of integers to permute.\n    \n    Returns:\n    list of int: A list containing the sums of the factorials of each number in each permutation.\n    list of list of int: A list containing all permutations of numbers.\n    \n    Raises:\n    TypeError: If numbers is not a list of integers.\n    ValueError: If input numbers are negative.\n    \"\"\"\n    \n    # Check if the input is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    \n    # Check if any of the numbers are negative\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must not be negative.\")\n    \n    # Handle the case where the input list is empty\n    if not numbers:\n        return [], []\n\n    # Generate all permutations of the input list\n    perm_list = list(permutations(numbers))\n    \n    # Calculate the sum of the factorials of each number in each permutation\n    factorial_sums = [sum(math.factorial(num) for num in perm) for perm in perm_list]\n    \n    return factorial_sums, [list(perm) for perm in perm_list]\n\n# Example usage\nfac, perm = task_func([0, 4])\nprint(fac)  # [25, 25]\nprint(perm)  # [[0, 4], [4, 0]]\n```\n\nThis code:\n- Validates that the input is a list of integers and that none of the integers are negative.\n- Returns empty lists if the input list is empty.\n- Generates all permutations of the input list.\n- Calculates the sum of the factorials of each number in each permutation.\n- Converts the permutations from tuples to lists to match the specified output format.\n- Raises appropriate exceptions for invalid inputs."}
{"task_id": "BigCodeBench/857", "solution": "```python\nimport warnings\nimport os\nimport glob\nimport shutil\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from SOURCE_DIR to DEST_DIR based on the specified EXTENSIONS.\n    \n    Parameters:\n    - SOURCE_DIR (str): The source directory to search files in.\n    - DEST_DIR (str): The destination directory where files will be moved.\n    - EXTENSIONS (list): List of file extensions to search for.\n\n    Returns:\n    - transferred_files (list): A list containing the names of files that were successfully transferred.\n    \"\"\"\n    \n    # Check if SOURCE_DIR exists\n    if not os.path.exists(SOURCE_DIR):\n        raise FileNotFoundError(f\"Source directory {SOURCE_DIR} does not exist.\")\n    \n    # Ensure the destination directory exists or create it\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    transferred_files = []\n\n    # If EXTENSIONS is empty, return an empty list\n    if not EXTENSIONS:\n        warnings.warn(\"No extensions provided. No files will be transferred.\")\n        return transferred_files\n\n    # Loop through each extension provided\n    for ext in EXTENSIONS:\n        # Construct the search pattern\n        search_pattern = os.path.join(SOURCE_DIR, f\"*.{ext}\")\n        \n        # Find all files matching the pattern\n        files_to_transfer = glob.glob(search_pattern)\n        \n        for file_path in files_to_transfer:\n            try:\n                # Get the file name\n                file_name = os.path.basename(file_path)\n                \n                # Construct the destination path\n                dest_path = os.path.join(DEST_DIR, file_name)\n                \n                # Transfer the file\n                shutil.move(file_path, dest_path)\n                \n                # Add the file name to the list of successfully transferred files\n                transferred_files.append(file_name)\n            \n            except FileNotFoundError:\n                warnings.warn(f\"File not found: {file_path}\")\n            except PermissionError:\n                warnings.warn(f\"Permission error while transferring file {file_path}\")\n            except Exception as e:\n                warnings.warn(f\"Could not transfer file {file_path}: {e}\")\n\n    return transferred_files\n\n# Example usage\nif __name__ == \"__main__\":\n    SOURCE_DIR = \"/path/to/source\"\n    DEST_DIR = \"/path/to/destination\"\n    EXTENSIONS = [\"txt\", \"jpg\", \"png\"]\n\n    transferred_files = task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS)\n    print(\"Transferred files:\", transferred_files)\n```"}
{"task_id": "BigCodeBench/865", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    \"\"\"\n    Normalize count and weight data.\n\n    Parameters:\n    data (list of tuples): A list where each tuple contains ('Item', Count, Weight).\n\n    Returns:\n    DataFrame: A pandas DataFrame with three columns: 'Item', 'Normalized Count', and 'Normalized Weight'.\n    \"\"\"\n\n    # Validate input\n    if not isinstance(data, list) or not all(isinstance(i, tuple) and len(i) == 3 for i in data):\n        raise ValueError(\"Input data must be a list of tuples with three elements each ('Item', Count, Weight).\")\n    \n    if not data:\n        return pd.DataFrame(columns=['Item', 'Normalized Count', 'Normalized Weight'])\n    \n    # Convert the input data to a DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n    \n    # Check for non-numeric values in 'Count' and 'Weight' columns\n    if not pd.api.types.is_numeric_dtype(df['Count']) or not pd.api.types.is_numeric_dtype(df['Weight']):\n        raise ValueError(\"Both 'Count' and 'Weight' must be numeric.\")\n    \n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n    \n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n    \n    # Select the relevant columns to return\n    result_df = df[['Item', 'Normalized Count', 'Normalized Weight']]\n    \n    return result_df\n\n# Example usage\ndata = [\n    ('item1', 10, 0.5),\n    ('item2', 20, 0.8),\n    ('item3', 15, 0.3),\n    ('item4', 25, 0.9)\n]\n\nresult = task_func(data)\nprint(result)\n```"}
{"task_id": "BigCodeBench/870", "solution": "```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    max_length = max(len(t) for t in data_list)\n    sums = [0] * max_length\n    counts = [0] * max_length\n\n    for tpl in data_list:\n        for i, val in enumerate(tpl):\n            if isinstance(val, (int, float)):\n                sums[i] += val\n                counts[i] += 1\n\n    means = [sums[i] / counts[i] if counts[i] != 0 else np.nan for i in range(max_length)]\n    index = [f'Position {i}' for i in range(max_length)]\n\n    df = pd.DataFrame({'Mean Value': means}, index=index)\n    return df\n\n# Example usage:\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)\nprint(df)\n```\n\nThis code defines the `task_func` function to calculate the mean of numerical values at each position in the tuples, ignoring non-numeric values. It returns a DataFrame with the mean values and appropriate positional indices. If the input list is empty, it returns an empty DataFrame."}
{"task_id": "BigCodeBench/879", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    \n    # Check if the columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both columns '{col1}' and '{col2}' are not in the DataFrame.\")\n    \n    # Convert columns to categorical if they are not already\n    if not pd.api.types.is_categorical_dtype(data[col1]):\n        data[col1] = data[col1].astype('category')\n    if not pd.api.types.is_categorical_dtype(data[col2]):\n        data[col2] = data[col2].astype('category')\n    \n    # Check if columns have multiple categories\n    if len(data[col1].unique()) == 1:\n        raise ValueError(f\"Column '{col1}' does not have multiple categories.\")\n    if len(data[col2].unique()) == 1:\n        raise ValueError(f\"Column '{col2}' does not have multiple categories.\")\n    \n    # Construct the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if each category has at least 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating chi-square test assumptions.\")\n    \n    # Perform the chi-square test of independence\n    chi2, p, _, _ = chi2_contingency(contingency_table)\n    \n    return p\n\n# Example usage\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})\n\nprint(task_func(data, 'a', 'b'))\n```\n\nThis code implements the `task_func` function, which performs a chi-square test of independence on two categorical columns in a given DataFrame. The function includes checks for various conditions and raises appropriate exceptions if any condition is violated. If all checks pass, it performs the chi-square test and returns the p-value."}
{"task_id": "BigCodeBench/897", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    \n    if rolls <= 0:\n        raise ValueError(\"Number of rolls must be a positive integer.\")\n    \n    # Simulate dice rolls using numpy for better performance\n    results = np.random.choice(NUMBERS, size=rolls)\n\n    # Calculate frequency of each outcome\n    frequency = np.array([np.sum(results == i) for i in NUMBERS])\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(1, 8) - 0.5, edgecolor='black')\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    \n    return frequency, ax\n\n# Example usage:\nfrequency, ax = task_func(rolls=100, seed=42)\n\n# Display the histogram\nplt.show()\n\n# Print frequency array\nprint(frequency)\n```"}
{"task_id": "BigCodeBench/906", "solution": "```python\nimport zipfile\nimport os\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files (identified by the '_processed' suffix) from the source directory to the target directory.\n\n    Args:\n        source_dir (str): The source directory containing processed files.\n        target_dir (str): The target directory where the archive will be saved.\n        archive_name (str, optional): The name of the archive file. Defaults to 'archive.zip'.\n\n    Returns:\n        str: The path to the created archive.\n    \"\"\"\n    try:\n        # Ensure the target directory exists\n        os.makedirs(target_dir, exist_ok=True)\n        \n        # Create the full path to the archive file\n        archive_path = os.path.join(target_dir, archive_name)\n        \n        # Initialize a list to hold paths of files that need to be archived\n        processed_files = []\n\n        # Walk through the source directory and find all processed files\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                if file.endswith('_processed'):\n                    processed_files.append(os.path.join(root, file))\n\n        # Create the ZIP file and add processed files to it\n        with zipfile.ZipFile(archive_path, 'w') as archive:\n            for file_path in processed_files:\n                # Add file to the archive with an arcname that preserves the relative path\n                relative_path = os.path.relpath(file_path, source_dir)\n                archive.write(file_path, arcname=relative_path)\n\n        return archive_path\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return \"\"\n\n# Example usage:\n# archive_path = task_func('/path/to/source_dir', '/path/to/target_dir')\n# print(f'Archive created at: {archive_path}')\n```"}
{"task_id": "BigCodeBench/914", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom datetime import timedelta\n\ndef task_func(df):\n    # Ensure the DataFrame has the correct columns\n    if 'timestamp' not in df.columns or 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'timestamp' and 'closing_price' columns.\")\n    \n    # Convert timestamp to seconds since epoch for linear regression\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['timestamp_seconds'] = df['timestamp'].view(np.int64) / 10**9  # Convert to seconds\n\n    # Prepare data for linear regression\n    X = df['timestamp_seconds'].values.reshape(-1, 1)\n    y = df['closing_price'].values\n    \n    # Create and fit the model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Predict the next 7 days\n    last_timestamp = df['timestamp'].max()\n    future_timestamps = [last_timestamp + timedelta(seconds=24*60*60*i) for i in range(1, 8)]\n    future_timestamps_seconds = np.array([ts.timestamp() for ts in future_timestamps]).reshape(-1, 1)\n    predicted_prices = model.predict(future_timestamps_seconds)\n    \n    # Create plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['timestamp'], df['closing_price'], label='Historical Prices')\n    ax.plot(future_timestamps, predicted_prices, label='Predicted Prices', linestyle='--')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Price Prediction')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    ax.grid(True)\n    \n    # Convert predicted prices to a list for output\n    predicted_prices_list = predicted_prices.tolist()\n    \n    return predicted_prices_list, ax\n\n# Example usage (with dummy data)\nif __name__ == \"__main__\":\n    # Create a sample DataFrame with historical stock data\n    data = {\n        'timestamp': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n        'closing_price': np.random.uniform(100, 200, 100)  # Random prices between 100 and 200\n    }\n    df = pd.DataFrame(data)\n    \n    predicted_prices, ax = task_func(df)\n    plt.show()\n    \n    print(\"Predicted prices for the next 7 days:\", predicted_prices)\n```"}
{"task_id": "BigCodeBench/915", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    # Check if 'closing_price' column exists in the DataFrame\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'closing_price' column\")\n    \n    # Calculate the Z-Scores of the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the z_threshold\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['closing_price'], label='Closing Prices')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    \n    # Customizing the plot\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    ax.grid(True)  # Adding grid lines\n    \n    # Return the outliers DataFrame and the plot\n    return outliers[['closing_price']], ax\n\n# Example usage:\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\nplt.show()\n```"}
{"task_id": "BigCodeBench/916", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram\n    within a single figure. This function helps understand the spread, central tendency, and\n    the distribution shape of stock closing prices.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame containing stock closing prices with a column named 'Close'\n    \n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot\n           and the second for the histogram.\n    \"\"\"\n    \n    # Check if the 'Close' column exists in the DataFrame\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column\")\n    \n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n    \n    # Box plot of closing prices\n    sns.boxplot(x=df['Close'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n    \n    # Histogram of closing prices\n    sns.histplot(df['Close'], kde=True, ax=ax2)\n    ax2.set_title('Histogram of Closing Prices')\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Show the plots\n    plt.show()\n    \n    return ax1, ax2\n\n# Example usage:\n# df = pd.read_csv('path_to_your_csv_file.csv')\n# task_func(df)\n```"}
{"task_id": "BigCodeBench/917", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame contains a 'Date' and 'Close' column\n    if 'Date' not in df.columns or 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Close' columns.\")\n    \n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # Take the 'Close' column as the time series\n    ts = df['Close']\n    \n    # Fit ARIMA model (p=5, d=1, q=0) as an example; parameters should be optimized\n    model = ARIMA(ts, order=(5, 1, 0))\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Plot the original series and the forecast\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ts.plot(ax=ax, label='Original')\n    forecast_dates = pd.date_range(start=ts.index[-1], periods=8, closed='right')\n    forecast_series = pd.Series(forecast, index=forecast_dates)\n    forecast_series.plot(ax=ax, label='Forecast', color='red')\n    \n    # Add titles and labels\n    ax.set_title('Share Closing Price Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n    \n    # Return the forecasted prices and the plot's Axes object\n    return forecast.tolist(), ax\n\n# Example usage:\n# Assuming `df` is your DataFrame with 'Date' and 'Close' columns\n# forecasted_prices, plot_axes = task_func(df)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/928", "solution": "```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Handle edge case for words with fewer than 2 characters\n    if len(word) < 2:\n        return {''.join(pair): 0 for pair in itertools.product(string.ascii_lowercase, repeat=2)}\n    \n    # Generate all possible two-letter combinations\n    combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize dictionary with all combinations set to 0\n    combination_counts = {combo: 0 for combo in combinations}\n    \n    # Count frequencies of two-letter combinations in the word\n    word_combinations = [word[i:i+2] for i in range(len(word) - 1)]\n    frequency = Counter(word_combinations)\n    \n    # Update the combination_counts dictionary with actual frequencies\n    combination_counts.update(frequency)\n    \n    return combination_counts\n\n# Example usage:\nword = \"exampleword\"\nresult = task_func(word)\nprint(result)\n```"}
{"task_id": "BigCodeBench/942", "solution": "Here's the self-contained code that generates and visualizes a sales report for different categories over a period of time as per the specified requirements:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Generate and visualize a sales report for different categories over a period of time.\n    \n    Parameters:\n    - start_date (str): The start date for the sales data.\n    - periods (int): Number of periods to generate data for.\n    - freq (str): Frequency of the periods.\n    - categories (list): List of product categories.\n    \n    Returns:\n    - df (pd.DataFrame): DataFrame containing the sales data with columns 'Date', 'Category', 'Sales'.\n    - ax (matplotlib.axes._subplots.AxesSubplot): Matplotlib Axes object for the plot.\n    \"\"\"\n    \n    # Set random seed for reproducibility\n    np.random.seed(42)\n    \n    # Generate dates\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame to store sales data\n    sales_data = {\n        'Date': np.repeat(dates, len(categories)),\n        'Category': categories * periods,\n        'Sales': np.random.randint(1000, 10000, size=len(dates) * len(categories))\n    }\n    \n    df = pd.DataFrame(sales_data)\n    \n    # Plotting the sales data\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], label=category)\n    \n    ax.set_title('Sales Report Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df, ax\n\n# Call the function\ndf, ax = task_func()\n```"}
{"task_id": "BigCodeBench/945", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Validate input parameters\n    if periods < 1:\n        raise ValueError(\"Periods must be a positive integer.\")\n    \n    # Generate date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate sales data if not provided\n    if sales_data is None:\n        np.random.seed(0)  # For reproducibility\n        sales_data = np.random.randint(100, 200, size=periods)\n    elif len(sales_data) != periods:\n        raise ValueError(\"Length of sales_data must match the number of periods.\")\n    \n    # Create a DataFrame for the date range and sales data\n    df = pd.DataFrame({'date': date_range, 'sales': sales_data})\n    \n    # Prepare data for linear regression\n    df['time_index'] = np.arange(len(df))\n    X = df[['time_index']]\n    y = df['sales']\n    \n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Forecast future sales for the same number of periods as the input data\n    future_time_index = np.arange(len(df), 2 * len(df)).reshape(-1, 1)\n    forecasted_sales = model.predict(future_time_index)\n    \n    # Ensure the forecasted sales are returned as a numpy array\n    return np.array(forecasted_sales)\n\n# Example usage\nforecasted_sales = task_func()\nprint(forecasted_sales)\n```"}
{"task_id": "BigCodeBench/952", "solution": "```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Validate input\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative.\")\n    \n    # Set random seed for reproducibility if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n    \n    # Select random tasks to assign\n    selected_tasks = random.sample(sanitized_tasks, min(n_tasks, len(sanitized_tasks)))\n    \n    # Select random employees to assign the tasks to\n    assigned_employees = random.choices(employees, k=len(selected_tasks))\n    \n    # Get current system date\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    # Create a DataFrame for the assignments\n    task_assignments = pd.DataFrame({\n        \"Task Name\": selected_tasks,\n        \"Assigned To\": assigned_employees,\n        \"Due Date\": [current_date] * len(selected_tasks)\n    })\n    \n    return task_assignments\n\n# Example usage:\nif __name__ == \"__main__\":\n    tasks = [\"Task A\", \"Task B\", \"Task C\", \"Task D\"]\n    n_tasks = 3\n    df = task_func(tasks, n_tasks)\n    print(df)\n```"}
{"task_id": "BigCodeBench/955", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text.strip():\n        raise ValueError(\"The input text is empty.\")\n    \n    # Process input text\n    text = text.lower()  # Convert to lowercase for case-insensitivity\n    words = re.findall(r'\\b\\w+\\b', text)  # Extract words using regex\n    \n    # Replace spaces in given words with underscores\n    # This step is for the provided text, not for the extracted words\n    modified_words = [word.replace(' ', '_') for word in words]\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(modified_words)\n    \n    # Plotting the frequency of each unique word\n    fig, ax = plt.subplots()\n    ax.bar(word_counts.keys(), word_counts.values())\n    ax.set_xlabel('Unique Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency Plot')\n    plt.xticks(rotation=90)\n    plt.show()\n    \n    return ax\n\n# Example usage:\nmystrings = \"Example strings input\"\ntext = \"This is a test. This test is only a test.\"\ntask_func(mystrings, text)\n```"}
{"task_id": "BigCodeBench/963", "solution": "```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory: str, target_directory: str, zip_name: str) -> str:\n    source_dir = Path(source_directory)\n    target_dir = Path(target_directory)\n    \n    # Ensure source directory exists\n    if not source_dir.exists():\n        raise OSError(\"Source directory does not exist\")\n    \n    # Create the target directory if it does not exist\n    try:\n        target_dir.mkdir(parents=True, exist_ok=True)\n    except Exception as e:\n        raise OSError(f\"Failed to create target directory: {e}\")\n    \n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create the full path for the zip file\n    zip_path = target_dir / f\"{zip_name}.zip\"\n    \n    # Initialize the zip file\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Iterate through all files in the source directory with valid extensions\n        for ext in valid_extensions:\n            for file in source_dir.rglob(f\"*{ext}\"):\n                zipf.write(file, arcname=file.relative_to(source_dir))\n    \n    return str(zip_path)\n\n# Example usage:\n# print(task_func(\"/path/to/source\", \"/path/to/target\", \"my_archive\"))\n```"}
{"task_id": "BigCodeBench/964", "solution": "```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists() or not source_path.is_dir():\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Ensure the target directory exists\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    # File extensions of interest\n    extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    converted_files_count = 0\n\n    # Iterate over files in the source directory\n    for file in source_path.glob('*'):\n        if file.is_file() and file.suffix in extensions:\n            text_content = \"\"\n            # Read the content based on file type\n            if file.suffix == '.txt':\n                with open(file, 'r', encoding='utf-8') as f:\n                    text_content = f.read()\n            elif file.suffix == '.docx':\n                doc = docx.Document(file)\n                text_content = \"\\n\".join([para.text for para in doc.paragraphs])\n            elif file.suffix == '.xlsx':\n                df = pd.read_excel(file)\n                text_content = df.to_csv(index=False)\n            elif file.suffix == '.csv':\n                df = pd.read_csv(file)\n                text_content = df.to_csv(index=False)\n            \n            # Create a DataFrame with a single 'Text' column\n            df_text = pd.DataFrame({'Text': [text_content]})\n            \n            # Save the DataFrame to a CSV file in the target directory\n            target_file = target_path / (file.stem + '.csv')\n            df_text.to_csv(target_file, index=False)\n            converted_files_count += 1\n    \n    return converted_files_count\n```"}
{"task_id": "BigCodeBench/969", "solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check for NaN values and non-numeric data types\n    for column in df.columns:\n        if df[column].isna().sum() > 0:\n            raise ValueError(f\"The DataFrame contains NaN values in column {column}.\")\n        if not np.issubdtype(df[column].dtype, np.number):\n            raise TypeError(f\"The DataFrame contains non-numeric data types in column {column}.\")\n    \n    # Compute the cumulative sum for each column\n    cumsum_df = df.cumsum()\n    \n    # Normalize each column using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_cumsum = scaler.fit_transform(cumsum_df)\n    \n    # Convert the normalized cumulative sum array back to a DataFrame\n    normalized_cumsum_df = pd.DataFrame(normalized_cumsum, columns=df.columns, index=df.index)\n    \n    return normalized_cumsum_df\n\n# Example usage:\n# df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n# print(task_func(df))\n```\n\nThis function ensures that:\n1. The DataFrame is not empty.\n2. There are no NaN values in the DataFrame.\n3. All columns contain numeric data types.\n\nIt then computes the cumulative sum for each numeric column, normalizes these cumulative sums using `MinMaxScaler`, and returns a DataFrame with the normalized cumulative sums, retaining the original column names and index."}
{"task_id": "BigCodeBench/971", "solution": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes a given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n    \n    Args:\n        directory_path (str): The path to the directory to analyze.\n        \n    Returns:\n        list of tuples: Each tuple contains (file name, file size in bytes,\n                        creation time in ISO format, modification time in ISO format).\n                        \n    Raises:\n        ValueError: If the provided directory does not exist.\n    \"\"\"\n    \n    # Check if the directory exists and is a directory\n    dir_path = Path(directory_path)\n    if not dir_path.exists() or not dir_path.is_dir():\n        raise ValueError(\"The provided directory does not exist or is not a directory.\")\n    \n    result = []\n    \n    # Iterate over each item in the directory\n    for item in dir_path.iterdir():\n        # Ensure the item is a file\n        if item.is_file():\n            # Get file size in bytes\n            file_size = item.stat().st_size\n            \n            # Get creation time (or last metadata change time if creation time is not available)\n            creation_time = datetime.fromtimestamp(item.stat().st_ctime, tz=timezone.utc).isoformat()\n            \n            # Get last modification time\n            modification_time = datetime.fromtimestamp(item.stat().st_mtime, tz=timezone.utc).isoformat()\n            \n            # Append the file details to the result list\n            result.append((item.name, file_size, creation_time, modification_time))\n    \n    return result\n\n# Example usage\n# result = task_func('/path/to/empty_directory')\n# print(result)\n```"}
{"task_id": "BigCodeBench/985", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        # Load the JSON data\n        data = json.loads(json_data)\n        \n        # Validation checks\n        if not isinstance(data, list):\n            raise ValueError(\"JSON data must be a list.\")\n        if not data:\n            raise ValueError(\"JSON data cannot be empty.\")\n        \n        countries = []\n        populations = []\n        \n        for item in data:\n            if not isinstance(item, dict):\n                raise ValueError(\"Each item in JSON data must be a dictionary.\")\n            \n            country = item.get(\"Country\")\n            population = item.get(\"Population\")\n            \n            if not isinstance(country, str):\n                raise ValueError(\"Country name must be a string.\")\n                \n            if not isinstance(population, (int, float)) or population < 0:\n                raise ValueError(\"Population must be a non-negative number.\")\n            \n            # Round down population if it's a float\n            population = math.floor(population)\n            \n            countries.append(country)\n            populations.append(population)\n        \n        # Create DataFrame\n        df = pd.DataFrame({\"Country\": countries, \"Population\": populations})\n        \n        # Define file path\n        file_path = os.path.join(output_dir, file_name)\n        \n        # Write DataFrame to CSV without an extra index column\n        try:\n            df.to_csv(file_path, index=False)\n        except IOError as e:\n            raise IOError(f\"Failed to write file to {output_dir}: {e}\")\n        \n        return file_path, df\n    \n    except (json.JSONDecodeError, ValueError) as e:\n        raise ValueError(f\"Invalid JSON data: {e}\")\n\n# Example usage:\n# json_data = '[{\"Country\": \"CountryA\", \"Population\": 123}, {\"Country\": \"CountryB\", \"Population\": 456.78}]'\n# task_func(json_data)\n```"}
{"task_id": "BigCodeBench/988", "solution": "```python\nimport os\nimport re\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files and directories) in a given directory against specified conditions.\n\n    Parameters:\n    dir_path (str): The path to the directory.\n    predicates (list): A list of conditions (either regex patterns or callable functions) to evaluate.\n\n    Returns:\n    dict: A dictionary with directory items as keys and the results of condition checks as values.\n\n    Raises:\n    FileNotFoundError: If the specified directory does not exist or is not a directory.\n    ValueError: If no valid predicates are provided.\n    \"\"\"\n    \n    # Ensure the directory path is valid\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Ensure there is at least one valid predicate\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Deduplicate predicates\n    predicates = list(set(predicates))\n    \n    # Define a function to evaluate a single predicate\n    def evaluate_predicate(item_name, predicate):\n        if isinstance(predicate, str):\n            return re.search(predicate, item_name) is not None\n        elif callable(predicate):\n            return predicate(item_name)\n        else:\n            raise ValueError(\"Predicates must be either a string (for regex) or a callable.\")\n    \n    # Dictionary to store the results\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        if os.path.isfile(item_path) or os.path.isdir(item_path):\n            results[item] = any(evaluate_predicate(item, predicate) for predicate in predicates)\n    \n    return results\n\n# Example usage:\n# dir_path = '/path/to/directory'\n# predicates = ['pattern', lambda x: x.startswith('test')]\n# result = task_func(dir_path, predicates)\n# print(result)\n```"}
{"task_id": "BigCodeBench/990", "solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Decode the hexadecimal string to bytes\n    byte_data = binascii.unhexlify(hex_string)\n\n    # Convert bytes to a UTF-8 string\n    utf8_str = byte_data.decode('utf-8')\n\n    # Encode the UTF-8 string into various formats\n    encoded_data = {\n        'hex': hex_string,\n        'base64': base64.b64encode(byte_data).decode('utf-8'),\n        'utf-8': utf8_str,\n        'utf-16': utf8_str.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_str.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_str if all(ord(c) < 128 for c in utf8_str) else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_str),\n        'ROT13': codecs.encode(utf8_str, 'rot_13')\n    }\n\n    return encoded_data\n\n# Example usage\nprint(task_func(\"68656c6c6f\"))\n```"}
{"task_id": "BigCodeBench/998", "solution": "```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    \"\"\"\n    Downloads a tar.gz file from a specified URL, validates its MD5 checksum against a predefined expected value.\n    If the checksum matches, it extracts the contents of the tar.gz file. Otherwise, it deletes the downloaded file.\n\n    Args:\n        url (str): The URL of the tar.gz file to download.\n\n    Returns:\n        bool: True if the file is successfully downloaded, its MD5 checksum matches the expected value, and it is\n              extracted. False if the checksum does not match the expected value or if the download fails.\n    \"\"\"\n    try:\n        # Download the tar.gz file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Validate the MD5 checksum\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            file_data = f.read()\n            md5_hash = hashlib.md5(file_data).hexdigest()\n        \n        if md5_hash != EXPECTED_MD5_CHECKSUM:\n            # Checksum mismatch\n            print(\"Checksum mismatch. Deleting the file.\")\n            os.remove(TARGET_TAR_FILE)\n            return False\n        \n        # Extract the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        \n        print(\"File downloaded, checksum matched, and extracted successfully.\")\n        return True\n    \n    except Exception as e:\n        # If any error occurs, remove the file if it was downloaded\n        print(f\"An error occurred: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False\n```"}
{"task_id": "BigCodeBench/999", "solution": "```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to download the file from the URL: {e}\")\n\n    try:\n        # Initialize a dictionary to count occurrences\n        occurrences = collections.Counter()\n        \n        # Open the CSV file and process its contents\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            for row in reader:\n                value = row[column_name]\n                occurrences[value] += 1\n        \n        return dict(occurrences)\n    \n    except ValueError as ve:\n        raise ve\n    \n    except Exception as e:\n        raise RuntimeError(f\"An error occurred while processing the CSV file: {e}\")\n    \n    finally:\n        # Ensure the downloaded file is deleted after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n```"}
{"task_id": "BigCodeBench/1003", "solution": "```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML content from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n        \n        # Parse the XML content\n        try:\n            tree = etree.fromstring(xml_content)\n        except etree.XMLSyntaxError:\n            raise ValueError(\"XML file has invalid syntax.\")\n        \n        # Check if the root element contains 'item' elements\n        items = tree.findall('.//item')\n        if not items:\n            raise ValueError(\"XML structure does not match expected format.\")\n        \n        # Extract data and convert to DataFrame\n        data = []\n        column_names = set()\n        \n        for item in items:\n            item_data = {}\n            for child in item:\n                item_data[child.tag] = child.text\n                column_names.add(child.tag)\n            data.append(item_data)\n        \n        if not data:\n            raise ValueError(\"XML structure does not match expected format.\")\n        \n        df = pd.DataFrame(data, columns=column_names)\n        return df\n    \n    except urllib.error.URLError:\n        raise ValueError(\"URL is invalid or the XML file cannot be fetched from the URL.\")\n\n# Example usage:\n# df = task_func('http://example.com/invalid_structure.xml')\n```\n\nThis code follows the specified requirements and handles all the mentioned scenarios where a `ValueError` should be raised. It fetches and parses an XML file from a given URL, checks for valid syntax and structure, and converts the data into a Pandas DataFrame."}
{"task_id": "BigCodeBench/1004", "solution": "```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, processes the text to count the frequency of each word,\n    and then plots a bar chart showing the ten most frequently occurring words. Assumes the URL points\n    to a plain text file and may not handle binary files or non-text content correctly. Words are identified\n    using a basic regular expression and are case-sensitive. The function does not remove common stopwords;\n    all words are counted as is. Requires internet access to download the file from the URL.\n\n    Parameters:\n        url (str): The URL pointing to the plain text file.\n\n    Returns:\n        tuple: A tuple containing two elements:\n            Counter: A Counter object from the collections module, containing word frequencies in the text.\n            Axes: A matplotlib Axes object that represents the plotted bar chart of the ten most common words.\n    \"\"\"\n    try:\n        # Download the text file from the specified URL\n        response = urllib.request.urlopen(url)\n        text = response.read().decode('utf-8')\n    except Exception as e:\n        print(f\"Error downloading or reading the file: {e}\")\n        return None, None\n    \n    # Use a regular expression to find all words (case-sensitive)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    common_words = word_counts.most_common(10)\n    \n    # Create a bar chart showing the ten most common words\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Frequent Words')\n    plt.xticks(rotation=45)\n    \n    # Show plot (optional)\n    # plt.show()\n    \n    return word_counts, ax\n\n# Example usage:\n# url = 'http://example.com/textfile.txt'\n# word_counts, ax = task_func(url)\n```"}
{"task_id": "BigCodeBench/1006", "solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        if not os.path.exists(download_path):\n            os.makedirs(download_path)\n        \n        # Define the path for the downloaded zip file\n        local_zip_path = os.path.join(download_path, \"downloaded_file.zip\")\n        \n        # Download the file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the content type is 'application/zip'\n        if response.headers.get('Content-Type') != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Write the downloaded content to a local file\n        with open(local_zip_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Try to extract the ZIP file\n        try:\n            with ZipFile(local_zip_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Return the path to the directory containing the extracted contents\n        return download_path\n    \n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage:\n# result = task_func('https://example.com/file.zip')\n# print(result)\n```"}
{"task_id": "BigCodeBench/1008", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url, table_id):\n    try:\n        # Make an HTTP GET request to fetch the webpage content\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n        \n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find the table with the specified ID\n        table = soup.find('table', {'id': table_id})\n        if table is None:\n            raise ValueError(\"Table with the specified ID not found.\")\n        \n        # Extract table headers if present\n        headers = []\n        header_row = table.find('tr')\n        if header_row:\n            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n        \n        # Extract table rows\n        rows = []\n        for tr in table.find_all('tr'):\n            row = [td.get_text(strip=True) for td in tr.find_all('td')]\n            if row:\n                rows.append(row)\n        \n        # If no rows found, return an empty DataFrame\n        if not rows:\n            return pd.DataFrame(columns=headers)\n        \n        # Create a DataFrame from the extracted data\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n        return df\n    \n    except requests.exceptions.HTTPError as e:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {e}\")\n    except ValueError as e:\n        raise ValueError(e)\n\n# Example usage:\n# df = task_func('https://example.com/data.html', 'table_id')\n# print(df)\n```\n\n### Explanation:\n1. **Imports**: Essential libraries (`requests`, `BeautifulSoup`, and `pandas`) are imported.\n2. **Function Definition**: The `task_func` function takes `url` and `table_id` as parameters.\n3. **HTTP Request**: Uses `requests.get` to fetch the webpage content and checks for HTTP errors using `raise_for_status`.\n4. **HTML Parsing**: Parses the HTML content using `BeautifulSoup`.\n5. **Table Search**: Searches for the table with the specified ID. If not found, raises a `ValueError`.\n6. **Header Extraction**: Extracts headers from the first row (`<tr>`) if present.\n7. **Row Extraction**: Iterates over all rows (`<tr>`), extracting cell data (`<td>`). Collects rows if they contain data.\n8. **Empty DataFrame**: If no data rows are found, returns an empty DataFrame with the extracted headers.\n9. **DataFrame Creation**: Creates and returns a DataFrame with the extracted data.\n\n### Error Handling:\n- **HTTPError**: Raised if the HTTP request fails.\n- **ValueError**: Raised if the table with the specified ID is not found, with a specific error message.\n\n### Example Usage:\nYou can uncomment the example usage lines to test the function with a real URL and table ID."}
{"task_id": "BigCodeBench/1012", "solution": "```python\nimport requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure directories exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    zip_path = DOWNLOAD_DIR / filename\n    file_list = []\n    \n    try:\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Write the zip file to the download directory\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n        \n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            file_list = zip_ref.namelist()\n        \n        status_message = \"Success\"\n    except requests.RequestException as e:\n        status_message = f\"Error: Network-related exception: {e}\"\n    except (zipfile.BadZipFile, IOError) as e:\n        status_message = f\"Error: File-related exception: {e}\"\n    finally:\n        # Optionally, clean up the downloaded zip file\n        if zip_path.exists():\n            zip_path.unlink()\n    \n    return (status_message, file_list)\n\n# Example Usage:\n# url = \"http://example.com/somefile.zip\"\n# filename = \"somefile.zip\"\n# print(task_func(url, filename))\n```"}
{"task_id": "BigCodeBench/1013", "solution": "```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes hyperlinks from a webpage, converts them to absolute URLs, and saves them to a CSV file.\n\n    Args:\n        url (str): The webpage URL to scrape.\n        base_url (str): The base URL to resolve relative URLs (default is \"https://www.example.com\").\n        csv_file (str): The name of the CSV file to save the links (default is \"scraped_data.csv\").\n\n    Returns:\n        int: The number of unique absolute links scraped from the webpage.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Ensure we notice bad responses\n\n        # Parse the content of the request with BeautifulSoup\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Find all hyperlinks\n        links = soup.find_all('a', href=True)\n\n        # Convert relative URLs to absolute URLs and gather unique links\n        absolute_links = set()\n        for link in links:\n            absolute_url = urljoin(base_url, link['href'])\n            absolute_links.add(absolute_url)\n\n        # Write the unique absolute links to a CSV file\n        with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            for link in absolute_links:\n                writer.writerow([link])\n\n        # Return the number of unique absolute links\n        return len(absolute_links)\n\n    except requests.RequestException as e:\n        print(f\"An error occurred while making the request: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return 0\n\n# Example usage:\n# print(task_func(\"https://www.example.com\"))\n```"}
{"task_id": "BigCodeBench/1015", "solution": "```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Validate URL format\n        if not webpage_url.startswith('http://') and not webpage_url.startswith('https://'):\n            raise ValueError(f\"Invalid URL format: {webpage_url}\")\n\n        # Fetch the HTML content from the URL\n        response = requests.get(webpage_url)\n        response.raise_for_status()\n        html_content = response.content\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Error fetching URL {webpage_url}: {e}\")\n    except ValueError as e:\n        raise ValueError(e)\n\n    try:\n        # Parse the HTML content using lxml\n        tree = html.fromstring(html_content)\n        tables = tree.xpath('//table')\n        \n        if not tables:\n            # No tables found in the HTML content\n            return 0\n\n        # Use pandas to read the first table in the HTML content\n        df = pd.read_html(html.tostring(tables[0]))[0]\n\n        if df.empty:\n            # Table is empty\n            return 0\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n\n        # Replace the table 'my_table' with new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Commit the changes and close the connection\n        conn.commit()\n        row_count = len(df)\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database error: {e}\")\n    except pd.errors.EmptyDataError as e:\n        raise pd.errors.EmptyDataError(f\"Pandas error: {e}\")\n    except lxml.etree.XMLSyntaxError as e:\n        raise lxml.etree.XMLSyntaxError(f\"LXML error: {e}\")\n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {e}\")\n    finally:\n        try:\n            if conn:\n                conn.close()\n        except NameError:\n            pass\n    \n    return row_count\n```"}
{"task_id": "BigCodeBench/1019", "solution": "```python\nfrom PIL import Image\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    def convert_text_encoding(text, from_enc, to_enc):\n        try:\n            return text.encode(from_enc).decode(to_enc)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(\"Error during text encoding conversion: \" + str(e))\n    \n    def extract_text_from_image(image):\n        try:\n            text = pytesseract.image_to_string(image)\n            return text.strip()\n        except Exception:\n            return \"\"\n\n    def extract_comment_from_image(image):\n        try:\n            if \"comment\" in image.info:\n                comment = image.info[\"comment\"]\n                if isinstance(comment, bytes):\n                    return comment.decode(from_encoding)\n                return comment\n            return \"\"\n        except Exception:\n            return \"\"\n\n    try:\n        image = Image.open(filename)\n    except Exception as e:\n        raise ValueError(f\"Failed to open image file: {e}\")\n\n    # Try to extract text using OCR\n    text = extract_text_from_image(image)\n    if text:\n        try:\n            return convert_text_encoding(text, from_encoding, to_encoding)\n        except ValueError as e:\n            raise e  # Raise the conversion error\n\n    # Fallback to extracting text from image comment\n    comment = extract_comment_from_image(image)\n    if comment:\n        try:\n            return convert_text_encoding(comment, from_encoding, to_encoding)\n        except ValueError as e:\n            raise e  # Raise the conversion error\n\n    # If both OCR and comment extraction fail\n    return \"\"\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        result = task_func()\n        print(f\"Extracted Comment/Text: {result}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/1020", "solution": "```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an HTTPError if the HTTP request returned an unsuccessful status code\n    except requests.RequestException as e:\n        raise RuntimeError(f\"HTTP request failed: {e}\")\n\n    content = response.content  # Get raw bytes of the response content\n\n    if not content:\n        return {}  # Return an empty dictionary if the content is empty\n\n    # Check if the content type is JSON to potentially skip encoding/decoding\n    if 'application/json' in response.headers.get('Content-Type', ''):\n        try:\n            return response.json()  # Directly return the parsed JSON data\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Error parsing JSON content: {e}\")\n\n    # Detect encoding if not provided\n    if from_encoding is None:\n        detected_encoding = chardet.detect(content)['encoding']\n        if not detected_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n        from_encoding = detected_encoding\n\n    try:\n        # Decode the content using the detected or specified encoding\n        decoded_content = content.decode(from_encoding)\n        # Re-encode the content to the target encoding\n        reencoded_content = decoded_content.encode(to_encoding)\n        # Decode again to a string so it can be parsed as JSON\n        json_str = reencoded_content.decode(to_encoding)\n        # Parse the JSON string into a Python dictionary\n        json_data = json.loads(json_str)\n        return json_data\n    except (UnicodeError, json.JSONDecodeError) as e:\n        raise ValueError(f\"Error processing content: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        data = task_func()\n        print(data)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```"}
{"task_id": "BigCodeBench/1022", "solution": "```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Processes a CSV file to filter rows based on the current date in a specified date column.\n\n    Parameters:\n    - csv_file_path (str): The path to the CSV file.\n    - column_name (str): The name of the date column to filter on.\n    - date_format (str): The format of the date in the specified column (default is \"%Y-%m-%d\").\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing rows where the date in the specified column matches the current date.\n    \"\"\"\n    \n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file at path {csv_file_path} does not exist.\")\n    \n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n        \n        # Check for empty CSV file\n        if df.empty:\n            return pd.DataFrame()\n        \n        # Validate the presence of the specified date column\n        if column_name not in df.columns:\n            raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n        \n        # Convert date values to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n        \n        # Drop rows where the date conversion resulted in NaT\n        df = df.dropna(subset=[column_name])\n        \n        # Get the current date\n        current_date = datetime.now().date()\n        \n        # Filter rows based on the current date\n        df = df[df[column_name].dt.date == current_date]\n        \n        # Sort the resulting data by the date column\n        df = df.sort_values(by=column_name)\n        \n        return df\n    \n    except EmptyDataError:\n        # Handle the special case of an empty CSV file\n        return pd.DataFrame()\n\n# Example usage:\n# result_df = task_func(\"path/to/file.csv\", \"date_column\")\n```"}
{"task_id": "BigCodeBench/1039", "solution": "```python\nimport ssl\nimport os\nimport hashlib\nimport socket\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Validate the file path\n        if not file_path or not os.path.isfile(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n        \n        # Compute the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            while True:\n                data = file.read(buffer_size)\n                if not data:\n                    break\n                sha256_hash.update(data)\n        \n        # Send the hash back to the client\n        hash_digest = sha256_hash.hexdigest()\n        secure_socket.sendall(hash_digest.encode('utf-8'))\n        return hash_digest\n    \n    except Exception as e:\n        error_message = f'Error: {str(e)}'\n        try:\n            secure_socket.sendall(error_message.encode('utf-8'))\n        except Exception:\n            pass\n        return error_message\n    \n    finally:\n        # Ensure the secure socket is closed\n        try:\n            secure_socket.shutdown(socket.SHUT_RDWR)\n        except Exception:\n            pass\n        secure_socket.close()\n```"}
{"task_id": "BigCodeBench/1040", "solution": "```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"Run a non-blocking echo server that appends the server's current time to received data and sends it back to the client.\"\"\"\n    \n    try:\n        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server.setblocking(0)\n        server.bind((server_address, server_port))\n        server.listen(5)\n    except socket.error as e:\n        return f\"Socket error: {e}\"\n    \n    inputs = [server]\n    outputs = []\n    message_queues = {}\n\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    print(f\"Server started at {start_time}, will run for {run_duration} seconds.\")\n\n    try:\n        while datetime.now() < end_time:\n            readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n            \n            for s in readable:\n                if s is server:\n                    connection, client_address = s.accept()\n                    connection.setblocking(0)\n                    inputs.append(connection)\n                    message_queues[connection] = queue.Queue()\n                else:\n                    data = s.recv(buffer_size)\n                    if data:\n                        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                        message_queues[s].put(data + b\" \" + current_time.encode())\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    outputs.remove(s)\n                else:\n                    s.send(next_msg)\n\n            for s in exceptional:\n                inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                del message_queues[s]\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n    finally:\n        for s in inputs:\n            s.close()\n        stop_time = datetime.now()\n        print(f\"Server stopped at {stop_time}, ran for {run_duration} seconds.\")\n        return f\"Server ran from {start_time} to {stop_time} for a duration of {run_duration} seconds.\"\n\n# Example usage\nif __name__ == \"__main__\":\n    print(task_func())\n```"}
{"task_id": "BigCodeBench/1042", "solution": "```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    try:\n        # Receive message from the client socket\n        data = client_socket.recv(BUFFER_SIZE)\n        if not data:\n            print(\"No data received from client.\")\n            return None\n        \n        # Decode the received data to string\n        message_body = data.decode('utf-8')\n        \n        # Get sender's email details\n        sender_email = input(\"Enter your email: \")\n        sender_password = getpass.getpass(\"Enter your email password: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        \n        # Create the email message\n        msg = EmailMessage()\n        msg.set_content(message_body)\n        msg['Subject'] = 'Message from Client Socket'\n        msg['From'] = sender_email\n        msg['To'] = recipient_email\n        \n        # Connect to the SMTP server\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()  # Secure the connection\n            server.login(sender_email, sender_password)  # Login to the SMTP server\n            server.send_message(msg)  # Send the email\n            print(\"Email sent successfully\")\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        client_socket.close()  # Close the client socket\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        # Create a server socket to accept connections from client\n        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n        server_socket.listen(1)  # Listen for incoming connections\n        \n        print(f\"Server listening on {SERVER_ADDRESS}:{SERVER_PORT}\")\n        \n        while True:\n            client_socket, address = server_socket.accept()\n            print(f\"Connection from {address}\")\n            task_func(client_socket)\n    except Exception as e:\n        print(f\"Server error: {e}\")\n    finally:\n        server_socket.close()\n```"}
{"task_id": "BigCodeBench/1053", "solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes a CSV file containing text data and generates a histogram of the ten most common words.\n    \n    Parameters:\n    - file_path (str): The path to the CSV file.\n    - save_path (str, optional): The path to save the histogram image. If None, the histogram is displayed.\n    \n    Returns:\n    - matplotlib.axes.Axes: The Axes object of the plot if save_path is not provided.\n    - None: If save_path is provided, the plot is saved to the specified path, and the function returns None.\n    \n    Raises:\n    - FileNotFoundError: If the specified file_path does not exist.\n    \"\"\"\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if the CSV has a header, else assume the first column is the text data\n        if 'Text' in df.columns:\n            text_data = df['Text'].astype(str)\n        else:\n            text_data = df.iloc[:, 0].astype(str)\n        \n        # Use CountVectorizer to count word frequencies, excluding the stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_counts = vectorizer.fit_transform(text_data)\n        \n        # Sum up the counts of each word\n        sum_words = word_counts.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 10 most common words\n        top_words = words_freq[:10]\n        words, counts = zip(*top_words)\n        \n        # Plot the histogram\n        fig, ax = plt.subplots()\n        ax.bar(words, counts)\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45)\n        \n        # Save or display the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close(fig)\n            return None\n        else:\n            return ax\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The specified file path '{file_path}' does not exist.\") from e\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```"}
{"task_id": "BigCodeBench/1057", "solution": "```python\nimport pandas as pd\nimport itertools\nimport random\n\ndef task_func(animals=None, foods=None):\n    # Predefined lists for animals and foods\n    predefined_animals = ['cat', 'dog', 'rabbit']\n    predefined_foods = ['carrot', 'meat', 'lettuce']\n\n    # Handling cases where animals or foods are not provided or are empty\n    if animals is None or len(animals) == 0:\n        animals = predefined_animals\n    if foods is None or len(foods) == 0:\n        foods = predefined_foods\n\n    # If both lists are empty after using predefined lists, return an empty DataFrame\n    if len(animals) == 0 and len(foods) == 0:\n        return pd.DataFrame()\n\n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n\n    # Shuffle the combinations to ensure variety in the DataFrame layout\n    random.shuffle(combinations)\n\n    # Create the DataFrame and fill it with 'animal:food' format\n    df = pd.DataFrame(index=animals, columns=foods)\n    for animal, food in combinations:\n        df.loc[animal, food] = f\"{animal}:{food}\"\n\n    return df\n\n# Example usage:\ndf = task_func(['cat', 'dog'], ['fish', 'meat'])\nprint(df)\n```\n\nThis code defines a function `task_func` that handles the specified cases and generates a DataFrame with the required format. The function uses default lists if the provided lists are empty or not provided. It also ensures that the resulting pairs are shuffled and arranged in a DataFrame where each cell contains a string in the 'animal:food' format."}
{"task_id": "BigCodeBench/1077", "solution": "```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Ensure the timezone is valid\n    try:\n        timezone = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(f\"Invalid timezone: {timezone}\")\n    \n    # Parse the time strings into timezone-aware datetime objects\n    try:\n        timestamps = [datetime.fromisoformat(ts).replace(tzinfo=pytz.UTC) for ts in time_strings]\n    except ValueError as e:\n        raise ValueError(f\"Invalid time string format: {e}\")\n    \n    # Convert timestamps to the specified timezone\n    timestamps = [ts.astimezone(timezone) for ts in timestamps]\n    \n    # Calculate the absolute time differences in seconds between consecutive timestamps\n    time_diffs = [\n        abs((timestamps[i + 1] - timestamps[i]).total_seconds())\n        for i in range(len(timestamps) - 1)\n    ]\n    \n    if not time_diffs:\n        return 0.0\n    \n    # Calculate the average time difference using numpy\n    average_time_diff = np.mean(time_diffs)\n    \n    return float(average_time_diff)\n\n# Example usage:\ntime_strings = [\n    \"2023-10-01T12:00:00\",\n    \"2023-10-01T14:30:00\",\n    \"2023-10-01T16:45:00\"\n]\ntimezone = \"America/New_York\"\n\nprint(task_func(time_strings, timezone))  # Output will be the average time difference in seconds\n```"}
{"task_id": "BigCodeBench/1085", "solution": "```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple, Any\n\ndef task_func(text: str) -> Tuple[List[Tuple[str, int]], Any]:\n    \"\"\"\n    Analyzes the frequency of words in a given text after lowercasing, removing punctuation, \n    splitting into words, and plots the top 10 most common words.\n\n    Parameters:\n    text (str): The input text to be analyzed.\n\n    Returns:\n    Tuple[List[Tuple[str, int]], Any]: A list of tuples containing the 10 most common words and their counts,\n                                       and the Matplotlib Axes object of the bar chart.\n    \"\"\"\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    if not words:\n        # Handle the case where the text is empty or contains no valid words\n        return [], None\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n    \n    if not common_words:\n        # Handle the case where there are no common words to display\n        return [], None\n    \n    # Unzip the words and their counts for plotting\n    words, counts = zip(*common_words)\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    \n    # Show the plot\n    plt.show()\n    \n    return common_words, ax\n\n# Example usage:\n# text = \"Your sample text goes here.\"\n# common_words, ax = task_func(text)\n# print(common_words)\n```"}
{"task_id": "BigCodeBench/1124", "solution": "```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = re.compile(\n        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    )\n    urls = url_pattern.findall(myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    url = urls[0]\n    \n    # Validate the URL structure\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme or not parsed_url.netloc:\n        return \"No valid URL found in the provided string.\"\n    \n    try:\n        response = requests.get(url, timeout=10)  # Added timeout\n        response.raise_for_status()  # Raise an HTTPError for bad responses\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    soup = BeautifulSoup(response.content, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if title_tag is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title_tag.string.strip()\n\n# Example usage:\n# print(task_func(\"Check out this website: https://www.example.com\"))\n```"}
{"task_id": "BigCodeBench/1129", "solution": "```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with the specified key,\n    downloads the file from the URL, and saves it with a timestamped filename.\n    \n    Args:\n        json_data (str): The JSON string containing the URL.\n        unknown_key (str): The key whose associated value is the URL.\n        save_dir (str, optional): The directory to save the downloaded file. Defaults to the current working directory.\n\n    Returns:\n        str: The absolute path of the downloaded file.\n    \n    Raises:\n        KeyError: If the specified key is not found in the JSON data.\n        Exception: If there is an issue with downloading the file.\n    \"\"\"\n    \n    # Parse the JSON string\n    data = json.loads(json_data)\n    \n    # Find the URL associated with the specified key\n    if unknown_key not in data:\n        raise KeyError(f\"Key '{unknown_key}' not found in the provided JSON data.\")\n    \n    url = data[unknown_key]\n    \n    # Generate a timestamp and create the filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    \n    # Ensure the save directory exists\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    # Define the full path for the file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Download the file from the URL and save it\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Check if the request was successful\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n    except requests.RequestException as e:\n        raise Exception(f\"Failed to download the file from URL: {url}\") from e\n    \n    # Return the absolute path of the downloaded file\n    return os.path.abspath(file_path)\n\n# Usage Example\nif __name__ == \"__main__\":\n    json_data = '{\"file_url\": \"https://example.com/sample.txt\"}'\n    unknown_key = \"file_url\"\n    save_dir = \"/path/to/save/directory\"  # Optional; can be omitted to use the current working directory\n\n    file_path = task_func(json_data, unknown_key, save_dir)\n    print(f\"File saved at: {file_path}\")\n```"}
{"task_id": "BigCodeBench/1137", "solution": "```python\nimport bs4\nimport requests\nimport re\nimport json\nimport os\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extract phone numbers from a given URL or local file and save them to a specified file in JSON format.\n\n    Parameters:\n    url (str): The URL or local file path to extract phone numbers from.\n    output_path (str): The file path to save the extracted phone numbers in JSON format.\n\n    Returns:\n    list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Regular expression pattern for extracting phone numbers\n    phone_pattern = re.compile(r'\\+?\\d[\\d\\s.-]{8,}\\d')\n    \n    # Fetch content from URL or local file\n    content = \"\"\n    if url.startswith('http://') or url.startswith('https://'):\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raise an error for bad status codes\n            content = response.text\n        except requests.RequestException as e:\n            print(f\"Error fetching URL content: {e}\")\n            return []\n    else:\n        if os.path.exists(url):\n            try:\n                with open(url, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            except IOError as e:\n                print(f\"Error reading file: {e}\")\n                return []\n        else:\n            print(f\"File not found: {url}\")\n            return []\n    \n    # Parse the content with BeautifulSoup\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    \n    # Extract text content from the HTML\n    text_content = soup.get_text()\n    \n    # Find all phone numbers using regex\n    phone_numbers = re.findall(phone_pattern, text_content)\n    \n    # Remove duplicates by converting the list to a set and back to a list\n    unique_phone_numbers = list(set(phone_numbers))\n    \n    # Save the phone numbers to the specified output path in JSON format\n    try:\n        with open(output_path, 'w', encoding='utf-8') as json_file:\n            json.dump(unique_phone_numbers, json_file, indent=4)\n    except IOError as e:\n        print(f\"Error writing to file: {e}\")\n        return []\n    \n    return unique_phone_numbers\n```"}
