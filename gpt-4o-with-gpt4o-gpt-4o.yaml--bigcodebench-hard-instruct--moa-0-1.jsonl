{"task_id": "BigCodeBench/13", "solution": "Model did not generate a response. Ignore this."}
{"task_id": "BigCodeBench/13", "solution": "```python\nimport subprocess\nimport ftplib\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # List the files in the directory\n    try:\n        file_names = ftp.nlst()\n    except Exception as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir}: {str(e)}\")\n    finally:\n        ftp.quit()\n\n    # Download each file using wget in a subprocess\n    for file_name in file_names:\n        file_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{file_name}\"\n        try:\n            subprocess.run(['wget', file_url], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {file_name} from {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    return file_names\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        downloaded_files = task_func()\n        print(\"Downloaded files:\", downloaded_files)\n    except Exception as e:\n        print(e)\n```"}
{"task_id": "BigCodeBench/15", "solution": "```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The commands file at {commands_file_path} does not exist.\")\n\n    # Create output directory if it does not exist\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    with open(commands_file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        for idx, row in enumerate(reader):\n            if not row:\n                continue\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f'command_{idx}_output.txt')\n            output_files.append(output_file_path)\n\n            try:\n                # Execute the command\n                result = subprocess.run(command, shell=True, text=True, capture_output=True)\n                # Write the output to the file\n                with open(output_file_path, mode='w') as output_file:\n                    if result.returncode == 0:\n                        output_file.write(result.stdout)\n                    else:\n                        output_file.write(f\"Error: Command failed with exit code {result.returncode}\\n\")\n                        output_file.write(result.stderr)\n            except Exception as e:\n                # Handle unexpected exceptions\n                with open(output_file_path, mode='w') as output_file:\n                    output_file.write(f\"Error: An unexpected error occurred: {str(e)}\\n\")\n\n    return output_files\n```\n\n### Explanation:\n\n1. **File Existence Check**: The function first checks if the `commands_file_path` exists. If it does not exist, it raises a `FileNotFoundError`.\n\n2. **Creating Output Directory**: If the specified output directory does not exist, it creates the directory.\n\n3. **Reading Commands from CSV**: The function reads the commands from the CSV file. It uses the `csv.reader` to iterate through each row in the CSV file.\n\n4. **Executing Commands**: For each command, it constructs an output file path and attempts to execute the command using `subprocess.run`. The `shell=True` argument allows for shell commands, and `capture_output=True` captures the standard output and standard error.\n\n5. **Writing Output**: The output (or error message) is written to the respective output file. If a command fails, the error message and exit code are written to the output file.\n\n6. **Return Value**: The function returns a list of paths to the output files created in the output directory.\n\n### Usage Example:\n```python\ncommands_file_path = 'commands.csv'\noutput_dir_path = 'output_directory'\noutput_files = task_func(commands_file_path, output_dir_path)\nprint(output_files)\n```\n\nThis will execute the commands listed in `commands.csv`, store their outputs in separate files in `output_directory`, and print the list of output file paths."}
{"task_id": "BigCodeBench/17", "solution": "```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process found, terminate it\n            proc.terminate()\n            proc.wait()  # Wait for the process to terminate\n            # Restart the process\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # Process not found, start it\n    subprocess.Popen([process_name])\n    return f\"Process not found. Starting {process_name}.\"\n\n# Example usage (uncomment to run):\n# print(task_func(\"your_process_name_here\"))\n```"}
{"task_id": "BigCodeBench/19", "solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"The specified directory '{directory}' does not exist.\")\n    \n    # Get all files in the directory (excluding subdirectories)\n    files = [f for f in glob.glob(os.path.join(directory, \"*\")) if os.path.isfile(f)]\n    \n    # If there are no files, return None\n    if not files:\n        return None\n    \n    # Path to the zip file\n    zip_file_path = os.path.join(directory, 'files.zip')\n    \n    # Create a zip file and add files to it\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))  # Add file with its basename\n    \n    # Return the path to the created zip file\n    return zip_file_path\n```"}
{"task_id": "BigCodeBench/34", "solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    cleaned_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Further clean the text by removing extra whitespace\n    cleaned_text = cleaned_text.strip()\n    \n    # Check if there are any words left after URL removal\n    if not cleaned_text:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud\n\n# Example usage:\n# text = \"Check out this link https://example.com and this one too http://example.org.\"\n# wordcloud = task_func(text)\n```"}
{"task_id": "BigCodeBench/37", "solution": "```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column):\n    # Separate the features and the target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    \n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Initialize and train the Random Forest Classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Extract feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n    \n    # Create a DataFrame for visualization\n    feature_importances = pd.DataFrame({\n        'Features': feature_names,\n        'Feature Importance Score': importances\n    })\n    \n    # Sort the DataFrame by importance score\n    feature_importances = feature_importances.sort_values(by='Feature Importance Score', ascending=False)\n    \n    # Plot the feature importances\n    plt.figure(figsize=(10, 8))\n    ax = sns.barplot(x='Feature Importance Score', y='Features', data=feature_importances)\n    ax.set_title('Visualizing Important Features')\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    plt.tight_layout()\n    plt.show()\n    \n    return clf, ax\n\n# Example usage:\n# df = pd.read_csv('your_dataset.csv')\n# target_column = 'your_target_column'\n# model, ax = task_func(df, target_column)\n```"}
{"task_id": "BigCodeBench/82", "solution": "```python\nfrom flask import Flask, render_template, redirect, url_for, request, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\n# In-memory user storage\nusers = {}\nnext_user_id = 1\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return users.get(int(user_id))\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            user = next((u for u in users.values() if u.username == username), None)\n            if user and user.check_password(password):\n                login_user(user)\n                flash('Logged in successfully.', 'success')\n                next_page = request.args.get('next')\n                return redirect(next_page or url_for('protected'))\n            else:\n                flash('Invalid username or password.', 'danger')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        flash('You have been logged out.', 'info')\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! This is a protected page.'\n\n    @app.route('/register', methods=['GET', 'POST'])\n    def register():\n        form = LoginForm()\n        if form.validate_on_submit():\n            global next_user_id\n            username = form.username.data\n            password = form.password.data\n            if username in [u.username for u in users.values()]:\n                flash('Username already exists.', 'danger')\n            else:\n                user = User(next_user_id, username, password)\n                users[next_user_id] = user\n                next_user_id += 1\n                flash('User registered successfully.', 'success')\n                return redirect(url_for('login'))\n        return render_template('register.html', form=form)\n    \n    return app\n\n# Example usage:\nif __name__ == \"__main__\":\n    app = task_func('supersecretkey', 'templates')\n    app.run(debug=True)\n```"}
{"task_id": "BigCodeBench/89", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based\n    on the Z-score and visualizes the data before and after outlier removal.\n\n    Parameters:\n    - data (np.ndarray): The input dataset.\n    - column (int): The column index to check for outliers.\n    - outlier_z_score (float): The Z-score threshold to identify outliers.\n\n    Returns:\n    - tuple: A tuple containing the original data (np.ndarray), the data without\n             outliers (np.ndarray), and the indices of the outliers (list).\n    \"\"\"\n\n    # Extract the specified column for outlier detection\n    column_data = data[:, column]\n\n    # Standardize the column data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data.reshape(-1, 1)).flatten()\n\n    # Calculate the Z-scores\n    z_scores = np.abs(stats.zscore(standardized_data))\n\n    # Identify the indices of the outliers\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n\n    # Remove the outliers from the data\n    data_without_outliers = np.delete(data, outlier_indices, axis=0)\n\n    # Plot the data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    # Original data scatter plot\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(column_data)), column_data, c='blue', label='Data Points')\n    plt.scatter(outlier_indices, column_data[outlier_indices], c='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n\n    # Data without outliers scatter plot\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[:, column])), data_without_outliers[:, column], c='green', label='Data Points')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Return the original data, the data without outliers, and the indices of the outliers\n    return data, data_without_outliers, outlier_indices.tolist()\n\n# Example usage:\ndata = np.array([[1, 2], [2, 3], [3, 100], [4, 5], [5, 6]])  # Sample dataset\ncolumn = 1  # Column to check for outliers\noutlier_z_score = 2.0  # Z-score threshold\n\noriginal_data, data_without_outliers, outlier_indices = task_func(data, column, outlier_z_score)\nprint(\"Original Data:\\n\", original_data)\nprint(\"Data without Outliers:\\n\", data_without_outliers)\nprint(\"Outlier Indices:\\n\", outlier_indices)\n```"}
{"task_id": "BigCodeBench/92", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    # Check if 'data' is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"The 'data' parameter should be a pandas DataFrame.\")\n    \n    # Check if 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"The 'n_clusters' parameter should be an integer greater than 1.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(data)\n    \n    # Get the cluster labels and centroids\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n    \n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis', marker='o')\n    centroid_scatter = ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=100, label='Centroids')\n    \n    # Add legend\n    handles, _ = scatter.legend_elements()\n    ax.legend(handles + [centroid_scatter], ['Cluster {}'.format(i) for i in range(n_clusters)] + ['Centroids'])\n    \n    # Return the labels and the Axes object\n    return labels, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    data = pd.DataFrame({\n        'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        'y': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    })\n    \n    labels, ax = task_func(data, n_clusters=3)\n    plt.show()\n```"}
{"task_id": "BigCodeBench/93", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert the transformed data to a DataFrame\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n    \n    # Generate the scatter plot\n    fig, ax = plt.subplots()\n    if n_components == 2:\n        ax.scatter(transformed_df['PC1'], transformed_df['PC2'])\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n    elif n_components == 3:\n        ax = plt.axes(projection='3d')\n        ax.scatter3D(transformed_df['PC1'], transformed_df['PC2'], transformed_df['PC3'])\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n        ax.set_zlabel('Principal Component 3')\n    else:\n        ax.scatter(transformed_df.iloc[:, 0], np.zeros_like(transformed_df.iloc[:, 0]))\n        ax.set_xlabel('Principal Component 1')\n        ax.set_title('PCA Scatter Plot (1D)')\n    \n    plt.title('PCA Transformed Data Scatter Plot')\n    plt.show()\n\n    return transformed_df, ax\n\n# Example usage (uncomment the following lines to test the function with a sample dataset):\n# data = np.random.rand(100, 5)  # Sample dataset with 100 samples and 5 features\n# transformed_df, ax = task_func(data, n_components=2)\n# print(transformed_df.head())\n```"}
{"task_id": "BigCodeBench/99", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target\n\n    # Map numeric species back to original names\n    species_map = dict(enumerate(iris.target_names))\n    iris_df['species'] = iris_df['species'].map(species_map)\n\n    # Set the global font to Arial for better readability\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create the pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species')\n\n    # Set the title for the overall plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16)\n\n    # Adjust the title position\n    pair_plot.fig.subplots_adjust(top=0.95)\n\n    # Set axis labels for each subplot\n    for ax in pair_plot.axes.flatten():\n        ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n        ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n\n    # Extracting the figure object\n    fig = pair_plot.fig\n\n    return fig\n\n# Call the function to generate the plot\nfig = task_func()\nplt.show()\n```"}
{"task_id": "BigCodeBench/100", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the last 30 days\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=29)\n        dates = pd.date_range(start=start_date, end=end_date).tolist()\n        \n        # Generate random values for each date\n        values = [random.uniform(0, 100) for _ in range(30)]\n        \n        # Create a DataFrame\n        data = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Plot the data\n        plt.figure(figsize=(10, 5))\n        plt.plot(data['Date'], data['Value'], marker='o', linestyle='-')\n        \n        # Set plot title and labels\n        plt.title('Random Time Series Data', fontname='Arial')\n        plt.xlabel('Date', fontname='Arial')\n        plt.ylabel('Value', fontname='Arial')\n        \n        # Improve readability\n        plt.xticks(fontname='Arial', rotation=45)\n        plt.yticks(fontname='Arial')\n        \n        # Generate the Axes object\n        ax = plt.gca()\n        \n        # Show the plot\n        plt.show()\n        \n        # Return the Axes object\n        return ax\n        \n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the data or plot: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    task_func()\n```"}
{"task_id": "BigCodeBench/101", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load the dataset\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        \n        # The dataset is split over two columns; concatenate into a single DataFrame\n        df1 = raw_df.iloc[:, :14]\n        df2 = raw_df.iloc[:, 14:].dropna(how='all')\n        data = pd.concat([df1, df2], axis=1)\n        \n        # Column names as per the dataset documentation\n        columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n        data.columns = columns\n        \n        # Set the random seed (though it's not actually needed here)\n        np.random.seed(seed)\n        \n        # Compute the correlation matrix\n        corr_matrix = data.corr()\n        \n        # Create a heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=.5)\n        plt.title('Correlation Heatmap of Boston Housing Dataset')\n        \n        # Save the heatmap if save_path is provided\n        if save_path:\n            plt.savefig(save_path, bbox_inches='tight')\n        \n        # Show the plot\n        plt.show()\n        \n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred in generating or saving the plot: {e}\")\n\n# Example usage:\n# ax = task_func(save_path='correlation_heatmap.png')\n```"}
{"task_id": "BigCodeBench/108", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom pandas.api.types import is_numeric_dtype\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    if not is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data.\")\n\n    # Validate frequency\n    try:\n        pd.date_range(start='2020-01-01', periods=2, freq=freq)\n    except ValueError:\n        raise ValueError(f\"'{freq}' is not a valid frequency string.\")\n    \n    # Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Decomposition model must be either 'additive' or 'multiplicative'.\")\n\n    # Ensure DataFrame index is datetime\n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        raise ValueError(\"DataFrame index must be of datetime type.\")\n    \n    # Perform decomposition\n    decomposition_result = seasonal_decompose(df['value'], model=decomposition_model, period=pd.Timedelta(freq).days)\n    \n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(15, 8), sharex=True)\n    decomposition_result.observed.plot(ax=axes[0], title='Observed')\n    decomposition_result.trend.plot(ax=axes[1], title='Trend')\n    decomposition_result.seasonal.plot(ax=axes[2], title='Seasonal')\n    decomposition_result.resid.plot(ax=axes[3], title='Residual')\n\n    plt.tight_layout()\n    plt.show()\n    \n    return (decomposition_result, axes)\n\n# Example usage:\n# df = pd.DataFrame({\n#     'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n#     'date': pd.date_range(start='2020-01-01', periods=12, freq='D')\n# })\n# df.set_index('date', inplace=True)\n# task_func(df)\n```"}
{"task_id": "BigCodeBench/120", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate input types\n    if not isinstance(start_date, datetime):\n        raise ValueError(\"start_date must be a datetime.datetime instance\")\n    if not isinstance(end_date, datetime):\n        raise ValueError(\"end_date must be a datetime.datetime instance\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date\")\n    \n    # Set the random seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days in the range\n    num_days = (end_date - start_date).days + 1\n    \n    # Generate random dates within the specified range\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n    \n    # Create a pandas Series from the list of random dates\n    return pd.Series(random_dates)\n\n# Example usage\nif __name__ == \"__main__\":\n    start_date = datetime(2020, 1, 1)\n    end_date = datetime(2020, 12, 31)\n    random_dates_series = task_func(start_date, end_date)\n    print(random_dates_series)\n```"}
{"task_id": "BigCodeBench/123", "solution": "```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is indeed a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"The input 'my_list' must be a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append(12)\n    \n    # Calculate the sum of the numbers in the list\n    try:\n        total_sum = sum(map(int, my_list))\n    except ValueError:\n        raise ValueError(\"The list contains non-integer values that cannot be summed.\")\n    \n    # Get all CSV files in the specified directory\n    csv_files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    \n    # Check if there are any CSV files in the directory\n    if not csv_files:\n        raise FileNotFoundError(f\"No files with extension {file_ext} found in directory {file_dir}.\")\n    \n    # Determine the number of files to concatenate based on the sum\n    num_files_to_concat = total_sum\n    \n    # Sort the files to ensure consistency in the selection process\n    csv_files.sort()\n    \n    # Select the required number of files\n    selected_files = csv_files[:num_files_to_concat]\n    \n    # Read and concatenate the selected CSV files into a single DataFrame\n    data_frames = [pd.read_csv(file) for file in selected_files]\n    concatenated_df = pd.concat(data_frames, ignore_index=True)\n    \n    return concatenated_df\n\n# Example usage:\n# my_list = [1, 2, 3]\n# df = task_func(my_list, './data_files/', '.csv')\n# print(df)\n```"}
{"task_id": "BigCodeBench/124", "solution": "```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Validate that 'my_list' is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Validate that all elements in 'my_list' are numeric (int or float)\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in my_list must be numeric (int or float)\")\n    \n    # Append the number 12 to 'my_list'\n    my_list.append(12)\n    \n    # Calculate the sum of elements in 'my_list'\n    sum_elements = sum(my_list)\n    \n    # Determine the size of the random integers list\n    random_list_size = min(sum_elements, size)\n    \n    # Seed the random number generator for reproducibility\n    random_seed(seed)\n    \n    # Measure the time taken to generate the list of random integers\n    start_time = time.time()\n    random_list = [randint(1, 100) for _ in range(random_list_size)]\n    end_time = time.time()\n    total_time = end_time - start_time\n    \n    # Plot the histogram of the generated random numbers\n    fig, ax = plt.subplots()\n    ax.hist(random_list, bins=range(1, 102), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Generated Random Numbers')\n    \n    return (total_time, ax)\n\n# Example usage:\nif __name__ == \"__main__\":\n    my_list = [1, 2, 3]\n    size = 50\n    seed = 42\n    \n    time_taken, ax = task_func(my_list, size, seed)\n    print(f\"Time taken: {time_taken} seconds\")\n    plt.show()\n```"}
{"task_id": "BigCodeBench/129", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the specified URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n    except requests.ConnectionError:\n        raise ConnectionError(\"There was an issue connecting to the URL.\")\n    except requests.HTTPError:\n        raise requests.HTTPError(f\"HTTP request to {url} failed with status code {response.status_code}.\")\n\n    try:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find the first table in the HTML\n        table = soup.find('table')\n        if table is None:\n            raise ValueError(\"No table data found on the page.\")\n        \n        # Extract headers, if present\n        headers = []\n        header_row = table.find('tr')\n        \n        if header_row:\n            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n        \n        # Extract table rows\n        rows = []\n        for row in table.find_all('tr')[1:]:  # Skip the header row\n            rows.append([td.get_text(strip=True) for td in row.find_all('td')])\n        \n        # Create a DataFrame\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n        \n        return df\n    except Exception as e:\n        raise ValueError(f\"Error parsing the page content: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    url = 'http://example.com'\n    try:\n        df = task_func(url)\n        print(df)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```"}
{"task_id": "BigCodeBench/139", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Validate input\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    # Select numeric columns\n    numeric_columns = df.select_dtypes(include=np.number).columns\n    \n    if numeric_columns.empty:\n        raise ValueError(\"There are no numeric columns in the DataFrame.\")\n    \n    # Create histograms for each numeric column\n    axes = []\n    for col in numeric_columns:\n        fig, ax = plt.subplots()\n        df[col].hist(ax=ax)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes\n\n# Example usage:\n# df = pd.DataFrame({\n#     'A': np.random.randn(100),\n#     'B': np.random.rand(100) * 100,\n#     'C': ['foo', 'bar', 'baz', 'qux'] * 25\n# })\n# axes = task_func(df)\n# plt.show()  # To display the histograms\n```"}
{"task_id": "BigCodeBench/147", "solution": "```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread, Lock\n\ndef task_func(ip_range, port):\n    def check_port(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(1)  # Set timeout to 1 second\n            try:\n                result = sock.connect_ex((ip, port))\n                with lock:\n                    port_status[ip] = result == 0\n            except Exception:\n                with lock:\n                    port_status[ip] = False\n\n    ip_network = IPv4Network(ip_range)\n    port_status = {}\n    threads = []\n    lock = Lock()\n\n    for ip in ip_network.hosts():\n        thread = Thread(target=check_port, args=(str(ip),))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return port_status\n\n# Example usage:\nif __name__ == \"__main__\":\n    ip_range = \"192.168.1.0/24\"\n    port = 80\n    result = task_func(ip_range, port)\n    print(result)\n```"}
{"task_id": "BigCodeBench/161", "solution": "```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Define the regex pattern for log entries\n    log_pattern = r'^(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.+)$'\n    log_entries = []\n\n    # Read the log file\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = re.match(log_pattern, line.strip())\n            if match:\n                log_type = match.group('type')\n                timestamp = match.group('timestamp')\n                message = match.group('message')\n                \n                # Validate the timestamp\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp found: {timestamp}\")\n                \n                log_entries.append((log_type, timestamp, message))\n    \n    # Raise exception if no valid log entries are found\n    if not log_entries:\n        raise ValueError(\"No valid log entries found.\")\n    \n    # Create a DataFrame from the log entries\n    df = pd.DataFrame(log_entries, columns=['Type', 'Timestamp', 'Message'])\n    \n    # Define the output CSV file path\n    output_csv_path = 'structured_log_data.csv'\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(output_csv_path, index=False)\n    \n    return output_csv_path\n```"}
{"task_id": "BigCodeBench/162", "solution": "```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Extract words from the text using regular expressions\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the lengths of the words\n    word_lengths = [len(word) for word in words]\n    \n    # Initialize the plot\n    fig, ax = plt.subplots()\n    \n    # If there are any word lengths, plot the histogram\n    if word_lengths:\n        ax.hist(word_lengths, bins=np.arange(1, max(word_lengths) + 2) - 0.5, rwidth=rwidth)\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths')\n    else:\n        ax.text(0.5, 0.5, 'No words to display', ha='center', va='center', fontsize=12)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths')\n    \n    # Return the Axes object\n    return ax\n\n# Example usage:\n# ax = task_func(\"This is an example text to visualize word lengths.\")\n# plt.show()\n```"}
{"task_id": "BigCodeBench/177", "solution": "```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef task_func(df):\n    # Check if the DataFrame is empty or missing necessary columns\n    if df.empty or not {'Title', 'Content'}.issubset(df.columns):\n        raise ValueError(\"The DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n    \n    # Define keywords and compile regex pattern for filtering\n    keywords = ['like', 'what']\n    pattern = re.compile('|'.join(keywords), re.IGNORECASE)\n    \n    # Filter DataFrame based on title containing specified keywords\n    filtered_df = df[df['Title'].str.contains(pattern, na=False)]\n    \n    # Combine all content from filtered DataFrame into a single string\n    combined_content = ' '.join(filtered_df['Content'].dropna().tolist())\n    \n    # Remove punctuation\n    combined_content = combined_content.translate(str.maketrans('', '', punctuation))\n    \n    # Tokenize the content into words\n    words = nltk.word_tokenize(combined_content)\n    \n    # Convert words to lowercase\n    words = [word.lower() for word in words]\n    \n    # Count the frequency of each word\n    word_freq = Counter(words)\n    \n    return dict(word_freq)\n\n# Ensure you have the necessary NLTK data files installed\n# You can download them using the following command:\n# nltk.download('punkt')\n```"}
{"task_id": "BigCodeBench/184", "solution": "```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Helper function to clean text data\n    def clean_text(text):\n        # Lowercase the text\n        text = text.lower()\n        # Remove punctuation and numbers\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text = re.sub(r'\\d+', '', text)\n        # Remove stopwords\n        text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n        return text\n\n    # Apply the cleaning function to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(clean_text)\n\n    # Initialize the CountVectorizer\n    vectorizer = CountVectorizer()\n    # Fit and transform the text data\n    text_matrix = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert the text matrix to a DataFrame\n    result_df = pd.DataFrame(text_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return result_df\n\n# Example usage:\n# df = pd.DataFrame({'text_column': ['This is a sample text.', 'Another sample text here!']})\n# print(task_func(df, 'text_column'))\n```"}
{"task_id": "BigCodeBench/187", "solution": "```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate dictionary keys and values\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"The dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values must be tuples.\")\n    \n    # Generate random coordinates within the specified ranges\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n    lons = np.random.uniform(lon_min, lon_max, len(cities))\n    lats = np.random.uniform(lat_min, lat_max, len(cities))\n    \n    # Create Point objects from the coordinates\n    geometry = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n    \n    # Create a GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': geometry}, geometry='Coordinates')\n    \n    return gdf\n\n# Example usage\ngdf = task_func()\nprint(gdf)\n```"}
{"task_id": "BigCodeBench/199", "solution": "```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate input\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError('utc_datetime must be a datetime object.')\n    \n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError('All cities must be strings.')\n    \n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError('All weather conditions must be strings.')\n    \n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError('timezones must be a dictionary with city names and timezone strings.')\n    \n    if not isinstance(seed, int):\n        raise ValueError('seed must be an integer.')\n\n    # Set random seed for reproducibility\n    set_seed(seed)\n    \n    # Prepare weather report data\n    data = []\n    \n    for city in cities:\n        # Get corresponding timezone\n        tz_name = timezones.get(city, None)\n        if not tz_name:\n            raise ValueError(f\"Timezone for city {city} is not specified in the timezones dictionary.\")\n        \n        local_tz = pytz.timezone(tz_name)\n        \n        # Convert UTC datetime to local time\n        local_time = utc_datetime.astimezone(local_tz)\n        local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Append data to list\n        data.append({\n            'City': city,\n            'Local Time': local_time_str,\n            'Weather Condition': weather_condition\n        })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['City', 'Local Time', 'Weather Condition'])\n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    utc_datetime = datetime(2023, 10, 1, 12, 0, 0, tzinfo=pytz.utc)\n    weather_report = task_func(utc_datetime)\n    print(weather_report)\n```"}
{"task_id": "BigCodeBench/208", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(elements, seed=0):\n    # Check if elements is a positive integer\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random steps of -1 or 1\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Calculate the random walk\n    random_walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    descriptive_stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'standard deviation': np.std(random_walk),\n        'minimum': np.min(random_walk),\n        '5th percentile': np.percentile(random_walk, 5),\n        '25th percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th percentile': np.percentile(random_walk, 75),\n        '95th percentile': np.percentile(random_walk, 95),\n        'maximum': np.max(random_walk)\n    }\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(random_walk, label='Random Walk')\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    ax.legend()\n    \n    return descriptive_stats, ax\n\n# Example usage:\n# stats, ax = task_func(100)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/211", "solution": "```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Get the filename from the URL\n    zip_filename = os.path.join(destination_directory, \"downloaded.zip\")\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Check if the request was successful\n    \n    # Save the zip file to the specified directory\n    with open(zip_filename, 'wb') as file:\n        file.write(response.content)\n    \n    # Extract the zip file\n    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n    \n    # List extracted files\n    extracted_files = [os.path.join(destination_directory, name) for name in zip_ref.namelist()]\n    \n    return extracted_files\n\n# Example usage\n# url = 'http://example.com/somefile.zip'\n# destination_directory = 'path/to/destination'\n# extracted_files = task_func(url, destination_directory)\n# print(extracted_files)\n```\n\n### Explanation:\n1. **Directory Creation**: Ensures the destination directory exists using `os.makedirs(destination_directory, exist_ok=True)`.\n2. **Filename Handling**: Sets the filename for the downloaded zip file as \"downloaded.zip\" within the destination directory.\n3. **Downloading**: Uses `requests.get(url, headers=headers)` to download the zip file from the provided URL and checks if the request was successful using `response.raise_for_status()`.\n4. **Saving**: Writes the content of the response to a file named \"downloaded.zip\".\n5. **Extracting**: Opens the downloaded zip file with `zipfile.ZipFile` and extracts its contents to the destination directory.\n6. **Listing Files**: Creates a list of full paths for the extracted files.\n7. **Return**: Returns the list of extracted file paths."}
{"task_id": "BigCodeBench/214", "solution": "```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Validate input range\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n    \n    # Set the random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    \n    # Generate a random RGB image\n    image = np.random.randint(range_low, range_high, image_size, dtype=np.uint8)\n    \n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(image)\n    ax.axis('off')  # Hide the axes for better visualization\n    plt.show()\n    \n    return ax, image\n\n# Example usage:\nax, image = task_func()\n```"}
{"task_id": "BigCodeBench/227", "solution": "```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file '{audio_file}' does not exist.\")\n    \n    # Read the audio file\n    data, samplerate = sf.read(audio_file)\n    \n    # Calculate the SPL\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n    \n    # Convert list L to an MxN matrix\n    if len(L) != M * N:\n        raise ValueError(\"The length of list L must be equal to M * N\")\n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize the matrix based on SPL\n    normalized_matrix = matrix / spl\n    \n    # Generate the spectrogram from the normalized matrix\n    fig, ax = plt.subplots()\n    S = np.abs(librosa.stft(normalized_matrix.flatten()))**2\n    S_db = librosa.amplitude_to_db(S, ref=np.max)\n    \n    img = librosa.display.specshow(S_db, x_axis='time', y_axis='log', ax=ax)\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n    plt.title(\"Spectrogram\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Frequency (Hz)\")\n    \n    return normalized_matrix, fig\n\n# Example usage:\n# L = [i for i in range(10000)]\n# M, N = 100, 100\n# audio_file = 'example.wav'\n# normalized_matrix, fig = task_func(L, M, N, audio_file)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/239", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = [item for sublist in original for item in sublist if isinstance(item, (int, float))]\n    \n    # Convert the list to a numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_array),\n        'std_dev': np.std(numeric_array),\n        'min': np.min(numeric_array),\n        'max': np.max(numeric_array),\n    }\n    \n    # Create the histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, bins='auto', density=True, alpha=0.6, color='g')\n    \n    # Compute the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, statistics['mean'], statistics['std_dev'])\n    \n    # Plot the PDF\n    ax.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mean = %.2f, std = %.2f\" % (statistics['mean'], statistics['std_dev'])\n    ax.set_title(title)\n    \n    # Return the results\n    return numeric_array, statistics, ax\n\n# Example usage\n# original = [(1, 2.5, 'a'), (3, 'b', 4.5), (5, 6.1, 'c')]\n# numeric_array, statistics, ax = task_func(original)\n# print(\"Numeric Array:\", numeric_array)\n# print(\"Statistics:\", statistics)\n# plt.show()  # Uncomment to display the plot\n```"}
{"task_id": "BigCodeBench/241", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using sklearn's MinMaxScaler\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Plot the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array, label='Normalized')\n    ax.legend()\n    ax.set_title('Original and Normalized Arrays')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the original array, normalized array, and the Axes object\n    return original_array, normalized_array, ax\n\n# Example usage:\noriginal = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\noriginal_array, normalized_array, ax = task_func(original)\n```"}
{"task_id": "BigCodeBench/267", "solution": "```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\"\n    signal = np.array(list(data.values()), dtype=float)\n    \n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    \n    # Step 4: Plot the FFT of the signal\n    freqs = fftpack.fftfreq(len(signal), 1/sample_rate)\n    \n    fig, ax = plt.subplots()\n    ax.plot(freqs, np.abs(fft_signal))\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('FFT of the Signal')\n    ax.grid(True)\n    plt.show()\n\n    # Return the FFT and the Axes object\n    return fft_signal, ax\n\n# Example usage:\n# data = {'key1': 0.5, 'key2': 0.75, 'key3': 0.25}\n# fft_signal, ax = task_func(data)\n# print(fft_signal)\n```"}
{"task_id": "BigCodeBench/273", "solution": "```python\nimport cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    class MyHTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n        \n        def do_POST(self):\n            # Check Content-Type\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_error(400, \"Content-Type header is not application/json\")\n                return\n\n            # Get the length of the data\n            content_length = int(self.headers.get('Content-Length', 0))\n            post_data = self.rfile.read(content_length)\n            \n            # Try to parse the data as JSON\n            try:\n                parsed_data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_error(400, \"Invalid JSON\")\n                return\n\n            # Check if 'data' key exists in the JSON\n            if 'data' not in parsed_data:\n                self.send_error(400, \"No data key in request\")\n                return\n\n            # Respond with success message\n            self.send_response(200)\n            response_json = json.dumps(SUCCESS_RESPONSE).encode('utf-8')\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', len(response_json))\n            self.end_headers()\n            self.wfile.write(response_json)\n\n    # Run the server\n    server_address = ('', 8000)\n    httpd = http.server.HTTPServer(server_address, MyHTTPRequestHandler)\n    print('Running server on port 8000...')\n    httpd.serve_forever()\n\nif __name__ == \"__main__\":\n    task_func()\n```"}
{"task_id": "BigCodeBench/274", "solution": "```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom json.decoder import JSONDecodeError\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \n    class EmailHTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', '16')\n                self.end_headers()\n                self.wfile.write(b'Invalid Content-Type')\n                return\n\n            content_length = int(self.headers.get('Content-Length', 0))\n            post_body = self.rfile.read(content_length)\n\n            try:\n                email_data = json.loads(post_body)\n            except JSONDecodeError:\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', '15')\n                self.end_headers()\n                self.wfile.write(b'Invalid JSON data')\n                return\n\n            try:\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n            except KeyError:\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', '24')\n                self.end_headers()\n                self.wfile.write(b'Missing required fields')\n                return\n\n            msg = MIMEText(message)\n            msg['Subject'] = subject\n            msg['From'] = smtp_username\n            msg['To'] = to\n\n            try:\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, [to], msg.as_string())\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', '21')\n                self.end_headers()\n                self.wfile.write(b'Authentication Failed')\n                return\n\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/plain')\n            response_message = 'Email sent'\n            self.send_header('Content-Length', len(response_message))\n            self.end_headers()\n            self.wfile.write(response_message.encode())\n\n    return EmailHTTPRequestHandler\n\n# Example usage:\nif __name__ == '__main__':\n    smtp_server = 'smtp.example.com'\n    smtp_port = 587\n    smtp_username = 'your_username@example.com'\n    smtp_password = 'your_password'\n    server_address = ('', 8080)\n    handler_class = task_func(smtp_server, smtp_port, smtp_username, smtp_password)\n    httpd = http.server.HTTPServer(server_address, handler_class)\n    print('Starting server at http://localhost:8080')\n    httpd.serve_forever()\n```"}
{"task_id": "BigCodeBench/287", "solution": "```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    word_counts = {}\n    total_words = 0\n    \n    # Iterate over all files in the specified directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    words = content.split()\n                    count = len(words)\n                    word_counts[file] = count\n                    total_words += count\n    \n    # Export the word counts to a JSON file\n    with open(filename, 'w', encoding='utf-8') as json_file:\n        json.dump(word_counts, json_file, indent=4)\n    \n    return total_words\n\n# Example usage:\n# total_words = task_func('word_counts.json', '/path/to/directory')\n# print(total_words)\n```"}
{"task_id": "BigCodeBench/302", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Validate the DataFrame input\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    if 'Value' not in df.columns:\n        raise ValueError(\"The DataFrame does not have a 'Value' column.\")\n    if not all(isinstance(i, list) for i in df['Value']):\n        raise ValueError(\"The 'Value' column must only contain lists.\")\n    \n    # Split lists in the 'Value' column into separate columns\n    value_df = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Calculate the Pearson correlation coefficient matrix\n    correlation_matrix = value_df.corr(method='pearson')\n    \n    # Plot the correlation matrix as a heatmap if plot is True\n    ax = None\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n    \n    return correlation_matrix, ax\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Date': ['2023-01-01', '2023-01-02', '2023-01-03'],\n#     'Value': [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# })\n# correlation_matrix, ax = task_func(df, plot=True)\n```"}
{"task_id": "BigCodeBench/308", "solution": "```python\nimport pandas as pd\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Combine predefined fields with additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Create a dictionary to store each student's grades\n    data = {student: [random.randint(0, 100) for _ in all_fields] for student in STUDENTS}\n    \n    # Create a DataFrame from the data dictionary\n    df = pd.DataFrame(data, index=all_fields).T\n    \n    # Calculate the average grade for each student and add it as a new column\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject and add it as a new row\n    avg_row = df.mean(axis=0)\n    avg_row.name = 'Average'\n    df = df.append(avg_row)\n    \n    # Reorder the columns to make 'Average Grade' the last column\n    cols = df.columns.tolist()\n    cols = cols[:-1] + ['Average Grade']\n    df = df[cols]\n\n    return df\n\n# Example usage\ndf = task_func()\nprint(df)\n```"}
{"task_id": "BigCodeBench/310", "solution": "```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef generate_random_name():\n    first_names = ['John', 'Jane', 'Alex', 'Emily', 'Chris', 'Katie', 'Michael', 'Sarah', 'David', 'Laura']\n    last_names = ['Smith', 'Johnson', 'Williams', 'Jones', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor']\n    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n\ndef generate_random_age():\n    return random.randint(18, 80)  # Age between 18 and 80\n\ndef generate_random_height():\n    return round(random.uniform(150, 200), 2)  # Height in cm between 150 and 200\n\ndef generate_random_weight():\n    return round(random.uniform(50, 100), 2)  # Weight in kg between 50 and 100\n\ndef task_func(filename):\n    # Generate data\n    people_data = []\n    for _ in range(PEOPLE_COUNT):\n        name = generate_random_name()\n        age = generate_random_age()\n        height = generate_random_height()\n        weight = generate_random_weight()\n        people_data.append([name, age, height, weight])\n\n    # Calculate averages\n    ages = [person[1] for person in people_data]\n    heights = [person[2] for person in people_data]\n    weights = [person[3] for person in people_data]\n    \n    average_age = mean(ages)\n    average_height = mean(heights)\n    average_weight = mean(weights)\n\n    # Append averages to the data\n    averages_row = [\"Average\", average_age, average_height, average_weight]\n    people_data.append(averages_row)\n\n    # Write data to CSV\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(people_data)\n    \n    return os.path.abspath(filename)\n\n# Example usage\nfile_path = task_func(\"people_data.csv\")\nprint(f\"CSV file created at: {file_path}\")\n```"}
{"task_id": "BigCodeBench/313", "solution": "```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Ensure the provided path is a directory\n    if not os.path.isdir(directory):\n        raise ValueError(\"The provided path is not a valid directory.\")\n    \n    # Dictionary to keep track of the files moved to each subdirectory\n    organized_files = {}\n    \n    # Regular expression to find the first text that is not enclosed in square brackets\n    regex = re.compile(r'([^[]+?)(?=\\s*\\[|$)')\n    \n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n        \n        # Search for the pattern\n        match = regex.match(filename)\n        if match:\n            subdir_name = match.group(1).strip()\n            subdir_path = os.path.join(directory, subdir_name)\n            \n            # Ensure the subdirectory exists\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n                organized_files[subdir_name] = []\n            \n            # Move the file to the appropriate subdirectory\n            shutil.move(file_path, subdir_path)\n            organized_files[subdir_name].append(filename)\n\n    # Return the directory path and the dictionary of organized files\n    return (directory, organized_files)\n\n# Example usage:\n# result = task_func('/path/to/your/directory')\n# print(result)\n```"}
{"task_id": "BigCodeBench/324", "solution": "```python\nimport subprocess\nimport time\nimport threading\n\ndef run_subprocess(file, result, index):\n    try:\n        # Execute the file as a subprocess\n        completed_process = subprocess.run(['python', file], check=True)\n        # Store the exit code in the result list\n        result[index] = completed_process.returncode\n    except subprocess.CalledProcessError as e:\n        # Store the exit code in case of an error\n        result[index] = e.returncode\n\ndef task_func(file_list):\n    # List to store the exit codes of the subprocesses\n    result = [None] * len(file_list)\n    # List to keep track of threads\n    threads = []\n\n    for i, file in enumerate(file_list):\n        # Create a new thread to run the subprocess\n        thread = threading.Thread(target=run_subprocess, args=(file, result, i))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    files = [\"script1.py\", \"script2.py\", \"script3.py\"]\n    exit_codes = task_func(files)\n    print(exit_codes)\n```"}
{"task_id": "BigCodeBench/326", "solution": "```python\nimport subprocess\nimport os\nimport glob\n\ndef task_func(directory_path):\n    results = []\n\n    # Create the full path pattern to search for .bat files\n    search_pattern = os.path.join(directory_path, '*.bat')\n\n    # Use glob to find all .bat files in the directory\n    bat_files = glob.glob(search_pattern)\n\n    for bat_file in bat_files:\n        try:\n            # Execute the .bat file and capture the exit code\n            process = subprocess.run(bat_file, shell=True)\n            exit_code = process.returncode\n        except Exception:\n            # If there is an exception, set the exit code to None\n            exit_code = None\n\n        # Append the file name and exit code to the results list\n        results.append((os.path.basename(bat_file), exit_code))\n\n    return results\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     directory_path = \"path_to_your_directory\"\n#     print(task_func(directory_path))\n```"}
{"task_id": "BigCodeBench/341", "solution": "```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Check if the input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame.\")\n    \n    # Check if the DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The input DataFrame must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The DataFrame does not contain the column '{col}'.\")\n    \n    # Generate a figure with two subplots\n    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n    \n    # Plot the histogram and KDE (for numerical data) on the first subplot\n    sns.histplot(df[col], kde=True, ax=axs[0])\n    axs[0].set_title(f'Histogram and KDE for {col}')\n    \n    # Plot the box plot on the second subplot\n    sns.boxplot(x=df[col], ax=axs[1])\n    axs[1].set_title(f'Box plot for {col}')\n    \n    # Adjust the layout\n    plt.tight_layout()\n    \n    return fig\n\n# Example usage:\n# df = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6]})\n# fig = task_func(df, 'A')\n# plt.show()\n```"}
{"task_id": "BigCodeBench/346", "solution": "```python\nimport subprocess\nimport os\nimport sys\n\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(\"The script does not exist.\")\n    \n    # Prepare the command with arguments\n    command = [sys.executable, script_path] + list(args)\n\n    try:\n        if wait:\n            # Run the script and wait for it to complete\n            result = subprocess.run(command, check=True)\n            return result.returncode\n        else:\n            # Run the script without waiting\n            subprocess.Popen(command)\n            return None\n    except subprocess.CalledProcessError as e:\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, output=e.output, stderr=e.stderr)\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        # Example of running a script with arguments and waiting for it to complete\n        return_code = task_func(\"example_script.py\", True, \"arg1\", \"arg2\")\n        print(f\"Script completed with return code: {return_code}\")\n\n        # Example of running a script without waiting\n        task_func(\"example_script.py\", False, \"arg1\", \"arg2\")\n        print(\"Script started without waiting\")\n\n    except ValueError as e:\n        print(f\"Error: {e}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Script raised an exception: {e}\")\n```"}
{"task_id": "BigCodeBench/360", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file at {file_location} does not exist.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\") from e\n\n    # Initialize the result dictionary\n    result_dict = {}\n    \n    # Calculate the mean and standard deviation for each column\n    for column in df.columns:\n        if np.issubdtype(df[column].dtype, np.number):  # Ensure the column is numeric\n            mean = df[column].mean()\n            std_dev = df[column].std()\n            result_dict[column] = {'mean': mean, 'std_dev': std_dev}\n    \n    # Prepare data for the bar chart\n    columns = list(result_dict.keys())\n    means = [result_dict[col]['mean'] for col in columns]\n    std_devs = [result_dict[col]['std_dev'] for col in columns]\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    x = np.arange(len(columns))\n    \n    width = 0.4  # the width of the bars\n    ax.bar(x - width/2, means, width, label='Mean')\n    ax.bar(x + width/2, std_devs, width, label='Standard Deviation')\n    \n    # Add labels, title, and legend\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns)\n    ax.legend()\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return result_dict, fig\n\n# Example usage:\n# file_location = 'path/to/your/file.xlsx'\n# sheet_name = 'Sheet1'\n# result_dict, fig = task_func(file_location, sheet_name)\n```"}
{"task_id": "BigCodeBench/367", "solution": "```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all elements in activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n    \n    # Dictionary to count activities per day of the week\n    activity_count = defaultdict(int)\n    \n    # Count the activities for each day of the week\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_count[day_of_week] += 1\n    \n    # Prepare data for plotting\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [activity_count[day] for day in days]\n    \n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax\n\n# Example usage:\n# activities = [\n#     datetime(2023, 10, 2, 10, 0), # Monday\n#     datetime(2023, 10, 3, 12, 0), # Tuesday\n#     datetime(2023, 10, 4, 14, 0), # Wednesday\n#     datetime(2023, 10, 4, 15, 0), # Wednesday\n#     datetime(2023, 10, 5, 9, 0),  # Thursday\n#     datetime(2023, 10, 6, 11, 0), # Friday\n#     datetime(2023, 10, 7, 16, 0), # Saturday\n#     datetime(2023, 10, 8, 18, 0)  # Sunday\n# ]\n# ax = task_func(activities)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/368", "solution": "```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Ensure the source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Raise an error if there are no files to move\n    if not files:\n        raise FileNotFoundError(f\"No files found in the source directory '{src_dir}'.\")\n\n    # Set the random seed for reproducibility\n    random.seed(seed)\n    \n    # Choose a random file from the list\n    random_file = random.choice(files)\n    \n    # Construct the full file paths\n    src_file_path = os.path.join(src_dir, random_file)\n    dest_file_path = os.path.join(dest_dir, random_file)\n    \n    # Move the file to the destination directory\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the moved file\n    return random_file\n\n# Example usage:\n# moved_file = task_func('source_directory', 'destination_directory')\n# print(f\"Moved file: {moved_file}\")\n```"}
{"task_id": "BigCodeBench/374", "solution": "```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Get a list of all xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Initialize the count of processed files\n    files_processed = 0\n    \n    # Iterate through each xlsx file\n    for file in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file)\n        modified = False\n        \n        # Iterate through each sheet in the workbook\n        for sheet in workbook:\n            # Iterate through each row in the sheet\n            for row in sheet.iter_rows():\n                # Iterate through each cell in the row\n                for cell in row:\n                    if cell.value and isinstance(cell.value, str):\n                        # Protect double quotes by prefixing with double backslash\n                        new_value = re.sub(r'\"', r'\\\\\"', cell.value)\n                        if new_value != cell.value:\n                            cell.value = new_value\n                            modified = True\n        \n        # Save the workbook only if modifications were made\n        if modified:\n            workbook.save(file)\n            files_processed += 1\n    \n    return files_processed\n\n# Example call to the function\n# The number of processed files will be printed\nif __name__ == \"__main__\":\n    count = task_func('./xlsx_files/')\n    print(f\"Number of Excel files processed: {count}\")\n```"}
{"task_id": "BigCodeBench/399", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    # Validate the frequency and sample size\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    # Generate the time values based on the sample size\n    t = np.linspace(0, 1, sample_size)\n    \n    # Generate the sine and cosine wave values\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    \n    # Add labels and legend\n    ax.set_xlabel('Time [s]')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    ax.set_title(f'Sine and Cosine Waves with frequency = {frequency} Hz')\n    \n    # Show grid\n    ax.grid(True)\n    \n    # Return the figure and axes objects\n    return fig, ax\n\n# Example usage\n# fig, ax = task_func(5)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/401", "solution": "```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create a Flask app instance\n    app = Flask(app_name)\n\n    # Set email server details from environment variables or default values\n    mail_settings = {\n        \"MAIL_SERVER\": os.getenv('MAIL_SERVER', 'localhost'),\n        \"MAIL_PORT\": int(os.getenv('MAIL_PORT', 25)),\n        \"MAIL_USE_TLS\": os.getenv('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't'],\n        \"MAIL_USERNAME\": os.getenv('MAIL_USERNAME', None),\n        \"MAIL_PASSWORD\": os.getenv('MAIL_PASSWORD', None)\n    }\n\n    # Update app configuration\n    app.config.update(mail_settings)\n\n    # Initialize Flask-Mail with the app\n    mail = Mail(app)\n\n    # Return the mail instance and the mail configuration\n    return (mail, mail_settings)\n\n# Example usage\nif __name__ == \"__main__\":\n    mail_instance, mail_config = task_func(\"my_flask_app\")\n    print(mail_instance)\n    print(mail_config)\n```"}
{"task_id": "BigCodeBench/409", "solution": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file at {full_path} does not exist.\")\n    \n    # Read the Excel file into a DataFrame\n    df = pd.read_excel(full_path)\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Calculate the mean, median, and standard deviation\n    mean_value = df[column_name].mean()\n    median_value = df[column_name].median()\n    std_dev_value = df[column_name].std()\n    \n    # Create the result dictionary\n    result = {\n        \"mean\": mean_value,\n        \"median\": median_value,\n        \"standard_deviation\": std_dev_value\n    }\n    \n    return result\n\n# Example usage\n# result = task_func('path/to/excel', 'file.xlsx', 'column_name')\n# print(result)\n```"}
{"task_id": "BigCodeBench/417", "solution": "```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    # Ensure the input dimension of X is always 2\n    assert X.shape[1] == 2, \"Input dimension of X must be 2\"\n\n    # Split the data into a training set (75%) and a test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(10, input_dim=2, activation='sigmoid'))  # One dense hidden layer with sigmoid activation\n    model.add(Dense(1, activation='sigmoid'))  # Output layer\n    \n    # Compile the model\n    sgd = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    \n    # Fit the model to the training data and evaluate on the test set\n    history = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), verbose=0)\n    \n    # Plot the model's training and validation loss over epochs\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n    \n    # Return the trained model and the Axes object\n    return model, ax\n\n# Example usage:\n# Assuming X and Y are already defined with appropriate shape\n# model, ax = task_func(X, Y)\n# plt.show()  # To display the plot\n```\n\n### Explanation:\n1. **Data Splitting**: The function splits the data into training (75%) and test (25%) sets using `train_test_split`.\n2. **Model Construction**: A `Sequential` model is created with one dense hidden layer (10 units) and a sigmoid activation function, followed by an output layer with a sigmoid activation function.\n3. **Compilation**: The model is compiled using the SGD optimizer with a learning rate of 0.01 and binary cross-entropy loss.\n4. **Training**: The model is trained on the training set for 100 epochs, with validation on the test set.\n5. **Plotting**: The training and validation loss are plotted over the epochs, with the plot titled 'Model loss', the x-axis labeled 'Epoch', and the y-axis labeled 'Loss'. A legend distinguishes between train and test losses.\n6. **Return**: The function returns the trained model and the Axes object for further customization.\n\nEnsure you have the necessary packages installed with:\n```sh\npip install sklearn matplotlib keras tensorflow\n```\n\nYou can use the function by providing `X` and `Y` as arguments and then display the plot using `plt.show()`."}
{"task_id": "BigCodeBench/418", "solution": "```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training (70%) and test (30%) sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create a Keras Sequential model\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(32, input_dim=X.shape[1], activation='sigmoid'))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    # Compile the model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n    \n    # Fit the model to the training data\n    model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n    \n    # Predict probabilities for the test set\n    y_pred_prob = model.predict(X_test).ravel()\n    \n    # Compute ROC curve and AUC score\n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.legend(loc=\"lower right\")\n    \n    plt.show()\n\n    return model, ax\n\n# Example usage (assuming X and Y are defined as your dataset features and labels respectively)\n# model, ax = task_func(X, Y)\n```"}
{"task_id": "BigCodeBench/424", "solution": "```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file does not exist at the specified path: {image_path}\")\n    \n    # Check if 'n_clusters' is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"'n_clusters' must be a positive integer\")\n    \n    # Read the image\n    original_image = cv2.imread(image_path)\n    if original_image is None:\n        raise FileNotFoundError(f\"Failed to read the image at the specified path: {image_path}\")\n    \n    # Convert the image from BGR to RGB\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n    \n    # Flatten the image array and normalize pixel values to [0, 1]\n    pixels = original_image.reshape(-1, 3).astype(np.float32) / 255.0\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n    cluster_labels = kmeans.predict(pixels)\n    cluster_centers = kmeans.cluster_centers_\n    \n    # Create a segmented image by replacing each pixel with its cluster's centroid color\n    segmented_image = cluster_centers[cluster_labels].reshape(original_image.shape)\n    segmented_image = (segmented_image * 255).astype(np.uint8)  # Convert back to [0, 255] range\n    \n    # Save each region as a separate image\n    for cluster_idx in range(n_clusters):\n        mask = (cluster_labels == cluster_idx)\n        region_image = np.zeros_like(original_image)\n        region_image[mask.reshape(original_image.shape[:2])] = original_image[mask.reshape(original_image.shape[:2])]\n        region_image_path = f'region_{cluster_idx}.png'\n        cv2.imwrite(region_image_path, cv2.cvtColor(region_image, cv2.COLOR_RGB2BGR))\n    \n    return original_image, segmented_image\n```\n\n### Explanation:\n1. **File Check and Validity**: The function starts by checking if the provided image path exists and if `n_clusters` is a positive integer.\n2. **Image Reading**: It reads the image using `cv2.imread` and converts it from BGR to RGB format.\n3. **Flattening and Normalization**: The image is flattened and pixel values are normalized to the [0, 1] range for better processing.\n4. **K-Means Clustering**: K-means clustering is applied to the pixel data to segment the image into `n_clusters` regions.\n5. **Creating Segmented Image**: Each pixel in the segmented image is replaced with the centroid color of its cluster.\n6. **Saving Regions**: Each region (cluster) is saved as a separate image.\n7. **Return Tuple**: Finally, it returns a tuple containing the original and segmented images as numpy arrays.\n\nMake sure to replace `'path/to/your/image.jpg'` with the actual path to your image file when using the function."}
{"task_id": "BigCodeBench/443", "solution": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Ensure P and T have compatible shapes for matrix multiplication\n    assert P.shape[1] == T.shape[0], \"The number of columns of P must match the first dimension of T\"\n\n    # Calculate the matrix product of P and T\n    # Perform matrix multiplication across the first dimension of T\n    result = np.tensordot(P, T, axes=(1, 0))\n\n    # Flatten the result\n    flattened_result = result.reshape(result.shape[0], -1)\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_result)\n\n    # Visualization\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis')\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax\n\n# Example usage:\n# P = np.random.rand(4, 3)  # Example matrix P\n# T = np.random.rand(3, 5, 2)  # Example 3D tensor T\n# cluster_result, ax = task_func(P, T)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/445", "solution": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Validate input\n    if not isinstance(points, (np.ndarray, list)):\n        raise ValueError(\"Input points must be a numpy array or a list.\")\n        \n    points = np.asarray(points)\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points must be a 2D array with shape (n, 2).\")\n        \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Apply jittering\n    jittered_points = points + np.random.normal(0, 1e-8, points.shape)\n    \n    # Compute the Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    # Add the original points to the plot\n    ax.plot(points[:, 0], points[:, 1], 'o')\n    ax.set_xlim(points[:, 0].min() - 1, points[:, 0].max() + 1)\n    ax.set_ylim(points[:, 1].min() - 1, points[:, 1].max() + 1)\n    \n    plt.show()\n    \n    return vor, ax\n\n# Example usage:\npoints = np.array([[0, 0], [1, 1], [1, -1], [-1, 1], [-1, -1], [2, 0]])\nvor, ax = task_func(points)\n```\n\n### Explanation:\n1. **Input Validation**:\n    - Ensures the input `points` is a list or a numpy array.\n    - Converts the input to a numpy array if necessary.\n    - Checks that the array is 2D and each point has exactly 2 coordinates.\n\n2. **Jittering**:\n    - Adds a small amount of random noise to each point to avoid degenerate cases. The noise has a mean of 0 and a very small standard deviation (`1e-8`).\n\n3. **Voronoi Diagram Calculation**:\n    - Computes the Voronoi diagram using the `Voronoi` class from `scipy.spatial`.\n\n4. **Plotting**:\n    - Creates a plot using Matplotlib.\n    - Plots the Voronoi diagram and the original points for reference.\n    - Adjusts the plot limits to ensure all points and Voronoi vertices are visible.\n\n5. **Return**:\n    - Returns a tuple containing the Voronoi object and the Axes object for the plot."}
{"task_id": "BigCodeBench/454", "solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with a specified extension from a source directory to a destination directory.\n    \n    Args:\n    src_dir (str): The source directory path.\n    dest_dir (str): The destination directory path.\n    ext (str): The file extension to search for (e.g., '.txt').\n    \n    Returns:\n    list: A list of the full paths of files that were successfully moved.\n    \n    Raises:\n    FileNotFoundError: If either the source or destination directory does not exist.\n    \"\"\"\n    # Check if the source directory exists\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    \n    # Check if the destination directory exists\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n    \n    # Form the search pattern for files with the specified extension in the source directory\n    search_pattern = os.path.join(src_dir, f'*{ext}')\n    \n    # Get list of files matching the extension\n    files_to_move = glob.glob(search_pattern)\n    \n    # List to keep track of successfully moved files\n    moved_files = []\n    \n    # Move each file to the destination directory if it doesn't already exist there\n    for file_path in files_to_move:\n        file_name = os.path.basename(file_path)\n        dest_path = os.path.join(dest_dir, file_name)\n        \n        if not os.path.exists(dest_path):\n            shutil.move(file_path, dest_path)\n            moved_files.append(dest_path)\n    \n    return moved_files\n\n# Example usage:\n# moved_files = task_func('/path/to/source', '/path/to/dest', '.txt')\n# print(moved_files)\n```"}
{"task_id": "BigCodeBench/458", "solution": "```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Check if the JSON string is empty\n    if not json_str.strip():\n        return pd.DataFrame()\n    \n    try:\n        # Load the JSON string into a dictionary\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    \n    # Function to double numerical values\n    def double_values(item):\n        if isinstance(item, (int, float)):\n            return item * 2\n        elif isinstance(item, str):\n            # Find all numbers in the string and double them\n            return re.sub(r'\\d+\\.?\\d*', lambda x: str(float(x.group()) * 2), item)\n        elif isinstance(item, list):\n            return [double_values(i) for i in item]\n        else:\n            return item\n\n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {k: double_values(v) for k, v in data_dict.items()}\n    \n    # Create a DataFrame from the normalized dictionary\n    df = pd.DataFrame([normalized_dict])\n    \n    # Convert all numerical columns to float\n    for col in df.select_dtypes(include=['object']).columns:\n        try:\n            df[col] = df[col].astype(float)\n        except ValueError:\n            continue\n    \n    return df\n\n# Example usage\njson_str = '{\"a\": \"123\", \"b\": [1, 2, 3], \"c\": 4, \"d\": \"text\"}'\ndf = task_func(json_str)\nprint(df)\n```"}
{"task_id": "BigCodeBench/461", "solution": "```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script path {script_path} does not exist.\")\n    \n    start_time = time.time()\n    \n    # Start the subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    try:\n        # Get the process id\n        process_pid = process.pid\n        \n        # Initialize accumulators for CPU and memory usage\n        accumulated_cpu_usage = 0.0\n        accumulated_memory_usage = 0.0\n        \n        while True:\n            current_time = time.time()\n            \n            # Check if the process is still running\n            if process.poll() is not None:\n                break\n            \n            if current_time - start_time > timeout:\n                process.terminate()\n                process.wait()  # Ensure the process has terminated\n                break\n            \n            try:\n                proc = psutil.Process(process_pid)\n                with proc.oneshot():\n                    # Aggregate CPU and memory usage\n                    accumulated_cpu_usage += proc.cpu_percent(interval=0.1)\n                    accumulated_memory_usage += proc.memory_info().rss\n                \n            except psutil.NoSuchProcess:\n                break\n            except psutil.ZombieProcess:\n                break\n            except Exception as e:\n                raise e\n\n        # Ensure to read stdout and stderr to avoid subprocess hanging\n        stdout, stderr = process.communicate()\n\n        return {\n            'CPU Usage': accumulated_cpu_usage,\n            'Memory Usage': accumulated_memory_usage\n        }\n        \n    except Exception as e:\n        process.terminate()\n        process.wait()  # Ensure the process has terminated\n        raise e\n\n# Example usage:\n# result = task_func('/path/to/your/script.sh')\n# print(result)\n```"}
{"task_id": "BigCodeBench/477", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    num_categories = len(CATEGORIES)\n    \n    if N >= num_categories:\n        # Ensure each category appears at least once\n        categories = np.random.choice(CATEGORIES, N - num_categories, replace=True).tolist()\n        categories.extend(CATEGORIES)\n        np.random.shuffle(categories)\n    else:\n        # Sample categories without replacement\n        categories = np.random.choice(CATEGORIES, N, replace=False).tolist()\n    \n    # Generate random values for x and y\n    x = np.random.randn(N)\n    y = np.random.randn(N)\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        \"x\": x,\n        \"y\": y,\n        \"category\": categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df[\"x\"], df[\"y\"], c=pd.Categorical(df[\"category\"]).codes, cmap='viridis')\n    \n    # Create a legend\n    handles, labels = scatter.legend_elements(prop=\"colors\", alpha=0.6)\n    legend_labels = dict(zip(range(len(CATEGORIES)), CATEGORIES))\n    labels = [legend_labels[int(label)] for label in labels]\n    ax.legend(handles, labels, title=\"Category\")\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func(100, [\"A\", \"B\", \"C\", \"D\", \"E\"], 42)\nplt.show()\n```"}
{"task_id": "BigCodeBench/486", "solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Convert epoch times to datetime\n    start_datetime = datetime.fromtimestamp(start_time)\n    end_datetime = datetime.fromtimestamp(end_time)\n\n    # Generate time series\n    current_time = start_datetime\n    timestamps = []\n    while current_time <= end_datetime:\n        timestamps.append(current_time)\n        current_time += timedelta(seconds=step)\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate values from a normal distribution\n    values = np.random.normal(loc=0, scale=1, size=len(timestamps))\n\n    # Add linear trend\n    trend_values = trend * np.arange(len(timestamps))\n    values_with_trend = values + trend_values\n\n    # Create DataFrame\n    df = pd.DataFrame({'Time': timestamps, 'Value': values_with_trend})\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    return ax\n\n# Example usage\nstart_time = 1609459200  # Epoch time for 2021-01-01 00:00:00\nend_time = 1609545600    # Epoch time for 2021-01-02 00:00:00\nstep = 3600              # 1 hour in seconds\ntrend = 0.1\n\ntask_func(start_time, end_time, step, trend)\n```"}
{"task_id": "BigCodeBench/492", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Validate the inputs\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer.\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"random_seed must be an integer.\")\n    if not isinstance(products, list) or not all(isinstance(p, str) for p in products):\n        raise ValueError(\"products must be a list of strings.\")\n    \n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000)\n    end_date = datetime.utcnow()\n\n    # Generate a list of all dates between start_date and end_date\n    total_days = (end_date - start_date).days\n    dates = [start_date + timedelta(days=i) for i in range(total_days + 1)]\n\n    # Generate sales data\n    data = []\n    for date in dates:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append({\n                \"Product\": product,\n                \"Date\": date,\n                \"Sales\": sales\n            })\n\n    # Convert to DataFrame\n    df = pd.DataFrame(data)\n    return df\n\n# Example usage\nif __name__ == \"__main__\":\n    epoch_time = 1609459200000  # This is the epoch time for 2021-01-01 00:00:00 UTC\n    df = task_func(epoch_time)\n    print(df)\n```"}
{"task_id": "BigCodeBench/501", "solution": "```python\nimport json\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    try:\n        # Check if the input is a valid string, bytes, or bytearray\n        if not isinstance(json_str, (str, bytes, bytearray)):\n            raise TypeError(\"json_str must be a string, bytes, or bytearray.\")\n        \n        # Parse the JSON string\n        data = json.loads(json_str)\n        \n        # Check if data is an empty array\n        if isinstance(data, list) and len(data) == 0:\n            # Create an empty DataFrame\n            df = pd.DataFrame()\n        else:\n            # Create a DataFrame from the JSON data\n            df = pd.json_normalize(data)\n        \n        # Create the Excel file\n        output_path = os.path.abspath(filename)\n        with pd.ExcelWriter(output_path) as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n        \n        return output_path\n    \n    except json.JSONDecodeError:\n        raise ValueError(\"Provided string is not valid JSON.\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n\n# Example usage:\n# json_str = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\n# filename = 'output.xlsx'\n# print(task_func(json_str, filename))\n```"}
{"task_id": "BigCodeBench/502", "solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Create an empty list to store the data\n    data = []\n\n    # Get the current date\n    end_date = datetime.now()\n\n    # Generate data for the specified number of days in the past\n    for i in range(days_in_past):\n        date = (end_date - timedelta(days=i)).strftime('%Y-%m-%d')\n        for activity in activities:\n            duration = random.randint(0, 120)\n            data.append({'Date': date, 'Activity': activity, 'Duration': duration})\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # Plot the data using seaborn\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(x='Date', y='Duration', hue='Activity', data=df, marker='o')\n    ax.set_title('Daily Activity Durations')\n    ax.set_ylabel('Duration (minutes)')\n    ax.set_xlabel('Date')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()\n\n    # Return the plot axis and DataFrame\n    return ax, df\n\n# Example usage\nax, df = task_func(days_in_past=7, random_seed=42)\n```"}
{"task_id": "BigCodeBench/503", "solution": "```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate a list of dates for the past `days_in_past` days\n    end_date = datetime.now()\n    date_range = [end_date - timedelta(days=i) for i in range(days_in_past)]\n    date_range.reverse()  # Ensure dates are in ascending order\n    \n    # Generate random stock prices between 0.0 and 1.0\n    data = np.random.rand(days_in_past, len(stock_names))\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data, index=date_range, columns=stock_names)\n    \n    return df\n\n# Example usage:\ndf = task_func(days_in_past=7)\nprint(df)\n```"}
{"task_id": "BigCodeBench/509", "solution": "```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Read the content of the first file\n        with open(file_path1, 'r', newline='', encoding='utf-8') as file1:\n            reader1 = list(csv.reader(file1, delimiter=delimiter, quotechar=quotechar))\n        \n        # Read the content of the second file\n        with open(file_path2, 'r', newline='', encoding='utf-8') as file2:\n            reader2 = list(csv.reader(file2, delimiter=delimiter, quotechar=quotechar))\n        \n        # Check if either file is empty\n        if not reader1:\n            raise ValueError(f\"The file {file_path1} is empty.\")\n        if not reader2:\n            raise ValueError(f\"The file {file_path2} is empty.\")\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\")\n    except ValueError as ve:\n        raise ve\n    except Exception as e:\n        raise Exception(f\"An error occurred: {str(e)}\")\n    \n    # Use ndiff to find differences\n    diffs = list(ndiff([''.join(row) for row in reader1], [''.join(row) for row in reader2]))\n    \n    differences = []\n    line_num_file1 = line_num_file2 = 0\n    \n    for diff in diffs:\n        code = diff[0]\n        line_content = diff[2:]\n        \n        if code == ' ':\n            line_num_file1 += 1\n            line_num_file2 += 1\n            differences.append({'Line Number': line_num_file1, 'Status': ' ', 'Content': line_content})\n        elif code == '-':\n            line_num_file1 += 1\n            differences.append({'Line Number': line_num_file1, 'Status': '-', 'Content': line_content})\n        elif code == '+':\n            line_num_file2 += 1\n            differences.append({'Line Number': line_num_file2, 'Status': '+', 'Content': line_content})\n    \n    # Create a DataFrame from the differences list\n    df = pd.DataFrame(differences)\n    \n    return df\n\n# Example usage:\n# df_diff = task_func('file1.csv', 'file2.csv')\n# print(df_diff)\n```"}
{"task_id": "BigCodeBench/511", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Convert the list of employee data to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the data is empty\n    if df.empty:\n        stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, 'No Data Available', horizontalalignment='center', verticalalignment='center')\n        plt.axis('off')\n        return stats, ax\n    \n    # Check if the specified column exists\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the data\")\n    \n    # Calculate statistics\n    col_data = df[column]\n    stats = {\n        'sum': col_data.sum(),\n        'mean': col_data.mean(),\n        'min': col_data.min(),\n        'max': col_data.max()\n    }\n    \n    # Create the pie chart\n    fig, ax = plt.subplots()\n    if 'Age' in df.columns:\n        ax.pie(col_data, labels=df['Age'], autopct='%1.1f%%')\n    else:\n        ax.pie(col_data, autopct='%1.1f%%')\n    ax.set_title(f'Pie Chart of {column} with Age as Labels')\n    \n    return stats, ax\n\n# Example usage:\ndata = [\n    {'Name': 'Alice', 'Age': 30, 'Salary': 70000},\n    {'Name': 'Bob', 'Age': 24, 'Salary': 50000},\n    {'Name': 'Charlie', 'Age': 28, 'Salary': 60000},\n    {'Name': 'David', 'Age': 35, 'Salary': 90000}\n]\n\ncolumn = 'Salary'\nstats, ax = task_func(column, data)\nprint(stats)\nplt.show()\n```"}
{"task_id": "BigCodeBench/513", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Check if data list is empty\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    # Create DataFrame from data\n    df = pd.DataFrame(data)\n    \n    # Ensure the specified column exists\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' is not valid.\")\n    \n    # Ensure necessary columns are present\n    required_columns = ['steps', 'calories burned', 'distance walked']\n    for col in required_columns:\n        if col not in df.columns:\n            raise KeyError(f\"Required column '{col}' is missing from data.\")\n    \n    # Validate numeric values are non-negative\n    if (df[['steps', 'calories burned', 'distance walked']] < 0).any().any():\n        raise ValueError(\"Numeric values for steps, calories burned, and distance walked must be non-negative.\")\n    \n    # Calculate sum, mean, min, and max for the specified column\n    column_sum = df[column].sum()\n    column_mean = df[column].mean()\n    column_min = df[column].min()\n    column_max = df[column].max()\n    \n    stats = {\n        'sum': column_sum,\n        'mean': column_mean,\n        'min': column_min,\n        'max': column_max\n    }\n    \n    # Plot the line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column])\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    plt.xticks(rotation=45)\n    \n    return stats, ax\n\n# Example usage:\ndata = [\n    {'Date': '2023-01-01', 'steps': 1000, 'calories burned': 50, 'distance walked': 1.0},\n    {'Date': '2023-01-02', 'steps': 2000, 'calories burned': 100, 'distance walked': 2.0},\n    {'Date': '2023-01-03', 'steps': 1500, 'calories burned': 75, 'distance walked': 1.5}\n]\n\n# Call the function with the specified column and data\nstats, ax = task_func('steps', data)\nplt.show()  # This will display the plot\nprint(stats)  # This will print the statistics dictionary\n```"}
{"task_id": "BigCodeBench/526", "solution": "```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a defaultdict to store lists of numeric values for each key\n    numeric_data = defaultdict(list)\n    \n    # Iterate over each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):\n                numeric_data[key].append(value)\n    \n    # Create lists to store results\n    keys = []\n    means = []\n    medians = []\n    \n    # Calculate mean and median for each key\n    for key in numeric_data:\n        values = numeric_data[key]\n        keys.append(key)\n        means.append(np.mean(values))\n        medians.append(np.median(values))\n    \n    # Create a DataFrame from the results\n    df = pd.DataFrame({\n        'mean': means,\n        'median': medians\n    }, index=keys)\n    \n    # Sort the DataFrame by index (variable names)\n    df.sort_index(inplace=True)\n    \n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    df = task_func(\"data.json\")\n    print(df)\n```"}
{"task_id": "BigCodeBench/528", "solution": "```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension\")\n\n    # Read the CSV file and collect rows\n    with open(file_path, newline='') as csvfile:\n        reader = csv.reader(csvfile)\n        header = next(reader)  # Skip header row\n        rows = [tuple(row) for row in reader]\n\n    # Count duplicate rows\n    row_counter = Counter(rows)\n    duplicates = {row: count for row, count in row_counter.items() if count > 1}\n\n    if not duplicates:\n        print(\"No duplicate rows found.\")\n        return {}, None\n\n    # Convert duplicates to a pandas DataFrame\n    duplicate_rows = list(duplicates.keys())\n    duplicate_df = pd.DataFrame(duplicate_rows, columns=header)\n\n    # Plot the duplicate rows\n    fig, ax = plt.subplots()\n    ax.bar(range(len(duplicates)), duplicates.values(), tick_label=[str(row) for row in duplicates.keys()])\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Duplicate Rows')\n    plt.xticks(rotation=90)\n    plt.tight_layout()  # Adjust layout to ensure labels fit\n\n    # Show plot\n    plt.show()\n\n    return duplicates, ax\n\n# Example usage:\n# duplicates, ax = task_func('your_file.csv')\n```"}
{"task_id": "BigCodeBench/530", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if required columns are present\n    if 'name' not in df.columns or 'age' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns.\")\n    \n    # Verify that there are no negative ages\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age must be non-negative.\")\n    \n    # Round down ages to the nearest integer\n    df['age'] = df['age'].apply(np.floor).astype(int)\n    \n    # Identify duplicates\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    \n    # If no duplicates, return an empty Counter and None for the plot\n    if duplicates.empty:\n        return Counter(), None\n    \n    # Get the age distribution of duplicate names\n    age_distribution = Counter(duplicates['age'])\n    \n    # Generate the histogram plot\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bin_edges = np.arange(min_age - 0.5, max_age + 1.5, 1)\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(duplicates['age'], bins=bin_edges, kde=False)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age Distribution of Duplicate Names')\n    \n    return age_distribution, ax\n\n# Example usage:\n# df = pd.DataFrame({'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'], 'age': [25, 30, 25.8, 35, 29.3]})\n# dist, plot = task_func(df)\n# print(dist)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/532", "solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, bins=4):\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates = {k: v for k, v in value_counts.items() if v > 1}\n    \n    # Plot histogram of 'value' column\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6, density=True)\n    \n    # Fit a normal distribution to the data and plot the curve\n    mu, std = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Plot settings\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n    \n    # Return the Counter of duplicate values and the matplotlib Axes object\n    return duplicates, ax\n\n# Example usage:\n# Creating a sample DataFrame for demonstration\nif __name__ == \"__main__\":\n    data = {'value': np.random.randint(0, 10, size=100)}\n    df = pd.DataFrame(data)\n    \n    # Call the function\n    duplicates, ax = task_func(df)\n    \n    # Print the Counter object\n    print(duplicates)\n    \n    # Show the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/553", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Determine the number of columns based on the length of list 'b'\n    num_columns = len(b)\n    \n    # Ensure we don't exceed the predefined COLUMNS list\n    if num_columns > len(COLUMNS):\n        raise ValueError(f\"List 'b' has more elements than expected. Maximum allowed is {len(COLUMNS)}.\")\n    \n    # Select the appropriate column names\n    column_names = COLUMNS[:num_columns]\n    \n    # Create a DataFrame with random values\n    data = np.random.rand(len(a), num_columns)\n    df = pd.DataFrame(data, index=a, columns=column_names)\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n    \n    # Display the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax\n\n# Example usage\na = ['row1', 'row2', 'row3']\nb = [1, 2, 3]\ntask_func(a, b)\n```"}
{"task_id": "BigCodeBench/560", "solution": "```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): DataFrame with columns 'month' and 'value'. \n                         'month' should be in the format 'YYYY-MM'.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object representing the plot.\n    \"\"\"\n    # Ensure the 'month' column is of datetime type\n    data['month'] = pd.to_datetime(data['month'])\n\n    # Extract the year from the first entry to use in the title\n    year = data['month'].dt.year.iloc[0]\n\n    # Set 'month' as the index for plotting\n    data.set_index('month', inplace=True)\n\n    # Create the plot\n    ax = data.plot(kind='bar', legend=False)\n\n    # Set the title and labels\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n\n    # Format x-axis to show month names\n    ax.set_xticklabels(data.index.strftime('%B'))\n\n    # Return the Axes object\n    return ax\n\n# Example usage:\nif __name__ == '__main__':\n    # Example data\n    example_data = pd.DataFrame({\n        'month': [f'2023-{i:02d}' for i in range(1, 13)],\n        'value': [5, 7, 8, 6, 4, 5, 6, 7, 8, 9, 10, 11]\n    })\n    \n    ax = task_func(example_data)\n    plt.show()\n```"}
{"task_id": "BigCodeBench/567", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input string to a list of numeric values\n    data_list = list(map(float, data.split()))\n    \n    # Convert the list to a Pandas Series for easier manipulation\n    data_series = pd.Series(data_list)\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(data_series.min(), data_series.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(data_series, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Show the plot (optional, depending on the environment)\n    plt.show()\n    \n    return ax\n\n# Example usage\ndata = '1 2 2 3 3 3 4 4 4 4 5 5 5 5 5'\nax = task_func(data)\n```"}
{"task_id": "BigCodeBench/574", "solution": "```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate the x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Generate the true sine wave\n    true_sine = np.sin(x)\n    \n    # Add noise to the sine wave\n    noise = noise_level * np.random.normal(size=array_length)\n    noisy_sine = true_sine + noise\n    \n    # Define the sine function to fit\n    def sine_function(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(frequency * x + phase) + offset\n    \n    # Initial guess for the parameters: amplitude, frequency, phase, offset\n    initial_guess = [1, 1, 0, 0]\n    \n    # Fit the curve\n    params, params_covariance = curve_fit(sine_function, x, noisy_sine, p0=initial_guess)\n    \n    # Generate the fitted sine wave\n    fitted_sine = sine_function(x, *params)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(x, noisy_sine, label='Noisy Sine Wave', linestyle='dotted')\n    ax.plot(x, fitted_sine, label='Fitted Sine Wave', color='red')\n\n    # Add labels and a legend\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n\n    # Show the plot\n    plt.show()\n\n    return ax\n\n# Call the function to see the plot\ntask_func()\n```"}
{"task_id": "BigCodeBench/579", "solution": "```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        # Open and read the CSV file\n        with open(csv_file, newline='', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = \" \".join([row[0] for row in reader])  # Assuming text is in the first column\n        \n        # Normalize text to ASCII\n        normalized_text = unicodedata.normalize('NFKD', text_data).encode('ascii', 'ignore').decode('ascii')\n        \n        # Split text into words and count frequencies\n        words = normalized_text.lower().split()\n        word_count = Counter(words)\n        \n        # Get the 10 most common words\n        most_common_words = word_count.most_common(10)\n        \n        # Create a bar plot\n        words, frequencies = zip(*most_common_words)\n        fig, ax = plt.subplots()\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        \n        # Return the plot and most common words\n        return ax, most_common_words\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"CSV file not found at path: {csv_file}\")\n    except IOError:\n        raise IOError(f\"Error reading the file: {csv_file}\")\n\n# Example usage:\n# ax, common_words = task_func('example.csv')\n# plt.show()  # To display the plot\n# print(common_words)  # To see the list of tuples\n```"}
{"task_id": "BigCodeBench/582", "solution": "```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot histogram of the data\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n    \n    # Plot the probability density function (PDF)\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, np.mean(data), np.std(data))\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set titles and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    return fig\n\n# Example usage\nif __name__ == \"__main__\":\n    fig = task_func(size=1000)\n    plt.show()\n```"}
{"task_id": "BigCodeBench/583", "solution": "```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Convert private key to PEM format\n    private_key_pem = private_key.save_pkcs1()\n    \n    # Generate random bytes for the filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # AES encryption setup\n    password = get_random_bytes(32)  # AES-256\n    nonce = get_random_bytes(12)     # GCM mode nonce\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n    \n    # Encrypt the private key\n    ciphertext, tag = cipher.encrypt_and_digest(private_key_pem)\n    \n    # Save the encrypted private key to a file\n    with open(filename, 'wb') as file_out:\n        file_out.write(nonce)\n        file_out.write(tag)\n        file_out.write(ciphertext)\n    \n    return public_key, filename, password, nonce\n\n# Example usage\nif __name__ == \"__main__\":\n    pub_key, file_name, enc_password, enc_nonce = task_func()\n    print(f\"RSA Public Key: \\n{pub_key}\")\n    print(f\"Encrypted private key saved to: {file_name}\")\n    print(f\"Encryption Password: {b64encode(enc_password).decode()}\")\n    print(f\"Encryption Nonce: {b64encode(enc_nonce).decode()}\")\n```\n\n### Explanation:\n1. **RSA Key Generation**: The code uses the `rsa` library to generate a 2048-bit RSA key pair.\n2. **Filename Generation**: It creates a filename based on 8 random bytes, converting them to a hexadecimal string.\n3. **AES Encryption Setup**: A 256-bit AES key is generated along with a 12-byte nonce for AES GCM mode.\n4. **Private Key Encryption**: The private key, in PEM format, is encrypted using AES-GCM.\n5. **File Writing**: The nonce, authentication tag, and ciphertext are written to a file.\n6. **Return Values**: The public key, filename, encryption password, and nonce are returned for further use.\n\nThis function ensures that the private key is securely encrypted and stored, and provides all necessary details for decryption."}
{"task_id": "BigCodeBench/587", "solution": "```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Read the file to encrypt\n    with open(file_path, 'rb') as f:\n        file_data = f.read()\n\n    # Generate a random AES key\n    aes_key = os.urandom(32)  # 256-bit key for AES-256\n\n    # Initialize AES\n    iv = os.urandom(16)  # Initialization vector for AES\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad the file data to be a multiple of the block size (AES block size is 128 bits)\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n\n    # Encrypt the file data\n    encrypted_file_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Save the encrypted file data\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as f:\n        f.write(iv + encrypted_file_data)  # Prepend the IV for use during decryption\n\n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save the encrypted AES key\n    encrypted_key_path = file_path + '.key'\n    with open(encrypted_key_path, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    return pubkey, encrypted_file_path, encrypted_key_path\n\n# Example usage:\nif __name__ == \"__main__\":\n    pubkey, enc_file, enc_key_file = task_func('example.txt')\n    print(f\"Public Key: {pubkey}\")\n    print(f\"Encrypted file: {enc_file}\")\n    print(f\"Encrypted AES key file: {enc_key_file}\")\n```\n\n### Explanation:\n1. **RSA Key Generation**: Generates a new RSA key pair (public and private keys).\n2. **File Reading**: Reads the content of the file specified by `file_path`.\n3. **AES Key Generation**: Creates a random 256-bit AES key.\n4. **AES Initialization**: Initializes AES encryption in CBC mode with a random IV.\n5. **Padding**: Pads the file data to make it a multiple of the AES block size.\n6. **AES Encryption**: Encrypts the padded file data using AES encryption.\n7. **File Saving**: Saves the encrypted file data (with the IV prepended) to a new file.\n8. **AES Key Encryption**: Encrypts the AES key using the RSA public key.\n9. **Key Saving**: Saves the encrypted AES key to a separate file.\n\nThe function returns the RSA public key, the path to the encrypted file, and the path to the encrypted AES key file."}
{"task_id": "BigCodeBench/590", "solution": "```python\nimport urllib.request\nfrom urllib.error import URLError\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nimport validators\n\ndef task_func(url):\n    # Validate the URL\n    if not url or not validators.url(url):\n        raise ValueError(\"Invalid or empty URL provided.\")\n    \n    try:\n        # Fetch the HTML content\n        response = urllib.request.urlopen(url)\n        html_content = response.read().decode('utf-8')\n    except URLError as e:\n        raise URLError(\"There was an issue with network connectivity or the server.\") from e\n    \n    # Parse the HTML content\n    document = pq(html_content)\n    \n    # Extract anchor tags\n    anchors = document('a')\n    \n    # Extract text and href attributes\n    data = []\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    for anchor in anchors.items():\n        text = anchor.text()\n        href = anchor.attr('href')\n        data.append({'text': text, 'href': href, 'fetch_time': fetch_time})\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['text', 'href', 'fetch_time'])\n    \n    return df\n```"}
{"task_id": "BigCodeBench/592", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate the filename based on the current timestamp\n    filename = os.path.join(output_dir, f'sensor_data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n    \n    # Open the CSV file for writing\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header\n        writer.writerow(['Time'] + SENSORS)\n        \n        # Generate data for the specified number of hours\n        current_time = datetime.now()\n        for i in range(hours):\n            # Generate random sensor data\n            temperature = randint(-10, 40)  # Temperature in Celsius\n            humidity = randint(0, 100)      # Humidity in percentage\n            pressure = randint(950, 1050)   # Pressure in hPa\n            \n            # Write the data to the CSV file\n            writer.writerow([\n                (current_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S'),\n                temperature,\n                humidity,\n                pressure\n            ])\n    \n    print(f\"Data has been written to {filename}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    task_func(5)  # Generate data for 5 hours\n```"}
{"task_id": "BigCodeBench/593", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Prepare file path\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    csv_file_path = os.path.join(output_dir, f'traffic_data_{timestamp}.csv')\n    \n    # Generate traffic data\n    data = []\n    for hour in range(hours):\n        row = {'Time': hour}\n        for vehicle in VEHICLE_TYPES:\n            row[vehicle] = randint(0, 100)  # Random count for each vehicle type\n        data.append(row)\n    \n    # Write data to CSV\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time'] + VEHICLE_TYPES)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    \n    # Read data into pandas DataFrame\n    df = pd.read_csv(csv_file_path)\n    \n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    for vehicle in VEHICLE_TYPES:\n        plt.plot(df['Time'], df[vehicle], label=vehicle)\n    \n    plt.xlabel('Time (Hours)')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data Over Time')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    \n    # Save plot to file\n    plot_file_path = os.path.join(output_dir, f'traffic_plot_{timestamp}.png')\n    plt.savefig(plot_file_path)\n    \n    # Show plot\n    plt.show()\n    \n    return csv_file_path, plt.gca()\n\n# Example usage:\n# csv_path, plot_ax = task_func(24)\n# print(f\"CSV file saved at: {csv_path}\")\n```"}
{"task_id": "BigCodeBench/594", "solution": "```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output and backup directories exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(BACKUP_DIR, exist_ok=True)\n    \n    # Generate the CSV filename\n    filename = f\"weather_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n    filepath = os.path.join(output_dir, filename)\n\n    # Generate weather data\n    current_time = datetime.now()\n    weather_data = []\n    for i in range(hours):\n        weather_data.append({\n            'Time': (current_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S'),\n            'Condition': choice(WEATHER_CONDITIONS)\n        })\n\n    # Write data to CSV file\n    with open(filepath, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=['Time', 'Condition'])\n        writer.writeheader()\n        writer.writerows(weather_data)\n\n    # Backup the file\n    backup_filepath = os.path.join(BACKUP_DIR, filename)\n    shutil.copy(filepath, backup_filepath)\n\n    return filepath\n\n# Example usage\nif __name__ == \"__main__\":\n    print(task_func(24))  # Generate weather data for 24 hours\n```"}
{"task_id": "BigCodeBench/618", "solution": "```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generate random match results\n    data = {\n        'Team': [TEAMS[randint(0, len(TEAMS) - 1)] for _ in range(len(goals))],\n        'Goals': goals,\n        'Penalties': penalties,\n        'Penalty Cost': [p * PENALTY_COST for p in penalties]\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Plotting\n    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Plot Goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=axs[0])\n    axs[0].set_title('Goals per Team')\n    axs[0].set_xlabel('Team')\n    axs[0].set_ylabel('Goals')\n    \n    # Plot Penalty Costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axs[1])\n    axs[1].set_title('Penalty Costs per Team')\n    axs[1].set_xlabel('Team')\n    axs[1].set_ylabel('Penalty Cost ($)')\n    \n    # Show plot\n    plt.tight_layout()\n    plt.show()\n    \n    return df, [axs[0], axs[1]]\n\n# Example usage\ngoals = [randint(0, 5) for _ in range(10)]\npenalties = [randint(0, 3) for _ in range(10)]\ndf, plots = task_func(goals, penalties)\nprint(df)\n```"}
{"task_id": "BigCodeBench/636", "solution": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Generate a DataFrame with random integers between 0 and 9\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create a bar plot\n    ax = non_zero_counts.plot(kind='bar', color='skyblue')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-zero Counts')\n    ax.set_title('Non-zero Counts in Each Column')\n    \n    # Show plot\n    plt.show()\n    \n    return df, ax\n\n# Example usage\n# df, ax = task_func(10)\n```"}
{"task_id": "BigCodeBench/637", "solution": "```python\nfrom random import randint\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Seed for reproducibility\n    np.random.seed(42)\n    \n    # Define course names\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    \n    # Generate random student names\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n    \n    # Generate random grades for each student in each course\n    grades = {course: [randint(50, 100) for _ in range(num_students)] for course in courses}\n    \n    # Create DataFrame\n    df = pd.DataFrame(grades, index=students)\n    \n    # Calculate average grade for each course\n    avg_grades = df.mean()\n    \n    # Calculate the number of students with passing grades (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot average grades\n    avg_grades.plot(kind='bar', color='blue', alpha=0.6, ax=ax, position=0, width=0.4, label='Average Grade')\n    \n    # Plot passing counts\n    passing_counts.plot(kind='bar', color='green', alpha=0.6, ax=ax, position=1, width=0.4, label='Passing Grade Count')\n    \n    # Set plot title and labels\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Courses')\n    ax.set_ylabel('Values')\n    ax.legend()\n    \n    # Show plot\n    plt.show()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func(30)\nprint(df)\n```"}
{"task_id": "BigCodeBench/654", "solution": "```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Filter the array for rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n    if filtered_array.size == 0:\n        raise ValueError(\"No elements match the target value in the first column.\")\n    \n    # Extract indices and second column values for fitting\n    indices = np.arange(filtered_array.shape[0])\n    y_values = filtered_array[:, 1]\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Fit the exponential decay function to the data\n    popt, pcov = optimize.curve_fit(exp_decay, indices, y_values)\n    \n    # Plot the data and the fitted curve\n    fig, ax = plt.subplots()\n    ax.scatter(indices, y_values, label='Data')\n    ax.plot(indices, exp_decay(indices, *popt), label='Fitted curve', color='red')\n    \n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    plt.title('Exponential Decay Fit')\n    \n    return popt, ax\n\n# Test the function with an example\nif __name__ == \"__main__\":\n    # Example array\n    example_array = np.array([\n        [1, 10],\n        [1, 8],\n        [1, 6],\n        [2, 15],\n        [2, 10],\n        [1, 4],\n        [2, 6],\n        [1, 2]\n    ])\n    \n    # Target value to filter the first column\n    target_value = 1\n    \n    # Execute the function\n    parameters, axis = task_func(example_array, target_value)\n    print(\"Optimized parameters:\", parameters)\n    plt.show()\n```"}
{"task_id": "BigCodeBench/655", "solution": "```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef preprocess_text(text):\n    # Remove non-alphanumeric characters (excluding spaces)\n    text = ALPHANUMERIC.sub(' ', text)\n    # Convert all characters to lowercase\n    text = text.lower()\n    # Remove stopwords\n    words = text.split()\n    words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(words)\n\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    preprocessed_texts = [preprocess_text(text) for text in texts]\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)\n\n    # Apply NMF to extract topics\n    nmf_model = NMF(n_components=num_topics, random_state=1)\n    nmf_model.fit(tfidf_matrix)\n\n    # Get the feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        # Get the most significant words for the topic\n        significant_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(significant_words)\n\n    return topics\n\n# Example usage\ntexts = [\n    \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\",\n    \"Machine Learning (ML) is a subset of Artificial Intelligence.\",\n    \"Deep Learning is a branch of Machine Learning.\",\n    \"NLP involves the processing and understanding of human languages.\"\n]\n\nnum_topics = 2\ntopics = task_func(texts, num_topics)\nfor idx, topic in enumerate(topics):\n    print(f\"Topic {idx+1}: {', '.join(topic)}\")\n```"}
{"task_id": "BigCodeBench/657", "solution": "```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\n\n# Ensure you have downloaded the stopwords\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    if stopwords is None:\n        stopwords_set = set(nltk.corpus.stopwords.words('english'))\n    else:\n        stopwords_set = set(stopwords)\n    \n    # Function to clean and preprocess a single text\n    def clean_text(text):\n        # Remove non-alphanumeric characters except space\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase the text\n        text = text.lower()\n        # Remove stop words\n        tokens = [word for word in text.split() if word not in stopwords_set]\n        return tokens\n\n    # Clean all texts\n    cleaned_texts = [clean_text(text) for text in texts]\n\n    # Train the Word2Vec model\n    word2vec_model = Word2Vec(sentences=cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n    return word2vec_model\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_texts = [\n        \"Hello world! This is a test sentence.\",\n        \"Word2Vec is a useful model for generating word vectors.\",\n        \"Cleaning texts is an important step in NLP.\"\n    ]\n    model = task_func(sample_texts)\n    print(model)\n    # Example of how to use the trained model\n    print(model.wv['word'])  # Get the vector for the word 'word'\n```"}
{"task_id": "BigCodeBench/678", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Create the 'processed' subdirectory if it does not exist\n    processed_path = os.path.join(path, 'processed')\n    os.makedirs(processed_path, exist_ok=True)\n    \n    # Initialize an empty DataFrame to collect all data\n    all_data = pd.DataFrame()\n    \n    # Get a sorted list of JSON files in the directory\n    files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n    \n    for file in files:\n        file_path = os.path.join(path, file)\n        \n        # Read JSON file into a DataFrame\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n            df = pd.DataFrame(data)\n        \n        # Insert the \"Source\" column\n        df['Source'] = file\n        \n        # Append the DataFrame to the collective DataFrame\n        all_data = pd.concat([all_data, df], ignore_index=True)\n        \n        # Move the processed file to the 'processed' subdirectory\n        shutil.move(file_path, os.path.join(processed_path, file))\n    \n    return all_data\n\n# Example usage:\n# df = task_func('/path/to/your/directory')\n# print(df)\n```"}
{"task_id": "BigCodeBench/720", "solution": "```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the directory and filename\n    directory = 'task_func_data'\n    filename = 'Output.txt'\n    filepath = os.path.join(directory, filename)\n    \n    # Ensure the directory exists\n    os.makedirs(directory, exist_ok=True)\n    \n    # Generate random sensor data\n    sensor_data = []\n    for _ in range(10):  # Generate 10 rows of data\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        temperature = round(random.uniform(20.0, 30.0), 2)  # Temperature between 20.0 and 30.0 degrees Celsius\n        humidity = round(random.uniform(30.0, 60.0), 2)     # Humidity between 30.0% and 60.0%\n        sensor_data.append([timestamp, temperature, humidity])\n    \n    # Write data to the CSV file\n    with open(filepath, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        # Write headers if the file is empty\n        if file.tell() == 0:\n            writer.writerow(['Timestamp', 'Temperature', 'Humidity'])\n        writer.writerows(sensor_data)  # Write sensor data\n    \n    # Return the path to the CSV file before deletion\n    return filepath\n\n# Call the function and print the file path before deletion\nfile_path = task_func()\nprint(f\"File path before deletion: {file_path}\")\n\n# Delete the file after use\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"File deleted: {file_path}\")\n```"}
{"task_id": "BigCodeBench/723", "solution": "```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    try:\n        # Fetch the contents of the URL\n        response = urllib.request.urlopen(url)\n        web_content = response.read()\n        \n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(web_content, 'html.parser')\n        \n        # Assuming we are scraping a table for demonstration purposes\n        table = soup.find('table')\n        \n        if not table:\n            raise ValueError('No table found on the given URL.')\n        \n        # Extract table headers\n        headers = [header.text.strip() for header in table.find_all('th')]\n        \n        # Extract table rows\n        rows = []\n        for row in table.find_all('tr')[1:]:  # Skip the header row\n            cells = row.find_all('td')\n            row_data = [cell.text.strip() for cell in cells]\n            rows.append(row_data)\n        \n        # Write the extracted data to a CSV file\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            if headers:\n                writer.writerow(headers)\n            if rows:\n                writer.writerows(rows)\n        \n        return CSV_FILE_PATH\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage\nif __name__ == '__main__':\n    url = 'http://example.com/table_page'  # Replace with the actual URL\n    csv_file_path = task_func(url)\n    if csv_file_path:\n        print(f\"Data successfully scraped and saved to {csv_file_path}\")\n    else:\n        print(\"Failed to scrape data.\")\n```"}
{"task_id": "BigCodeBench/752", "solution": "```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validate input parameters\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data is not a DataFrame.\")\n    \n    if data.empty:\n        raise ValueError(\"data is empty.\")\n    \n    if target_column not in data.columns:\n        raise ValueError(\"target_column is not a column of data.\")\n    \n    if not np.issubdtype(data.dtypes, np.number).all():\n        raise ValueError(\"data contains values that are not numeric.\")\n    \n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state is not an integer.\")\n    \n    if not (0 < test_size < 1):\n        raise ValueError(\"test_size is not between 0 and 1.\")\n    \n    # Separate the features and the target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Create and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    return model.score(X_test, y_test)\n\n# Example usage:\n# import numpy as np\n# rng = np.random.default_rng(42)\n# data = pd.DataFrame({\n#     'x1': rng.random(500),\n# })\n# data['y'] = data['x1'] * 2 + 1\n# result = task_func(data, 'y', random_state=9, test_size=0.1)\n# print(result)\n```"}
{"task_id": "BigCodeBench/760", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef fix_encoding(name: str) -> str:\n    \"\"\"\n    Fix improperly encoded Latin characters in name.\n\n    Args:\n        name (str): A string with potentially improperly encoded characters.\n\n    Returns:\n        str: A string with fixed encoding.\n    \"\"\"\n    return codecs.decode(codecs.encode(name, 'latin1'), 'utf-8')\n\ndef generate_random_date(start_year, end_year, seed=None):\n    \"\"\"\n    Generate a random date between start_year and end_year.\n    \n    Args:\n        start_year (int): The start year for the random date.\n        end_year (int): The end year for the random date.\n        seed (int, optional): The seed for the random number generator.\n        \n    Returns:\n        datetime: A randomly generated date.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n        \n    start_date = datetime.datetime(start_year, 1, 1)\n    end_date = datetime.datetime(end_year, 12, 31)\n    delta = end_date - start_date\n    random_days = np.random.randint(0, delta.days)\n    return start_date + datetime.timedelta(days=random_days)\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    \n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Combine names list\n    all_names = latin_names + other_names\n\n    # Generate IDs\n    ids = np.arange(1, 101)\n\n    # Select random names\n    names = np.random.choice(all_names, size=100, replace=True)\n\n    # Generate random dates of birth\n    dob = [generate_random_date(start_year, end_year, seed=rng_seed) for _ in range(100)]\n\n    # Fix encoding for names\n    fixed_names = [fix_encoding(name) for name in names]\n\n    # Generate emails\n    emails = [f\"{re.sub(r'[^a-zA-Z0-9]', '', name.lower())}{dob[i].year % 10000}@{email_domain}\" for i, name in enumerate(fixed_names)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': fixed_names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n    \n    return df\n\n# Example usage\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\nprint(df)\n```"}
{"task_id": "BigCodeBench/763", "solution": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    # Load data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Check if data is a list of dictionaries\n    if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n        raise ValueError(\"Input JSON file must contain a list of dictionaries\")\n\n    # Initialize a dictionary to store the values for each field\n    values_dict = defaultdict(list)\n\n    # Populate the dictionary with values from each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):  # Ensure that the value is numeric\n                values_dict[key].append(value)\n\n    # Calculate mean and median for each field\n    stats_dict = {}\n    for key, values in values_dict.items():\n        # Convert values to a numpy array for calculation\n        values_array = np.array(values)\n        mean_value = np.mean(values_array)\n        median_value = np.median(values_array)\n        stats_dict[key] = {'mean': mean_value, 'median': median_value}\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['Field', 'Mean', 'Median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for key, stats in stats_dict.items():\n            writer.writerow({'Field': key, 'Mean': stats['mean'], 'Median': stats['median']})\n\n    return stats_dict\n\n# Example usage:\n# stats = task_func('input.json', 'output.csv')\n# print(stats)\n```"}
{"task_id": "BigCodeBench/765", "solution": "```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n\n    # Ensure the target directory exists\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    for key, file_path in kwargs.items():\n        file_path = Path(file_path)\n        # Check if the file exists and has content\n        if file_path.exists() and file_path.is_file() and file_path.stat().st_size > 0:\n            # Define the target file path\n            target_file_path = target_path / file_path.name\n            # Copy the file to the target directory\n            shutil.copy(file_path, target_file_path)\n            # Add the full target file path to the copied files list\n            copied_files.append(str(target_file_path))\n\n    return copied_files\n\n# Example usage\nif __name__ == \"__main__\":\n    files_dict = {\n        \"file1\": \"/path/to/file1.txt\",\n        \"file2\": \"/path/to/nonexistent_or_empty_file.txt\",\n        \"file3\": \"/path/to/file3.txt\"\n    }\n    copied_files = task_func(files_dict)\n    print(\"Copied files:\", copied_files)\n```"}
{"task_id": "BigCodeBench/771", "solution": "```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    # Compile the regular expression pattern for performance\n    compiled_pattern = re.compile(pattern)\n    \n    # Create a Path object for the directory\n    dir_path = Path(directory)\n    \n    # Ensure the directory exists\n    if not dir_path.is_dir():\n        raise NotADirectoryError(f\"The provided path: {directory} is not a valid directory.\")\n    \n    # List to store the new file names\n    new_files = []\n    \n    # Iterate over each file in the directory\n    for file_path in dir_path.iterdir():\n        if file_path.is_file() and file_path.suffix == '.csv':\n            # Match the file name against the pattern\n            match = compiled_pattern.match(file_path.name)\n            if match:\n                # Extract the base name from the match groups\n                base_name = match.group(1)\n                new_file_name = f\"{base_name}_processed.csv\"\n                new_file_path = dir_path / new_file_name\n                \n                # Copy the content of the original CSV to the new CSV\n                with open(file_path, mode='r', newline='', encoding='utf-8') as original_file:\n                    reader = csv.reader(original_file)\n                    with open(new_file_path, mode='w', newline='', encoding='utf-8') as new_file:\n                        writer = csv.writer(new_file)\n                        for row in reader:\n                            writer.writerow(row)\n                \n                # Add the new file name to the list\n                new_files.append(new_file_name)\n    \n    return new_files\n\n# Example usage:\n# directory = '/path/to/csv/files'\n# new_files = task_func(directory)\n# print(new_files)\n```"}
{"task_id": "BigCodeBench/777", "solution": "```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*-)\\d+\\.zip$'):\n    extracted_dirs = []\n    \n    # Compile the regex pattern for matching filenames\n    pattern = re.compile(pattern)\n    \n    # Iterate through all files in the given directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        match = pattern.match(filename)\n        if match:\n            # Extract the prefix part of the filename by splitting at the last '-'\n            prefix = filename.rsplit('-', 1)[0]\n            # Define the extraction directory\n            extraction_dir = os.path.join(directory, prefix)\n            # Create the extraction directory if it doesn't exist\n            if not os.path.exists(extraction_dir):\n                os.makedirs(extraction_dir)\n            # Full path to the zip file\n            zip_path = os.path.join(directory, filename)\n            # Extract the zip file\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(extraction_dir)\n            # Add the extraction directory to the list\n            extracted_dirs.append(extraction_dir)\n    \n    return extracted_dirs\n\n# Example usage:\n# extracted_dirs = task_func('/path/to/directory')\n# print(extracted_dirs)\n```\n\n### Explanation:\n\n1. **Import required modules**: The `re` module for regular expressions, `os` module for file and directory operations, and `zipfile` module for handling zip files.\n2. **Define `task_func`**: This function takes two parameters:\n   - `directory`: The path of the directory containing the zip files.\n   - `pattern`: A regex pattern to match filenames, with a default value.\n3. **Initialize an empty list `extracted_dirs`**: This will store directories where the files are extracted.\n4. **Compile the regex pattern**: This optimizes the pattern matching process.\n5. **Iterate through files in the directory**:\n   - For each file, check if it matches the pattern.\n   - If it matches, split the filename at the last '-' to extract the prefix.\n6. **Create an extraction directory**: Named using the prefix, if it doesn't already exist.\n7. **Extract the zip file**: Using `zipfile.ZipFile` into the specified directory.\n8. **Add the extraction directory to the list**.\n9. **Return the list of extracted directories**.\n\nThis script is ready to be used in any Python environment to handle zip files according to the specified pattern."}
{"task_id": "BigCodeBench/785", "solution": "```python\nimport subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n    \n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    \n    if not files_to_archive:\n        raise FileNotFoundError(f\"No files matched the pattern: {pattern}\")\n    \n    # Create the archive file path\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n    \n    # Create a tar.gz archive of the matching files\n    with subprocess.Popen(['tar', 'czf', archive_file] + files_to_archive, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as proc:\n        stdout, stderr = proc.communicate()\n        \n        if proc.returncode != 0:\n            raise Exception(f\"Error creating archive: {stderr.decode('utf-8')}\")\n    \n    # Delete the original files\n    for file in files_to_archive:\n        os.remove(file)\n    \n    return archive_file\n\n# Example usage:\n# pattern = '/path/to/files/*.txt'\n# archive_file_path = task_func(pattern)\n# print(f\"Archive created at: {archive_file_path}\")\n```"}
{"task_id": "BigCodeBench/800", "solution": "```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Ensure the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n\n    # Initialize a Counter to count goals and penalties\n    count = Counter()\n\n    # Read the CSV file and count the existing goals and penalties\n    with open(csv_file_path, mode='r', newline='') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n\n    # Update the count with the new goals and penalties\n    count['goals'] += goals\n    count['penalties'] += penalties\n\n    # Append the new row to the CSV file\n    with open(csv_file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['New Entry', goals, penalties])\n\n    return count\n\n# Example function call\nif __name__ == \"__main__\":\n    setup_csv()  # Set up the example CSV file\n    updated_counts = task_func(4, 3, 'test_data/test_case_2.csv')\n    print(updated_counts)\n```"}
{"task_id": "BigCodeBench/826", "solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    if not os.path.exists(source_dir):\n        raise ValueError(f\"The source directory '{source_dir}' does not exist.\")\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    pattern = re.compile(file_pattern)\n    moved_files_count = 0\n\n    for filename in os.listdir(source_dir):\n        source_file_path = os.path.join(source_dir, filename)\n        \n        if os.path.isfile(source_file_path) and pattern.match(filename):\n            target_file_path = os.path.join(target_dir, filename)\n            shutil.move(source_file_path, target_file_path)\n            moved_files_count += 1\n    \n    return moved_files_count\n\n# Example usage:\n# moved_count = task_func('/path/to/source', '/path/to/target')\n# print(f\"Moved files count: {moved_count}\")\n```"}
{"task_id": "BigCodeBench/845", "solution": "```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text):\n    \"\"\"Remove non-alphanumeric characters (except spaces) and convert to lowercase.\"\"\"\n    return ALPHANUMERIC.sub(' ', text).lower()\n\ndef cosine_similarity(text1, text2):\n    \"\"\"Calculate the cosine similarity between two cleaned texts.\"\"\"\n    # Tokenize and count term frequencies\n    tokens1 = text1.split()\n    tokens2 = text2.split()\n    freq1 = Counter(tokens1)\n    freq2 = Counter(tokens2)\n    \n    # Convert to term frequency vectors\n    terms = set(freq1).union(set(freq2))\n    vec1 = np.array([freq1.get(term, 0) for term in terms])\n    vec2 = np.array([freq2.get(term, 0) for term in terms])\n    \n    # Calculate cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n    \n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n    \n    return dot_product / (norm1 * norm2)\n\ndef task_func(text1, text2):\n    \"\"\"Calculate cosine similarity and Levenshtein ratio for two texts.\"\"\"\n    # Clean the texts\n    cleaned_text1 = clean_text(text1)\n    cleaned_text2 = clean_text(text2)\n    \n    # Calculate cosine similarity\n    cos_sim = cosine_similarity(cleaned_text1, cleaned_text2)\n    \n    # Calculate Levenshtein ratio\n    lev_ratio = ratio(cleaned_text1, cleaned_text2)\n    \n    return (cos_sim, lev_ratio)\n\n# Example usage\ntext1 = \"Hello, World!\"\ntext2 = \"Hello world.\"\nprint(task_func(text1, text2))  # Output: (cosine similarity, Levenshtein ratio)\n```"}
{"task_id": "BigCodeBench/854", "solution": "```python\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Check for input type and value errors\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list.\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"All elements in the list must be integers.\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"All elements in the list must be non-negative.\")\n    \n    # If the input list is empty, return empty lists\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations of the input list\n    perm_list = list(permutations(numbers))\n    \n    # Calculate the sum of the factorials of each number in each permutation\n    fac_list = [sum(math.factorial(num) for num in perm) for perm in perm_list]\n    \n    return fac_list, perm_list\n\n# Example usage:\nfac, perm = task_func([0, 4])\nprint(fac)  # Output: [25, 25]\nprint(perm)  # Output: [(0, 4), (4, 0)]\n```"}
{"task_id": "BigCodeBench/857", "solution": "```python\nimport warnings\nimport os\nimport glob\nimport shutil\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    # Ensure the destination directory exists\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    # Initialize the list of transferred files\n    transferred_files = []\n\n    # Loop through each specified extension\n    for ext in EXTENSIONS:\n        # Find all files in the source directory with the given extension\n        files = glob.glob(os.path.join(SOURCE_DIR, f\"*.{ext}\"))\n        \n        for file_path in files:\n            try:\n                # Extract the file name from the full path\n                file_name = os.path.basename(file_path)\n                # Define the destination file path\n                dest_path = os.path.join(DEST_DIR, file_name)\n                # Move the file to the destination directory\n                shutil.move(file_path, dest_path)\n                # Add the file name to the list of transferred files\n                transferred_files.append(file_name)\n            except Exception as e:\n                # Issue a warning if the file could not be transferred\n                warnings.warn(f\"Could not transfer {file_path}: {e}\")\n\n    return transferred_files\n\n# Example usage:\n# transferred = task_func('/path/to/source', '/path/to/destination', ['txt', 'jpg'])\n# print(\"Transferred files:\", transferred)\n```"}
{"task_id": "BigCodeBench/865", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items, counts, weights = zip(*data)\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1, 1)).flatten()\n    \n    # Create a DataFrame with the items, normalized counts, and normalized weights\n    result_df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return result_df\n\n# Example usage:\ndata = [\n    ('apple', 10, 1.5),\n    ('banana', 15, 2.0),\n    ('cherry', 5, 1.0)\n]\n\nresult_df = task_func(data)\nprint(result_df)\n```\n\n### Explanation:\n1. **Import Necessary Libraries**: Import `pandas`, `numpy`, `zscore` from `scipy.stats`, and `MinMaxScaler` from `sklearn.preprocessing`.\n2. **Extract Data**: Unpack the list of tuples into separate lists for items, counts, and weights.\n3. **Normalize Counts**: Apply z-score normalization to the counts using `scipy.stats.zscore`.\n4. **Normalize Weights**: Apply min-max scaling to the weights using `sklearn.preprocessing.MinMaxScaler`.\n5. **Create DataFrame**: Create a pandas DataFrame with the items, normalized counts, and normalized weights.\n6. **Return DataFrame**: Return the resulting DataFrame.\n\nThis function processes any list of tuples with items, counts, and weights as described. The example usage demonstrates how to call the function and print the resulting DataFrame."}
{"task_id": "BigCodeBench/870", "solution": "```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Determine the maximum length of the tuples\n    max_length = max(len(t) for t in data_list)\n    \n    # Create a list to hold the numeric values for each position\n    values_per_position = [[] for _ in range(max_length)]\n    \n    # Iterate through the list of tuples\n    for tup in data_list:\n        for i, value in enumerate(tup):\n            if isinstance(value, (int, float)):\n                values_per_position[i].append(value)\n    \n    # Calculate means\n    means = [np.mean(pos_values) if pos_values else np.nan for pos_values in values_per_position]\n    \n    # Create the DataFrame\n    index = [f'Position {i}' for i in range(max_length)]\n    df = pd.DataFrame({'Mean Value': means}, index=index)\n    \n    return df\n\n# Example usage:\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)\nprint(df)\n```"}
{"task_id": "BigCodeBench/879", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty\")\n    \n    # Check if the specified columns are in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"'{col1}' or '{col2}' not found in the DataFrame\")\n    \n    # Check if the columns contain categorical data\n    if not (pd.api.types.is_categorical_dtype(data[col1]) or data[col1].dtype == object):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data\")\n    if not (pd.api.types.is_categorical_dtype(data[col2]) or data[col2].dtype == object):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data\")\n    \n    # Create the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if both columns have multiple categories\n    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n        raise ValueError(\"One or both columns do not have multiple categories\")\n    \n    # Check if any category has less than 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations\")\n    \n    # Perform the chi-square test\n    chi2, p, _, _ = chi2_contingency(contingency_table)\n    \n    return p\n\n# Example usage\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})\nprint(task_func(data, 'a', 'b'))  # This should print 1.0\n```"}
{"task_id": "BigCodeBench/897", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.array([results.count(i) for i in NUMBERS])\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(1, 8) - 0.5, edgecolor='black', rwidth=0.8)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    \n    # Return frequency array and histogram axes\n    return frequency, ax\n\n# Example usage:\nfrequency, ax = task_func(rolls=1000, seed=42)\nplt.show()\n```"}
{"task_id": "BigCodeBench/906", "solution": "```python\nimport zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure source directory exists\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    \n    # Create target directory if it does not exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Path to the archive file\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a new zip file\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as archive:\n        # Iterate over files in the source directory\n        for root, _, files in os.walk(source_dir):\n            for file in files:\n                # Check if the file has the '_processed' suffix\n                if re.search(r'_processed$', os.path.splitext(file)[0]):\n                    file_path = os.path.join(root, file)\n                    # Add the processed file to the archive\n                    archive.write(file_path, os.path.relpath(file_path, source_dir))\n                    # Optionally, move the processed file to the target directory\n                    shutil.move(file_path, os.path.join(target_dir, file))\n\n    return archive_path\n\n# Example usage:\n# print(task_func('/path/to/source', '/path/to/target'))\n```"}
{"task_id": "BigCodeBench/914", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure the index is of datetime type\n    df.index = pd.to_datetime(df.index)\n    \n    # Prepare the data for linear regression\n    df['Timestamp'] = df.index.map(pd.Timestamp.timestamp)\n    X = df['Timestamp'].values.reshape(-1, 1)\n    y = df['Close'].values\n    \n    # Create and train the model\n    model = LinearRegression().fit(X, y)\n    \n    # Predict the next 7 days\n    time_step = 24 * 60 * 60  # 1 day in seconds\n    last_timestamp = df['Timestamp'].iloc[-1]\n    future_timestamps = np.array([last_timestamp + i * time_step for i in range(1, 8)]).reshape(-1, 1)\n    future_dates = [pd.Timestamp.fromtimestamp(ts) for ts in future_timestamps.flatten()]\n    predicted_prices = model.predict(future_timestamps).tolist()\n    \n    # Plot the data\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Close'], label='Historical Prices')\n    ax.plot(future_dates, predicted_prices, 'r--', label='Predicted Prices')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.legend()\n    \n    return (predicted_prices, ax)\n\n# Example usage:\n# df = pd.read_csv('path_to_csv_file.csv', index_col='Date', parse_dates=True)\n# predicted_prices, ax = task_func(df)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/915", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\n\ndef task_func(df, z_threshold=2):\n    # Calculate the Z-Scores of the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['closing_price'], label='Closing Prices', marker='o')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers', marker='x')\n    \n    # Labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    plt.show()\n    \n    # Return the outliers DataFrame and the plot axes\n    return outliers[['closing_price']], ax\n\n# Example usage\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\nprint(outliers2)\n```"}
{"task_id": "BigCodeBench/916", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Ensure the DataFrame contains the necessary 'Close' column\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column\")\n    \n    # Extract the closing prices\n    closing_prices = df['Close']\n\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 7))\n\n    # Box plot\n    sns.boxplot(ax=axes[0], data=closing_prices)\n    axes[0].set_title('Box Plot of Closing Prices')\n\n    # Histogram\n    sns.histplot(ax=axes[1], data=closing_prices, kde=True)\n    axes[1].set_title('Histogram of Closing Prices')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Return the axes objects as a tuple\n    return tuple(axes)\n\n# Example usage:\n# sample_data = {\n#     'Close': [120, 125, 130, 128, 132, 135, 140, 138, 136, 130, 128, 126, 124]\n# }\n# df = pd.DataFrame(sample_data)\n# task_func(df)\n```"}
{"task_id": "BigCodeBench/917", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the 'Close' column exists in the DataFrame\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column.\")\n    \n    # Ensure the data is sorted by date\n    df = df.sort_index()\n    \n    # Extract the closing prices\n    closing_prices = df['Close']\n    \n    # Fit the ARIMA model (Using a basic ARIMA(5,1,0) configuration as an example)\n    model = ARIMA(closing_prices, order=(5, 1, 0))\n    model_fit = model.fit()\n    \n    # Forecast for the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecasted_prices = forecast.tolist()\n    \n    # Plot the actual data and the forecast\n    fig, ax = plt.subplots()\n    closing_prices.plot(ax=ax, label='Actual Closing Prices')\n    \n    # Generate forecast index\n    forecast_index = pd.date_range(start=closing_prices.index[-1], periods=7, closed='right')\n    \n    # Create a Series for the forecast to ensure correct plotting\n    forecast_series = pd.Series(forecasted_prices, index=forecast_index)\n    forecast_series.plot(ax=ax, label='Forecasted Prices', color='red', linestyle='--')\n    \n    # Adding titles and labels\n    ax.set_title('Closing Price Forecast')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Price')\n    ax.legend()\n    \n    return forecasted_prices, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample DataFrame with closing prices for demonstration purposes\n    data = {'Close': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]}\n    df = pd.DataFrame(data, index=pd.date_range(start='2023-01-01', periods=11, freq='D'))\n    \n    # Call the forecasting function\n    forecasted_prices, ax = task_func(df)\n    print(\"Forecasted Prices for the next 7 days:\", forecasted_prices)\n    \n    # Display the plot\n    plt.show()\n```\n\n### Explanation:\n1. **DataFrame Validation**: The function first checks if the input DataFrame contains a 'Close' column.\n2. **Sort Data**: Ensures the data is sorted by date to avoid any issues with date order.\n3. **Extract Prices**: The 'Close' column is extracted from the DataFrame.\n4. **Model Fitting**: The ARIMA model is fit on the closing prices. Here, we use ARIMA(5,1,0) as an example. You can adjust the parameters as necessary based on the data.\n5. **Forecasting**: The model forecasts the closing prices for the next 7 days.\n6. **Plotting**: The function plots the actual closing prices and the forecasted prices on the same plot. It also sets titles, labels, and a legend.\n7. **Return**: The function returns a tuple containing the list of forecasted prices and the Axes object of the plot.\n\nEnsure you have the necessary packages installed:\n```sh\npip install pandas matplotlib statsmodels\n```"}
{"task_id": "BigCodeBench/928", "solution": "```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of lowercase English alphabets\n    all_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    combination_dict = {comb: 0 for comb in all_combinations}\n    \n    # Count the frequencies of each two-letter combination in the word\n    word_combinations = [word[i:i+2] for i in range(len(word)-1)]\n    word_combinations_count = Counter(word_combinations)\n    \n    # Update the dictionary with the actual counts\n    for comb, count in word_combinations_count.items():\n        if comb in combination_dict:\n            combination_dict[comb] = count\n            \n    return combination_dict\n\n# Example usage:\nword = \"exampleword\"\nresult = task_func(word)\nprint(result)\n```"}
{"task_id": "BigCodeBench/942", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a list to hold sales data\n    sales_data = []\n\n    # Generate sales data for each category and date\n    for date in dates:\n        for category in categories:\n            sales = np.random.randint(1000, 5000)  # Random sales figure between 1000 and 5000\n            sales_data.append([date, category, sales])\n\n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=['Date', 'Category', 'Sales'])\n\n    # Pivot the DataFrame to make it suitable for plotting\n    df_pivot = df.pivot(index='Date', columns='Category', values='Sales')\n\n    # Plot the data\n    ax = df_pivot.plot(kind='line', marker='o', figsize=(12, 8))\n    ax.set_title('Sales Report')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    plt.legend(title='Category')\n    plt.grid(True)\n    plt.tight_layout()\n    \n    # Show plot\n    plt.show()\n\n    # Return the DataFrame and Axes object\n    return df, ax\n\n# Example of how to call the function\nsales_df, sales_ax = task_func()\nprint(sales_df.head())\n```"}
{"task_id": "BigCodeBench/945", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # If sales data is not provided, generate random sales data\n    if sales_data is None:\n        np.random.seed(0)  # For reproducibility\n        sales_data = np.random.randint(100, 200, size=periods)\n    \n    # Create a DataFrame\n    data = pd.DataFrame({'Date': dates, 'Sales': sales_data})\n    \n    # Prepare data for linear regression\n    data['Date_ordinal'] = data['Date'].map(pd.Timestamp.toordinal)\n    X = data['Date_ordinal'].values.reshape(-1, 1)\n    y = data['Sales'].values\n    \n    # Fit the Linear Regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Forecast future sales for the same number of periods as the input data\n    future_dates = pd.date_range(start=dates[-1] + pd.Timedelta(days=1), periods=periods, freq=freq)\n    future_dates_ordinal = future_dates.map(pd.Timestamp.toordinal).values.reshape(-1, 1)\n    future_sales_forecast = model.predict(future_dates_ordinal)\n    \n    # Output the forecasted future sales as a numpy array\n    return future_sales_forecast\n\n# Example usage\nforecasted_sales = task_func()\nprint(forecasted_sales)\n```"}
{"task_id": "BigCodeBench/952", "solution": "```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Raise an exception if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    \n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n    \n    # Set the current system date as the due date\n    due_date = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    # Initialize random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Randomly assign tasks to employees\n    assigned_tasks = []\n    for _ in range(min(n_tasks, len(sanitized_tasks))):\n        task = random.choice(sanitized_tasks)\n        employee = random.choice(employees)\n        assigned_tasks.append({\"Task Name\": task, \"Assigned To\": employee, \"Due Date\": due_date})\n        sanitized_tasks.remove(task)  # To prevent duplicate assignment of the same task\n    \n    # Create a DataFrame with the assignments\n    df_assignments = pd.DataFrame(assigned_tasks, columns=[\"Task Name\", \"Assigned To\", \"Due Date\"])\n    \n    return df_assignments\n\n# Example usage\n# task_list = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\"]\n# n_tasks = 3\n# print(task_func(task_list, n_tasks))\n```\n\n### Explanation:\n1. **Imports**: The necessary libraries (`pandas`, `random`, and `datetime`) are imported.\n2. **Function Definition**: The function `task_func` is defined with parameters for a list of tasks, the number of tasks to assign, a list of employees (with a default list), and an optional seed for reproducibility.\n3. **Negative n_tasks Check**: The function raises a `ValueError` if `n_tasks` is negative.\n4. **Task Sanitization**: Task names are sanitized by replacing spaces with underscores.\n5. **Due Date**: The current system date is set as the due date.\n6. **Random Seed**: If a seed is provided, it is used to initialize the random number generator for reproducibility.\n7. **Task Assignment**: Tasks are randomly assigned to employees. Each assignment includes the sanitized task name, the assigned employee, and the due date.\n8. **DataFrame Creation**: A pandas DataFrame is created with the assigned tasks and returned.\n\nYou can use the example usage provided to test the function with a sample task list and a specified number of tasks to assign."}
{"task_id": "BigCodeBench/955", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n    \n    # Convert words to lowercase and replace spaces with underscores\n    modified_words = [word.lower().replace(' ', '_') for word in mystrings]\n    \n    # Make text case-insensitive\n    text = text.lower()\n    \n    # Find all words in text\n    words_in_text = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count frequencies of each word\n    word_counts = Counter(words_in_text)\n    \n    # Create a dictionary for modified words\n    modified_word_counts = {word: word_counts[word] for word in modified_words if word in word_counts}\n    \n    # Plotting the frequency of each unique word\n    fig, ax = plt.subplots()\n    ax.bar(modified_word_counts.keys(), modified_word_counts.values())\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Each Unique Word')\n\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\nmystrings = [\"Hello World\", \"This is a Test\", \"Example\"]\ntext = \"This is a test. Hello world! Example example world.\"\nax = task_func(mystrings, text)\nplt.show()\n```"}
{"task_id": "BigCodeBench/963", "solution": "```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Ensure source_directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory {source_directory} does not exist.\")\n    \n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create the target directory if it does not exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Prepare the full path for the zip file\n    zip_path = target_path / f\"{zip_name}.zip\"\n    \n    # Initialize the ZipFile object\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Iterate over all files in the source directory\n        for ext in valid_extensions:\n            for file in glob.glob(os.path.join(source_directory, f\"*{ext}\")):\n                zipf.write(file, os.path.basename(file))\n    \n    # Return the full path to the created zip file\n    return str(zip_path)\n\n# Example usage:\n# print(task_func(\"/path/to/source_directory\", \"/path/to/target_directory\", \"my_archive\"))\n```"}
{"task_id": "BigCodeBench/964", "solution": "```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Verify the source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists():\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Create the target directory if it doesn't exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Initialize count of successfully converted files\n    converted_files_count = 0\n    \n    # Supported file extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Iterate over files in the source directory\n    for file_path in source_path.iterdir():\n        if file_path.suffix.lower() in supported_extensions:\n            # Read the file content based on its extension\n            if file_path.suffix.lower() == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            \n            elif file_path.suffix.lower() == '.docx':\n                doc = docx.Document(file_path)\n                content = \"\\n\".join([para.text for para in doc.paragraphs])\n            \n            elif file_path.suffix.lower() == '.xlsx':\n                df = pd.read_excel(file_path)\n                content = df.to_csv(index=False, header=False)\n            \n            elif file_path.suffix.lower() == '.csv':\n                df = pd.read_csv(file_path)\n                content = df.to_csv(index=False, header=False)\n            \n            # Save content to a CSV file in the target directory\n            target_file_path = target_path / (file_path.stem + '.csv')\n            df = pd.DataFrame({'Text': [content]})\n            df.to_csv(target_file_path, index=False)\n            \n            # Increment the count of successfully converted files\n            converted_files_count += 1\n    \n    return converted_files_count\n\n# Example usage:\n# print(task_func('source_directory_path', 'target_directory_path'))\n```"}
{"task_id": "BigCodeBench/969", "solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check for NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Check for non-numeric columns\n    if not all(np.issubdtype(dtype, np.number) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    # Compute the cumulative sum for each numeric column\n    cumsum_df = df.cumsum()\n    \n    # Normalize the cumulative sum using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_cumsum = scaler.fit_transform(cumsum_df)\n    \n    # Convert the normalized cumulative sum back to a DataFrame retaining original columns\n    normalized_cumsum_df = pd.DataFrame(normalized_cumsum, columns=df.columns, index=df.index)\n    \n    return normalized_cumsum_df\n\n# Example usage:\n# df = pd.DataFrame({\n#     'A': [1, 2, 3, 4],\n#     'B': [4, 3, 2, 1]\n# })\n# result = task_func(df)\n# print(result)\n```"}
{"task_id": "BigCodeBench/971", "solution": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Ensure the provided directory exists and is a directory\n    directory = Path(directory_path)\n    if not directory.exists() or not directory.is_dir():\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    result = []\n\n    # Iterate over the files in the directory (no recursion into subdirectories)\n    for item in directory.iterdir():\n        if item.is_file():\n            # Get file stats\n            stat = item.stat()\n            \n            # Get file size\n            file_size = stat.st_size\n            \n            # Get creation time (or last metadata change time if creation time is not available)\n            try:\n                creation_time = datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc).isoformat()\n            except AttributeError:\n                creation_time = None  # Handle platforms where creation time is not available\n            \n            # Get last modification time\n            modification_time = datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc).isoformat()\n            \n            # Append tuple (file name, file size, creation time, modification time) to the result list\n            result.append((item.name, file_size, creation_time, modification_time))\n    \n    return result\n\n# Example usage\n# result = task_func('/path/to/empty_directory')\n# print(result)\n```"}
{"task_id": "BigCodeBench/985", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n        \n        # Validate the parsed data\n        if not isinstance(data, list) or not data:\n            raise ValueError(\"Input JSON data is malformed or empty.\")\n        \n        country_population = []\n        \n        for entry in data:\n            if not isinstance(entry, dict) or 'Country' not in entry or 'Population' not in entry:\n                raise ValueError(\"Malformed JSON data entry.\")\n            \n            country = entry['Country']\n            population = entry['Population']\n            \n            if not isinstance(country, str):\n                raise ValueError(f\"Country name must be a string. Found: {type(country).__name__}\")\n            if not (isinstance(population, int) or isinstance(population, float)):\n                raise ValueError(f\"Population must be numeric. Found: {type(population).__name__}\")\n            if population < 0:\n                raise ValueError(\"Population must be non-negative.\")\n            \n            # Round down float population values\n            rounded_population = math.floor(population)\n            country_population.append((country, rounded_population))\n        \n        # Create DataFrame\n        df = pd.DataFrame(country_population, columns=[\"Country\", \"Population\"])\n        \n        # Ensure output directory exists\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Define the file path\n        file_path = os.path.join(output_dir, file_name)\n        \n        # Write DataFrame to CSV without an extra index column\n        df.to_csv(file_path, index=False)\n        \n        return file_path, df\n    \n    except json.JSONDecodeError:\n        raise ValueError(\"Input JSON data is malformed.\")\n    except OSError:\n        raise IOError(\"Failed to write the file to the specified directory.\")\n\n# Example usage:\n# json_data = '[{\"Country\": \"USA\", \"Population\": 331002651}, {\"Country\": \"India\", \"Population\": 1380004385}, {\"Country\": \"Brazil\", \"Population\": 212559417}]'\n# output_dir = \"./output\"\n# file_name = \"country_population_report.csv\"\n# file_path, df = task_func(json_data, output_dir, file_name)\n# print(file_path)\n# print(df)\n```"}
{"task_id": "BigCodeBench/988", "solution": "```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Deduplicate predicates\n    predicates = list(set(predicates))\n\n    # Check for valid predicates\n    if not predicates or not all(callable(p) for p in predicates):\n        raise ValueError(\"No valid predicates are provided.\")\n\n    # Check if the directory exists and is a directory\n    dir_path_obj = Path(dir_path)\n    if not dir_path_obj.exists() or not dir_path_obj.is_dir():\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n\n    result = {}\n    \n    # Iterate over items in the directory\n    for item in dir_path_obj.iterdir():\n        item_name = item.name\n        # Evaluate item name against all predicates\n        result[item_name] = [pred(item_name) for pred in predicates]\n\n    return result\n\n# Example usage:\n# Define some sample predicates\ndef is_hidden(name):\n    return name.startswith('.')\n\ndef has_extension(extension):\n    def predicate(name):\n        return name.endswith(extension)\n    return predicate\n\ndef contains_digit(name):\n    return any(char.isdigit() for char in name)\n\n# Define directory path and predicates list\ndir_path = \"/path/to/your/directory\"\npredicates = [is_hidden, has_extension('.txt'), contains_digit]\n\n# Execute the function\ntry:\n    result = task_func(dir_path, predicates)\n    print(result)\nexcept (ValueError, FileNotFoundError) as e:\n    print(e)\n```"}
{"task_id": "BigCodeBench/990", "solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    try:\n        # Decode the hexadecimal string to bytes\n        byte_data = binascii.unhexlify(hex_string)\n        \n        # Convert bytes to a UTF-8 string\n        utf8_string = byte_data.decode('utf-8')\n        \n        # Initialize the result dictionary\n        result = {\n            'hex': hex_string,\n            'base64': base64.b64encode(byte_data).decode('utf-8'),\n            'utf-8': utf8_string,\n            'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n            'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        }\n\n        # Check if the UTF-8 string can be represented in ASCII\n        try:\n            result['ASCII'] = utf8_string.encode('ascii').decode('ascii')\n        except UnicodeEncodeError:\n            result['ASCII'] = 'Not representable in ASCII'\n\n        # URL encoding\n        result['URL'] = urllib.parse.quote(utf8_string)\n        \n        # ROT13 encoding\n        result['ROT13'] = codecs.encode(utf8_string, 'rot_13')\n        \n        return result\n    \n    except Exception as e:\n        return str(e)\n\n# Example usage\nprint(task_func(\"68656c6c6f\"))\n```"}
{"task_id": "BigCodeBench/998", "solution": "```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(byte_block)\n        downloaded_md5_checksum = md5_hash.hexdigest()\n        \n        # Validate the MD5 checksum\n        if downloaded_md5_checksum != EXPECTED_MD5_CHECKSUM:\n            os.remove(TARGET_TAR_FILE)\n            return False\n        \n        # Extract the contents of the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        \n        return True\n    \n    except Exception as e:\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False\n```"}
{"task_id": "BigCodeBench/999", "solution": "```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a counter for counting occurrences in the specified column\n        occurrences = collections.Counter()\n        \n        # Open the downloaded CSV file and process its contents\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists in the CSV file\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                occurrences[row[column_name]] += 1\n        \n        # Convert the Counter to a dictionary and return it\n        return dict(occurrences)\n    \n    except urllib.error.URLError as e:\n        raise Exception(f\"Error downloading the file: {e}\")\n    \n    except ValueError as ve:\n        raise ve\n    \n    except Exception as e:\n        raise Exception(f\"An unexpected error occurred: {e}\")\n    \n    finally:\n        # Ensure the downloaded CSV file is deleted after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n\n# Example usage (Uncomment to test with actual URL and path)\n# url = 'http://example.com/path/to/your/csvfile.csv'\n# column_name = 'your_column_name'\n# csv_file_path = '/path/to/save/csvfile.csv'\n# print(task_func(url, column_name, csv_file_path))\n```"}
{"task_id": "BigCodeBench/1003", "solution": "```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML content from the URL\n        response = urllib.request.urlopen(url)\n        xml_content = response.read()\n    except Exception as e:\n        raise ValueError(\"Invalid URL or cannot fetch XML file\") from e\n    \n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(\"Invalid XML syntax\") from e\n    \n    # Extract items and their child elements into a list of dictionaries\n    items = []\n    for item in root.findall('item'):\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        items.append(item_data)\n\n    # Check if items list is empty indicating wrong structure\n    if not items:\n        raise ValueError(\"XML structure does not match expected format\")\n\n    # Convert the list of dictionaries to a DataFrame\n    try:\n        df = pd.DataFrame(items)\n    except Exception as e:\n        raise ValueError(\"Error converting XML data to DataFrame\") from e\n    \n    return df\n\n# Example usage\n# df = task_func('http://example.com/invalid_structure.xml')\n# print(df)\n```"}
{"task_id": "BigCodeBench/1004", "solution": "```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file from the specified URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n    \n    # Use regular expression to find all words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    common_words = word_counts.most_common(10)\n    \n    # Separate the words and their counts for plotting\n    words, counts = zip(*common_words)\n    \n    # Plot the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    return word_counts, ax\n\n# Example usage\nurl = 'https://www.gutenberg.org/files/1342/1342-0.txt'  # URL to a plain text file (e.g., \"Pride and Prejudice\" by Jane Austen)\nword_counts, ax = task_func(url)\nprint(word_counts.most_common(10))\n```"}
{"task_id": "BigCodeBench/1006", "solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n        \n        # Attempt to download the file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the Content-Type is 'application/zip'\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Save the downloaded file to the specified path\n        zip_file_path = os.path.join(download_path, \"downloaded_file.zip\")\n        with open(zip_file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Extract the ZIP file\n        try:\n            with ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        return download_path\n    \n    except requests.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage:\n# print(task_func(\"https://example.com/file.zip\"))\n```"}
{"task_id": "BigCodeBench/1008", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        # Send a GET request to the specified URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad requests\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {e}\")\n    \n    # Parse the HTML content of the webpage\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the table with the specified ID\n    table = soup.find('table', id=table_id)\n    \n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n    \n    # Extract the headers\n    headers = []\n    header_row = table.find('tr')\n    if header_row:\n        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n    \n    # Extract the rows of the table\n    rows = []\n    data_rows = table.find_all('tr')[1:]  # Skip the header row\n    \n    for row in data_rows:\n        cells = row.find_all('td')\n        row_data = [cell.get_text(strip=True) for cell in cells]\n        if row_data:  # Ensure that there is data in the row\n            rows.append(row_data)\n    \n    # Create and return the DataFrame\n    if rows:\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n    else:\n        df = pd.DataFrame(columns=headers)\n    \n    return df\n\n# Example usage:\n# df = task_func('https://example.com/data.html', 'existing_table_id')\n# print(df)\n```"}
{"task_id": "BigCodeBench/1012", "solution": "```python\nimport requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure the directories exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    zip_file_path = DOWNLOAD_DIR / filename\n    extracted_files = []\n    \n    try:\n        # Download the zip file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n\n        # Write the zip file to the download directory\n        with open(zip_file_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Extract the zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            extracted_files = zip_ref.namelist()\n        \n        return (\"Success\", extracted_files)\n    \n    except requests.exceptions.RequestException as e:\n        return (f\"Error: Network-related exception - {str(e)}\", [])\n    \n    except (zipfile.BadZipFile, IOError, OSError) as e:\n        return (f\"Error: File-related exception - {str(e)}\", [])\n    \n    finally:\n        # Clean up the downloaded zip file if it exists\n        if zip_file_path.exists():\n            zip_file_path.unlink()\n\n# Example usage:\n# status, files = task_func(\"https://example.com/file.zip\", \"example.zip\")\n# print(status)\n# print(files)\n```"}
{"task_id": "BigCodeBench/1013", "solution": "```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    links = soup.find_all('a', href=True)\n    \n    unique_links = set()\n    \n    for link in links:\n        href = link.get('href')\n        if href:\n            absolute_url = urljoin(base_url, href)\n            unique_links.add(absolute_url)\n    \n    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['URL'])  # Write the header\n        for link in unique_links:\n            writer.writerow([link])\n    \n    return len(unique_links)\n```"}
{"task_id": "BigCodeBench/1015", "solution": "```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the specified URL\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise HTTPError for bad responses\n        webpage_content = response.content\n\n        # Parse the HTML content\n        tree = html.fromstring(webpage_content)\n        tables = tree.xpath('//table')\n\n        if not tables:\n            # No table found in the HTML content\n            return 0\n\n        # Convert the first table found into a pandas DataFrame\n        df = pd.read_html(html.tostring(tables[0]))[0]\n\n        if df.empty:\n            # The table is empty\n            return 0\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n\n        # Replace the table \"my_table\" with the new data\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n\n        # Commit the transaction and close the connection\n        conn.commit()\n        conn.close()\n\n        # Return the number of rows parsed and stored\n        return len(df)\n\n    except requests.RequestException as e:\n        raise requests.RequestException(f\"Network-related error occurred: {e}\")\n\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database-related error occurred: {e}\")\n\n# Example usage:\n# rows = task_func(\"https://example.com/table_page\")\n# print(f\"Number of rows parsed and stored: {rows}\")\n```\n\nThis function performs the following tasks:\n1. Fetches HTML content from a given URL using `requests`.\n2. Parses the HTML and extracts the first table found using `lxml`.\n3. Converts the table to a DataFrame using `pandas`.\n4. Replaces the existing `my_table` in the specified SQLite database with the new data, handling any database errors that may occur.\n5. Returns the number of rows in the parsed HTML table.\n\nMake sure you have the necessary libraries installed:\n```\npip install requests lxml pandas\n```"}
{"task_id": "BigCodeBench/1019", "solution": "```python\nfrom PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    comment = \"\"\n\n    # Try to open the image file\n    try:\n        img = Image.open(filename)\n    except IOError as e:\n        raise ValueError(f\"Unable to open image file: {e}\")\n\n    # Try to extract text using OCR\n    try:\n        extracted_text = pytesseract.image_to_string(img)\n        if extracted_text:\n            try:\n                # Convert extracted text encoding\n                comment = extracted_text.encode(from_encoding).decode(to_encoding)\n                return comment\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(f\"Encoding conversion error: {e}\")\n    except Exception as e:\n        # Log OCR extraction failure but do not raise an exception, move to comment processing\n        print(f\"Failed to extract text using OCR: {e}\")\n\n    # If OCR extraction fails or no text extracted, try to process image comment\n    try:\n        if \"comment\" in img.info:\n            img_comment = img.info[\"comment\"]\n            if isinstance(img_comment, bytes):\n                img_comment = img_comment.decode(from_encoding)\n            try:\n                comment = img_comment.encode(from_encoding).decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(f\"Encoding conversion error: {e}\")\n        else:\n            comment = \"\"\n    except Exception as e:\n        # Log comment extraction failure but do not raise an exception\n        print(f\"Failed to extract comment from image: {e}\")\n\n    return comment\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        result = task_func()\n        print(f\"Extracted Text/Comment: {result}\")\n    except ValueError as err:\n        print(f\"Error: {err}\")\n```"}
{"task_id": "BigCodeBench/1020", "solution": "```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Initiating an HTTP GET request to the specified URL with a 5-second timeout.\n        response = requests.get(url, timeout=5)\n        # Retrieving the response content in raw bytes.\n        raw_content = response.content\n        \n        # Handling cases with empty content\n        if not raw_content:\n            return {}\n        \n        # Detecting the encoding if not provided\n        if not from_encoding:\n            detection = chardet.detect(raw_content)\n            from_encoding = detection.get('encoding')\n        \n        # Raise ValueError if the encoding is undetectable and content is not empty\n        if not from_encoding:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n        \n        # Decoding and re-encoding the content\n        decoded_content = raw_content.decode(from_encoding)\n        re_encoded_content = decoded_content.encode(to_encoding)\n        \n        # Parsing the JSON data\n        return json.loads(re_encoded_content.decode(to_encoding))\n    \n    except requests.RequestException as e:\n        # Handle errors in the HTTP request\n        raise RuntimeError(f\"HTTP request failed: {e}\")\n    except (json.JSONDecodeError, ValueError) as e:\n        # Handle JSON parsing errors and custom ValueError\n        raise RuntimeError(f\"Error processing the response content: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        data = task_func()\n        print(data)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```"}
{"task_id": "BigCodeBench/1022", "solution": "```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The specified file was not found: {csv_file_path}\")\n\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # If the CSV file is empty, return an empty DataFrame\n        return pd.DataFrame()\n\n    # Check if the specified date column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column is not present in the CSV file: {column_name}\")\n\n    # Convert the date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Drop rows where the date conversion failed (NaT values)\n    df = df.dropna(subset=[column_name])\n\n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort the data by the date column\n    df = df.sort_values(by=column_name)\n\n    return df\n```"}
{"task_id": "BigCodeBench/1039", "solution": "```python\nimport ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        requested_file_path = secure_socket.recv(buffer_size).decode('utf-8').strip()\n        \n        # Check if the file exists\n        if not os.path.isfile(requested_file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n\n        # Calculate SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(requested_file_path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b''):\n                sha256_hash.update(byte_block)\n\n        # Send the SHA256 hash to the client\n        hash_hex = sha256_hash.hexdigest()\n        secure_socket.sendall(hash_hex.encode('utf-8'))\n\n        return hash_hex\n    except Exception as e:\n        error_message = f\"An error occurred: {str(e)}\"\n        try:\n            secure_socket.sendall(error_message.encode('utf-8'))\n        except:\n            pass\n        return error_message\n    finally:\n        secure_socket.close()\n```"}
{"task_id": "BigCodeBench/1040", "solution": "```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    # Setup the server socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setblocking(False)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n    \n    # Initialize the inputs and outputs lists for select\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n    \n    # Track the start time and end time for the run duration\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n    \n    print(f\"Server started at {start_time} on {server_address}:{server_port}\")\n\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs, 1)\n        \n        for s in readable:\n            if s is server_socket:\n                # Accept a new connection\n                connection, client_address = s.accept()\n                connection.setblocking(False)\n                inputs.append(connection)\n                message_queues[connection] = queue.Queue()\n            else:\n                # Receive data from a client\n                data = s.recv(buffer_size)\n                if data:\n                    # Append current time to received data\n                    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                    response = data.decode('utf-8') + ' ' + current_time\n                    message_queues[s].put(response.encode('utf-8'))\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Handle client disconnection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n        \n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                s.send(next_msg)\n        \n        for s in exceptional:\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n    \n    # Close all remaining sockets\n    for s in inputs:\n        s.close()\n    for s in outputs:\n        s.close()\n\n    end_time_actual = datetime.now()\n    duration = (end_time_actual - start_time).total_seconds()\n    status_message = f\"Server stopped at {end_time_actual} after running for {duration:.2f} seconds\"\n    print(status_message)\n    return status_message\n\n# Example usage:\nif __name__ == \"__main__\":\n    task_func()\n```"}
{"task_id": "BigCodeBench/1042", "solution": "```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    try:\n        # Receive the message from the client socket\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        \n        if not message:\n            print(\"No message received from client.\")\n            return\n        \n        print(\"Message received from client:\", message)\n        \n        # Get sender's email details\n        sender_email = input(\"Enter your email: \")\n        recipient_email = input(\"Enter recipient's email: \")\n        sender_password = getpass.getpass(\"Enter your email password: \")\n\n        # Create an email message\n        email_msg = EmailMessage()\n        email_msg.set_content(message)\n        email_msg['Subject'] = 'Message from Client Socket'\n        email_msg['From'] = sender_email\n        email_msg['To'] = recipient_email\n\n        # Connect to the SMTP server and send the email\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n            smtp.starttls()  # Secure the connection\n            smtp.login(sender_email, sender_password)\n            smtp.send_message(email_msg)\n        \n        print(\"Email sent successfully.\")\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    \n    finally:\n        # Close the client socket\n        client_socket.close()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a server socket to accept a connection from a client\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    server_socket.listen(1)\n    print(\"Waiting for a connection...\")\n    \n    client_socket, client_address = server_socket.accept()\n    print(f\"Connection from {client_address}\")\n    \n    task_func(client_socket)\n    \n    server_socket.close()\n```"}
{"task_id": "BigCodeBench/1053", "solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        \n        # Check if the first column has a header 'Text'\n        if 'Text' in df.columns:\n            text_data = df['Text']\n        else:\n            text_data = df.iloc[:, 0]  # Use the first column if no header is found\n\n        # Combine all text data into a single string\n        all_text = ' '.join(text_data.dropna().astype(str).tolist())\n\n        # Initialize CountVectorizer with stop_words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_counts = vectorizer.fit_transform([all_text])\n\n        # Sum up the counts of each vocabulary word\n        sum_words = word_counts.sum(axis=0)\n        word_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n\n        # Get the top 10 most common words\n        top_words = word_freq[:10]\n        words, counts = zip(*top_words)\n\n        # Plot the histogram\n        fig, ax = plt.subplots()\n        ax.bar(words, counts)\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45)\n\n        # If save_path is provided, save the plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage:\n# ax = task_func(\"path/to/your/file.csv\")\n# task_func(\"path/to/your/file.csv\", \"path/to/save/histogram.png\")\n```"}
{"task_id": "BigCodeBench/1057", "solution": "```python\nimport pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Predefined lists for animals and foods\n    predefined_animals = ['Cat', 'Dog', 'Elephant']\n    predefined_foods = ['Fish', 'Meat', 'Grass']\n    \n    # Handling empty or None inputs\n    if animals is None or len(animals) == 0:\n        animals = predefined_animals\n    if foods is None or len(foods) == 0:\n        foods = predefined_foods\n    \n    # If both lists are empty or None, return an empty DataFrame\n    if (animals is None or len(animals) == 0) and (foods is None or len(foods) == 0):\n        return pd.DataFrame()\n    \n    # Create all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations randomly\n    np.random.shuffle(combinations)\n    \n    # Create an empty DataFrame with animals as rows and foods as columns\n    df = pd.DataFrame(index=animals, columns=foods)\n    \n    # Populate the DataFrame with the 'animal:food' format\n    for animal, food in combinations:\n        df.at[animal, food] = f\"{animal}:{food}\"\n    \n    # Replace any NaN values with an empty string (in case some cells were not filled)\n    df.fillna('', inplace=True)\n    \n    return df\n\n# Example usage:\ndf = task_func()\nprint(df)\n```"}
{"task_id": "BigCodeBench/1077", "solution": "```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Convert the timezone string to a pytz timezone object\n    tz = pytz.timezone(timezone)\n    \n    # Convert each timestamp in the list to the specified timezone\n    datetime_list = [datetime.fromisoformat(ts).astimezone(tz) for ts in time_strings]\n    \n    # Calculate the absolute time differences in seconds between each consecutive pair of timestamps\n    time_differences = [\n        abs((datetime_list[i+1] - datetime_list[i]).total_seconds())\n        for i in range(len(datetime_list) - 1)\n    ]\n    \n    # If no time differences are found, return 0.0\n    if not time_differences:\n        return 0.0\n    \n    # Calculate and return the average time difference using numpy's mean function\n    return float(np.mean(time_differences))\n\n# Example usage:\ntime_strings = [\n    \"2023-03-01T12:00:00+00:00\",\n    \"2023-03-01T13:00:00+00:00\",\n    \"2023-03-01T14:30:00+00:00\"\n]\ntimezone = \"America/New_York\"\nprint(task_func(time_strings, timezone))  # Example output\n```"}
{"task_id": "BigCodeBench/1085", "solution": "```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words\n    common_words = word_counts.most_common(10)\n    \n    # Plot the top 10 most common words\n    words, counts = zip(*common_words)\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    \n    return common_words, ax\n\n# Example usage:\n# text = \"Sample text with several words. Some words appear more frequently than others. Words, words, words.\"\n# common_words, ax = task_func(text)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/1124", "solution": "```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to extract URL\n    url_pattern = re.compile(r'https?://[^\\s]+')\n    match = url_pattern.search(myString)\n    \n    if not match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = match.group(0)\n    \n    # Check if the URL is valid\n    try:\n        result = urlparse(url)\n        if not all([result.scheme, result.netloc]):\n            return \"No valid URL found in the provided string.\"\n    except:\n        return \"No valid URL found in the provided string.\"\n    \n    # Fetch the content of the URL\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code != 200:\n            return f\"Unable to fetch the content of the URL: {url}\"\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the web page to find the title\n    try:\n        soup = BeautifulSoup(response.content, 'html.parser')\n        title_tag = soup.find('title')\n        if title_tag and title_tag.string:\n            return title_tag.string.strip()\n        else:\n            return \"No title tag found in the webpage.\"\n    except:\n        return \"No title tag found in the webpage.\"\n\n# Example usage:\n# print(task_func(\"Check out this website: https://www.example.com\"))\n```"}
{"task_id": "BigCodeBench/1129", "solution": "```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON string\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON data\") from e\n\n    # Retrieve the URL associated with the specified key\n    url = data.get(unknown_key)\n    if not url:\n        raise KeyError(f\"Key '{unknown_key}' not found in JSON data\")\n\n    # Download the file from the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to download the file from the URL: {url}\") from e\n\n    # Generate the timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f'{unknown_key}_{timestamp}.txt'\n\n    # Determine the save directory\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n    else:\n        save_dir = os.getcwd()\n\n    # Construct the full path for the file\n    file_path = os.path.join(save_dir, filename)\n\n    # Save the file\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n\n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)\n\n# Example usage:\n# json_data = '{\"example_key\": \"https://example.com/file.txt\"}'\n# print(task_func(json_data, 'example_key', '/path/to/save'))\n```"}
{"task_id": "BigCodeBench/1137", "solution": "```python\nimport bs4\nimport requests\nimport re\nimport json\nimport os\n\ndef task_func(url: str, output_path: str) -> list:\n    # Define a regular expression pattern to match phone numbers\n    phone_regex = re.compile(r'\\(?\\b[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b')\n    \n    # Check if the input is a URL or a local file path\n    if os.path.isfile(url):\n        # If it's a local file, read its content\n        with open(url, 'r') as file:\n            text = file.read()\n    else:\n        # Else, assume it's a URL and fetch content from the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        text = response.text\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = bs4.BeautifulSoup(text, 'html.parser')\n    \n    # Extract all text from the HTML content\n    extracted_text = soup.get_text()\n    \n    # Find all phone numbers in the text\n    phone_numbers = phone_regex.findall(extracted_text)\n    \n    # Remove duplicates by converting to a set and back to a list\n    phone_numbers = list(set(phone_numbers))\n    \n    # Save the phone numbers to the specified file in JSON format\n    with open(output_path, 'w') as output_file:\n        json.dump(phone_numbers, output_file, indent=4)\n    \n    return phone_numbers\n\n# Example usage:\n# phone_numbers = task_func('https://example.com', 'output.json')\n# print(phone_numbers)\n# phone_numbers = task_func('localfile.html', 'output.json')\n# print(phone_numbers)\n```"}
