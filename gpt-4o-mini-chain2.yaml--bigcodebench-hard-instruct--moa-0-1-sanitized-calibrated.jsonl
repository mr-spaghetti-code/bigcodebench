{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport logging\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    logging.basicConfig(level=logging.INFO)\n\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n\n    try:\n        # Log into the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n\n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    # List files in the directory\n    filenames = ftp.nlst()\n    ftp.quit()\n\n    # Prepare the wget command\n    for filename in filenames:\n        url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\".replace('//', '/')\n        try:\n            subprocess.run(['wget', url], check=True)\n        except subprocess.CalledProcessError as e:\n            logging.error(f\"Failed to download {filename}: {str(e)}\")\n\n    return filenames"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes shell commands from a CSV file and saves outputs to unique files.\n\n    :param commands_file_path: Path to the CSV file containing commands.\n    :param output_dir_path: Directory to save output files.\n    :return: List of paths to output files created.\n    :raises FileNotFoundError: If the commands_file_path does not exist.\n    \"\"\"\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file '{commands_file_path}' does not exist.\")\n    \n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir_path, exist_ok=True)\n    \n    output_files = []\n    \n    # Read commands from the CSV file\n    with open(commands_file_path, mode='r', newline='') as csvfile:\n        command_reader = csv.reader(csvfile)\n        \n        for index, row in enumerate(command_reader):\n            command = row[0] if row else ''\n            output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n            output_files.append(output_file_path)\n            \n            try:\n                # Execute the command\n                result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)\n                # Write the standard output to the file\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(result.stdout)\n                    \n            except subprocess.CalledProcessError as e:\n                # Write the error message and exit code to the file\n                with open(output_file_path, 'w') as output_file:\n                    output_file.write(f\"Error executing command: '{command}'\\n\")\n                    output_file.write(f\"Error Message: {e.stderr}\\n\")\n                    output_file.write(f\"Exit Code: {e.returncode}\\n\")\n    \n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    \"\"\"Check if a process is running by its name, start or restart the process accordingly.\n    \n    Args:\n        process_name (str): The name of the process to check and manage.\n    \n    Returns:\n        str: A message indicating the action taken.\n    \"\"\"\n    # Check if the process is running\n    process_found = False\n    pid = None\n    for proc in psutil.process_iter(attrs=['pid', 'name']):\n        if proc.info['name'] == process_name:\n            process_found = True\n            pid = proc.info['pid']\n            break\n\n    if not process_found:\n        # Process not found, starting it\n        try:\n            subprocess.Popen(process_name)\n            return f\"Process not found. Starting {process_name}.\"\n        except Exception as e:\n            return f\"Error starting process {process_name}: {str(e)}\"\n    else:\n        # Process found, restarting it\n        try:\n            psutil.Process(pid).terminate()  # Terminate the process\n            psutil.Process(pid).wait(timeout=5)  # Wait for the process to terminate\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n        except psutil.NoSuchProcess:\n            return f\"Process {process_name} not found for termination.\"\n        except Exception as e:\n            return f\"Error restarting process {process_name}: {str(e)}\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The specified directory does not exist: {directory}\")\n    \n    # Create the path for the zip file\n    zip_file_path = os.path.join(directory, 'files.zip')\n\n    # Find all files in the directory (excluding subdirectories)\n    file_pattern = os.path.join(directory, '*')  # Pattern to match all files\n    files = [f for f in glob.glob(file_pattern) if os.path.isfile(f)]\n\n    # If there are no files, return None\n    if not files:\n        return None\n    \n    # Create a zip file and add all files to it\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))  # Add file to zip with just the filename\n\n    # Return the path to the created zip file\n    return zip_file_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Generate and plot a word cloud from the input text after removing URLs.\n\n    Parameters:\n    text (str): The input text from which URLs will be removed.\n\n    Returns:\n    WordCloud: The generated word cloud object.\n\n    Raises:\n    ValueError: If no words are available to generate a word cloud after removing URLs.\n    TypeError: If input is not a string.\n    \"\"\"\n    if not isinstance(text, str):\n        raise TypeError(\"Input must be a string.\")\n    \n    # Remove URLs using a regular expression\n    text_without_urls = re.sub(r'http[s]?://\\S+', '', text)\n    \n    # Split the text into words and filter out any empty strings\n    words = text_without_urls.split()\n    words = [word for word in words if word]  # Filter out empty strings\n    \n    # Raise an exception if no words are available\n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')  # Turn off the axis\n    plt.show()\n    \n    # Return the WordCloud object\n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, target_column):\n    \"\"\"\n    Train a Random Forest classifier and plot feature importances.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame containing features and the target column.\n    target_column (str): The name of the target column in the DataFrame.\n\n    Returns:\n    sklearn.model.RandomForestClassifier: The trained Random Forest classifier.\n    matplotlib.axes.Axes: The Axes object of the plotted data.\n    \"\"\"\n    # Validate input\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in DataFrame.\")\n    \n    # Separate features and target\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # Check for missing values\n    if X.isnull().any().any() or y.isnull().any():\n        raise ValueError(\"Input data contains missing values.\")\n\n    # Initialize the Random Forest Classifier\n    clf = RandomForestClassifier(random_state=42)\n\n    # Fit the classifier\n    clf.fit(X, y)\n\n    # Get feature importances\n    feature_importances = clf.feature_importances_\n\n    # Create a DataFrame for feature importances\n    feature_importance_df = pd.DataFrame({\n        'Feature': X.columns,\n        'Importance': feature_importances\n    })\n\n    # Sort the DataFrame by importance\n    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n\n    # Return the trained classifier and the Axes object\n    ax = plt.gca()\n    return clf, ax"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = generate_password_hash(password)\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\nusers = {}\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.secret_key = secret_key\n    login_manager.init_app(app)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            user = next((u for u in users.values() if u.username == username), None)\n            if user and check_password_hash(user.password, password):\n                login_user(user)\n                return redirect(url_for('protected'))\n            flash('Invalid username or password', 'danger')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Hello, {current_user.username}! You are logged in.'\n\n    # Create a sample user (for demonstration purposes only)\n    if not users:\n        users['1'] = User('1', 'testuser', 'password123')\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    \n    Parameters:\n    - data (pd.DataFrame): The input DataFrame containing the data.\n    - column (str): The name of the column to check for outliers.\n    - outlier_z_score (float): The Z-score threshold for identifying outliers.\n    \n    Returns:\n    - tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Ensure the specified column exists in the DataFrame\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n    \n    # Standardize the specified column\n    scaler = StandardScaler()\n    data[column + '_scaled'] = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data[column + '_scaled']))\n    \n    # Identify outliers\n    outlier_indices = np.where(z_scores > outlier_z_score)[0]\n    \n    # Create a new DataFrame without outliers\n    data_without_outliers = data.drop(index=outlier_indices).reset_index(drop=True)\n\n    # Visualize the data before and after outlier removal\n    plt.figure(figsize=(12, 6))\n\n    # Scatter plot of original data with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='blue', label='Data with Outliers')\n    plt.scatter(outlier_indices, data[column].iloc[outlier_indices], color='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n\n    # Scatter plot of data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Return original data, data without outliers, and indices of outliers\n    return data, data_without_outliers, outlier_indices.tolist()"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on the provided dataset and generate a scatter plot showing the clusters and their centroids.\n\n    Parameters:\n    data (pd.DataFrame): Input data for clustering, must be a DataFrame.\n    n_clusters (int): The number of clusters to form, must be an integer greater than 1.\n\n    Returns:\n    tuple: A tuple containing:\n        np.ndarray: An array of cluster labels assigned to each sample.\n        plt.Axes: An Axes object with the scatter plot showing the clusters and centroids.\n    \"\"\"\n    # Validate inputs\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input 'data' must be a pd.DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Input 'n_clusters' must be an integer greater than 1.\")\n    \n    # Ensure there are at least two features for visualization\n    if data.shape[1] < 2:\n        raise ValueError(\"Input 'data' must contain at least two features for visualization.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Create scatter plot\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', marker='o', edgecolor='k', s=50)\n    \n    # Plot the centroids\n    centroids = kmeans.cluster_centers_\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n    \n    # Customize the plot\n    plt.title('K-means Clustering')\n    plt.xlabel(data.columns[0])\n    plt.ylabel(data.columns[1])\n    plt.legend()\n    plt.grid()\n    \n    # Create Axes object\n    ax = plt.gca()  # Get current Axes\n    plt.show()\n    \n    return cluster_labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, n_components=2):\n    \"\"\"\n    Performs PCA on the provided dataset and returns the transformed data and a scatter plot of the first two components.\n\n    Parameters:\n    data (pd.DataFrame): The input dataset (must be numerical).\n    n_components (int): The number of principal components to compute (must be a positive integer).\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the principal components.\n    matplotlib.axes: The Axes object containing the scatter plot.\n\n    Raises:\n    ValueError: If n_components is not a positive integer or if data is not a DataFrame with numerical values.\n    \"\"\"\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n\n    # Check if the input data is a DataFrame with numerical values\n    if not isinstance(data, pd.DataFrame) or not all(data.dtypes.apply(np.issubdtype, args=(np.number,))):\n        raise ValueError(\"Input data must be a DataFrame with numerical values.\")\n\n    # Standardizing the data\n    data_standardized = StandardScaler().fit_transform(data)\n\n    # Performing PCA\n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(data_standardized)\n\n    # Creating a DataFrame for the transformed data\n    principal_df = pd.DataFrame(data=principal_components, \n                                 columns=[f'Principal Component {i+1}' for i in range(n_components)])\n    \n    # Plotting the PCA result\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.scatter(principal_df.iloc[:, 0], principal_df.iloc[:, 1], edgecolor='k', s=50)\n    ax.set_title('PCA Result (Showing PC1 vs PC2)')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.grid()\n\n    plt.show()\n    \n    return principal_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    \"\"\"\n    Generates a Seaborn pair plot of the Iris dataset using Arial font.\n    \n    Returns:\n        plt.Figure: A matplotlib Figure object containing the seaborn pair plot of the iris dataset.\n    \"\"\"\n    # Set global font to Arial for better readability\n    plt.rcParams['font.family'] = 'Arial'\n    \n    # Load the iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = iris.target_names[iris.target]\n    \n    # Create a pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species', markers=[\"o\", \"s\", \"D\"])\n    \n    # Set the title\n    plt.suptitle('Iris Dataset Pair Plot', y=1.02)\n    \n    # Adjust the layout\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the current figure containing the plot\n    return plt.gcf()"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime, timedelta\nimport matplotlib.dates as mdates\ndef task_func(seed=42) -> plt.Axes:\n    \"\"\"\n    Generates a plot of random time series data for the past 30 days.\n\n    Parameters:\n        seed (int): An optional seed parameter for reproducibility.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing a line plot of the time series data.\n    \n    Raises:\n        ValueError: If there is an issue generating the data or the plot.\n    \"\"\"\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n\n        # Generate date range for the past 30 days in ascending order\n        date_range = [datetime.now() - timedelta(days=i) for i in range(30)]\n\n        # Generate random values for the time series data\n        values = [random.uniform(0, 100) for _ in range(30)]\n\n        # Create a pandas DataFrame\n        time_series_data = pd.DataFrame({'Date': date_range, 'Value': values})\n\n        # Plotting the data\n        fig, ax = plt.subplots()\n        ax.plot(time_series_data['Date'], time_series_data['Value'], marker='o', color='blue')\n\n        # Formatting the plot\n        ax.set_xlabel('Date', fontsize=12, fontname='Arial')\n        ax.set_ylabel('Value', fontsize=12, fontname='Arial')\n        ax.set_title('Random Time Series Data', fontsize=14, fontname='Arial')\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n        plt.xticks(rotation=45)\n        plt.grid()\n\n        # Show the plot\n        plt.tight_layout()\n        plt.show()\n\n        # Return the Axes object\n        return ax\n\n    except Exception as e:\n        raise ValueError(\"An issue occurred while generating the data or plot: \" + str(e))"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", save_path=None, seed=42):\n    \"\"\"\n    Generates a correlation heatmap for the Boston Housing dataset.\n\n    Parameters:\n    - data_url (str): URL to load the dataset from.\n    - save_path (str): Optional path to save the heatmap plot.\n    - seed (int): Seed for random number generation.\n\n    Returns:\n    - matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n\n    Raises:\n    - ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    try:\n        # Load the Boston Housing dataset from the provided URL\n        # The dataset is typically in a .txt format, so we'll read and process it properly.\n        boston_data = pd.read_csv(data_url, delim_whitespace=True, header=None)\n\n        # For simplicity, we will create a sample DataFrame resembling the Boston Housing dataset\n        # Normally, you would replace this with the actual DataFrame structure after loading\n        np.random.seed(seed)\n        data = {\n            'CRIM': np.random.rand(506),\n            'ZN': np.random.rand(506) * 100,\n            'INDUS': np.random.rand(506) * 12,\n            'NOX': np.random.rand(506) * 0.1 + 0.3,\n            'RM': np.random.rand(506) * 5 + 5,\n            'AGE': np.random.rand(506) * 100,\n            'DIS': np.random.rand(506) * 10,\n            'RAD': np.random.randint(1, 25, size=506),\n            'TAX': np.random.randint(100, 700, size=506),\n            'PTRATIO': np.random.rand(506) * 10,\n            'B': np.random.rand(506) * 100,\n            'LSTAT': np.random.rand(506) * 30,\n            'MEDV': np.random.rand(506) * 50\n        }\n        df = pd.DataFrame(data)\n\n        # Calculate the correlation matrix\n        corr = df.corr()\n\n        # Set up the matplotlib figure\n        plt.figure(figsize=(10, 8))\n\n        # Draw the heatmap\n        ax = sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n\n        # Title for the heatmap\n        plt.title('Correlation Heatmap of Boston Housing Dataset')\n\n        # Save the plot if a path is specified\n        if save_path:\n            plt.savefig(save_path)\n        \n        plt.show()\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating or saving the plot: {e}\")"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes a time series in the 'value' column of a DataFrame into trend, \n    seasonality, and residuals.\n    \n    Parameters:\n    - df: pandas DataFrame containing a 'value' column and a DateTime index.\n    - freq: Frequency string ('D', 'W', 'M', 'Q', 'Y').\n    - decomposition_model: Type of decomposition ('additive' or 'multiplicative').\n    \n    Returns:\n    - tuple: A tuple containing the decomposition result and the matplotlib Axes object.\n    \"\"\"\n    \n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must contain numeric data types.\")\n    \n    # Validate frequency string\n    valid_frequencies = ['D', 'W', 'M', 'Q', 'Y']\n    if freq not in valid_frequencies:\n        raise ValueError(f\"Frequency '{freq}' is not valid. Valid options are: {valid_frequencies}.\")\n    \n    # Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"decomposition_model must be either 'additive' or 'multiplicative'.\")\n    \n    # Ensure the DataFrame is indexed by a DateTime index\n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        raise ValueError(\"DataFrame index must be a DateTimeIndex.\")\n    \n    # Check if frequency can be inferred\n    inferred_freq = pd.infer_freq(df.index)\n    if inferred_freq is None:\n        raise ValueError(\"Could not infer frequency from the DateTime index.\")\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=pd.infer_freq(df.index))\n\n    # Plotting\n    fig, axs = plt.subplots(4, 1, figsize=(10, 8))\n    axs[0].plot(df['value'], label='Original', color='blue')\n    axs[0].set_title('Original Time Series')\n    axs[0].legend(loc='upper left')\n    \n    axs[1].plot(decomposition.trend, label='Trend', color='orange')\n    axs[1].set_title('Trend Component')\n    axs[1].legend(loc='upper left')\n    \n    axs[2].plot(decomposition.seasonal, label='Seasonality', color='green')\n    axs[2].set_title('Seasonal Component')\n    axs[2].legend(loc='upper left')\n    \n    axs[3].plot(decomposition.resid, label='Residuals', color='red')\n    axs[3].set_title('Residuals Component')\n    axs[3].legend(loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n    \n    return decomposition, axs"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import sample, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate input types\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than or equal to 'end_date'.\")\n\n    # Set the seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days between start and end date\n    delta_days = (end_date - start_date).days + 1  # Include end_date\n\n    # Generate all possible dates in the range\n    all_dates = [start_date + timedelta(days=i) for i in range(delta_days)]\n    \n    # Sample random dates from all possible dates\n    random_dates = sample(all_dates, delta_days)  # Ensure unique dates\n    \n    # Create and return a pandas Series\n    return pd.Series(random_dates)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies the input list by adding the element '12' and concatenates\n    a specified number of CSV files from a directory based on the sum of\n    the numbers in the modified list.\n\n    Parameters:\n    my_list (list): A list of numbers.\n    file_dir (str): Directory path where CSV files are located.\n    file_ext (str): The file extension of the files to concatenate.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing the concatenated data from the CSV files.\n    \n    Raises:\n    TypeError: If 'my_list' is not a list.\n    FileNotFoundError: If no files are found in the specified directory.\n    \"\"\"\n    # Check if my_list is indeed a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    \n    # Modify the list by adding the element '12'\n    my_list.append(12)\n    \n    # Calculate the number of files to concatenate\n    num_files_to_concat = sum(my_list)\n    \n    # Create a search pattern for CSV files\n    search_pattern = os.path.join(file_dir, '*' + file_ext)\n    \n    # Get a list of all CSV files in the specified directory\n    csv_files = glob.glob(search_pattern)\n    \n    # Raise FileNotFoundError if no files are found\n    if not csv_files:\n        raise FileNotFoundError(\"No files found in the specified directory.\")\n    \n    # Limit the number of files to the number calculated\n    csv_files_to_use = csv_files[:num_files_to_concat]\n    \n    # Concatenate the CSV files into a single DataFrame\n    df_list = [pd.read_csv(file) for file in csv_files_to_use]\n    \n    # Check if df_list is empty before concatenating\n    if not df_list:\n        raise ValueError(\"No data to concatenate from the specified files.\")\n    \n    concatenated_df = pd.concat(df_list, ignore_index=True)\n    \n    return concatenated_df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Validate input types\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input 'my_list' must be a list.\")\n    \n    # Ensure every element in my_list is numeric\n    for element in my_list:\n        if not isinstance(element, (int, float)):\n            raise ValueError(\"All elements in 'my_list' must be numeric (int or float).\")\n    \n    # Append the number 12 to my_list\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    total_sum = sum(my_list)\n    \n    # Limit the size of the random integers list by 'size'\n    total_sum = min(total_sum, size)  # Ensure total_sum does not exceed size\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Measure the time taken to generate the list\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    \n    # Calculate the time taken\n    time_taken = end_time - start_time\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=range(1, 102), align='left', edgecolor='black')\n    \n    # Set labels\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Generated Random Numbers')\n    ax.set_xticks(range(1, 101))\n    \n    # Show the histogram\n    plt.show()\n    \n    return (time_taken, ax)"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n    \n    except requests.ConnectionError:\n        raise ConnectionError(\"Unable to connect to the URL provided.\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP request failed: {e}\")\n\n    # Parse the content of the page\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the first table in the parsed HTML\n    table = soup.find('table')\n\n    if table is None:\n        raise ValueError(\"No table found on the page.\")\n\n    # Extract headers if present\n    headers = [th.get_text(strip=True) for th in table.find_all('th')]\n    \n    # Extract rows\n    rows = []\n    for tr in table.find_all('tr'):\n        cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n        if cells:  # Only add rows that have cell data\n            rows.append(cells)\n\n    if not rows:\n        raise ValueError(\"No table data found on the page.\")\n\n    # Create a DataFrame\n    if headers:\n        df = pd.DataFrame(rows, columns=headers)\n    else:\n        df = pd.DataFrame(rows)  # No headers, unnamed columns\n\n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draw histograms for each numeric column in the provided DataFrame.\n    \n    Parameters:\n    df (pd.DataFrame): The DataFrame containing numeric data.\n    \n    Returns:\n    list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \n    Raises:\n    ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    \n    # Validate input\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    # Select numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    if len(numeric_cols) == 0:\n        raise ValueError(\"No numeric columns found in the DataFrame.\")\n    \n    # Create a list to hold axes objects\n    axes_list = []\n    \n    # Create histograms for each numeric column\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df[col], bins=30, edgecolor='black')  # Adjust the number of bins as needed\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n    \n    plt.tight_layout()  # Adjust layout to prevent overlap\n    plt.show()  # Show all plots\n    \n    return axes_list"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread, BoundedSemaphore\nmax_threads = 100\nsemaphore = BoundedSemaphore(max_threads)\ndef check_port(ip, port):\n    \"\"\"Check if a specific port is open on a given IP address.\"\"\"\n    with semaphore:\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                sock.settimeout(1)  # Set a timeout for the connection attempt\n                return sock.connect_ex((ip, port)) == 0  # Return True if port is open\n        except Exception:\n            return False  # Return False if there's an error\ndef task_func(ip_range, port):\n    \"\"\"Scan the specified IP range for a given port and return open ports.\"\"\"\n    network = IPv4Network(ip_range)\n    results = {}\n\n    # Function to handle each thread's task\n    def thread_task(ip):\n        if check_port(str(ip), port):\n            results[str(ip)] = True  # Only add if the port is open\n\n    threads = []\n    for ip in network.hosts():\n        thread = Thread(target=thread_task, args=(ip,))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Regular expression to match the log format\n    log_pattern = re.compile(r'^(.*?): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.*)$')\n    \n    log_entries = []\n    \n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line.strip())\n            if match:\n                log_type = match.group(1)\n                timestamp_str = match.group(2)\n                message = match.group(3)\n                \n                # Validate the timestamp\n                try:\n                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f'Invalid timestamp: {timestamp_str}')\n                \n                # Append the valid log entry\n                log_entries.append({\n                    'Type': log_type,\n                    'Timestamp': timestamp,\n                    'Message': message\n                })\n    \n    # Raise exception if no valid log entries found\n    if not log_entries:\n        raise ValueError('No valid log entries found.')\n    \n    # Create a DataFrame from the log entries\n    df = pd.DataFrame(log_entries)\n    \n    # Define the output CSV file path\n    csv_file_path = log_file.rsplit('.', 1)[0] + '_structured_logs.csv'\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False, quotechar='\"')\n    \n    return csv_file_path"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a given text.\n    \n    Parameters:\n    text (str): The input text to analyze.\n    rwidth (float): The relative width of the bars in the histogram (between 0 and 1).\n    \n    Returns:\n    matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \"\"\"\n    # Use regex to find words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the lengths of the words\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and an Axes object\n    plt.figure(figsize=(10, 6))\n    ax = plt.subplot()\n    \n    # Check if there are words to analyze\n    if not word_lengths:\n        # Create an empty histogram\n        ax.hist([], bins=0)  # No bins for empty histogram\n        ax.set_title('Distribution of Word Lengths')\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_xticks([])  # No x-ticks for empty\n        ax.grid(axis='y', alpha=0.75)\n        plt.show()  # Display the empty histogram\n        return ax  # Return the empty Axes object\n    \n    # Create a histogram of word lengths\n    bins = np.arange(1, max(word_lengths) + 2) - 0.5  # Centering bins on integer values\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, color='blue', alpha=0.7)\n\n    # Labeling the axes\n    ax.set_title('Distribution of Word Lengths')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_xticks(np.arange(1, max(word_lengths) + 1))  # Set x-ticks at each word length\n    ax.grid(axis='y', alpha=0.75)\n\n    plt.show()  # Display the histogram\n    return ax  # Return the Axes object containing the histogram"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport pandas as pd\nfrom string import punctuation\ndef task_func(df):\n    \"\"\"\n    Extracts articles from a DataFrame based on specific keywords in their titles\n    and analyzes the frequency of each word in the content of these articles, excluding punctuation.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing 'Title' and 'Content' columns.\n\n    Returns:\n    dict: A dictionary with words as keys and their corresponding frequency as values.\n    \"\"\"\n    # Check if the DataFrame is empty or does not contain the necessary columns\n    if df.empty or not {'Title', 'Content'}.issubset(df.columns):\n        raise ValueError(\"DataFrame must not be empty and must contain 'Title' and 'Content' columns.\")\n    \n    # Keywords for filtering titles\n    keywords = ['like', 'what']\n    \n    # Filter articles whose titles contain the keywords (case-insensitive)\n    filtered_df = df[df['Title'].str.contains('|'.join(keywords), case=False, na=False)]\n    \n    # Return an empty dictionary if no articles match\n    if filtered_df.empty:\n        return {}\n    \n    # Combine all content from the filtered articles\n    combined_content = ' '.join(filtered_df['Content'].dropna())\n    \n    # Remove punctuation and convert to lower case\n    content_without_punctuation = combined_content.translate(str.maketrans('', '', punctuation)).lower()\n    \n    # Split the content into words\n    words = content_without_punctuation.split()\n    \n    # Count word frequencies\n    word_freq = {}\n    for word in words:\n        if word:  # Check if the word is not empty\n            word_freq[word] = word_freq.get(word, 0) + 1\n    \n    return word_freq\ndata = {\n    'Title': ['What is AI?', 'How to like Python', 'Data Science Overview', 'What are the benefits of AI?'],\n    'Content': ['AI is a field of study.', 'Python is liked by many.', 'Data Science is an interdisciplinary field.', 'AI has many benefits.']\n}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n                  'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', \n                  'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n                  'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n                  'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n                  'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n                  'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n                  'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n                  'further', 'then', 'once'])\ndef preprocess_text(text):\n    if pd.isna(text) or text.strip() == \"\":\n        return \"\"\n    # Remove punctuation and numbers\n    text = re.sub(r'[\\W\\d]', ' ', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n    return text\ndef task_func(dataframe, text_column):\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    \n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the preprocessed text to a document-term matrix\n    dtm = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the document-term matrix to a DataFrame\n    dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return dtm_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Generates a GeoPandas DataFrame containing random coordinates for a list of cities based on specified longitude and latitude ranges.\n\n    Parameters:\n    dic (dict): A dictionary with 'Lon' and 'Lat' keys, where values are tuples defining the range for longitude and latitude.\n    cities (list): A list of city names for which coordinates will be generated.\n\n    Returns:\n    gdf (GeoDataFrame): A GeoPandas DataFrame containing 'City' and 'Coordinates' (Point objects).\n    \"\"\"\n    # Check if 'Lon' and 'Lat' keys are present and their values are tuples\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Missing 'Lon' or 'Lat' keys in the dictionary.\")\n    \n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values of 'Lon' and 'Lat' must be tuples.\")\n    \n    # Generate random coordinates for each city\n    coordinates = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        coordinates.append(Point(lon, lat))\n    \n    # Create a GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': coordinates}, geometry='Coordinates')\n    \n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate the utc_datetime input\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object.\")\n    \n    # Validate the cities input\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings.\")\n    \n    # Validate the weather_conditions input\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings.\")\n    \n    # Validate the timezones input\n    if not isinstance(timezones, dict) or not all(isinstance(city, str) and isinstance(tz, str) for city, tz in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with city names as keys and timezone strings as values.\")\n    \n    # Set the seed for reproducibility\n    set_seed(seed)\n    \n    # Create a list to hold the weather report\n    weather_report = []\n    \n    for city in cities:\n        if city not in timezones:\n            continue  # Skip city if timezone is not defined\n        \n        # Convert UTC datetime to local time\n        local_tz = pytz.timezone(timezones[city])\n        local_time = utc_datetime.astimezone(local_tz)\n        \n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Append the weather report for the city\n        weather_report.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S') + ' ' + local_tz.zone,\n            'Weather Condition': weather_condition\n        })\n    \n    # Create a DataFrame from the weather report\n    df_weather_report = pd.DataFrame(weather_report)\n    \n    return df_weather_report"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate a random walk of given number of steps and return descriptive statistics.\n\n    Parameters:\n    elements (int): The number of steps in the random walk (must be a positive integer).\n    seed (int): Seed for random number generation for reproducibility.\n\n    Returns:\n    dict: A dictionary containing the descriptive statistics of the random walk.\n    matplotlib.axes.Axes: The Axes object with the plotted random walk.\n\n    Raises:\n    ValueError: If elements is not a positive integer.\n    \"\"\"\n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random steps\n    steps = np.random.choice([-1, 1], size=elements)\n\n    # Calculate the random walk\n    random_walk = np.cumsum(steps)\n\n    # Calculate descriptive statistics\n    descriptive_stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'std_dev': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th_percentile': np.percentile(random_walk, 5),\n        '25th_percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th_percentile': np.percentile(random_walk, 75),\n        '95th_percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk)\n    }\n\n    # Plotting the random walk\n    fig, ax = plt.subplots()\n    ax.plot(random_walk, marker='o', linestyle='-', markersize=3)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Position')\n    ax.grid()\n\n    plt.show()\n\n    return descriptive_stats, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a ZIP file from the specified URL, extracts its contents \n    to the given directory, and returns a list of the extracted filenames.\n\n    Parameters:\n    url (str): The URL of the ZIP file to download.\n    destination_directory (str): The directory where the ZIP file will be extracted.\n    headers (dict, optional): Optional headers to include in the request.\n\n    Returns:\n    list: A list of filenames of the extracted files.\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Define the local path for the downloaded zip file\n    zip_file_path = os.path.join(destination_directory, 'downloaded.zip')\n    \n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Raise an error for bad responses\n\n    # Write the content to a zip file\n    with open(zip_file_path, 'wb') as f:\n        f.write(response.content)\n\n    # Extract the zip file\n    extracted_files = []\n    try:\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n            extracted_files = zip_ref.namelist()  # Get the list of extracted files\n    except zipfile.BadZipFile:\n        print(\"Error: The downloaded file is not a valid ZIP file.\")\n    \n    # Return the list of extracted files\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate a random RGB image and display it.\n\n    Parameters:\n    - seed (int): Seed for random number generation.\n    - image_size (tuple): Size of the image (height, width, channels).\n    - range_low (int): Lower bound of the random pixel values.\n    - range_high (int): Upper bound of the random pixel values.\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot.\n    - image (numpy.ndarray): The numpy array of the generated image.\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Check if image_size is valid\n    if not (isinstance(image_size, tuple) and len(image_size) == 3 and all(isinstance(dim, int) and dim > 0 for dim in image_size)):\n        raise ValueError(\"image_size must be a tuple of three positive integers (height, width, channels).\")\n\n    # Set the random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(range_low, range_high, image_size, dtype=np.uint8)\n\n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis numbers and ticks\n    plt.show()\n\n    return ax, image"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.isfile(audio_file):\n        raise FileNotFoundError(f\"The specified audio file does not exist: {audio_file}\")\n\n    # Create an MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Read audio data from the file\n    data, samplerate = sf.read(audio_file)\n\n    # If the audio data has multiple channels, convert to mono\n    if data.ndim > 1:\n        data = np.mean(data, axis=1)\n\n    # Calculate the Sound Pressure Level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix * (10**(spl / 20))\n\n    # Generate the spectrogram from the normalized matrix\n    plt.figure(figsize=(10, 5))\n    plt.specgram(normalized_matrix.flatten(), NFFT=256, Fs=samplerate, Fc=0, noverlap=128, cmap='plasma', sides='default', mode='default')\n\n    plt.colorbar(label='Intensity (dB)')\n    plt.title('Spectrogram')\n    plt.xlabel('Time (samples)')\n    plt.ylabel('Frequency (Hz)')\n    plt.yscale('log')  # Logarithmic scale for frequency\n    plt.xscale('linear')  # Linear scale for time\n    plt.tight_layout()\n\n    # Return the normalized matrix and the figure object\n    return normalized_matrix, plt.gcf()"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    \"\"\"\n    Extract numeric values from a list of tuples, compute basic statistics,\n    and generate a histogram with an overlaid probability density function (PDF).\n\n    Parameters:\n    - original: list of tuples containing numeric values\n\n    Returns:\n    - np.array: A numpy array of the extracted numeric values.\n    - dict: Basic statistics for the array including mean, standard deviation, minimum, and maximum.\n    - Axes: A matplotlib Axes object showing the histogram with overlaid PDF.\n    \"\"\"\n    # Validate input\n    if not isinstance(original, list) or not all(isinstance(item, tuple) for item in original):\n        raise ValueError(\"Input must be a list of tuples.\")\n    \n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([item for sublist in original for item in sublist if isinstance(item, (int, float))])\n    \n    # Handle case with no numeric values\n    if numeric_values.size == 0:\n        return np.array([]), {'mean': np.nan, 'std_dev': np.nan, 'min': np.nan, 'max': np.nan}, None\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n    \n    # Create histogram and overlay PDF\n    fig, ax = plt.subplots()\n    counts, bins, patches = ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='g', edgecolor='black')\n    \n    # Create a PDF from the data\n    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n    pdf = stats.gaussian_kde(numeric_values)\n    \n    # Plot the PDF\n    ax.plot(bin_centers, pdf(bin_centers), 'r-', linewidth=2)\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.set_title('Histogram with PDF Overlay')\n    \n    # Return the numpy array, statistics dictionary, and Axes object\n    return numeric_values, statistics, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    \"\"\"\n    Normalize the input list and plot the original and normalized arrays.\n\n    Parameters:\n    original (list): A list of numeric values to be normalized.\n\n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: Normalized array.\n    matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Input validation\n    if not original:\n        raise ValueError(\"The input list is empty.\")\n    if not all(isinstance(x, (int, float)) for x in original):\n        raise TypeError(\"All elements in the input list must be numeric.\")\n\n    # Convert the original list to a NumPy array\n    original_array = np.array(original)\n    \n    # Normalize the array\n    normalized_array = preprocessing.normalize(original_array.reshape(1, -1)).flatten()\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original', marker='o', color='blue')\n    ax.plot(normalized_array, label='Normalized', marker='x', color='orange')\n    \n    # Adding labels and title\n    ax.set_title('Original and Normalized Arrays')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True)  # Adding grid for better readability\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the original and normalized arrays, and the axes object\n    return original_array, normalized_array, ax"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Processes the input dictionary to perform FFT on its values.\n    \n    Parameters:\n    data (dict): Input dictionary with numeric values.\n    sample_rate (int): The sample rate for the FFT (default is 8000).\n    \n    Returns:\n    tuple: A tuple containing the FFT of the signal (ndarray) and the plot Axes (Axes).\n    \"\"\"\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # Validate that all values are numeric and extract them\n    values = [v for v in data.values() if isinstance(v, (int, float))]\n    \n    if len(values) < 2:\n        raise ValueError(\"Not enough numeric values to perform FFT (at least 2 required).\")\n\n    # Step 2: Generate a signal based on the numeric values in \"data\"\n    signal = np.array(values)\n\n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n\n    # Step 4: Plot the FFT of the signal\n    N = len(signal)\n    freqs = fftpack.fftfreq(N, d=1/sample_rate)\n\n    # Create a plot\n    plt.figure(figsize=(12, 6))\n    plt.plot(freqs[:N // 2], np.abs(fft_result)[:N // 2])  # Plotting only positive frequencies\n    plt.title('FFT of the Signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Magnitude')\n    plt.grid()\n    plt.xlim(0, sample_rate / 2)  # Limit x-axis to half the sample rate\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()\n\n    # Return the FFT result and the Axes object\n    return fft_result, plt.gca()\ndata = {'x': 3, 'y': 5, 'z': 7}"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE_CONTENT_TYPE = {\n    'status': 'error',\n    'message': 'Content-Type header is not application/json'\n}\nERROR_RESPONSE_NO_DATA_KEY = {\n    'status': 'error',\n    'message': 'No data key in request'\n}\nERROR_RESPONSE_INVALID_JSON = {\n    'status': 'error',\n    'message': 'Invalid JSON'\n}\nclass SimpleHTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers.get('Content-Type')\n        \n        # Check if Content-Type is application/json\n        if content_type != 'application/json':\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', str(len(json.dumps(ERROR_RESPONSE_CONTENT_TYPE))))\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE_CONTENT_TYPE).encode('utf-8'))\n            return\n        \n        # Read and decode the body\n        content_length = int(self.headers.get('Content-Length', 0))\n        body = self.rfile.read(content_length)\n        \n        # Try to parse the JSON data\n        try:\n            data = json.loads(body)\n        except json.JSONDecodeError:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', str(len(json.dumps(ERROR_RESPONSE_INVALID_JSON))))\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE_INVALID_JSON).encode('utf-8'))\n            return\n        \n        # Check for 'data' key in JSON data\n        if 'data' not in data:\n            self.send_response(400)\n            self.send_header('Content-Type', 'application/json')\n            self.send_header('Content-Length', str(len(json.dumps(ERROR_RESPONSE_NO_DATA_KEY))))\n            self.end_headers()\n            self.wfile.write(json.dumps(ERROR_RESPONSE_NO_DATA_KEY).encode('utf-8'))\n            return\n        \n        # If everything is valid, send success response\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.send_header('Content-Length', str(len(json.dumps(SUCCESS_RESPONSE))))\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\ndef task_func():\n    server_class = http.server.HTTPServer\n    handler_class = SimpleHTTPRequestHandler\n    port = 8000\n    server_address = ('', port)\n    httpd = server_class(server_address, handler_class)\n    print(f'Starting httpd server on port {port}...')\n    httpd.serve_forever()"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Set the response code to 200 initially\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n\n            try:\n                # Parse the incoming JSON data\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n                email_data = json.loads(post_data)\n\n                # Validate required fields\n                if 'subject' not in email_data or 'message' not in email_data or 'to' not in email_data:\n                    raise ValueError(\"Missing 'subject', 'message', or 'to' key\")\n\n                subject = email_data['subject']\n                message = email_data['message']\n                to_email = email_data['to']\n\n                # Create the email message\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_email\n\n                # Send the email\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()  # Upgrade the connection to a secure encrypted SSL/TLS connection\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, [to_email], msg.as_string())\n\n                response = {'status': 'success', 'message': 'Email sent successfully'}\n\n            except json.JSONDecodeError:\n                self.send_response(400)\n                response = {'error': 'Invalid JSON data'}\n            except ValueError as ve:\n                self.send_response(400)\n                response = {'error': str(ve)}\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                response = {'error': 'Authentication failed'}\n            except Exception:\n                self.send_response(500)\n                response = {'error': 'Internal server error'}\n\n            # End headers and send response\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(response).encode('utf-8'))\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    total_words = 0\n    word_counts = {}\n\n    try:\n        # Iterate through all files in the specified directory\n        for file in os.listdir(directory):\n            if file.endswith('.txt'):\n                file_path = os.path.join(directory, file)\n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        # Read the content of the file line by line\n                        for line in f:\n                            words = line.split()\n                            word_count = len(words)\n                            total_words += word_count\n                            # Update the word count for the file\n                            word_counts[file] = word_counts.get(file, 0) + word_count\n                except IOError:\n                    print(f\"Error reading file {file_path}. Skipping this file.\")\n\n        # Export the word counts to a JSON file\n        with open(filename, 'w', encoding='utf-8') as json_file:\n            json.dump(word_counts, json_file)\n\n    except FileNotFoundError:\n        print(f\"Directory {directory} does not exist.\")\n    except IOError:\n        print(f\"Error writing to file {filename}.\")\n\n    # Return the total number of words\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, plot=False):\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if 'Value' column is present and contains valid non-empty lists\n    if 'Value' not in df.columns or not all(isinstance(x, list) and len(x) > 0 for x in df['Value']):\n        raise ValueError(\"The 'Value' column must contain non-empty lists.\")\n\n    # Split the lists in the 'Value' column into separate columns\n    value_df = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Check if the resulting DataFrame has at least two columns to calculate correlation\n    if value_df.shape[1] < 2:\n        raise ValueError(\"There must be at least two lists in the 'Value' column to calculate correlation.\")\n\n    # Calculate the Pearson correlation matrix\n    correlation_matrix = value_df.corr(method='pearson')\n\n    # Optional: Visualize the correlation matrix using a heatmap\n    axes = None\n    if plot:\n        plt.figure(figsize=(10, 8))\n        axes = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n    \n    return correlation_matrix, axes"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with additional fields if any\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {field: [random.randint(0, 100) for _ in range(len(STUDENTS))] for field in all_fields}\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # Calculate the average grade for each student and add it to the DataFrame\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject and add it as a new row\n    df.loc['Average'] = df.mean(axis=0)\n    \n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename: str) -> str:\n    \"\"\"Generates a CSV file of simulated data for 100 people\n    \n    Args:\n        filename (str): The name of the CSV file to create.\n\n    Returns:\n        str: The path of the created CSV file.\n    \"\"\"\n    # Generate simulated data\n    people_data = []\n    names = [f'Person_{i+1}' for i in range(PEOPLE_COUNT)]\n    \n    for name in names:\n        age = random.randint(18, 65)  # Age between 18 and 65\n        height = random.uniform(150, 200)  # Height in cm between 150 and 200\n        weight = random.uniform(50, 100)  # Weight in kg between 50 and 100\n        people_data.append([name, age, height, weight])\n    \n    # Calculate averages\n    ages = [row[1] for row in people_data]\n    heights = [row[2] for row in people_data]\n    weights = [row[3] for row in people_data]\n    \n    average_age = mean(ages)\n    average_height = mean(heights)\n    average_weight = mean(weights)\n    \n    # Append average data\n    people_data.append(['Average', average_age, average_height, average_weight])\n    \n    # Write to CSV\n    try:\n        with open(filename, mode='w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow(COLUMNS)  # Write header\n            writer.writerows(people_data)  # Write data rows\n    except Exception as e:\n        print(f\"Error writing to file: {e}\")\n    \n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Ensure the directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(\"The provided path is not a valid directory.\")\n    \n    # Dictionary to hold the results\n    organized_files = {}\n    \n    # Loop through each file in the directory\n    for filename in os.listdir(directory):\n        # Create the full path to the file\n        file_path = os.path.join(directory, filename)\n        \n        # Skip directories\n        if os.path.isdir(file_path):\n            continue\n        \n        # Attempt to find the first text not enclosed in square brackets\n        match = re.match(r'^[^\\[\\]\\s]+', filename)  # Match text before any brackets or spaces\n        if match:\n            # Extract the text and strip any leading or trailing whitespace\n            subdir_name = match.group(0).strip()\n            subdir_path = os.path.join(directory, subdir_name)\n            \n            # Create the subdirectory if it doesn't exist\n            os.makedirs(subdir_path, exist_ok=True)\n            \n            # Move the file into the appropriate subdirectory\n            shutil.move(file_path, os.path.join(subdir_path, filename))\n            \n            # Add the filename to the corresponding subdirectory in the organized_files dictionary\n            if subdir_name not in organized_files:\n                organized_files[subdir_name] = []\n            organized_files[subdir_name].append(filename)\n    \n    return directory, organized_files"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport threading\nfrom queue import Queue\ndef task_func(file_list):\n    exit_codes = Queue()  # Use Queue to store exit codes safely\n    threads = []\n\n    def run_file(file):\n        try:\n            result = subprocess.run(['python', file], check=True)\n            exit_codes.put(result.returncode)\n        except subprocess.CalledProcessError as e:\n            exit_codes.put(e.returncode)\n        except Exception:\n            exit_codes.put(-1)  # Indicate an error with a special exit code\n\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return list(exit_codes.queue)  # Convert Queue to list before returning"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in the specified directory.\n\n    Args:\n        directory_path (str): The path to the directory containing .bat files.\n\n    Returns:\n        list: A list of tuples where each tuple contains the file name and its exit code.\n              The exit code is None if the file could not be executed.\n    \"\"\"\n    results = []\n    \n    # Check if the provided path is a directory\n    if not os.path.isdir(directory_path):\n        raise ValueError(\"Provided path is not a valid directory.\")\n    \n    # Find all .bat files in the specified directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    \n    for bat_file in bat_files:\n        try:\n            # Run the .bat file and capture the exit code\n            result = subprocess.run(bat_file, shell=True, check=True)\n            exit_code = result.returncode\n        except subprocess.CalledProcessError as e:\n            exit_code = e.returncode\n        except Exception:\n            exit_code = None\n        \n        # Append the file name and exit code to results\n        results.append((os.path.basename(bat_file), exit_code))\n    \n    return results"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    \"\"\"\n    Generate a histogram with a kernel density estimate and a box plot \n    for the specified column in the given DataFrame.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame containing the data.\n    col (str): The name of the column to plot.\n\n    Returns:\n    matplotlib.figure.Figure: A matplotlib figure object containing the histogram and box plot.\n\n    Raises:\n    ValueError: If df is not a DataFrame, is empty, or if col is not a string or does not exist in df.\n    \"\"\"\n    # Validate the input\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    if not isinstance(col, str):\n        raise ValueError(\"The specified column name must be a string.\")\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' must be in the DataFrame.\")\n    if not pd.api.types.is_numeric_dtype(df[col]):\n        raise ValueError(f\"The specified column '{col}' must contain numeric data.\")\n\n    # Create a matplotlib figure with two subplots\n    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n\n    # First subplot: Histogram with kernel density estimate\n    sns.histplot(df[col], kde=True, ax=axs[0], bins=30)\n    axs[0].set_title(f\"Histogram and Kernel Density Estimate of '{col}'\")\n    axs[0].set_xlabel(col)\n    axs[0].set_ylabel(\"Frequency\")\n\n    # Second subplot: Box plot\n    sns.boxplot(x=df[col], ax=axs[1])\n    axs[1].set_title(f\"Box Plot of '{col}'\")\n    axs[1].set_xlabel(col)\n\n    plt.tight_layout()\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"The script '{script_path}' does not exist.\")\n    \n    # Prepare the command to run the script\n    command = [sys.executable, script_path] + list(args)\n    \n    try:\n        # Run the subprocess\n        process = subprocess.run(command, check=True)\n        \n        # If wait is True, return the return code\n        if wait:\n            return process.returncode\n        else:\n            return None\n            \n    except subprocess.CalledProcessError as e:\n        # Raise an exception if the script fails\n        raise subprocess.CalledProcessError(e.returncode, e.cmd) from e"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Load data from an Excel spreadsheet, calculate the mean and standard deviation of each column,\n    and draw a bar chart. \n\n    Parameters:\n    - file_location (str): Path to the Excel file.\n    - sheet_name (str): Name of the sheet to read.\n\n    Returns:\n    - dict: A dictionary with mean and standard deviation of each column.\n    - matplotlib.figure.Figure: The figure object containing the bar chart.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(f\"The specified sheet '{sheet_name}' does not exist in the workbook.\") from e\n\n    # Filter for numeric columns only\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    # Calculate mean and standard deviation of each numeric column\n    mean_std_dict = {\n        \"mean\": numeric_df.mean(),\n        \"std\": numeric_df.std()\n    }\n\n    # Prepare data for the bar chart\n    means = mean_std_dict[\"mean\"]\n    stds = mean_std_dict[\"std\"]\n    x_labels = means.index\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    bar_width = 0.35\n    index = np.arange(len(x_labels))\n\n    # Bar positions\n    bars1 = ax.bar(index, means, bar_width, label='Mean', color='b')\n    bars2 = ax.bar(index + bar_width, stds, bar_width, label='Standard Deviation', color='r')\n\n    # Adding labels and title\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(x_labels)\n    ax.legend()\n    ax.grid(axis='y')  # Adding gridlines for better readability\n\n    return mean_std_dict, fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    \"\"\"\n    Generates a bar chart of the number of activities performed on each day of the week.\n\n    Parameters:\n    activities (list): A list of datetime objects representing activities.\n\n    Returns:\n    matplotlib.axes.Axes: Axes object representing the bar chart.\n    \n    Raises:\n    TypeError: If any of the activities are not datetime objects.\n    \"\"\"\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n    \n    # Create a dictionary to count activities per day of the week\n    activity_count = defaultdict(int)\n    \n    # Count activities for each day of the week\n    for activity in activities:\n        # Get the day of the week (0=Monday, 6=Sunday)\n        day_of_week = activity.strftime('%A')  # Get the full name of the day\n        activity_count[day_of_week] += 1\n    \n    # Prepare data for the bar chart\n    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    counts = [activity_count[day] for day in days]\n\n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts)\n    \n    # Set the labels and title\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax  # Return the Axes object"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the specified destination directory.\n    \n    Parameters:\n        src_dir (str): The source directory to move files from.\n        dest_dir (str): The destination directory to move files to.\n        seed (int): The seed for random number generation (default is 100).\n    \n    Returns:\n        str: The name of the file moved, formatted as 'filename.extension'.\n    \n    Raises:\n        FileNotFoundError: If the source or destination directory does not exist.\n        ValueError: If no files are found in the source directory.\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir, exist_ok=True)  # Create destination directory if it doesn't exist\n\n    # Set the seed for reproducibility\n    random.seed(seed)\n    \n    # Get a list of all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files to move\n    if not files:\n        raise ValueError(\"No files found in the source directory.\")\n    \n    # Select a random file\n    random_file = random.choice(files)\n    \n    # Define the full path for the source and destination\n    src_file_path = os.path.join(src_dir, random_file)\n    dest_file_path = os.path.join(dest_dir, random_file)\n    \n    # Move the file\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the moved file\n    return random_file"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Processes all Excel (.xlsx) files in the specified directory by prefixing\n    all double quotes with a double backslash. Returns the count of processed\n    files.\n\n    :param directory_path: Path to the directory containing .xlsx files\n    :return: int - Number of Excel files processed\n    \"\"\"\n    processed_count = 0\n    \n    # Get all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    for file_path in xlsx_files:\n        try:\n            processed_count += 1  # Increment the count for each file processed\n            \n            # Load the workbook\n            workbook = load_workbook(file_path)\n            \n            # Iterate through all sheets in the workbook\n            for sheet in workbook.worksheets:\n                # Iterate through all rows and columns in the sheet\n                for row in sheet.iter_rows():\n                    for cell in row:\n                        if isinstance(cell.value, str):  # Check if cell value is a string\n                            # Replace double quotes with double backslash and double quotes\n                            cell.value = re.sub(r'\"', r'\\\\\"', cell.value)\n            \n            # Save the modified workbook back to the same file\n            workbook.save(file_path)\n        \n        except Exception as e:\n            print(f\"Error processing file {file_path}: {e}\")\n    \n    return processed_count"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Generate a sine wave and cosine wave plot.\n\n    Parameters:\n    frequency (float): Frequency of the sine and cosine waves (must be non-negative).\n    sample_size (int): Number of sample points (must be positive).\n\n    Returns:\n    tuple: A tuple containing:\n        - matplotlib.figure.Figure: The figure object containing the plot.\n        - matplotlib.axes.Axes: The axes object of the plot.\n    \n    Raises:\n    ValueError: If the frequency is negative.\n    ValueError: If the sample size is non-positive.\n    \"\"\"\n    # Raise exceptions for invalid input\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n    \n    # Generate time values\n    t = np.linspace(0, 2 * math.pi, sample_size)\n    \n    # Calculate sine and cosine values\n    sine_wave = np.sin(frequency * t)\n    cosine_wave = np.cos(frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave', color='blue')\n    ax.plot(t, cosine_wave, label='Cosine Wave', color='red')\n    \n    # Customize the plot\n    ax.set_title('Sine and Cosine Waves')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    ax.grid(True)\n    \n    # Return the figure and axes objects\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    # Create a Flask application instance\n    app = Flask(app_name)\n\n    # Email configuration with defaults\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    mail_port = int(os.environ.get('MAIL_PORT', 25))\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    mail_username = os.environ.get('MAIL_USERNAME')  # Default is None\n    mail_password = os.environ.get('MAIL_PASSWORD')  # Default is None\n\n    # Set the configuration for Flask-Mail\n    app.config['MAIL_SERVER'] = mail_server\n    app.config['MAIL_PORT'] = mail_port\n    app.config['MAIL_USE_TLS'] = mail_use_tls\n    app.config['MAIL_USERNAME'] = mail_username\n    app.config['MAIL_PASSWORD'] = mail_password\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Return the Flask-Mail instance and the configuration used\n    mail_config = {\n        'MAIL_SERVER': mail_server,\n        'MAIL_PORT': mail_port,\n        'MAIL_USE_TLS': mail_use_tls,\n        'MAIL_USERNAME': mail_username,\n        'MAIL_PASSWORD': mail_password\n    }\n\n    return mail, mail_config"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full file path\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.isfile(full_path):\n        raise FileNotFoundError(f\"The file '{file_name}' does not exist at the specified path '{excel_file_path}'.\")\n    \n    # Read the Excel file\n    try:\n        data = pd.read_excel(full_path)\n    except Exception as e:\n        raise Exception(f\"An error occurred while reading the Excel file: {e}\")\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in data.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not found in the Excel file.\")\n    \n    # Calculate statistics\n    column_data = data[column_name].dropna()  # Drop NaN values for calculation\n    mean = np.mean(column_data)\n    median = np.median(column_data)\n    std_dev = np.std(column_data, ddof=0)  # Population standard deviation\n    \n    # Return results as a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'standard_deviation': std_dev\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Ensure the input dimension is 2\n    if X.shape[1] != 2:\n        raise ValueError(\"Input dimension must be 2.\")\n\n    # Split the data into a training set (75%) and a test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct a Sequential model with one dense hidden layer and a sigmoid activation function\n    model = Sequential()\n    model.add(Dense(5, input_dim=2, activation='sigmoid'))  # Hidden layer with 5 neurons\n    model.add(Dense(1, activation='sigmoid'))  # Output layer\n    \n    # Compile the model using binary cross-entropy loss and SGD optimizer\n    optimizer = SGD(lr=0.01)  # Set learning rate to 0.01\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fit the model to the training data, also evaluating it on the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plot the model's training and validation loss over epochs\n    plt.figure(figsize=(10, 6))\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Return the trained Keras Sequential model and the Axes object of the plot\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Validate input shapes\n    assert X.shape[0] == Y.shape[0], \"Features and labels must have the same number of samples.\"\n    assert len(set(Y)) == 2, \"Labels must be binary.\"\n\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(10, activation='sigmoid', input_shape=(X_train.shape[1],)))  # Hidden layer\n    model.add(keras.layers.Dense(1, activation='sigmoid'))  # Output layer\n\n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n\n    # Fit the model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_probs = model.predict(X_test).ravel()\n\n    # Compute ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_probs)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc='lower right')\n    plt.grid()\n\n    # Show the plot\n    plt.show()\n\n    return model, plt.gca()  # Return the trained model and the current Axes object"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Segments an RGB image into n_clusters regions using K-means clustering.\n\n    Parameters:\n        image_path (str): Path to the input image file.\n        n_clusters (int): Number of clusters for segmentation (must be a positive integer).\n        random_seed (int): Random seed for reproducibility.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays:\n            - Original RGB image.\n            - Segmented image with colors replaced by cluster centroids.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist at the specified path.\n        ValueError: If n_clusters is not a positive integer.\n    \"\"\"\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file at {image_path} does not exist.\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    # Read the image\n    original_image = cv2.imread(image_path)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n\n    # Reshape the image to a 2D array of pixels\n    pixel_values = original_image.reshape((-1, 3))\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n    \n    # Get the cluster centers (centroids)\n    centroids = kmeans.cluster_centers_.astype(int)\n    \n    # Predict the cluster for each pixel\n    labels = kmeans.predict(pixel_values)\n    \n    # Create the segmented image\n    segmented_image = centroids[labels].reshape(original_image.shape)\n    \n    # Save each region as a separate image\n    for i in range(n_clusters):\n        mask = (labels == i)\n        region = np.zeros_like(original_image)\n        region[mask] = centroids[i]\n        cv2.imwrite(f'region_{i + 1}.png', cv2.cvtColor(region, cv2.COLOR_RGB2BGR))\n\n    return original_image, segmented_image.astype(np.uint8)"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Ensure P and T are compatible for multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"The number of columns in P must match the number of rows in T.\")\n    \n    # Calculate the product of P and T\n    result = np.tensordot(P, T, axes=(1, 0))  # Matrix multiplication\n\n    # Flatten the result to 2D for KMeans\n    flattened_result = result.reshape(-1, result.shape[-1])\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_result)\n\n    # Visualization\n    ax = plt.figure().add_subplot(111)\n    \n    # Check if the flattened result has at least 2 dimensions for plotting\n    if flattened_result.shape[1] >= 2:\n        plt.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis', alpha=0.5)\n        plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')  # Mark cluster centers\n        plt.title('KMeans Clustering Visualization')\n        plt.xlabel('Feature 1')\n        plt.ylabel('Feature 2')\n    else:\n        plt.scatter(np.arange(flattened_result.shape[0]), flattened_result, c=cluster_result, cmap='viridis', alpha=0.5)\n        plt.title('KMeans Clustering Visualization (1D)')\n        plt.xlabel('Index')\n        plt.ylabel('Value')\n\n    return cluster_result, ax\nP = np.random.rand(4, 3)\nT = np.random.rand(3, 5, 2)"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    # Validate input\n    if not isinstance(points, (np.ndarray, list)):\n        raise ValueError(\"Input points must be a numpy array or a list.\")\n    \n    points = np.asarray(points)\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points must be a 2D array with shape (n, 2).\")\n    \n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Apply jittering to the points\n    jitter = np.random.normal(0, 0.01, points.shape)\n    jittered_points = points + jitter\n    \n    # Calculate Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plotting the Voronoi diagram\n    fig, ax = plt.subplots(figsize=(8, 8))\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, point_size=10)\n    \n    # Plot the original points\n    ax.plot(points[:, 0], points[:, 1], 'o', color='blue', markersize=10, label='Original Points')\n    ax.plot(jittered_points[:, 0], jittered_points[:, 1], 'o', color='red', markersize=5, label='Jittered Points')\n    \n    ax.legend()\n    ax.set_title('Voronoi Diagram with Jittered Points')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_xlim(np.min(points[:, 0]) - 1, np.max(points[:, 0]) + 1)\n    ax.set_ylim(np.min(points[:, 1]) - 1, np.max(points[:, 1]) + 1)\n    plt.grid()\n    \n    plt.show()\n    \n    return (vor, ax)"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Validate inputs\n    if not isinstance(src_dir, str) or not isinstance(dest_dir, str) or not isinstance(ext, str):\n        raise ValueError(\"Source directory, destination directory, and extension must be strings.\")\n    \n    if not ext:\n        raise ValueError(\"Extension cannot be an empty string.\")\n    \n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"The source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"The destination directory '{dest_dir}' does not exist.\")\n    \n    # Normalize the extension to lower case\n    ext = ext.lower()\n    \n    # Prepare the pattern for matching files with the specified extension\n    pattern = os.path.join(src_dir, f'*.{ext}')\n    \n    # List to hold paths of successfully moved files\n    moved_files = []\n    \n    # Find all files with the specified extension in the source directory\n    for src_file in glob.glob(pattern):\n        # Get the name of the file\n        file_name = os.path.basename(src_file)\n        dest_file = os.path.join(dest_dir, file_name)\n        \n        # Check if a file with the same name already exists in the destination directory\n        if not os.path.exists(dest_file):\n            # Move the file\n            shutil.move(src_file, dest_file)\n            # Add the full path of the moved file to the list\n            moved_files.append(dest_file)\n    \n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    # Check if the input JSON string is empty\n    if not json_str.strip():\n        return pd.DataFrame()  # Return an empty DataFrame if the input is empty\n    \n    # Load the JSON string into a dictionary\n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()  # Return an empty DataFrame if JSON is invalid\n    \n    # Function to double numerical values\n    def double_values(value):\n        if isinstance(value, (int, float)):\n            return float(value) * 2  # Ensure all numerical values are stored as floats\n        elif isinstance(value, list):\n            return [double_values(item) for item in value]  # Process each item in the list\n        elif isinstance(value, str):\n            # Attempt to convert the string to a float\n            match = re.match(r\"^-?\\d+(\\.\\d+)?$\", value)\n            if match:\n                return float(value) * 2  # Double the numeric string\n            return value  # Return the string as-is if it can't be converted\n        return value  # Return the value as-is if it is not a number or a list\n    \n    # Normalize the dictionary by doubling numerical values\n    normalized_dict = {key: double_values(value) for key, value in data_dict.items()}\n    \n    # Create a Pandas DataFrame from the normalized dictionary\n    return pd.DataFrame([normalized_dict])  # Wrap the dict in a list to create a DataFrame"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    Executes a given bash script and monitors its CPU and memory usage.\n\n    Args:\n        script_path (str): The path to the bash script to execute.\n        timeout (int): The maximum time in seconds to allow the script to run.\n\n    Returns:\n        dict: A dictionary containing:\n            'CPU Usage': The accumulated CPU usage in percentage.\n            'Memory Usage': The accumulated memory usage in bytes.\n    \"\"\"\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} does not exist.\")\n    \n    # Start the subprocess\n    process = subprocess.Popen(['bash', script_path])\n    pid = process.pid\n    process_psutil = psutil.Process(pid)\n\n    # Initialize metrics\n    total_cpu_usage = 0.0\n    total_memory_usage = 0\n    \n    start_time = time.time()\n    \n    try:\n        while True:\n            if process_psutil.is_running():\n                # Accumulate CPU usage\n                cpu_usage = process_psutil.cpu_percent(interval=0.1)\n                total_cpu_usage += cpu_usage\n                \n                # Monitor memory usage and accumulate\n                memory_info = process_psutil.memory_info()\n                total_memory_usage += memory_info.rss\n\n                # Check for timeout\n                if time.time() - start_time > timeout:\n                    process.terminate()\n                    break\n            else:\n                break  # Process has finished\n\n    except psutil.NoSuchProcess:\n        print(\"The process is no longer available.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        # Ensure the process is cleaned up\n        if process_psutil.is_running():\n            process_psutil.terminate()\n        \n    # Return the accumulated CPU and memory usage\n    return {\n        'CPU Usage': total_cpu_usage,\n        'Memory Usage': total_memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    Creates a DataFrame with random values and generates a scatter plot.\n\n    Parameters:\n    N (int): Number of rows in the DataFrame.\n    CATEGORIES (list): List of categories to sample from.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    tuple: A tuple containing the generated DataFrame and Axes object of the scatter plot.\n    \"\"\"\n    # Check for valid input\n    if N < 0:\n        raise ValueError(\"Number of rows (N) must be non-negative.\")\n    if not CATEGORIES:\n        raise ValueError(\"CATEGORIES list must not be empty.\")\n\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random values for \"x\" and \"y\"\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Assign categories ensuring each appears at least once if N >= len(CATEGORIES)\n    if N >= len(CATEGORIES):\n        categories = np.random.choice(CATEGORIES, N, replace=True)\n        # Shuffle to ensure randomness and replace the first len(CATEGORIES) entries with unique categories\n        np.random.shuffle(categories)\n        categories[:len(CATEGORIES)] = np.random.choice(CATEGORIES, len(CATEGORIES), replace=False)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create the scatter plot\n    fig, ax = plt.subplots()\n    for category in df['category'].unique():\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot of x vs y colored by category')\n    ax.legend(title='Categories')\n    ax.grid(True)  # Add grid lines for better readability\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the DataFrame and Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generates a time series from epoch start time to end time with specified step and trend.\n    \n    Parameters:\n        start_time (int): The starting epoch time (in seconds).\n        end_time (int): The ending epoch time (in seconds).\n        step (str): The frequency string for date range (e.g., 'H' for hourly).\n        trend (float): The slope of the linear trend to be added to the generated values.\n        seed (int): Seed for random number generation for reproducibility.\n    \n    Returns:\n        ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Convert epoch times to datetime objects\n    start = datetime.fromtimestamp(start_time)\n    end = datetime.fromtimestamp(end_time)\n\n    # Generate date range with specified step\n    date_range = pd.date_range(start=start, end=end, freq=step)\n\n    # Generate values from a normal distribution and add a linear trend\n    num_points = len(date_range)\n    values = np.random.normal(loc=0, scale=1, size=num_points) + np.arange(num_points) * trend\n\n    # Create a DataFrame\n    time_series = pd.DataFrame({'Time': date_range, 'Value': values})\n\n    # Plotting the time series\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(time_series['Time'], time_series['Value'], marker='o', linestyle='-')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Trend')\n\n    # Rotate x-axis labels for better visibility\n    plt.xticks(rotation=45)\n    plt.tight_layout()  # Adjust layout to make room for rotated labels\n\n    plt.show()  # Display the plot\n\n    return ax"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for five products from a given epoch time to the current time.\n\n    Parameters:\n        epoch_milliseconds (int or float): The starting point in time in milliseconds since epoch.\n        random_seed (int): A seed for random number generation to ensure reproducibility.\n        products (list): A list of product names for which sales data will be generated.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing sales data with columns 'Product', 'Date', and 'Sales'.\n    \"\"\"\n    # Check the validity of epoch_milliseconds\n    if not isinstance(epoch_milliseconds, (int, float)) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer or float.\")\n\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    end_date = datetime.now()\n\n    # Generate a date range from start_date to end_date\n    date_range = pd.date_range(start=start_date, end=end_date)\n\n    # Create an empty list to store sales data\n    sales_data = []\n\n    # Generate random sales data for each product for each date\n    for date in date_range:\n        for product in products:\n            sales_quantity = random.randint(10, 50)  # Random sales quantity\n            sales_data.append({\"Product\": product, \"Date\": date, \"Sales\": sales_quantity})\n\n    # Create a DataFrame from the sales data\n    sales_df = pd.DataFrame(sales_data)\n\n    return sales_df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if the input is a string, bytes, or bytearray\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"Input json_str must be a string, bytes, or bytearray.\")\n\n    # Attempt to parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Input json_str is not valid JSON.\") from e\n\n    # If data is an empty array, create an empty DataFrame\n    if isinstance(data, list) and len(data) == 0:\n        df = pd.DataFrame()  # empty DataFrame\n    elif isinstance(data, list):\n        df = pd.DataFrame(data)\n    else:\n        raise ValueError(\"The JSON data must be an array (list) of objects.\")\n\n    # Create an Excel writer object and save the DataFrame to it\n    try:\n        # Define the absolute path for the output file\n        abs_path = os.path.abspath(filename)\n        with pd.ExcelWriter(abs_path, engine='xlwt') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n        \n        return abs_path\n    except Exception as e:\n        raise Exception(f\"An error occurred while writing the Excel file: {str(e)}\")"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates a DataFrame of daily activity durations for a specified number of days in the past\n    along with a plot of these durations over time.\n    \n    Parameters:\n    - days_in_past (int): The number of days in the past to generate data for (default is 7).\n    - random_seed (int): Random seed for reproducibility (default is 0).\n    \n    Returns:\n    - Tuple containing:\n        ax (matplotlib.pyplot.Axes): Axes object of the generated plot.\n        df (pd.DataFrame): DataFrame containing the generated activity data.\n    \"\"\"\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates for the specified number of days in the past\n    dates = [(datetime.now() - timedelta(days=i)).date() for i in range(days_in_past)]\n    \n    # Create a list to hold the generated data\n    data = []\n    \n    # Generate random durations for each activity on each date\n    for date in dates:\n        for activity in activities:\n            duration = random.randint(0, 120)  # Duration between 0 and 120 minutes\n            data.append({'Date': date, 'Activity': activity, 'Duration': duration})\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data)\n    \n    # Create the plot\n    plt.figure(figsize=(12, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    plt.title('Daily Activity Durations')\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generates a DataFrame of random stock prices for a specified number of days in the past.\n\n    Parameters:\n    days_in_past (int): Number of days in the past to generate stock prices for.\n    stock_names (list): List of stock names to include in the DataFrame.\n    random_seed (int): Seed for the random number generator for reproducibility.\n\n    Returns:\n    pd.DataFrame: DataFrame containing random stock prices.\n    \"\"\"\n    if not isinstance(days_in_past, int) or days_in_past < 0:\n        raise ValueError(\"days_in_past must be a non-negative integer.\")\n    if not isinstance(stock_names, list) or not all(isinstance(name, str) for name in stock_names):\n        raise ValueError(\"stock_names must be a list of strings.\")\n\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate the date range for the specified number of days in the past\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Generate random stock prices in the range [0.0, 1.0)\n    stock_prices = np.random.rand(len(date_range), len(stock_names))\n    \n    # Create a DataFrame with the stock prices\n    df = pd.DataFrame(stock_prices, index=date_range, columns=stock_names)\n    \n    return df"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nimport os\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files line by line and create a difference report.\n\n    Parameters:\n    - file_path1: Path to the first CSV file.\n    - file_path2: Path to the second CSV file.\n    - delimiter: Character used to separate values in the CSV files (default: ',').\n    - quotechar: Character used to quote fields containing special characters (default: '\").\n\n    Returns:\n    - A pandas DataFrame containing the differences.\n    \"\"\"\n    try:\n        # Check if files exist\n        if not os.path.isfile(file_path1):\n            raise FileNotFoundError(f\"File not found: {file_path1}\")\n        if not os.path.isfile(file_path2):\n            raise FileNotFoundError(f\"File not found: {file_path2}\")\n\n        # Read the CSV files\n        lines1 = pd.read_csv(file_path1, delimiter=delimiter, quotechar=quotechar, header=None).values.tolist()\n        lines2 = pd.read_csv(file_path2, delimiter=delimiter, quotechar=quotechar, header=None).values.tolist()\n\n        # Check for empty files\n        if not lines1:\n            raise ValueError(f\"File is empty: {file_path1}\")\n        if not lines2:\n            raise ValueError(f\"File is empty: {file_path2}\")\n\n        # Prepare the difference report\n        differences = []\n        max_len = max(len(lines1), len(lines2))\n\n        for i in range(max_len):\n            line1 = lines1[i] if i < len(lines1) else None\n            line2 = lines2[i] if i < len(lines2) else None\n            \n            if line1 is None:\n                differences.append({'Line Number': i + 1, 'Status': '+', 'Content': line2})\n            elif line2 is None:\n                differences.append({'Line Number': i + 1, 'Status': '-', 'Content': line1})\n            elif line1 == line2:\n                differences.append({'Line Number': i + 1, 'Status': ' ', 'Content': line1})\n            else:\n                differences.append({'Line Number': i + 1, 'Status': '!', 'Content': f\"File1: {line1}, File2: {line2}\"})\n\n        # Create a DataFrame from the differences\n        df_diff = pd.DataFrame(differences)\n\n        return df_diff\n\n    except FileNotFoundError as fnf_error:\n        print(fnf_error)\n        raise\n    except ValueError as ve:\n        print(ve)\n        raise\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"Analyzes a list of employee data and calculates statistics for a given column.\n    \n    Args:\n        column (str): The column name for which to calculate statistics.\n        data (list): A list of dictionaries containing employee data.\n    \n    Returns:\n        tuple: A tuple containing:\n            dict: A dictionary with 'sum', 'mean', 'min', and 'max' of the column.\n            Axes object: The pie chart visualizing the column data.\n    \"\"\"\n    # Convert data to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the input data is empty\n    if df.empty:\n        result = {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n        plt.figure(figsize=(8, 6))\n        plt.pie([], labels=[], autopct='%1.1f%%', startangle=90)  # Empty pie chart\n        plt.title('No Data Available')\n        plt.axis('equal')\n        return result, plt.gca()\n\n    # Check if the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' does not exist in the data.\")\n\n    # Calculate statistics\n    result = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Create a pie chart using the 'Age' column as labels\n    plt.figure(figsize=(8, 6))\n    plt.pie(df['Age'], labels=df['Age'].astype(str), autopct='%1.1f%%', startangle=90)\n    plt.title('Age Distribution')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie chart is circular.\n\n    # Show the plot\n    plt.show()\n\n    return result, plt.gca()  # Return the result and the current Axes object"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyzes a specified column of fitness data and generates a line chart.\n    \n    Parameters:\n    - column (str): The name of the column to analyze (e.g., 'steps', 'calories_burned', 'distance_walked').\n    - data (list): A list of dictionaries containing fitness data.\n    \n    Returns:\n    - tuple: A tuple containing:\n        - dict: A dictionary with the sum, mean, min, max of the column.\n        - matplotlib.axes.Axes: The Axes object of the plotted line chart.\n    \"\"\"\n    # Validate the input data\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n\n    # Convert the data list into a DataFrame\n    df = pd.DataFrame(data)\n\n    # Check if the specified column exists in the DataFrame\n    if column not in df.columns:\n        raise KeyError(f\"The specified column '{column}' is not valid.\")\n\n    # Validate that steps, calories burned, and distance walked are non-negative and numeric\n    if not pd.api.types.is_numeric_dtype(df['steps']) or not pd.api.types.is_numeric_dtype(df['calories_burned']) or not pd.api.types.is_numeric_dtype(df['distance_walked']):\n        raise ValueError(\"Steps, calories burned, and distance walked must be numeric.\")\n    \n    if (df['steps'] < 0).any() or (df['calories_burned'] < 0).any() or (df['distance_walked'] < 0).any():\n        raise ValueError(\"Steps, calories burned, and distance walked must be non-negative.\")\n\n    # Parsing dates\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Calculate the required statistics for the specified column\n    column_data = df[column]\n    result = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    # Plotting the line chart\n    plt.figure(figsize=(10, 5))\n    plt.plot(df['date'], column_data, marker='o', color='blue', linestyle='-', label=column)\n    plt.title(f'Line Chart of {column}')\n    plt.xlabel('Date')\n    plt.ylabel(column)\n    plt.xticks(rotation=45)\n    plt.grid()\n    plt.tight_layout()\n    \n    # Get the current Axes object\n    ax = plt.gca()\n\n    # Show the plot\n    plt.show()\n\n    return result, ax\ndata = [\n    {'date': '2023-10-01', 'steps': 1000, 'calories_burned': 50, 'distance_walked': 0.5},\n    {'date': '2023-10-02', 'steps': 2000, 'calories_burned': 100, 'distance_walked': 1.0},\n    {'date': '2023-10-03', 'steps': 3000, 'calories_burned': 150, 'distance_walked': 1.5},\n]"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Attempt to read the JSON file\n    try:\n        with open(input_file, 'r') as file:\n            data = json.load(file)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        return f\"Error reading the file: {str(e)}\"\n\n    # Initialize a defaultdict to hold lists of numeric values for each key\n    numeric_data = defaultdict(list)\n\n    # Collect numeric values for each key, filtering out non-numeric and None values\n    for record in data:\n        for key, value in record.items():\n            if isinstance(value, (int, float)):\n                numeric_data[key].append(value)\n            elif isinstance(value, str):  # Handle numeric strings\n                try:\n                    numeric_value = float(value)\n                    numeric_data[key].append(numeric_value)\n                except ValueError:\n                    continue  # Ignore non-numeric strings\n\n    # Prepare a DataFrame to hold the results\n    results = {\n        'mean': {},\n        'median': {}\n    }\n\n    # Calculate mean and median for each key\n    for key, values in numeric_data.items():\n        if values:  # Ensure there are values to compute\n            results['mean'][key] = np.mean(values)\n            results['median'][key] = np.median(values)\n\n    # Create a DataFrame from the results\n    df = pd.DataFrame(results).T  # Transpose to have keys as index\n    df.index.name = 'Variable'\n    df = df.sort_index()  # Sort by index (variable names)\n\n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must be a CSV file with a .csv extension.\")\n\n    # Attempt to read the CSV file and handle exceptions\n    try:\n        # Read the CSV file using pandas\n        df = pd.read_csv(file_path)\n    except Exception as e:\n        raise ValueError(f\"Error reading the CSV file: {e}\")\n\n    # Count duplicate rows\n    duplicates = df[df.duplicated(keep=False)]\n    duplicate_counts = duplicates.value_counts().to_dict()\n\n    # Convert duplicates to a DataFrame\n    duplicates_df = pd.DataFrame(duplicate_counts.items(), columns=['Row', 'Count'])\n\n    # Plotting the duplicate rows count\n    fig, ax = plt.subplots()\n    duplicates_df.set_index('Row')['Count'].plot(kind='bar', ax=ax, color='skyblue')\n    ax.set_title('Duplicate Row Counts')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.tick_params(axis='x', rotation=45)\n    ax.grid(axis='y', linestyle='--')\n\n    plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n    plt.show()          # Show the plot\n\n    return duplicate_counts, ax"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Ensure age is non-negative and numeric\n    if not pd.api.types.is_numeric_dtype(df['age']) or (df['age'] < 0).any():\n        raise ValueError(\"Age must not be negative and should be numeric.\")\n    \n    # Round down the age to the nearest integer\n    df['age'] = np.floor(df['age']).astype(int)\n\n    # Identify duplicates based on 'name'\n    duplicate_names = df[df.duplicated('name', keep=False)]\n    \n    # If there are no duplicate names, return Counter and None for plot\n    if duplicate_names.empty:\n        return Counter(), None\n\n    # Get the age distribution for the duplicate names\n    age_distribution = duplicate_names['age']\n    \n    # Count occurrences of each age\n    age_counter = Counter(age_distribution)\n\n    # Plotting the histogram\n    plt.figure(figsize=(10, 6))\n    \n    # Create bins based on min and max ages\n    min_age = age_distribution.min()\n    max_age = age_distribution.max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)  # Bin edges\n    \n    # Create histogram\n    sns.histplot(age_distribution, bins=bins, kde=False, color='blue', discrete=True)\n    \n    plt.title('Age Distribution for Duplicate Names')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.xticks(np.arange(min_age, max_age + 1, 1))\n    plt.grid(axis='y')\n\n    # Show plot\n    plt.tight_layout()\n    \n    return age_counter, plt.gca()"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(df, bins=4):\n    # Check if 'value' column exists in the DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column\")\n\n    # Check if 'value' column contains numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must contain numeric data\")\n\n    # Count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates = {k: v for k, v in value_counts.items() if v > 1}\n\n    # Prepare data for histogram\n    data = df['value'].dropna()\n\n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(data, bins=bins, color='green', alpha=0.6, edgecolor='black', density=True)\n\n    # Fit a normal distribution to the data\n    mu, std = norm.fit(data)\n\n    # Plot the normal distribution curve\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    ax.plot(x, p, 'k', linewidth=2)\n\n    # Set titles and labels\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    # Show the plot\n    plt.show()\n\n    # Return the Counter and Axes object\n    return (duplicates, ax)\ndata = {'value': [1, 2, 2, 3, 4, 4, 4, 5, 6, 7]}\ndf = pd.DataFrame(data)"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b',\n    and plots it as a bar chart.\n\n    Parameters:\n    a (list): List for row indices.\n    b (list): List to determine number of columns.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    # Validate inputs\n    if len(a) == 0:\n        raise ValueError(\"List 'a' must not be empty.\")\n    if len(b) > len(COLUMNS):\n        raise ValueError(f\"Number of columns requested exceeds available columns: {len(COLUMNS)}\")\n    \n    # Create a DataFrame with random values\n    num_rows = len(a)\n    num_cols = len(b)\n    \n    # Create random data and assign it to the DataFrame\n    data = np.random.rand(num_rows, num_cols)\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_cols])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    plt.title(\"Bar Chart of Random Values\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Values\")\n    plt.xticks(rotation=45)\n    plt.legend(title=\"Columns\")\n    plt.tight_layout()  # Adjust layout to make room for labels\n\n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax\na = ['row1', 'row2', 'row3']\nb = ['A', 'B', 'C']"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"Plots a bar chart of monthly data values for a single year.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing 'month' and 'value' columns.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object representing the plot.\n\n    Raises:\n        ValueError: If the input data is not a DataFrame or does not contain \n                    the required columns or if the month format is invalid.\n    \"\"\"\n    # Ensure that 'data' is a pandas DataFrame with 'month' and 'value' columns\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if 'month' not in data.columns or 'value' not in data.columns:\n        raise ValueError(\"DataFrame must contain 'month' and 'value' columns.\")\n\n    # Validate month format and extract the year\n    try:\n        data['month'] = pd.to_datetime(data['month'], format='%Y-%m')\n        year = data['month'].dt.year.iloc[0]\n    except Exception as e:\n        raise ValueError(\"Invalid date format in 'month' column. Expected format is 'YYYY-MM'.\") from e\n\n    # Create the bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(data['month'].dt.strftime('%b'), data['value'])  # Use month abbreviation for x-ticks\n    \n    # Set the title and labels\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n\n    # Rotate x-tick labels for better readability\n    plt.xticks(rotation=45)\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    \"\"\"\n    Draws a histogram to visualize the frequency distribution of numeric values \n    provided in a string format.\n\n    Parameters:\n    data (str): A string of comma-separated numeric values.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): The Axes object of the created histogram.\n    \"\"\"\n    # Validate input\n    if not isinstance(data, str):\n        raise ValueError(\"Input data must be a string.\")\n    \n    # Convert the string data to a list of numbers\n    try:\n        numeric_values = pd.Series(data.split(',')).astype(float)\n    except ValueError:\n        raise ValueError(\"Input data must contain valid numeric values.\")\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(numeric_values.min(), numeric_values.max() + 2) - 0.5\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef sine_wave(x, amplitude, frequency, phase, offset):\n    \"\"\"Sine wave function for curve fitting.\"\"\"\n    return amplitude * np.sin(frequency * x + phase) + offset\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"Generates a noisy sine wave, fits a sine function, and plots the results.\n    \n    Args:\n        array_length (int): Length of the generated arrays.\n        noise_level (float): Standard deviation of the Gaussian noise added to the sine wave.\n    \n    Returns:\n        Axes: The Axes object containing the plot.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(0, 4 * np.pi, array_length)\n    \n    # Create a clean sine wave\n    clean_y = np.sin(x)\n    \n    # Add noise to the sine wave\n    noise = np.random.normal(0, noise_level, array_length)\n    noisy_y = clean_y + noise\n\n    # Initial guess for sine wave parameters: amplitude, frequency, phase, offset\n    initial_guess = [1, 1, 0, 0]\n    \n    # Fit the noisy data to the sine wave function\n    try:\n        params, _ = curve_fit(sine_wave, x, noisy_y, p0=initial_guess)\n    except RuntimeError as e:\n        print(f\"An error occurred during curve fitting: {e}\")\n        return None\n\n    # Generate the fitted curve using the optimized parameters\n    fitted_y = sine_wave(x, *params)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(x, noisy_y, 'o', label='Noisy Data', markersize=5)\n    ax.plot(x, fitted_y, 'r-', label='Fitted Curve', linewidth=2)\n    ax.plot(x, clean_y, 'g--', label='Original Sine Wave', linewidth=2)\n    ax.legend()\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.grid(True)\n    plt.show()\n\n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef normalize_text(text):\n    \"\"\"Normalize text to ASCII by removing diacritical marks.\"\"\"\n    return ''.join(\n        c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn'\n    )\ndef task_func(csv_file):\n    \"\"\"Reads a CSV file, normalizes the text, counts words, \n    and returns a bar plot and the 10 most common words with frequencies.\n    \n    Args:\n        csv_file (str): The path to the CSV file.\n    \n    Returns:\n        tuple: A tuple containing the matplotlib.axes.Axes object for the bar plot \n               and a list of the 10 most common words with their frequencies.\n    \"\"\"\n    try:\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = []\n            for row in reader:\n                text_data.extend(row)  # Assuming each row can contain multiple columns of text\n            \n            # Join all text data into a single string\n            full_text = ' '.join(text_data)\n            \n            # Normalize the full text\n            normalized_text = normalize_text(full_text)\n            \n            # Split the text into words and count occurrences\n            words = normalized_text.lower().split()\n            word_counts = Counter(words)\n            \n            # Get the 10 most common words\n            common_words = word_counts.most_common(10)\n            \n            # Prepare for plotting\n            words, frequencies = zip(*common_words)\n\n            # Create a bar plot\n            fig, ax = plt.subplots()\n            ax.bar(words, frequencies)\n            ax.set_xlabel('Words')\n            ax.set_ylabel('Frequencies')\n            ax.set_title('Top 10 Most Common Words')\n            plt.xticks(rotation=45)\n            plt.grid(axis='y')\n\n            return ax, common_words  # Return the Axes object and list of tuples\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{csv_file}' was not found.\")\n    except IOError:\n        raise IOError(f\"An error occurred while reading the file '{csv_file}'.\")"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    \"\"\"\n    Generate normally distributed random numbers and plot their histogram and PDF.\n\n    Parameters:\n        size (int): Number of random samples to generate (default is 1000).\n\n    Returns:\n        matplotlib.figure.Figure: A figure object containing the histogram and PDF plot.\n        np.ndarray: An array of generated random numbers.\n    \"\"\"\n    if size <= 0:\n        raise ValueError(\"Size must be a positive integer.\")\n    \n    # Generate normally distributed random numbers\n    mu, sigma = 0, 1  # mean and standard deviation\n    data = np.random.normal(mu, sigma, size)\n\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Create a histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\n\n    # Plot the PDF of the normal distribution\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, sigma)\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n\n    # Add labels and title\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n\n    # Show the plot\n    plt.show()\n\n    return fig, data"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode\ndef task_func() -> tuple:\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate a random password for AES encryption\n    password = get_random_bytes(32)  # AES-256 uses a 32-byte key\n\n    # Generate a random nonce for AES (GCM mode)\n    nonce = get_random_bytes(16)  # 16 bytes is standard for GCM nonce\n\n    # Create AES cipher\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n\n    # Encrypt the private key\n    encrypted_private_key, tag = cipher.encrypt_and_digest(private_key.save_pkcs1(format='PEM'))\n\n    # Generate a random number for the filename\n    random_number = get_random_bytes(8).hex()\n    filename = f\"private_key_{random_number}.txt\"\n\n    # Save the encrypted private key to a file\n    try:\n        with open(filename, 'wb') as file:\n            file.write(nonce + tag + encrypted_private_key)  # Store nonce, tag, and encrypted key\n        print(f\"Private key saved successfully to {filename}\")\n    except IOError as e:\n        print(f\"Error writing to file: {e}\")\n        return None, None, None, None\n\n    return public_key, filename, password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\ndef task_func(file_path):\n    \"\"\"\n    Encrypt a file using AES and encrypt the AES key using RSA.\n    \n    Parameters:\n    - file_path (str): The path to the file to be encrypted.\n\n    Returns:\n    - rsa.PublicKey: The RSA public key used for encryption.\n    - str: The filename of the encrypted file.\n    - str: The filename of the file containing the encrypted AES key.\n    \"\"\"\n    # Generate RSA keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Generate AES key\n    aes_key = os.urandom(32)  # AES-256 key\n\n    # Encrypt the file using AES\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    with open(file_path, 'rb') as f:\n        plaintext = f.read()\n    \n    # Create a random IV for AES\n    iv = os.urandom(16)\n\n    # Pad the plaintext to be AES block size compliant\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(plaintext) + padder.finalize()\n\n    # Encrypt using AES\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    ciphertext = iv + encryptor.update(padded_data) + encryptor.finalize()  # Prepend IV to the ciphertext\n\n    # Save the encrypted file\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as ef:\n        ef.write(ciphertext)\n\n    # Encrypt the AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Save the encrypted AES key\n    encrypted_aes_key_file_path = 'encrypted_aes_key.bin'\n    with open(encrypted_aes_key_file_path, 'wb') as ef:\n        ef.write(encrypted_aes_key)\n\n    # Return the public key and file paths\n    return public_key, encrypted_file_path, encrypted_aes_key_file_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom urllib.error import URLError\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    \"\"\"\n    Extracts the text and href attributes of all anchor tags from a given URL's HTML content,\n    and returns this data in a pandas DataFrame along with the time of data extraction.\n\n    Parameters:\n        url (str): The URL to fetch and parse.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with columns 'text', 'href', and 'fetch_time'.\n    \"\"\"\n    # Check for empty or invalid URL\n    if not url or not isinstance(url, str):\n        raise ValueError(\"The provided URL is invalid or empty.\")\n    \n    try:\n        # Fetch the HTML content from the URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read().decode('utf-8')\n        \n        # Parse the HTML content\n        document = pq(html_content)\n        \n        # Extract the anchor tags\n        anchors = document('a')\n        \n        # Prepare data for DataFrame\n        data = []\n        fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        \n        for anchor in anchors.items():\n            text = anchor.text()\n            href = anchor.attr('href')\n            data.append({'text': text, 'href': href, 'fetch_time': fetch_time})\n        \n        # Create a DataFrame\n        df = pd.DataFrame(data)\n        \n        return df\n    \n    except URLError as e:\n        raise URLError(f\"Network connectivity issue or server error: {e}\")"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint, uniform\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Prepare file name\n    file_name = f\"sensor_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Generate data\n    start_time = datetime.now()\n    data = []\n    \n    for hour in range(hours):\n        for minute in range(60):\n            timestamp = start_time + timedelta(hours=hour, minutes=minute)\n            temperature = round(uniform(-10, 35), 2)  # Random temperature between -10 and 35 degrees Celsius\n            humidity = randint(0, 100)  # Random humidity percentage\n            pressure = round(uniform(950, 1050), 2)  # Random pressure between 950 and 1050 hPa\n            \n            data.append([timestamp.strftime('%Y-%m-%d %H:%M:%S'), temperature, humidity, pressure])\n    \n    # Write data to CSV\n    with open(file_path, mode='w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        # Write header\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        # Write data rows\n        writer.writerows(data)\n\n    print(f\"Sensor data generated and saved to {file_path}\")"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"Generate traffic data and plot it.\n\n    Args:\n        hours (int): The number of hours to generate data for.\n        output_dir (str): The directory to save output files.\n\n    Returns:\n        tuple: Path to the CSV file and the matplotlib axes object of the line plot.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Prepare the data\n    data = []\n    start_time = datetime.now()\n    \n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour)\n        \n        # Generate random counts for each vehicle type\n        vehicle_counts = {\n            'Car': randint(0, 100),\n            'Bus': randint(0, 20),\n            'Truck': randint(0, 30),\n            'Bike': randint(0, 50)\n        }\n        \n        # Append the time and counts to the data list\n        data.append([current_time, vehicle_counts['Car'], vehicle_counts['Bus'], vehicle_counts['Truck'], vehicle_counts['Bike']])\n    \n    # Define CSV file path\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    \n    # Write data to a CSV file\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Car', 'Bus', 'Truck', 'Bike'])  # Write header\n        writer.writerows(data)  # Write data rows\n    \n    # Create a DataFrame for plotting\n    df = pd.DataFrame(data, columns=['Time', 'Car', 'Bus', 'Truck', 'Bike'])\n    df['Time'] = pd.to_datetime(df['Time'])  # Keep full timestamp for plotting\n    df.set_index('Time', inplace=True)\n    \n    # Plotting\n    plt.figure(figsize=(12, 6))\n    for vehicle in VEHICLE_TYPES:\n        plt.plot(df.index, df[vehicle], label=vehicle)\n    \n    plt.title('Vehicle Counts Over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.grid(True)  # Add grid\n    plt.tight_layout()\n    \n    # Save the plot\n    plot_file_path = os.path.join(output_dir, 'traffic_plot.png')\n    plt.savefig(plot_file_path)\n    \n    # Return the path to the CSV file and the matplotlib axes object\n    return csv_file_path, plt.gca()"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\ndef task_func(hours, output_dir=OUTPUT_DIR, backup_dir=BACKUP_DIR):\n    \"\"\"\n    Generate weather data for a specified number of hours, save it in a CSV file,\n    and back up the file to a backup directory.\n\n    Args:\n        hours (int): The number of hours for which to generate weather data.\n        output_dir (str): The directory where the output CSV file will be saved.\n        backup_dir (str): The directory where the backup CSV file will be saved.\n\n    Returns:\n        str: The path of the generated CSV file.\n    \"\"\"\n    # Validate input\n    if not isinstance(hours, int) or hours <= 0:\n        raise ValueError(\"Hours must be a positive integer.\")\n\n    # Create output and backup directories if they do not exist\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(backup_dir, exist_ok=True)\n\n    # Generate weather data\n    weather_data = []\n    current_time = datetime.now()\n    \n    for hour in range(hours):\n        time_str = current_time + timedelta(hours=hour)\n        condition = choice(WEATHER_CONDITIONS)\n        weather_data.append((time_str.strftime(\"%Y-%m-%d %H:%M:%S\"), condition))\n    \n    # Define the file path\n    csv_file_path = os.path.join(output_dir, 'weather_data.csv')\n    \n    # Write data to CSV\n    try:\n        with open(csv_file_path, mode='w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow(['Time', 'Condition'])  # Write header\n            writer.writerows(weather_data)  # Write data\n    except Exception as e:\n        raise IOError(f\"Failed to write CSV file: {e}\")\n    \n    # Backup the file\n    backup_file_path = os.path.join(backup_dir, f'weather_data_backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n    try:\n        shutil.copy(csv_file_path, backup_file_path)\n    except Exception as e:\n        raise IOError(f\"Failed to create backup file: {e}\")\n    \n    return csv_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a Pandas DataFrame of football match results with random goals and penalties.\n    \n    Parameters:\n        goals (int): Maximum number of goals a team can score.\n        penalties (int): Maximum number of penalties a team can incur.\n        \n    Returns:\n        pd.DataFrame: A DataFrame containing match results.\n        list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    \n    # Parameter validation\n    if goals < 0 or penalties < 0:\n        raise ValueError(\"Goals and penalties must be non-negative integers.\")\n    \n    # Generate random goals and penalties for the teams\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in TEAMS],\n        'Penalties': [randint(0, penalties) for _ in TEAMS]\n    }\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate penalty fines\n    df['Penalty Cost'] = df['Penalties'] * PENALTY_COST\n    \n    # Visualize Goals\n    plt.figure(figsize=(12, 6))\n    goal_ax = sns.barplot(x='Team', y='Goals', data=df, palette='viridis')\n    goal_ax.set_title('Goals Scored by Teams')\n    goal_ax.set_ylabel('Number of Goals')\n    plt.xticks(rotation=45)\n    \n    # Visualize Penalty Costs\n    plt.figure(figsize=(12, 6))\n    penalty_ax = sns.barplot(x='Team', y='Penalty Cost', data=df, palette='magma')\n    penalty_ax.set_title('Penalty Costs by Teams')\n    penalty_ax.set_ylabel('Penalty Cost (in dollars)')\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n    \n    return df, [goal_ax, penalty_ax]"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows: int) -> tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generate a DataFrame with random integer values and plot the non-zero counts.\n\n    Parameters:\n    rows (int): Number of rows to generate in the DataFrame.\n\n    Returns:\n    tuple: A tuple containing the generated DataFrame and the Axes object of the bar plot.\n    \"\"\"\n    # Validate input\n    if not isinstance(rows, int) or rows <= 0:\n        raise ValueError(\"The number of rows must be a positive integer.\")\n\n    # Generate a DataFrame with random integers between 0 and 9\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create a bar plot of non-zero counts\n    fig, ax = plt.subplots(figsize=(8, 5))  # Set figure size\n    non_zero_counts.plot(kind='bar', ax=ax, color='skyblue')\n    ax.set_title('Non-Zero Value Counts by Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-Zero Count')\n    ax.grid(axis='y', linestyle='--', alpha=0.7)  # Add grid lines for clarity\n    ax.set_xticks(range(len(COLUMNS)))\n    ax.set_xticklabels(COLUMNS)\n\n    # Return the DataFrame and the Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates a DataFrame of student grades in multiple courses,\n    calculates average grades and passing counts, and visualizes the results.\n\n    Parameters:\n    num_students (int): The number of students to generate grades for.\n\n    Returns:\n    Tuple[pd.DataFrame, plt.Axes]: A tuple containing the generated DataFrame and the bar plot's Axes object.\n    \"\"\"\n    if num_students <= 0:\n        raise ValueError(\"Number of students must be a positive integer.\")\n    \n    # Set a random seed for reproducibility\n    np.random.seed(42)\n\n    # Define courses\n    courses = ['Math', 'Science', 'History', 'Art', 'Physical Education']\n    \n    # Generate random grades for each student in each course\n    data = {\n        'Student': [f'Student {i+1}' for i in range(num_students)],\n        **{course: sample(range(0, 101), num_students) for course in courses}\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate average grades and passing counts\n    average_grades = df[courses].mean()\n    passing_counts = (df[courses] >= 60).sum()\n    \n    # Create a new DataFrame to hold the summary\n    summary_df = pd.DataFrame({\n        'Average Grade': average_grades,\n        'Passing Count': passing_counts\n    })\n    \n    # Plotting\n    ax = summary_df.plot(kind='bar', figsize=(10, 6))\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_ylabel('Grades / Count')\n    ax.set_xlabel('Courses')\n    \n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return df, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fit an exponential decay function to the indices of the array\n    where the first column matches the specified target value.\n    \n    Parameters:\n    array (np.ndarray): A 2D NumPy array with at least two columns.\n    target_value (any): The target value to match in the first column.\n    \n    Returns:\n    tuple: Optimized parameters of the fitting function (popt) and the matplotlib Axes object.\n    \"\"\"\n    # Validate input array\n    if not isinstance(array, np.ndarray) or array.ndim != 2 or array.shape[1] < 2:\n        raise ValueError(\"Input must be a 2D NumPy array with at least two columns.\")\n    \n    # Filter rows where the first column matches the target value\n    filtered_indices = np.where(array[:, 0] == target_value)[0]\n    \n    if len(filtered_indices) == 0:\n        raise ValueError(\"No matching target value found in the first column.\")\n    \n    # Create x and y data for fitting\n    x_data = filtered_indices\n    y_data = array[filtered_indices, 1]  # Assuming the second column contains the y-values\n    \n    # Check for enough data points to fit\n    if len(y_data) < 2:\n        raise ValueError(\"Not enough data points to perform curve fitting.\")\n    \n    # Define the exponential decay function to fit\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Initial guess for parameters a, b, c\n    initial_guess = (np.max(y_data), 0.1, np.min(y_data))\n    \n    # Perform the curve fitting\n    popt, pcov = optimize.curve_fit(exp_decay, x_data, y_data, p0=initial_guess)\n\n    # Generate x values for the fitted curve\n    x_fit = np.linspace(np.min(x_data), np.max(x_data), 100)\n    y_fit = exp_decay(x_fit, *popt)\n\n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.scatter(x_data, y_data, label='Data', color='red')\n    ax.plot(x_fit, y_fit, label='Fitted Exponential Decay', color='blue')\n    ax.set_xlabel('Indices')\n    ax.set_ylabel('Values')\n    ax.set_title('Exponential Decay Fit')\n    ax.legend()\n    ax.grid()\n\n    return popt, ax"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef preprocess_text(text):\n    \"\"\"Preprocess the input text by removing non-alphanumeric characters, \n    converting to lowercase, and removing stopwords.\"\"\"\n    text = ALPHANUMERIC.sub(' ', text)  # Remove non-alphanumeric characters\n    text = text.lower()  # Convert to lowercase\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS)  # Remove stopwords\n    return text\ndef task_func(texts, num_topics):\n    \"\"\"Perform topic extraction from a collection of text documents using NMF.\n\n    Args:\n        texts (list of str): The collection of text documents.\n        num_topics (int): The number of topics to extract.\n\n    Returns:\n        list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    if not texts:\n        raise ValueError(\"The input texts list is empty.\")\n    if num_topics <= 0:\n        raise ValueError(\"The number of topics must be a positive integer.\")\n\n    # Preprocess the texts\n    processed_texts = [preprocess_text(text) for text in texts]\n\n    # Vectorize the texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=1)\n    nmf.fit(tfidf_matrix)\n\n    # Get the words corresponding to the features\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words_indices = topic.argsort()[-10:][::-1]  # Get the top 10 words for each topic\n        top_words = [feature_names[i] for i in top_words_indices]\n        topics.append(top_words)\n\n    return topics"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nALPHANUMERIC = re.compile('[\\W_]+')\ndef clean_text(text, stopwords=None):\n    \"\"\"Cleans the input text by removing non-alphanumeric characters,\n    lowercasing, and removing stopwords.\"\"\"\n    # Remove non-alphanumeric characters and lower the case\n    cleaned_text = ALPHANUMERIC.sub(' ', text).lower()\n    # Tokenize the text\n    tokens = cleaned_text.split()\n    # Remove stopwords if provided\n    if stopwords is not None:\n        tokens = [word for word in tokens if word not in stopwords]\n    return tokens\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using the Word2Vec model.\n    \n    Args:\n        texts (list of str): The list of texts to be processed.\n        stopwords: A set of stopwords to remove. If None, will use NLTK's stopwords.\n    \n    Returns:\n        Word2Vec: A trained Word2Vec model.\n    \"\"\"\n    if stopwords is None:\n        stopwords = set(stopwords.words('english'))\n    \n    # Preprocess the texts\n    processed_texts = [clean_text(text, stopwords) for text in texts]\n    \n    # Train the Word2Vec model\n    model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, sg=0)\n    \n    return model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in a specified directory, reading each file into a DataFrame,\n    adding a 'Source' column with the filename, and moving the processed files to a 'processed' \n    subdirectory.\n\n    Args:\n        path (str): The directory path containing JSON files.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the data from all processed files.\n    \"\"\"\n    # Ensure the path exists and is a directory\n    if not os.path.exists(path):\n        os.makedirs(path)  # Create the directory if it does not exist\n\n    # Create a processed directory if it doesn't exist\n    processed_dir = os.path.join(path, \"processed\")\n    os.makedirs(processed_dir, exist_ok=True)\n\n    # Initialize an empty list to hold DataFrames\n    df_list = []\n\n    # Get a sorted list of JSON files in the directory\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n\n    if not json_files:\n        print(\"No JSON files found in the directory.\")\n        return pd.DataFrame()  # Return empty DataFrame if no files to process\n\n    for json_file in json_files:\n        file_path = os.path.join(path, json_file)\n        \n        # Read the JSON file into a DataFrame\n        try:\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n                df = pd.DataFrame(data)\n\n            # Insert the 'Source' column\n            df['Source'] = json_file\n\n            # Append the DataFrame to the list\n            df_list.append(df)\n\n            # Move the processed file to the \"processed\" directory\n            shutil.move(file_path, os.path.join(processed_dir, json_file))\n            print(f\"Processed and moved: {json_file}\")\n\n        except Exception as e:\n            print(f\"Error processing file {json_file}: {e}\")\n\n    # Concatenate all DataFrames into a single DataFrame\n    combined_df = pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()\n\n    return combined_df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the directory and file name\n    directory = \"task_func_data\"\n    file_name = \"Output.txt\"\n    file_path = os.path.join(directory, file_name)\n\n    # Create the directory if it doesn't exist\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Generate random sensor data and write to the file\n    with open(file_path, mode='a+', newline='') as file:\n        writer = csv.writer(file)\n        # Write header if the file is empty\n        if file.tell() == 0:  # Check if the file is empty\n            writer.writerow(['Timestamp', 'Temperature', 'Humidity'])\n        \n        # Generate random temperature and humidity\n        temperature = random.uniform(-10.0, 40.0)  # Temperature in \u00b0C\n        humidity = random.uniform(0.0, 100.0)      # Humidity in %\n\n        # Write the current timestamp, temperature, and humidity to the file\n        writer.writerow([datetime.now().isoformat(), temperature, humidity])\n\n    # Return the path to the CSV file\n    return file_path"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves it to a CSV file.\n\n    Parameters:\n    url (str): The URL to scrape data from.\n\n    Returns:\n    str: The path of the CSV file where the scraped data is saved.\n    \"\"\"\n    try:\n        # Fetch the content from the URL\n        with urllib.request.urlopen(url) as response:\n            web_content = response.read()\n    except Exception as e:\n        print(f\"Error fetching URL: {e}\")\n        return None\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(web_content, 'html.parser')\n\n    # Example: Scrape data from all <h2> tags (modify as needed)\n    data = []\n    for item in soup.find_all('h2'):  # Modify this as required\n        data.append(item.get_text(strip=True))\n\n    # Save the scraped data to a CSV file\n    try:\n        with open(CSV_FILE_PATH, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['Scraped Data'])  # Write header\n            for row in data:\n                writer.writerow([row])  # Write each row of data\n    except Exception as e:\n        print(f\"Error writing to CSV: {e}\")\n        return None\n\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"ValueError: data must be a DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"ValueError: data must not be empty.\")\n    \n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(\"ValueError: target_column must be a column of data.\")\n    \n    # Check if all values in the DataFrame are numeric\n    if not all(np.issubdtype(data[col].dtype, np.number) for col in data.columns):\n        raise ValueError(\"ValueError: all columns must contain numeric values.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"ValueError: random_state must be an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"ValueError: test_size must be between 0 and 1.\")\n\n    # Split the data into features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n\n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Create and train the linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # Return the model score on the test set\n    return model.score(X_test, y_test)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport datetime\ndef correct_latin_characters(name):\n    # A simple function to correct improperly encoded Latin characters\n    # This example assumes some common replacements; adjust as necessary.\n    replacements = {\n        'Sopet\u00f3n': 'Sopet\u00f3n',\n        'M\u00e9ndez': 'M\u00e9ndez',\n        'G\u00f3mez': 'G\u00f3mez',\n        'P\u00e9rez': 'P\u00e9rez',\n        'Mu\u00f1oz': 'Mu\u00f1oz',\n        # Add more replacements as needed\n    }\n    return replacements.get(name, name)\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n               latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n               other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n               rng_seed=None):\n    \"\"\"\n    Generates a DataFrame with random user data including ID, Name, Date of Birth, and Email.\n    \n    Parameters:\n    - start_year: Integer, the starting year for generating dates of birth.\n    - end_year: Integer, the ending year for generating dates of birth.\n    - email_domain: String, the domain to use for generating email addresses.\n    - latin_names: List of strings, names with Latin characters.\n    - other_names: List of strings, names without Latin characters.\n    - rng_seed: Integer, optional seed for random number generation.\n    \n    Returns:\n    - A pandas DataFrame containing the generated user data.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    names = latin_names + other_names\n    ids = np.arange(1, 101)\n    random_names = np.random.choice(names, size=100)\n    \n    start_date = datetime.date(start_year, 1, 1)\n    end_date = datetime.date(end_year, 12, 31)\n    random_dates = [start_date + datetime.timedelta(days=np.random.randint(0, (end_date - start_date).days)) for _ in range(100)]\n    \n    years_of_birth = [date.year for date in random_dates]\n    emails = [f\"{correct_latin_characters(name).lower()}{year}@{email_domain}\" for name, year in zip(random_names, years_of_birth)]\n    \n    # Format the dates\n    formatted_dates = [date.strftime('%Y-%m-%d') for date in random_dates]\n\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': random_names,\n        'Date of Birth': formatted_dates,\n        'Email': emails\n    })\n\n    return df"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\nimport os\ndef task_func(input_file, output_file):\n    \"\"\"Reads a JSON file, calculates mean and median for each key, and writes the results to a CSV file.\n    \n    Args:\n        input_file (str): Path to the input JSON file.\n        output_file (str): Path to the output CSV file.\n        \n    Returns:\n        dict: A dictionary where each key is a field from the input JSON and each value is another dictionary \n              with the mean and median of that field.\n    \"\"\"\n    \n    if not os.path.exists(input_file):\n        raise FileNotFoundError(f\"The file {input_file} does not exist.\")\n        \n    # Read JSON data from the input file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    if not isinstance(data, list) or not all(isinstance(entry, dict) for entry in data):\n        raise ValueError(\"Input JSON must be a list of dictionaries.\")\n\n    # Initialize a dictionary to hold sums, counts, and values for calculations\n    sums = defaultdict(float)\n    counts = defaultdict(int)\n    all_values = defaultdict(list)\n\n    # Process each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                sums[key] += value\n                counts[key] += 1\n                all_values[key].append(value)\n            else:\n                print(f\"Warning: Non-numeric value encountered for key '{key}': {value}\")\n\n    # Prepare the result dictionary\n    result = {}\n    for key in sums.keys():\n        mean = sums[key] / counts[key] if counts[key] > 0 else 0\n        median = np.median(all_values[key]) if all_values[key] else 0\n        result[key] = {'mean': mean, 'median': median}\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['Field', 'Mean', 'Median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for key, stats in result.items():\n            writer.writerow({'Field': key, 'Mean': stats['mean'], 'Median': stats['median']})\n\n    return result"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from a dictionary by checking if the file exists,\n    and if it has content, then copies it to a target directory.\n    \n    Args:\n        kwargs (dict): A dictionary where keys are file identifiers and \n                       values are file paths.\n        target_dir (str): The directory where non-empty files will be copied. \n                          Defaults to 'non_none_files'.\n                          \n    Returns:\n        list: A list of full file paths that were copied.\n    \"\"\"\n    # Create target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n    \n    copied_files = []\n    \n    for file_path in kwargs.values():\n        file_path = Path(file_path)  # Convert to Path object for easier manipulation\n        \n        # Check if the file exists and is not empty\n        if file_path.exists() and file_path.stat().st_size > 0:\n            try:\n                # Define the target file path\n                target_file_path = Path(target_dir) / file_path.name\n                \n                # Copy the file to the target directory\n                shutil.copy(file_path, target_file_path)\n                copied_files.append(str(target_file_path))  # Append the full path to the list\n                \n            except Exception as e:\n                print(f\"Error copying {file_path}: {e}\")\n        else:\n            print(f\"File does not exist or is empty: {file_path}\")\n    \n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in a specified directory based on a given pattern,\n    creates new files with altered names while preserving their content.\n    \n    Args:\n        directory (str): The path to the directory containing CSV files.\n        pattern (str): The regex pattern used to match files.\n\n    Returns:\n        list: A list of strings, where each string is the filename of a new CSV file created.\n    \"\"\"\n    # Convert the directory string to a Path object\n    dir_path = Path(directory)\n\n    # Ensure the directory exists\n    if not dir_path.is_dir():\n        raise ValueError(f\"Directory '{directory}' does not exist.\")\n    \n    # Compile the regex pattern\n    regex = re.compile(pattern)\n    \n    new_files = []  # List to store names of new files created\n\n    # Loop through all CSV files in the directory\n    for csv_file in dir_path.glob('*.csv'):\n        match = regex.match(csv_file.name)\n        if match:\n            # Extract the base name without the number and extension\n            base_name = match.group(1)\n            \n            # Create a new filename\n            new_filename = f\"{base_name}-processed.csv\"\n            new_file_path = dir_path / new_filename\n            \n            # Prevent overwriting existing files\n            if new_file_path.exists():\n                raise FileExistsError(f\"The file '{new_filename}' already exists.\")\n            \n            # Copy the content from the old file to the new file\n            try:\n                with open(csv_file, 'r', newline='', encoding='utf-8') as infile:\n                    reader = csv.reader(infile)\n                    with open(new_file_path, 'w', newline='', encoding='utf-8') as outfile:\n                        writer = csv.writer(outfile)\n                        for row in reader:\n                            writer.writerow(row)\n            except Exception as e:\n                raise IOError(f\"An error occurred while processing the file '{csv_file.name}': {e}\")\n            \n            # Append the new file name to the list\n            new_files.append(new_filename)\n    \n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"Unzips all zip files in the specified directory that match the given pattern.\n    \n    Args:\n        directory (str): The path to the directory containing zip files.\n        pattern (str): The regex pattern to match filename format.\n    \n    Returns:\n        list: A list of directories where the files were extracted.\n    \"\"\"\n    if not os.path.exists(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n    \n    extracted_dirs = []\n    regex = re.compile(pattern)\n\n    for filename in os.listdir(directory):\n        match = regex.match(filename)\n        if match:\n            # Split the filename using the last occurrence of \"-\"\n            prefix = filename.rsplit('-', 1)[0]\n            extraction_dir = os.path.join(directory, prefix)\n            os.makedirs(extraction_dir, exist_ok=True)\n            zip_file_path = os.path.join(directory, filename)\n\n            try:\n                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n                    zip_ref.extractall(extraction_dir)\n                extracted_dirs.append(extraction_dir)\n            except zipfile.BadZipFile:\n                print(f\"Warning: {zip_file_path} is not a zip file or it is corrupted.\")\n            except Exception as e:\n                print(f\"Error extracting {zip_file_path}: {e}\")\n\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archives all files matching the specified pattern and deletes the original files.\n    \n    Args:\n        pattern (str): The file pattern to match files for archiving.\n        \n    Returns:\n        str: The archive file path if successful, None if an error occurs.\n    \"\"\"\n    # Create the archive directory if it does not exist\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the given pattern\n    files_to_archive = glob.glob(pattern)\n\n    if not files_to_archive:\n        print(\"No files found matching the pattern.\")\n        return None\n\n    # Define the archive file path\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n\n    try:\n        # Create a tar.gz archive of the files\n        subprocess.run(['tar', '-czf', archive_file] + files_to_archive, check=True)\n        print(f\"Archived files into: {archive_file}\")\n        \n        # Delete the original files\n        for file in files_to_archive:\n            os.remove(file)\n            print(f\"Deleted original file: {file}\")\n        \n        return archive_file\n    \n    except subprocess.CalledProcessError as e:\n        print(f\"Error occurred while creating archive: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"Counts and updates goals and penalties in the CSV file.\"\"\"\n    if not isinstance(goals, int) or not isinstance(penalties, int):\n        raise ValueError(\"Goals and penalties must be integers.\")\n    if goals < 0 or penalties < 0:\n        raise ValueError(\"Goals and penalties must be non-negative.\")\n\n    # Initialize counts\n    total_goals = 0\n    total_penalties = 0\n    \n    # Read the CSV file and count goals and penalties\n    with open(csv_file_path, newline='') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            total_goals += int(row['goals'])\n            total_penalties += int(row['penalties'])\n    \n    # Update counts with the provided goals and penalties\n    total_goals += goals\n    total_penalties += penalties\n    \n    # Write the updated counts back to the CSV file\n    with open(csv_file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Total', total_goals, total_penalties])\n    \n    # Create a Counter object with the results\n    count = Counter({'goals': total_goals, 'penalties': total_penalties})\n    \n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Compile the regex pattern for efficiency\n    pattern = re.compile(file_pattern)\n    moved_files_count = 0\n    \n    # Ensure the target directory exists\n    try:\n        os.makedirs(target_dir, exist_ok=True)\n    except Exception as e:\n        print(f\"Error creating target directory: {e}\")\n        return moved_files_count\n    \n    # Iterate through all files in the source directory\n    try:\n        for filename in os.listdir(source_dir):\n            if pattern.match(filename):\n                source_file_path = os.path.join(source_dir, filename)\n                target_file_path = os.path.join(target_dir, filename)\n                \n                # Move the file and increment count\n                try:\n                    shutil.move(source_file_path, target_file_path)\n                    moved_files_count += 1\n                except Exception as e:\n                    print(f\"Error moving file {filename}: {e}\")\n    except FileNotFoundError:\n        print(\"Source directory not found.\")\n    \n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef clean_text(text):\n    \"\"\"Cleans the input text by removing non-alphanumeric characters (except spaces) and converting to lowercase.\"\"\"\n    text = text.lower()  # Convert to lowercase\n    text = ALPHANUMERIC.sub(' ', text)  # Remove non-alphanumeric characters\n    return text\ndef cosine_similarity(text1, text2):\n    \"\"\"Calculates the cosine similarity between two texts.\"\"\"\n    # Tokenize and count term frequencies\n    tokens1 = text1.split()\n    tokens2 = text2.split()\n    \n    counter1 = Counter(tokens1)\n    counter2 = Counter(tokens2)\n    \n    # Create a set of all unique words\n    all_words = set(counter1.keys()).union(set(counter2.keys()))\n    \n    # Create term frequency vectors\n    vector1 = np.array([counter1[word] for word in all_words])\n    vector2 = np.array([counter2[word] for word in all_words])\n    \n    # Calculate cosine similarity\n    dot_product = np.dot(vector1, vector2)\n    magnitude1 = np.linalg.norm(vector1)\n    magnitude2 = np.linalg.norm(vector2)\n    \n    if magnitude1 == 0 or magnitude2 == 0:\n        return 0.0  # To handle division by zero\n    \n    return dot_product / (magnitude1 * magnitude2)\ndef task_func(text1, text2):\n    \"\"\"Calculates the cosine similarity and Levenshtein ratio between two texts.\"\"\"\n    # Clean the texts\n    clean_text1 = clean_text(text1)\n    clean_text2 = clean_text(text2)\n    \n    # Calculate cosine similarity\n    cos_sim = cosine_similarity(clean_text1, clean_text2)\n    \n    # Calculate Levenshtein ratio\n    levenshtein_sim = ratio(clean_text1, clean_text2)\n    \n    return (cos_sim, levenshtein_sim)\ntext1 = \"Hello, World!\"\ntext2 = \"Hello World\""}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Check if the input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers.\")\n    \n    # Check if the list contains only integers and if there are negative numbers\n    for number in numbers:\n        if not isinstance(number, int):\n            raise TypeError(\"All elements in the list must be integers.\")\n        if number < 0:\n            raise ValueError(\"Negative numbers are not allowed.\")\n    \n    # If the input list is empty, return empty outputs\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations of the numbers\n    perms = list(permutations(numbers))\n    \n    # Compute sums of factorials for each permutation\n    factorial_sums = []\n    for perm in perms:\n        factorial_sum = sum(math.factorial(num) for num in perm)\n        factorial_sums.append(factorial_sum)\n    \n    return factorial_sums, perms"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from SOURCE_DIR to DEST_DIR based on specified file EXTENSIONS.\n    \n    Parameters:\n    SOURCE_DIR (str): The path to the source directory.\n    DEST_DIR (str): The path to the destination directory.\n    EXTENSIONS (list): A list of file extensions to filter by (e.g., ['txt', 'jpg']).\n    \n    Returns:\n    list: A list containing the names of files that were successfully transferred.\n    \"\"\"\n    transferred_files = []\n    \n    # Ensure the source directory exists\n    if not os.path.exists(SOURCE_DIR):\n        warnings.warn(f\"Source directory '{SOURCE_DIR}' does not exist.\")\n        return transferred_files\n    \n    # Ensure the destination directory exists, create it if not\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    # Iterate over each extension and transfer matching files\n    for ext in EXTENSIONS:\n        pattern = os.path.join(SOURCE_DIR, f'*.{ext}')\n        \n        for file_path in glob.glob(pattern):\n            try:\n                # Extract the file name from the file path\n                file_name = os.path.basename(file_path)\n                # Construct full destination path\n                dest_path = os.path.join(DEST_DIR, file_name)\n                \n                # Transfer the file\n                shutil.move(file_path, dest_path)\n                transferred_files.append(file_name)\n            \n            except (FileNotFoundError, PermissionError) as e:\n                warnings.warn(f\"Could not transfer file '{file_path}': {e}\")\n    \n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Validate input data\n    if not isinstance(data, list) or not all(isinstance(item, tuple) and len(item) == 3 for item in data):\n        raise ValueError(\"Input must be a list of tuples with three elements: (item, count, weight).\")\n\n    # Unzip the input data into separate lists for items, counts, and weights\n    items, counts, weights = zip(*data)\n\n    # Convert counts and weights to numpy arrays for processing\n    counts_array = np.array(counts)\n    weights_array = np.array(weights)\n\n    # Normalize counts using z-score normalization\n    if counts_array.std() == 0:  # Handle edge case\n        normalized_counts = np.zeros_like(counts_array)\n    else:\n        normalized_counts = zscore(counts_array)\n\n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(weights_array.reshape(-1, 1)).flatten()\n\n    # Create a DataFrame with the results\n    result_df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n\n    return result_df\ndata = [\n    ('item1', 10, 0.5),\n    ('item2', 20, 0.75),\n    ('item3', 30, 0.2),\n    ('item4', 40, 0.1)\n]"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Return an empty DataFrame if the input list is empty\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n\n    # Transpose the list of tuples to get columns for each position\n    transposed_data = list(zip(*data_list))\n    \n    means = []\n    \n    for position_data in transposed_data:\n        # Filter out non-numeric values\n        numeric_values = [value for value in position_data if isinstance(value, (int, float))]\n        \n        # Calculate the mean if there are any numeric values\n        if numeric_values:\n            mean_value = np.mean(numeric_values)\n        else:\n            mean_value = np.nan  # Assign NaN if no numeric values are present\n        \n        means.append(mean_value)\n\n    # Create a DataFrame with the means\n    result_df = pd.DataFrame(means, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(means))])\n    \n    return result_df"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the specified columns are in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both of the columns '{col1}' or '{col2}' are not in the DataFrame.\")\n    \n    # Check if the columns are categorical or can be converted to categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]):\n        if pd.api.types.is_object_dtype(data[col1]):\n            data[col1] = data[col1].astype('category')\n        else:\n            raise TypeError(f\"The column '{col1}' must contain categorical data.\")\n    \n    if not pd.api.types.is_categorical_dtype(data[col2]):\n        if pd.api.types.is_object_dtype(data[col2]):\n            data[col2] = data[col2].astype('category')\n        else:\n            raise TypeError(f\"The column '{col2}' must contain categorical data.\")\n    \n    # Get the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if there are multiple categories in both columns\n    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Check for categories with less than 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating the assumptions of the chi-square test.\")\n    \n    # Perform the chi-square test\n    chi2, p, _, _ = chi2_contingency(contingency_table)\n    \n    return float(p)  # Ensure returning a float\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate a number of dice rolls, calculate the frequency of each result, \n    and return both the frequency array and a histogram of the results.\n\n    Parameters:\n        rolls (int): Number of dice rolls.\n        seed (int, optional): Seed for random number generation for reproducibility.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.array: A numpy array with the frequency of each outcome.\n            - matplotlib.Axes: Axes object representing the histogram.\n    \"\"\"\n    # Validate input\n    if not isinstance(rolls, int) or rolls <= 0:\n        raise ValueError(\"The number of rolls must be a positive integer.\")\n\n    # Set the random seed for reproducibility if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.array([results.count(i) for i in NUMBERS])\n    \n    # Create a histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(0.5, 7.5, 1), density=False, align='mid', color='blue', alpha=0.7)\n    \n    # Customize the histogram\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    \n    plt.show()  # Display the histogram\n    \n    return frequency, ax"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from the source directory to the target directory.\n\n    Parameters:\n    - source_dir (str): The directory containing files to be archived.\n    - target_dir (str): The directory where the archive will be saved.\n    - archive_name (str): The name of the archive file. Default is 'archive.zip'.\n\n    Returns:\n    - str: The path to the created archive or a message indicating no files were archived.\n    \"\"\"\n    # Ensure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Define the path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Create a zip file to archive processed files\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        processed_files_found = False\n        \n        # Loop through all files in the source directory\n        for filename in os.listdir(source_dir):\n            # Check if the file has the '_processed' suffix\n            if filename.endswith('_processed'):\n                processed_files_found = True\n                file_path = os.path.join(source_dir, filename)\n                # Add the file to the zip file\n                zipf.write(file_path, arcname=os.path.basename(filename))\n\n    if processed_files_found:\n        # Return the path to the created archive\n        return archive_path\n    else:\n        return \"No processed files found to archive.\""}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the DataFrame has a 'date' and 'close' column\n    if 'date' not in df.columns or 'close' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'date' and 'close' columns.\")\n    \n    # Convert 'date' column to datetime and ensure it's sorted\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values('date')\n\n    # Prepare the data for linear regression\n    df['timestamp'] = df['date'].view(np.int64) // 10**9  # Convert datetime to timestamp\n    X = df['timestamp'].values.reshape(-1, 1)  # Feature: timestamp\n    y = df['close'].values  # Target: closing prices\n\n    # Create and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n\n    # Predict for the next 7 days\n    last_timestamp = df['timestamp'].iloc[-1]\n    future_timestamps = np.array([last_timestamp + i * 24 * 60 * 60 for i in range(1, 8)]).reshape(-1, 1)\n    predicted_prices = model.predict(future_timestamps)\n\n    # Create a new DataFrame for the future dates\n    future_dates = pd.date_range(start=df['date'].iloc[-1] + pd.Timedelta(days=1), periods=7)\n    future_df = pd.DataFrame({'date': future_dates, 'predicted_close': predicted_prices})\n\n    # Plotting the historical and predicted prices\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['date'], df['close'], label='Historical Close Prices', color='blue', marker='o')\n    ax.plot(future_df['date'], future_df['predicted_close'], label='Predicted Close Prices', color='orange', marker='x')\n    \n    ax.set_title('Stock Closing Prices Prediction')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.grid(True)\n    plt.tight_layout()\n\n    # Return the predicted prices and the Axes object as a tuple\n    return list(predicted_prices), ax"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a given DataFrame using the Z-Score method.\n\n    Parameters:\n    - df: pandas.DataFrame containing a 'closing_price' column.\n    - z_threshold: float, the Z-Score threshold for identifying outliers.\n\n    Returns:\n    - tuple: A tuple containing:\n        - pandas.DataFrame: A DataFrame containing the outliers in the 'closing_price' column.\n        - matplotlib.axes._axes.Axes: The plot object displaying the outliers.\n    \"\"\"\n    \n    # Check if 'closing_price' column exists\n    if 'closing_price' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'closing_price' column.\")\n    \n    # Calculate the Z-Scores of the closing_price column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.scatter(df.index, df['closing_price'], label='Data Points', color='blue')\n    ax.scatter(outliers.index, outliers['closing_price'], label='Outliers', color='red', s=100, edgecolor='black')\n    \n    # Label the axes and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    # Return the outliers DataFrame and the plot object as a tuple\n    return outliers[['closing_price']], ax"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using a box plot and a histogram.\n    \n    Parameters:\n    df (pd.DataFrame): DataFrame containing a 'Closing Price' column.\n    \n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects for the boxplot and histogram.\n    \"\"\"\n    # Validate input DataFrame\n    if 'Closing Price' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Closing Price' column.\")\n    \n    # Set up the figure and axes\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Box plot on the first axis\n    sns.boxplot(x=df['Closing Price'], ax=ax1)\n    ax1.set_title('Box Plot of Closing Prices')\n    \n    # Histogram on the second axis\n    sns.histplot(df['Closing Price'], bins=30, kde=True, ax=ax2)\n    ax2.set_title('Histogram of Closing Prices')\n    \n    # Show the plots\n    plt.tight_layout()\n    plt.show()\n    \n    # Return the axes for further manipulation if needed\n    return ax1, ax2"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame with a DateTime index and a 'Close' column for closing prices.\n\n    Returns:\n        Tuple[List[float], Axes]: A tuple containing the forecasted prices and a matplotlib Axes object.\n    \"\"\"\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column for closing prices.\")\n    \n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame index must be a DateTimeIndex.\")\n\n    # Fit the ARIMA model\n    model = ARIMA(df['Close'], order=(5, 1, 0))  # ARIMA parameters can be adjusted as needed\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Close'], label='Historical Prices', color='blue')\n    plt.plot(pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=7, freq='B'), forecast, label='Forecasted Prices', color='orange')\n    plt.title('Share Closing Price Forecast')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.legend()\n    plt.grid()\n    \n    # Get the current Axes object\n    ax = plt.gca()\n    \n    # Show the plot\n    plt.show()\n    \n    return forecast.tolist(), ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Creates a dictionary of all possible two-letter combinations from\n    the lowercase English alphabet and counts their occurrences in the\n    given word. Combinations not found in the word will have a value of 0.\n\n    Parameters:\n    word (str): The input word to analyze.\n\n    Returns:\n    dict: A dictionary with two-letter combinations as keys and their\n    counts in the word as values.\n    \"\"\"\n    # Generate all possible two-letter combinations from lowercase letters\n    letters = string.ascii_lowercase\n    combinations = [''.join(pair) for pair in itertools.product(letters, repeat=2)]\n    \n    # Create a Counter to count occurrences of two-letter combinations in the word\n    counter = Counter()\n    \n    # Convert word to lowercase to ensure case insensitivity\n    word = word.lower()\n    \n    # Count two-letter combinations in the input word\n    for i in range(len(word) - 1):\n        pair = word[i:i + 2]\n        counter[pair] += 1  # Count the combination directly\n    \n    # Create a dictionary with all combinations and their counts\n    result = {combo: counter[combo] for combo in combinations}\n    \n    return result\nword = \"hello world\""}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # If sales_data is not provided, generate random sales data\n    if sales_data is None:\n        np.random.seed(0)  # For reproducibility\n        sales_data = np.random.randint(low=100, high=500, size=periods)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Sales': sales_data})\n    \n    # Prepare the data for linear regression\n    df['Time'] = np.arange(len(df))  # Create a time variable\n    X = df[['Time']]  # Features\n    y = df['Sales']   # Target variable\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Forecast future sales for the same number of periods as input data\n    future_time = np.arange(len(df), len(df) + periods).reshape(-1, 1)  # New time values\n    forecasted_sales = model.predict(future_time)\n    \n    # Return forecasted sales as a numpy array\n    return forecasted_sales"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\nfrom typing import List\ndef task_func(\n    task_list: List[str],\n    n_tasks: int,\n    employees: List[str] = [\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed: int = None,\n) -> pd.DataFrame:\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees for the current date.\n\n    Parameters:\n    - task_list: List of tasks to choose from.\n    - n_tasks: Number of tasks to assign.\n    - employees: List of employees to assign tasks to (default provided).\n    - seed: Optional seed for random number generation for reproducibility.\n\n    Returns:\n    - A DataFrame with columns 'Task Name', 'Assigned To', and 'Due Date'.\n    \"\"\"\n    \n    # Raise ValueError if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"The number of tasks cannot be negative.\")\n    \n    # Set the random seed for reproducibility, if provided\n    if seed is not None:\n        random.seed(seed)\n\n    # Ensure we do not assign more tasks than available\n    n_tasks = min(n_tasks, len(task_list))\n\n    # Create a list to hold the tasks\n    tasks = []\n    \n    # Assign tasks randomly to employees\n    for task in random.sample(task_list, n_tasks):\n        task_name = task.replace(\" \", \"_\")  # Sanitize task name\n        assigned_to = random.choice(employees)  # Randomly assign to an employee\n        due_date = datetime.now().date()  # Get today's date\n        \n        tasks.append({\n            'Task Name': task_name,\n            'Assigned To': assigned_to,\n            'Due Date': due_date\n        })\n    \n    return pd.DataFrame(tasks)"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    \"\"\"Replaces spaces in given words with underscores, calculates frequencies,\n    and plots the frequency of each unique word.\n\n    Args:\n        mystrings (list of str): List of strings to be modified and counted.\n        text (str): Input text from which the frequency of words will be calculated.\n\n    Raises:\n        ValueError: If the input text is empty.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object of the plot.\n    \"\"\"\n    # Check if the input text is empty\n    if not text:\n        raise ValueError(\"The input text is empty.\")\n    \n    # Replace spaces with underscores and normalize case\n    modified_strings = [s.replace(\" \", \"_\").lower() for s in mystrings]\n    \n    # Split the text into words, normalize case, and count frequencies\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    word_count = Counter(words)\n    \n    # Prepare data for plotting in the order of modified_strings\n    filtered_count = {s: word_count[s.lower()] for s in modified_strings if s.lower() in word_count}\n    \n    # Ensure the order of unique words matches their appearance in modified_strings\n    unique_words = list(filtered_count.keys())\n    frequencies = [filtered_count[s] for s in modified_strings if s in unique_words]\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.bar(unique_words, frequencies)\n    ax.set_xlabel('Unique Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of Unique Words')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\nmystrings = [\"hello world\", \"data science\", \"python programming\"]\ntext = \"Hello world! Welcome to the world of data science. Python programming is fun.\""}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zips files with specified extensions from the source directory and saves it to the target directory.\n    \n    Args:\n        source_directory (str): The path to the source directory.\n        target_directory (str): The path to the target directory where the zip file will be saved.\n        zip_name (str): The name for the created zip file (without extension).\n    \n    Returns:\n        str: The full path to the created zip file.\n    \n    Raises:\n        OSError: If the source_directory does not exist.\n    \"\"\"\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Create the target directory if it does not exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # Create the full path for the zip file\n    zip_file_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    \n    # Gather all files with valid extensions\n    files_to_zip = []\n    for ext in valid_extensions:\n        files_to_zip.extend(glob.glob(os.path.join(source_directory, f\"*{ext}\")))\n    \n    # Create a zip file and add the files\n    with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n        for file in files_to_zip:\n            zip_file.write(file, arcname=os.path.basename(file))\n\n    # Return the full path to the created zip file\n    return zip_file_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # Initialize a counter for successfully converted files\n    converted_count = 0\n    \n    # Iterate through each file in the source directory\n    for file_name in os.listdir(source_directory):\n        file_path = os.path.join(source_directory, file_name)\n        name, ext = os.path.splitext(file_name)\n        ext = ext.lower()\n        \n        # Prepare the path for the CSV file (overwrites existing files)\n        target_file_path = os.path.join(target_directory, f\"{name}.csv\")\n        \n        # Process based on file extension\n        try:\n            if ext == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n            elif ext == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([para.text for para in doc.paragraphs])\n            elif ext == '.xlsx':\n                df = pd.read_excel(file_path)\n                content = df.to_string(index=False)\n            elif ext == '.csv':\n                df = pd.read_csv(file_path)\n                content = df.to_string(index=False)\n            else:\n                continue  # Skip unsupported file types\n            \n            # Save the content to a CSV file\n            df_output = pd.DataFrame({'Text': [content]})\n            df_output.to_csv(target_file_path, index=False)  # Overwrite if exists\n            converted_count += 1\n            \n        except Exception as e:\n            print(f\"Error processing file '{file_name}': {e}\")\n    \n    return converted_count"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the MinMax-normalized cumulative sum for each numeric column in the given DataFrame.\n    \n    Parameters:\n    df (pd.DataFrame): Input DataFrame containing numeric data.\n    \n    Returns:\n    pd.DataFrame: A DataFrame where each column contains the normalized cumulative sum\n    of the respective column in the input DataFrame, retaining the original column names.\n    \n    Raises:\n    ValueError: If the DataFrame is empty or contains NaN values.\n    TypeError: If the DataFrame contains non-numeric data types.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check for NaN values\n    if df.isnull().any().any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Check for non-numeric data types\n    if df.select_dtypes(include=[np.number]).shape[1] != df.shape[1]:\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    # Calculate the cumulative sum for each numeric column\n    cumulative_sum = df.cumsum()\n    \n    # Normalize the cumulative sum using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_cumsum = scaler.fit_transform(cumulative_sum)\n    \n    # Create a new DataFrame with the normalized cumulative sums\n    normalized_df = pd.DataFrame(normalized_cumsum, columns=df.columns)\n    \n    return normalized_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    # Check if the provided directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # Create a Path object for the directory\n    directory = Path(directory_path)\n\n    # List to hold the result\n    result = []\n\n    # Iterate through each file in the directory\n    for file in directory.iterdir():\n        if file.is_file():  # Ensure we only process files, not directories\n            # Get file size\n            file_size = file.stat().st_size\n            \n            # Get creation time and modification time\n            # Note: Creation time may not be available on all platforms\n            creation_time = datetime.fromtimestamp(file.stat().st_ctime, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(file.stat().st_mtime, tz=timezone.utc).isoformat()\n            \n            # Append the file details to the result list\n            result.append((file.name, file_size, creation_time, modification_time))\n\n    return result"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n    \n    Parameters:\n    - json_data (str): A JSON formatted string with country and population data.\n    - output_dir (str): Directory where the CSV file will be saved. Defaults to current directory.\n    - file_name (str): Name of the output CSV file. Defaults to 'country_population_report.csv'.\n    \n    Returns:\n    - str: The file path of the generated CSV report.\n    - pd.DataFrame: The DataFrame containing country-population data with columns \"Country\" and \"Population\".\n    \n    Raises:\n    - ValueError: If the JSON data is malformed, empty, contains non-string country names, non-numeric, or negative populations.\n    - IOError: If the file cannot be written to the specified directory.\n    \"\"\"\n    # Parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Malformed JSON data.\")\n\n    # Validate JSON data\n    if not isinstance(data, list) or len(data) == 0:\n        raise ValueError(\"JSON data is empty or not a list.\")\n\n    country_population = []\n\n    for entry in data:\n        if not isinstance(entry, dict):\n            raise ValueError(\"Each entry in JSON data must be an object.\")\n        \n        country = entry.get(\"country\")\n        population = entry.get(\"population\")\n\n        if not isinstance(country, str) or not country.strip():\n            raise ValueError(\"Country names must be non-empty strings.\")\n        \n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Population must be a non-negative number.\")\n        \n        # Round down the population if it's a float\n        population = math.floor(population)\n        \n        country_population.append({\"Country\": country, \"Population\": population})\n\n    # Create DataFrame\n    df = pd.DataFrame(country_population)\n\n    # Prepare output path\n    output_file_path = os.path.join(output_dir, file_name)\n\n    # Save DataFrame to CSV\n    try:\n        df.to_csv(output_file_path, index=False)\n    except IOError as e:\n        raise IOError(f\"Unable to write to file: {output_file_path}. Error: {str(e)}\")\n\n    return output_file_path, df"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if the provided predicates list is valid\n    if not predicates or not isinstance(predicates, list):\n        raise ValueError(\"No valid predicates provided. Please provide a list of predicates.\")\n    \n    # Validate and deduplicate predicates\n    unique_predicates = []\n    for predicate in predicates:\n        if isinstance(predicate, str):\n            try:\n                re.compile(predicate)  # Check if the pattern is valid\n                if predicate not in unique_predicates:\n                    unique_predicates.append(predicate)\n            except re.error:\n                raise ValueError(f\"Invalid regex pattern provided: '{predicate}'\")\n        else:\n            raise ValueError(f\"Predicate must be a string, got '{type(predicate).__name__}'\")\n\n    # Check if the directory exists and is indeed a directory\n    dir_path_obj = Path(dir_path)\n    if not dir_path_obj.exists() or not dir_path_obj.is_dir():\n        raise FileNotFoundError(f\"The specified directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Prepare the result dictionary\n    result = {}\n    \n    # Iterate through items in the directory\n    for item in dir_path_obj.iterdir():\n        item_name = item.name\n        # Initialize the evaluation result for the current item\n        evaluation_results = []\n        \n        # Check each predicate against the item name\n        for predicate in unique_predicates:\n            if re.match(predicate, item_name):\n                evaluation_results.append(True)\n            else:\n                evaluation_results.append(False)\n        \n        # Store the evaluation results in the result dictionary\n        result[item_name] = evaluation_results\n    \n    return result"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    \"\"\"\n    Convert a hexadecimal string to various encodings.\n\n    Parameters:\n    hex_string (str): A hexadecimal string to be converted.\n\n    Returns:\n    dict: A dictionary containing the input string encoded in various formats.\n          If ASCII is not possible, the value for 'ASCII' will be 'Not representable in ASCII'.\n    \"\"\"\n    # Decode the hexadecimal string to bytes\n    try:\n        byte_data = binascii.unhexlify(hex_string)\n    except binascii.Error:\n        return {'error': 'Invalid hexadecimal string'}\n    \n    # Convert bytes to UTF-8 string\n    utf8_string = byte_data.decode('utf-8', errors='replace')\n\n    # Prepare the result dictionary\n    result = {\n        'hex': hex_string,\n        'base64': base64.b64encode(byte_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': byte_data.decode('utf-16', errors='replace'),\n        'utf-32': byte_data.decode('utf-32', errors='replace'),\n        'ASCII': utf8_string if all(ord(c) < 128 for c in utf8_string) else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n    \n    return result"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef calculate_md5(file_path):\n    \"\"\"Calculate the MD5 checksum of a file.\"\"\"\n    hash_md5 = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\ndef task_func(url):\n    \"\"\"\n    Downloads a tar.gz file, validates its MD5 checksum, and extracts its contents.\n\n    Parameters:\n        url (str): The URL of the tar.gz file to download.\n\n    Returns:\n        bool: True if the file is successfully downloaded, checksum is valid, and file is extracted.\n              False if the checksum does not match or the download fails.\n    \"\"\"\n    try:\n        # Download the file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Validate MD5 checksum\n        md5_checksum = calculate_md5(TARGET_TAR_FILE)\n        if md5_checksum != EXPECTED_MD5_CHECKSUM:\n            print(\"Checksum does not match. Deleting the file.\")\n            os.remove(TARGET_TAR_FILE)\n            return False\n        \n        # Extract the tar.gz file\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        \n        print(\"File downloaded and extracted successfully.\")\n        return True\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        # Ensure the file is deleted if it was created\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n        return False"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    # Download the CSV file\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise RuntimeError(f\"Error downloading the file from {url}: {e}\")\n\n    # Initialize a counter for occurrences\n    occurrences = collections.defaultdict(int)\n\n    try:\n        # Open the downloaded CSV file and read its contents\n        with open(csv_file_path, mode='r', newline='') as csvfile:\n            reader = csv.DictReader(csvfile)\n\n            # Check if column_name exists in the CSV\n            if column_name not in reader.fieldnames:\n                os.remove(csv_file_path)  # Delete the file if column is missing\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value = row[column_name]\n                occurrences[value] += 1\n\n    except csv.Error as e:\n        os.remove(csv_file_path)  # Delete the file on CSV processing error\n        raise RuntimeError(f\"CSV processing error: {e}\")\n    except Exception as e:\n        os.remove(csv_file_path)  # Delete the file on other processing error\n        raise RuntimeError(f\"Error processing the CSV file: {e}\")\n\n    # Delete the downloaded CSV file after processing\n    os.remove(csv_file_path)\n\n    # Convert the defaultdict to a regular dict before returning\n    return dict(occurrences)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n\n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n\n        # Check if the expected structure exists\n        items = root.findall('.//item')\n        if not items:\n            raise ValueError(\"XML structure does not match expected format.\")\n\n        # Extract data and create a DataFrame\n        data = []\n        for item in items:\n            item_data = {}\n            for child in item:\n                item_data[child.tag] = child.text if child.text is not None else ''\n            data.append(item_data)\n\n        # Create DataFrame\n        df = pd.DataFrame(data)\n\n        return df\n\n    except urllib.error.URLError as e:\n        raise ValueError(\"Invalid URL or failed to fetch the XML file.\") from e\n    except etree.XMLSyntaxError as e:\n        raise ValueError(\"The XML file has invalid syntax.\") from e\n    except Exception as e:\n        raise ValueError(\"An error occurred while processing the XML file.\") from e"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from a specified URL, counts the frequency of each word,\n    and plots a bar chart showing the ten most frequently occurring words.\n    \n    Parameters:\n        url (str): The URL of the plain text file to download.\n    \n    Returns:\n        tuple: A tuple containing:\n            - Counter: A Counter object with word frequencies.\n            - Axes: A matplotlib Axes object representing the plotted bar chart.\n    \"\"\"\n    try:\n        # Step 1: Download the text file from the specified URL\n        response = urllib.request.urlopen(url)\n        text = response.read().decode('utf-8')  # Decode the bytes to a string\n    except Exception as e:\n        print(f\"Error downloading or decoding the file: {e}\")\n        return None, None  # Return None for both if an error occurs\n\n    # Step 2: Process the text to count the frequency of each word\n    words = re.findall(r'\\b\\w+\\b', text)  # Case-sensitive word extraction\n    word_counts = Counter(words)\n\n    # Step 3: Get the ten most common words\n    most_common_words = word_counts.most_common(10)\n    words, counts = zip(*most_common_words)  # Unzip the tuples into two lists\n\n    # Step 4: Plot a bar chart of the ten most common words\n    fig, ax = plt.subplots()\n    ax.bar(words, counts, color='skyblue')\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45)\n    ax.grid(axis='y')  # Add gridlines for better readability\n    plt.tight_layout()\n\n    # Show the plot (optional - can be commented out if not needed)\n    plt.show()\n\n    return word_counts, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n\n        # Check if the content type is a ZIP file\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Define the path for the downloaded file\n        zip_file_path = os.path.join(download_path, \"downloaded_file.zip\")\n\n        # Write the content to a ZIP file\n        with open(zip_file_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract the ZIP file\n        try:\n            with ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        return download_path  # Return the directory containing the extracted contents\n\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts data from a specified HTML table and returns it as a Pandas DataFrame.\n    \n    Parameters:\n    - url (str): The URL of the webpage containing the table.\n    - table_id (str): The ID of the table to extract.\n    \n    Returns:\n    - pd.DataFrame: DataFrame containing the extracted data. Returns an empty DataFrame if the table exists but has no rows.\n    \n    Raises:\n    - requests.exceptions.HTTPError: If the HTTP request fails (e.g., connection issues).\n    - ValueError: If no table with the specified 'table_id' is found on the webpage.\n    \"\"\"\n    # Send a GET request to the specified URL\n    response = requests.get(url)\n    \n    # Raise an HTTPError for unsuccessful requests\n    response.raise_for_status()\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find the table with the specified ID\n    table = soup.find('table', id=table_id)\n    \n    # Check if the table exists\n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n    \n    # Find all rows in the table\n    rows = table.find_all('tr')\n    \n    # If there are no rows, return an empty DataFrame\n    if not rows:\n        return pd.DataFrame()\n    \n    # Extract headers from the first row\n    headers = [header.get_text(strip=True) for header in rows[0].find_all('th')]\n    \n    # If there are no headers, use a default column labeling\n    if not headers:\n        headers = [f'Column {i+1}' for i in range(len(rows[1].find_all('td')))]\n    \n    # Extract data from the subsequent rows\n    data = []\n    for row in rows[1:]:\n        cells = row.find_all('td')\n        data.append([cell.get_text(strip=True) for cell in cells])\n    \n    # Create a DataFrame using the extracted headers and data\n    df = pd.DataFrame(data, columns=headers)\n    \n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads a ZIP file from a specified URL and extracts it to a specified directory.\n    \n    Args:\n        url (str): The URL of the ZIP file to download.\n        filename (str): The name to save the downloaded ZIP file as.\n    \n    Returns:\n        tuple: A tuple containing a status message and a list of filenames in the unzipped directory, \n               or an empty list if extraction fails.\n    \"\"\"\n    # Ensure the download and unzip directories exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n\n    zip_file_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the zip file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n        \n        # Write the content to a file\n        with open(zip_file_path, 'wb') as file:\n            file.write(response.content)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Unable to download the file. \" + str(e), [])\n    \n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            # Get the list of extracted filenames\n            extracted_files = zip_ref.namelist()\n        return (\"Success: File downloaded and extracted.\", extracted_files)\n\n    except (zipfile.BadZipFile, FileNotFoundError, PermissionError) as e:\n        return (\"Error: Unable to extract the zip file. \" + str(e), [])"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        # Send a GET request to the specified URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n    except requests.RequestException as e:\n        print(f\"Error fetching {url}: {e}\")\n        return 0\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Set to hold unique absolute URLs\n    unique_links = set()\n\n    # Find all anchor tags with href attributes\n    for link in soup.find_all('a', href=True):\n        # Create absolute URL\n        absolute_url = urljoin(url, link['href'])\n        # Optionally filter the links here\n        if absolute_url.startswith(\"http\"):\n            unique_links.add(absolute_url)\n\n    # Write unique absolute URLs to a CSV file\n    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\nimport os\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    # Check if the input is a local file or a URL\n    if os.path.isfile(webpage_url):\n        with open(webpage_url, 'r') as f:\n            content = f.read()\n    else:\n        try:\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Raises an HTTPError for bad responses\n            content = response.content\n        except requests.RequestException as e:\n            raise requests.RequestException(f\"Network issue occurred: {e}\")\n\n    # Parse the HTML content\n    tree = html.fromstring(content)\n    tables = tree.xpath('//table')\n\n    if not tables:\n        return 0  # No tables found\n\n    # Use pandas to read the first HTML table\n    table_html = tables[0]\n    \n    try:\n        df = pd.read_html(html.tostring(table_html))[0]\n    except ValueError:\n        return 0  # Table is empty or not readable\n\n    # Connect to the SQLite database\n    try:\n        with sqlite3.connect(database_name) as conn:\n            # Replace the existing table\n            df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        raise sqlite3.DatabaseError(f\"Database error occurred: {e}\")\n\n    return len(df)  # Return the number of rows stored in the table"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    # Initialize the comment variable to an empty string\n    comment = \"\"\n\n    try:\n        # Open the image file\n        with Image.open(filename) as img:\n            # Perform OCR on the image to extract text\n            extracted_text = pytesseract.image_to_string(img)\n\n        # Convert the extracted text to the target encoding\n        comment = extracted_text.encode(from_encoding).decode(to_encoding)\n\n    except (UnicodeDecodeError, LookupError) as e:\n        raise ValueError(f\"Encoding conversion error: {e}\")\n\n    except Exception as e:\n        print(f\"OCR extraction failed: {e}\")\n\n    # If OCR extraction fails or comment is still empty, attempt to read the image comment\n    if not comment:\n        try:\n            with Image.open(filename) as img:\n                img_comment = img.info.get('comment', '')\n                if img_comment:\n                    comment = img_comment.encode(from_encoding).decode(to_encoding)\n        except (UnicodeDecodeError, LookupError) as e:\n            raise ValueError(f\"Encoding conversion error for image comment: {e}\")\n        except Exception as e:\n            print(f\"Image comment processing failed: {e}\")\n\n    return comment"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf-8\") -> dict:\n    try:\n        # Initiate an HTTP GET request with a timeout of 5 seconds\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Get the raw bytes content\n        raw_content = response.content\n        \n        # Handle empty content\n        if not raw_content:\n            return {}\n        \n        # Detect encoding if not provided\n        if from_encoding is None:\n            result = chardet.detect(raw_content)\n            from_encoding = result['encoding']\n            \n            # Raise an exception if encoding is undetectable\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the raw bytes using the detected or specified encoding\n        decoded_content = raw_content.decode(from_encoding)\n\n        # Re-encode the content to the specified 'to_encoding'\n        re_encoded_content = decoded_content.encode(to_encoding)\n\n        # Parse the JSON from the re-encoded content\n        json_data = json.loads(re_encoded_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        return {}\n    except ValueError as ve:\n        raise ValueError(f\"ValueError: {ve}\")\n    except json.JSONDecodeError as je:\n        return {}\n    except Exception as e:\n        return {}"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"Reads a CSV file, processes its date-related data, and returns a sorted DataFrame.\n\n    Args:\n        csv_file_path (str): The path to the CSV file.\n        column_name (str): The name of the date column to process.\n        date_format (str): The format of the date column (default is \"%Y-%m-%d\").\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered and sorted date data.\n\n    Raises:\n        FileNotFoundError: If the specified CSV file is not found at the given path.\n        ValueError: If the specified column is not present in the CSV file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"The file '{csv_file_path}' was not found.\")\n    \n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()  # Return an empty DataFrame if the file is empty\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not present in the CSV file.\")\n    \n    # Convert the date column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n    \n    # Filter out rows with invalid dates\n    df = df[df[column_name].notna()]\n    \n    # Get the current date\n    current_date = datetime.now()\n    \n    # Filter rows based on the current date\n    filtered_df = df[df[column_name] >= current_date]\n    \n    # Sort the resulting DataFrame by the date column\n    sorted_df = filtered_df.sort_values(by=column_name)\n    \n    return sorted_df.reset_index(drop=True)"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    Secures a client socket using SSL/TLS and computes the SHA256 hash of a requested file.\n\n    Args:\n        client_socket: The client socket to secure.\n        cert_file: Path to the SSL certificate file.\n        key_file: Path to the SSL key file.\n        buffer_size: The size of the buffer to read files (default is 1024 bytes).\n\n    Returns:\n        str: The SHA256 hash of the requested file or an error message.\n    \"\"\"\n    # Wrap the socket with SSL\n    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n    # Secure the client socket\n    secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n    try:\n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode('utf-8').strip()\n\n        if not file_path:\n            return \"No file path provided\"\n\n        # Check if the file exists\n        if not os.path.isfile(file_path):\n            return \"File not found\"\n\n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b\"\"):\n                sha256_hash.update(byte_block)\n\n        # Return the SHA256 hash as a hexadecimal string\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n    finally:\n        secure_socket.close()"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Starts a non-blocking echo server that appends the current time to received data\n    and sends it back to clients. Handles exceptional conditions for each socket.\n\n    Parameters:\n    - server_address: The address the server listens on.\n    - server_port: The port the server listens on.\n    - buffer_size: The size of the buffer for incoming data.\n    - run_duration: The time in seconds for which the server will run.\n    \n    Returns:\n    - str: A status message indicating server operation and run duration.\n    \"\"\"\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen()\n\n    # Set the socket to non-blocking mode\n    server_socket.setblocking(False)\n\n    # Create lists to manage sockets and queues\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n\n    end_time = datetime.now() + timedelta(seconds=run_duration)\n    status_message = \"Server is running...\"\n\n    print(f\"{status_message} (Duration: {run_duration} seconds)\")\n\n    while datetime.now() < end_time:\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server_socket:\n                # Handle the server socket\n                client_socket, client_address = s.accept()\n                print(f\"Connection from {client_address}\")\n                client_socket.setblocking(False)\n                inputs.append(client_socket)\n                message_queues[client_socket] = queue.Queue()\n            else:\n                # Handle client sockets\n                try:\n                    data = s.recv(buffer_size)\n                    if data:\n                        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                        response = f\"{data.decode()} - Received at {current_time}\"\n                        message_queues[s].put(response)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        # No data means the client has closed the connection\n                        print(f\"Closing connection to {s.getpeername()}\")\n                        clean_up_socket(s, inputs, outputs, message_queues)\n                except Exception as e:\n                    print(f\"Error handling data from {s.getpeername()}: {e}\")\n                    clean_up_socket(s, inputs, outputs, message_queues)\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                outputs.remove(s)\n            else:\n                print(f\"Sending message to {s.getpeername()}\")\n                s.send(next_msg.encode())\n\n        for s in exceptional:\n            print(f\"Handling exceptional condition for {s.getpeername()}\")\n            clean_up_socket(s, inputs, outputs, message_queues)\n\n    # Clean up remaining sockets\n    for s in inputs:\n        s.close()\n\n    return status_message\ndef clean_up_socket(s, inputs, outputs, message_queues):\n    \"\"\"Helper function to clean up sockets and associated queues.\"\"\"\n    if s in outputs:\n        outputs.remove(s)\n    inputs.remove(s)\n    s.close()\n    del message_queues[s]"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\nimport re\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef is_valid_email(email):\n    \"\"\"Validate the email format using a regular expression.\"\"\"\n    regex = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\n    return re.match(regex, email) is not None\ndef task_func(client_socket):\n    \"\"\"Receive a message from a client socket and send it as an email via an SMTP server.\"\"\"\n    # Receive the message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Ask for sender's email and password\n    sender_email = input(\"Enter sender's email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    \n    # Validate email addresses\n    if not is_valid_email(sender_email) or not is_valid_email(recipient_email):\n        print(\"Invalid email address format. Please try again.\")\n        client_socket.close()\n        return None\n    \n    sender_password = getpass.getpass(\"Enter sender's email password: \")\n\n    # Create the email message\n    email_message = EmailMessage()\n    email_message.set_content(message)\n    email_message['Subject'] = 'Message from Client Socket'\n    email_message['From'] = sender_email\n    email_message['To'] = recipient_email\n\n    try:\n        # Connect to the SMTP server\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n            smtp.starttls()  # Upgrade the connection to secure\n            smtp.login(sender_email, sender_password)  # Log in to the SMTP server\n            smtp.send_message(email_message)  # Send the email\n            print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Failed to send email: {e}\")\n    finally:\n        # Close the client socket\n        client_socket.close()\n    \n    return None  # Explicit return as per the requirement"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes a CSV file to generate a histogram of the ten most common words,\n    excluding predefined stopwords.\n\n    Parameters:\n    file_path (str): Path to the CSV file.\n    save_path (str, optional): Path to save the histogram image. If None, the histogram will be displayed.\n\n    Returns:\n    matplotlib.axes.Axes or None: Axes object if save_path is not provided, otherwise None.\n    \"\"\"\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n\n        # Check if the DataFrame has a 'Text' column or use the first column\n        if 'Text' in df.columns:\n            text_data = df['Text']\n        elif df.shape[1] > 0:\n            text_data = df.iloc[:, 0]\n        else:\n            raise ValueError(\"The CSV file does not contain any usable text data.\")\n\n        # Validate text data\n        if text_data.isnull().all() or not issubclass(text_data.dtype.type, str):\n            raise ValueError(\"The text data is empty or not valid.\")\n\n        # Initialize CountVectorizer with the stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_counts = vectorizer.fit_transform(text_data)\n\n        # Sum up the counts of each word\n        word_sum = word_counts.sum(axis=0)\n\n        # Create a dictionary of words and their counts\n        word_freq = {word: word_sum[0, idx] for word, idx in vectorizer.vocabulary_.items()}\n\n        # Get the ten most common words\n        sorted_words = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)[:10]\n        words, counts = zip(*sorted_words)\n\n        # Create a histogram\n        fig, ax = plt.subplots()\n        ax.bar(words, counts)\n        ax.set_ylabel('Frequency')\n        ax.set_title('Top 10 Most Common Words')\n\n        # Rotate the x labels for better readability\n        plt.xticks(rotation=45)\n\n        if save_path:\n            plt.savefig(save_path)\n            plt.close(fig)  # Close the figure to free memory\n            return None\n        else:\n            plt.show()\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Predefined lists\n    default_animals = ['Dog', 'Cat', 'Bird', 'Fish']\n    default_foods = ['Meat', 'Fish', 'Seeds', 'Vegetables']\n    \n    # Handle cases where animals or foods are not provided\n    if animals is None or len(animals) == 0:\n        animals = default_animals\n    if foods is None or len(foods) == 0:\n        foods = default_foods\n    \n    # If both lists are empty, return an empty DataFrame\n    if not animals or not foods:\n        return pd.DataFrame()\n    \n    # Generate combinations using itertools.product\n    combinations = list(itertools.product(animals, foods))\n    \n    # Create an empty DataFrame with animals as index and foods as columns\n    df = pd.DataFrame(index=animals, columns=foods)\n    \n    # Fill the DataFrame with 'animal:food' strings\n    for animal, food in combinations:\n        df.at[animal, food] = f'{animal}:{food}'\n    \n    # Shuffle the DataFrame randomly\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    return df"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates the average time difference in seconds between each consecutive\n    pair of timestamps in a given list after converting them to a specified timezone.\n    \n    Parameters:\n    - time_strings: List of timestamp strings in the format 'YYYY-MM-DD HH:MM:SS'.\n    - timezone: String representation of the timezone.\n    \n    Returns:\n    - float: The mean (average) time difference in seconds between each consecutive \n             pair of timestamps. Returns 0.0 if there are less than two timestamps \n             or if there are no time differences.\n    \"\"\"\n    \n    # Check if the list contains less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Create a timezone object and handle invalid timezone\n    try:\n        tz = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(\"Invalid timezone provided.\")\n    \n    # Convert each time string to the specified timezone\n    timestamps = []\n    for time_string in time_strings:\n        try:\n            # Parse the time string into a naive datetime object\n            naive_time = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n            # Localize it to UTC\n            utc_time = pytz.utc.localize(naive_time)\n            # Convert to the specified timezone\n            localized_time = utc_time.astimezone(tz)\n            # Append the timestamp in seconds since epoch for easy calculation\n            timestamps.append(localized_time.timestamp())\n        except ValueError as e:\n            raise ValueError(f\"Error parsing timestamp '{time_string}': {e}\")\n    \n    # Calculate the absolute time differences in seconds between consecutive timestamps\n    time_diffs = np.abs(np.diff(timestamps))\n    \n    # If there are no time differences, return 0.0\n    if len(time_diffs) == 0:\n        return 0.0\n    \n    # Calculate and return the mean of the time differences\n    return float(np.mean(time_diffs))\ntime_strings = [\"2023-10-01 12:00:00\", \"2023-10-01 12:05:00\", \"2023-10-01 12:10:00\"]\ntimezone = \"America/New_York\""}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    \"\"\"\n    Analyzes the frequency of words in the given text, plots the top 10 most common words,\n    and returns the results.\n\n    Parameters:\n    text (str): The input text to analyze.\n\n    Returns:\n    list: A list of tuples containing the 10 most common words and their counts.\n    Axes: The matplotlib Axes object of the bar chart.\n    \"\"\"\n    if not isinstance(text, str):\n        raise ValueError(\"Input must be a string.\")\n    \n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation using regex and split into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    if not words:\n        return [], plt.gca()  # Return empty list and current Axes if no words found\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words and their counts\n    most_common = word_counts.most_common(10)\n    \n    # Plotting the top 10 most common words\n    words, counts = zip(*most_common)\n    \n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    plt.xticks(rotation=45)\n    ax.grid(axis='y')  # Add grid lines for better visibility\n    \n    # Show plot\n    plt.tight_layout()\n    plt.show()\n    \n    return most_common, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = r'https?://[^\\s]+'\n    match = re.search(url_pattern, myString)\n    \n    if not match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = match.group(0)\n    \n    # Check if the URL is valid\n    parsed_url = urlparse(url)\n    if not all([parsed_url.scheme in ('http', 'https'), parsed_url.netloc]):\n        return \"No valid URL found in the provided string.\"\n    \n    try:\n        # Fetch the content of the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the HTML content and extract the title\n    soup = BeautifulSoup(response.content, 'html.parser')\n    title_tag = soup.title\n    \n    if title_tag is None:\n        return \"No title tag found in the webpage.\"\n    \n    return title_tag.string.strip() if title_tag.string else \"No title tag found in the webpage.\""}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with a specified key, downloads the file from that URL,\n    and saves it with a timestamped filename.\n\n    Parameters:\n    - json_data (str): JSON string containing the URL.\n    - unknown_key (str): Key to look up the URL in the JSON data.\n    - save_dir (str, optional): Directory to save the downloaded file. Defaults to current working directory.\n\n    Returns:\n    - str: The absolute path of the downloaded file.\n    \n    Raises:\n    - ValueError: If the JSON is invalid or the key does not exist.\n    - Exception: If the file download fails.\n    \"\"\"\n    # Parse the JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data provided.\")\n    \n    # Extract the URL associated with the specified unknown_key\n    url = data.get(unknown_key)\n    if not url or not (url.startswith(\"http://\") or url.startswith(\"https://\")):\n        raise ValueError(f\"No valid URL found for key: {unknown_key}\")\n\n    # Get current timestamp for the filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    \n    # Create the filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the directory to save the file\n    if save_dir is None:\n        save_dir = os.getcwd()\n    \n    # Ensure the save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Full path for the saved file\n    file_path = os.path.join(save_dir, filename)\n    \n    # Download the file\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n    else:\n        raise Exception(f\"Failed to download file from URL: {url}, Status Code: {response.status_code}\")\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n\n    Args:\n        url (str): The URL to fetch content from or the path to a local file.\n        output_path (str): The file path to save the extracted phone numbers in JSON format.\n\n    Returns:\n        list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Define a regex pattern for phone numbers\n    phone_pattern = re.compile(r'\\+?(\\d[\\d -]{7,}\\d)')\n\n    try:\n        # Check if the input is a URL or a local file\n        if url.startswith('http://') or url.startswith('https://'):\n            # Fetch the content from the URL\n            response = requests.get(url)\n            response.raise_for_status()  # Raise an error for bad responses\n            content = response.text\n        else:\n            # Read the content from a local file\n            with open(url, 'r', encoding='utf-8') as file:\n                content = file.read()\n\n        # Parse the HTML content\n        soup = bs4.BeautifulSoup(content, 'html.parser')\n\n        # Extract text from the parsed HTML\n        text = soup.get_text()\n\n        # Find all phone numbers using the regex pattern\n        phone_numbers = phone_pattern.findall(text)\n\n        # Remove duplicates by converting to a set\n        unique_phone_numbers = list(set(phone_numbers))\n\n        # Save the phone numbers to the specified output file in JSON format\n        with open(output_path, 'w', encoding='utf-8') as output_file:\n            json.dump(unique_phone_numbers, output_file, ensure_ascii=False, indent=4)\n\n        return unique_phone_numbers\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []  # Return an empty list on error"}
