{"task_id": "BigCodeBench/13", "solution": "Below is a self-contained Python code snippet that fulfills the requirements you've outlined. This code attempts to download all files from a specified directory on an FTP server using `wget` in a subprocess. It handles exceptions for connection, login, and directory change failures, and returns a list of filenames that were attempted to be downloaded.\n\n```python\nimport subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    # Attempt to connect to the FTP server\n    try:\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    # Attempt to log in to the FTP server\n    try:\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    # Attempt to change to the specified directory\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    # Attempt to list files in the directory\n    try:\n        filenames = ftp.nlst()\n    except ftplib.error_perm as e:\n        # Handle empty directory or access denied\n        raise Exception(f\"Failed to list directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    finally:\n        ftp.quit()\n    \n    downloaded_files = []\n    for filename in filenames:\n        # Construct the full URL for the file\n        file_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        # Use wget to download the file\n        try:\n            subprocess.run(['wget', file_url], check=True)\n            downloaded_files.append(filename)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {filename}: {str(e)}\")\n    \n    return downloaded_files\n\n# Example usage\nif __name__ == \"__main__\":\n    downloaded_files = task_func()\n    print(\"Downloaded files:\", downloaded_files)\n```\n\nThis script does the following:\n- Connects to the FTP server using the provided credentials.\n- Changes to the specified directory on the FTP server.\n- Lists all files in the directory.\n- Attempts to download each file using `wget` via a subprocess call.\n- Handles exceptions for connection, login, directory change failures, and download errors.\n- Returns a list of filenames that were attempted to be downloaded.\n\nPlease ensure you have `wget` installed on your system for this script to work. This script assumes that `wget` is available in the system's PATH."}
{"task_id": "BigCodeBench/15", "solution": "Below is a self-contained Python code snippet that fulfills the described requirements. This code reads a list of shell commands from a CSV file, executes each command, and saves the output or error message into separate files within a specified output directory.\n\n```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists, raise FileNotFoundError if it does not\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Ensure the output directory exists, create it if it does not\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    output_files = []  # To store paths of the output files\n    \n    with open(commands_file_path, mode='r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        for index, row in enumerate(reader):\n            if row:  # Check if the row is not empty\n                command = row[0]  # Assuming each row contains a single command\n                output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n                output_files.append(output_file_path)\n                \n                try:\n                    # Execute the command\n                    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                    # Write stdout to the file if command succeeds\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(result.stdout)\n                except subprocess.CalledProcessError as e:\n                    # Write stderr and exit code to the file if command fails\n                    with open(output_file_path, 'w') as output_file:\n                        output_file.write(f\"Error executing command: {e.stderr}\\nExit code: {e.returncode}\")\n    \n    return output_files\n\n# Example usage\nif __name__ == \"__main__\":\n    commands_file_path = 'commands.csv'  # Path to your CSV file containing commands\n    output_dir_path = 'command_outputs'  # Desired output directory for command outputs\n    try:\n        output_files = task_func(commands_file_path, output_dir_path)\n        print(\"Output files created:\")\n        for output_file in output_files:\n            print(output_file)\n    except FileNotFoundError as e:\n        print(e)\n```\n\nThis script assumes that the CSV file contains one command per row, and it does not handle CSV files with multiple columns where the command might be split across columns. Adjustments would be needed to handle such cases."}
{"task_id": "BigCodeBench/17", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This code uses the `psutil` library to check if a process is running by name, and the `subprocess` library to start or terminate and restart the process. Note that to use `psutil`, you might need to install it first using `pip install psutil` if it's not already installed in your environment.\n\n```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(attrs=['name']):\n        if proc.info['name'] == process_name:\n            # Process found, terminate it\n            proc.terminate()\n            proc.wait()  # Wait for the process to terminate\n            # Restart the process\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n\n    # If the process was not found, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\"\n\n# Example usage\nif __name__ == \"__main__\":\n    process_name = \"notepad.exe\"  # Example process name, change as needed\n    print(task_func(process_name))\n```\n\n### Important Notes:\n- This code assumes that the process can be started by simply calling its name as a command. Depending on the specific process you're trying to manage, additional path or command line arguments may be necessary.\n- The `Popen` method from `subprocess` is used to start the process. This method does not wait for the process to complete, making it suitable for starting background processes.\n- The `terminate` method sends a SIGTERM signal on Unix or uses TerminateProcess on Windows. Some processes might handle this signal to clean up before exiting, but others might not terminate as expected. If the process does not terminate, you might need to use `kill()` instead, which is a more forceful way of stopping the process.\n- This script requires appropriate permissions to terminate and start processes on the system.\n- Always test scripts that terminate and restart processes in a safe environment to ensure they work as expected and do not inadvertently affect critical system processes."}
{"task_id": "BigCodeBench/19", "solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The specified directory does not exist: {directory}\")\n    \n    # Find all files (not directories) in the specified directory\n    files = [f for f in glob.glob(os.path.join(directory, \"*\")) if os.path.isfile(f)]\n    \n    # Proceed only if there are files to zip\n    if not files:\n        return None\n    \n    # Define the zip file name\n    zip_filename = os.path.join(directory, \"files.zip\")\n    \n    # Create a zip file and add files to it\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        for file in files:\n            # Add file to zip, arcname is the name within the zip file\n            # os.path.basename(file) ensures only the file name is added, not the path\n            zipf.write(file, arcname=os.path.basename(file))\n    \n    # Return the path to the created zip file\n    return zip_filename\n\n# Example usage\n# directory_path = \"/path/to/your/directory\"\n# print(task_func(directory_path))\n```\nThis code snippet defines a function `task_func` that takes a directory path as input. It first checks if the directory exists, raising a `FileNotFoundError` if it does not. It then finds all files (excluding directories) within the specified directory. If there are files, it creates a zip file named `files.zip` in the same directory and adds the files to it. The function returns the path to the created zip file or `None` if there were no files to zip."}
{"task_id": "BigCodeBench/34", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This code first defines a function `task_func` that takes a string `text` as input. It then removes URLs from the text using a regular expression, checks if there are any words left to generate a word cloud, and if so, generates and plots the word cloud. If there are no words left after removing URLs, it raises a `ValueError`.\n\n```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Regular expression to match URLs\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    \n    # Remove URLs from the text\n    text_without_urls = re.sub(url_pattern, '', text)\n    \n    # Check if there are words left in the text\n    if not text_without_urls.strip():\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate a word cloud\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = set(), \n                min_font_size = 10).generate(text_without_urls)\n    \n    # Plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n  \n    plt.show()\n    \n    return wordcloud\n\n# Example usage\ntext = \"This is a sample text with a URL https://example.com and some other words.\"\ntry:\n    wordcloud = task_func(text)\nexcept ValueError as e:\n    print(e)\n```\n\nThis code uses the `re` module to remove URLs from the input text. The `WordCloud` object from the `wordcloud` library is then used to generate a word cloud from the remaining text. If the text is empty after URL removal, a `ValueError` is raised. The generated word cloud is plotted using `matplotlib`.\n\nPlease ensure you have the `wordcloud` and `matplotlib` libraries installed in your Python environment. You can install them using pip if necessary:\n\n```sh\npip install wordcloud matplotlib\n```"}
{"task_id": "BigCodeBench/37", "solution": "Below is a self-contained code snippet that defines the `task_func` function. This function takes a pandas DataFrame (`df`) and a string (`target_column`) indicating the column in the DataFrame to be used as the target for a classification task. The function trains a RandomForestClassifier on the data, plots the feature importances in a bar plot, and returns the trained RandomForestClassifier and the Axes object of the plot.\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef task_func(df, target_column):\n    # Splitting the dataframe into features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    # Splitting the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initializing and training the RandomForestClassifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_train, y_train)\n    \n    # Getting the feature importances\n    feature_importances = clf.feature_importances_\n    \n    # Creating a pandas series with feature names and their importance scores\n    features = pd.Series(feature_importances, index=X.columns).sort_values(ascending=False)\n    \n    # Plotting the feature importances\n    plt.figure(figsize=(10, 8))\n    sns.barplot(x=features, y=features.index)\n    \n    # Setting the plot labels and title\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    \n    # Returning the classifier and the Axes object of the plot\n    return clf, plt.gca()\n\n# Example usage (assuming you have a suitable DataFrame `df` and a target column name `target_column`):\n# clf, ax = task_func(df, 'target_column_name')\n```\n\nThis code assumes that you have a suitable DataFrame `df` and a target column name `target_column`. It uses `train_test_split` from `sklearn.model_selection` to split the data into training and testing sets, trains a `RandomForestClassifier` on the training data, and then plots the feature importances using seaborn's barplot function. The feature importances are sorted in descending order as required."}
{"task_id": "BigCodeBench/82", "solution": "Certainly! Below is a self-contained code snippet that creates a Flask application configured for user authentication using Flask-Login and Flask-WTF for the login form. It includes routes for login, logout, and a protected page. The application uses dynamic configuration for the secret key and template folder.\n\n```python\nfrom flask import Flask, render_template, redirect, url_for, request, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\n# Define the LoginForm class\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\n# Initialize Flask-Login's login manager\nlogin_manager = LoginManager()\n\n# User class\nclass User(UserMixin):\n    # For simplicity, we're using a static user\n    user_database = {\n        \"admin\": {\"password\": generate_password_hash(\"adminpass\"), \"id\": 1}\n    }\n    def __init__(self, username):\n        self.id = self.user_database[username][\"id\"]\n        self.username = username\n\n    @classmethod\n    def get(cls, id):\n        for username, data in cls.user_database.items():\n            if data[\"id\"] == id:\n                return cls(username)\n        return None\n\n    @classmethod\n    def authenticate(cls, username, password):\n        user = cls.user_database.get(username)\n        if user and check_password_hash(user['password'], password):\n            return cls(username)\n        return None\n\n# User loader callback for Flask-Login\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.get(user_id)\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.secret_key = secret_key\n    login_manager.init_app(app)\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            user = User.authenticate(username, password)\n            if user:\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                flash('Invalid username or password')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('home'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Protected area, Hi {current_user.username}! <a href=\"/logout\">Logout</a>'\n\n    return app\n\n# Example usage\nif __name__ == '__main__':\n    app = task_func('your_secret_key', 'path/to/your/templates')\n    app.run(debug=True)\n```\n\nThis code defines a simple user authentication system with a single user for demonstration purposes. The `User` class is a basic representation of a user that Flask-Login can work with, including methods for authentication and user loading. The `task_func` function dynamically configures the Flask application with a secret key and template folder path, which you need to replace with your actual secret key and the path to your templates directory.\n\nRemember, for a real-world application, you would need to implement a more secure user management system, likely involving a database to store user credentials securely."}
{"task_id": "BigCodeBench/89", "solution": "Certainly! Below is a self-contained Python code snippet that defines a function `task_func`. This function takes a dataset (`data`), a specified column name (`column`), and a Z-score threshold (`outlier_z_score`) as inputs. It identifies and removes outliers from the specified column based on the Z-score, visualizes the data before and after outlier removal, and returns a tuple containing the original data, the data without outliers, and the indices of the outliers.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Ensure the column exists in the dataset\n    if column not in data.columns:\n        raise ValueError(f\"Column '{column}' not found in the dataset.\")\n    \n    # Standardize the specified column\n    scaler = StandardScaler()\n    data_scaled = data.copy()\n    data_scaled[column] = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data_scaled[column]))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)\n    outlier_indices = outliers[0]\n    \n    # Remove outliers\n    data_without_outliers = data.drop(index=outlier_indices)\n    \n    # Visualization\n    plt.figure(figsize=(12, 6))\n    \n    # Plot original data\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='blue', label='Original Data')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    \n    # Plot data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_without_outliers, outlier_indices)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    np.random.seed(0)\n    data = np.random.normal(0, 1, 100)\n    # Introduce outliers\n    data[::10] = np.random.normal(10, 1, 10)\n    df = pd.DataFrame(data, columns=['SampleColumn'])\n    \n    # Call the function\n    original_data, cleaned_data, outliers = task_func(df, 'SampleColumn', 3)\n    print(f\"Identified outliers at indices: {outliers}\")\n```\n\nThis code snippet includes everything needed to run the `task_func` function, including necessary imports and an example usage block. The example generates a sample dataset with outliers, applies the function, and visualizes the results. Note that you need to have `pandas` installed (`import pandas as pd` is assumed to be available for creating the DataFrame in the example usage), as well as `numpy`, `matplotlib`, `scipy`, and `sklearn` for the function itself."}
{"task_id": "BigCodeBench/92", "solution": "Certainly! Below is a self-contained code snippet that performs K-means clustering on a dataset provided as a pandas DataFrame. It also generates a scatter plot visualizing the clusters and their centroids. The function `task_func` will raise exceptions if the input `data` is not a pandas DataFrame or if `n_clusters` is not an integer greater than 1. It returns a tuple containing an array of cluster labels and a matplotlib Axes object with the scatter plot.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    # Validate input parameters\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pd.DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"n_clusters must be an integer greater than 1\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Generate scatter plot\n    fig, ax = plt.subplots()\n    # Scatter plot for clusters\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6, edgecolors='w')\n    \n    # Plot centroids\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n    \n    # Optional: Add legend for centroids\n    # This part is optional and can be commented out if not needed\n    for i, center in enumerate(centers):\n        ax.text(center[0], center[1], str(i), color='white', ha='center', va='center')\n    \n    # Optional: Enhance plot aesthetics\n    ax.set_title('K-means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.tight_layout()\n    \n    return cluster_labels, ax\n\n# Example usage:\n# Assuming you have a DataFrame `df` with the data to cluster\n# df = pd.DataFrame({\n#     'Feature1': np.random.rand(100),\n#     'Feature2': np.random.rand(100)\n# })\n# labels, ax = task_func(df, n_clusters=3)\n# plt.show()\n```\n\nThis code snippet includes:\n- Input validation for the `data` and `n_clusters` parameters.\n- K-means clustering of the data using `sklearn.cluster.KMeans`.\n- Generation of a scatter plot visualizing the clusters with different colors and the centroids marked with red 'X'.\n- Optional code to add text labels to centroids and enhance plot aesthetics.\n\nRemember to replace the example usage with your actual data by defining `df` with your dataset."}
{"task_id": "BigCodeBench/93", "solution": "Certainly! Below is a self-contained code snippet that performs Principal Component Analysis (PCA) on a given dataset. It includes error handling for the `n_components` parameter and generates both the transformed DataFrame and a scatter plot of the transformed data.\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Create DataFrame for transformed data\n    columns = [f'PC{i+1}' for i in range(n_components)]\n    transformed_df = pd.DataFrame(transformed_data, columns=columns)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df.iloc[:, 0], transformed_df.iloc[:, 1])\n    ax.set_title('PCA Transformed Data')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    \n    # Return the DataFrame and Axes object\n    return transformed_df, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generating random data for demonstration\n    np.random.seed(0)\n    data = np.random.rand(100, 5)  # 100 samples, 5 features\n    \n    # Perform PCA and plot\n    transformed_df, ax = task_func(data, n_components=2)\n    print(transformed_df.head())\n    plt.show()\n```\n\nThis code defines a function `task_func` that takes a dataset (`data`) and the number of principal components (`n_components`) as inputs. It first checks if `n_components` is a positive integer and raises a `ValueError` if not. Then, it performs PCA on the data using scikit-learn's `PCA` class, transforms the data, and creates a DataFrame with the principal components. Finally, it generates a scatter plot of the transformed data using matplotlib. The function returns the transformed DataFrame and the matplotlib Axes object containing the scatter plot."}
{"task_id": "BigCodeBench/99", "solution": "Certainly! Below is a self-contained code snippet that accomplishes the described task. This code first sets the global font to Arial using Matplotlib's `rcParams`. It then loads the Iris dataset using scikit-learn, converts it into a pandas DataFrame for easier manipulation, and finally generates a seaborn pair plot. Each subplot in the pair plot visualizes the relationship between two features of the Iris dataset, colored by species. The plot is titled 'Iris Dataset Pair Plot', and each subplot's axes are labeled with the corresponding feature names.\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n    \n    # Load the Iris dataset\n    iris = load_iris()\n    iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n    \n    # Generate a pair plot\n    pair_plot = sns.pairplot(iris_df, hue='species', palette='bright')\n    \n    # Set the title of the plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)  # Adjust title position\n    \n    # Return the matplotlib Figure object containing the plot\n    return pair_plot.fig\n\n# Call the function to display the plot\nfig = task_func()\nplt.show()\n```\n\nThis code snippet defines a function `task_func` that, when called, will generate and display the pair plot as described. Note that the `plt.show()` command is used outside the function to display the plot when the function is called. This is a common practice in scripts to control when and how plots are shown, especially in environments that do not automatically display plots (like some IDEs or when running Python scripts from the command line)."}
{"task_id": "BigCodeBench/100", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        np.random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=30)\n        dates = pd.date_range(start=start_date, end=end_date)\n        \n        # Generate random data\n        data = np.random.randn(len(dates))\n        \n        # Create a DataFrame\n        df = pd.DataFrame(data, index=dates, columns=['Value'])\n        \n        # Plotting\n        plt.figure(figsize=(10, 6))\n        plt.plot(df.index, df['Value'], marker='o', linestyle='-')\n        \n        # Styling\n        plt.title('Random Time Series Data', fontname='Arial', fontsize=14)\n        plt.xlabel('Date', fontname='Arial', fontsize=12)\n        plt.ylabel('Value', fontname='Arial', fontsize=12)\n        plt.xticks(fontname='Arial', fontsize=10)\n        plt.yticks(fontname='Arial', fontsize=10)\n        \n        # Ensure the x-axis ticks format as dates properly\n        plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))\n        plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.DayLocator())\n        plt.gcf().autofmt_xdate() # Auto-format the x-axis labels for better readability\n        \n        return plt.gca()\n    except Exception as e:\n        raise ValueError(\"There was an issue generating the data or plot.\") from e\n\n# Example of how to use the function\nax = task_func()\nplt.show()\n```\nThis code snippet defines a function `task_func` that generates a plot of random time series data for the past 30 days. The randomness can be controlled by an optional `seed` parameter for reproducibility. The plot uses Arial font for better readability and is styled with labels for both axes and a title. The function raises a `ValueError` if there's an issue generating the data or plot, encapsulating potential errors in data generation or plotting within a try-except block."}
{"task_id": "BigCodeBench/101", "solution": "Certainly! Below is a self-contained code snippet that completes the specified task. This function will download the Boston Housing dataset from the provided URL, generate a correlation heatmap using Seaborn, and optionally save it to a file. It also includes error handling as requested.\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load the dataset\n        raw_data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None).dropna()\n        raw_data = raw_data.values.flatten()\n        X = np.hstack(raw_data.reshape(-1, 14)[:, :-1])\n        y = raw_data[::14]\n        columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n        data = pd.DataFrame(X, columns=columns)\n        data['MEDV'] = y\n\n        # Generate the correlation matrix\n        corr = data.corr()\n\n        # Plot the heatmap\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5)\n        plt.title('Correlation Heatmap of the Boston Housing Dataset')\n\n        # Save the plot if a save_path is provided\n        if save_path is not None:\n            plt.savefig(save_path)\n\n        # Return the Axes object containing the plot\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    ax = task_func(save_path=\"boston_housing_heatmap.png\")\n    plt.show()\n```\n\nThis code does the following:\n- It fetches the Boston Housing dataset from the specified URL.\n- Prepares the dataset by reshaping and assigning column names.\n- Calculates the correlation matrix of the dataset.\n- Draws a heatmap of the correlation matrix using Seaborn.\n- Optionally saves the heatmap to a file if a path is provided.\n- It includes error handling to raise a `ValueError` with an appropriate message if anything goes wrong during the process.\n\nPlease ensure you have the required libraries (`matplotlib`, `pandas`, `seaborn`, and `numpy`) installed in your Python environment to run this code."}
{"task_id": "BigCodeBench/108", "solution": "To complete the given problem, we'll first validate the inputs as per the requirements. Then, we'll proceed with the decomposition of the time series data using the `seasonal_decompose` method from the `statsmodels` library. Finally, we'll plot the decomposed components (trend, seasonal, and residual) and return the required outputs. Here's the self-contained code:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Validate input DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame lacks required 'value' column.\")\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"DataFrame 'value' column must be of a numeric data type.\")\n    \n    # Validate frequency\n    valid_freqs = ['D', 'M', 'Y', 'W', 'Q']  # Example of valid frequencies, can be expanded\n    if freq not in valid_freqs:\n        raise ValueError(\"Invalid 'freq' value. Must be one of {}\".format(valid_freqs))\n    \n    # Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid 'decomposition_model'. Must be 'additive' or 'multiplicative'\")\n    \n    # Ensure the DataFrame index is a DatetimeIndex\n    if not isinstance(df.index, pd.DatetimeIndex):\n        try:\n            df.index = pd.to_datetime(df.index)\n        except ValueError as e:\n            raise ValueError(\"DataFrame index must be convertible to DatetimeIndex\") from e\n    \n    # Decompose the time series\n    decomposed = seasonal_decompose(df['value'], model=decomposition_model, period=freq)\n    \n    # Plotting the decomposed components\n    fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n    \n    ax[0].plot(decomposed.observed)\n    ax[0].set_title('Observed')\n    \n    ax[1].plot(decomposed.trend)\n    ax[1].set_title('Trend')\n    \n    ax[2].plot(decomposed.seasonal)\n    ax[2].set_title('Seasonal')\n    \n    ax[3].plot(decomposed.resid)\n    ax[3].set_title('Residual')\n    \n    plt.tight_layout()\n    \n    # Return the DecomposeResult object and the matplotlib Axes object\n    return decomposed, ax\n\n# Example usage:\n# Ensure your DataFrame 'df' is properly formatted with a 'value' column and a DatetimeIndex.\n# df = pd.DataFrame({'value': np.random.randn(365)}, index=pd.date_range(start='2020-01-01', periods=365))\n# decomposed, ax = task_func(df, freq=7, decomposition_model='additive')\n```\n\nThis code snippet includes input validation for the DataFrame, frequency, and decomposition model. It also ensures that the DataFrame's index is a `DatetimeIndex`, which is a requirement for the `seasonal_decompose` function. The plotting section creates a 4-row subplot showing each component of the decomposition. Note that the `freq` parameter in `seasonal_decompose` is set to interpret the frequency of the time series, but this example assumes a daily frequency ('D') and uses it to demonstrate the functionality. You might need to adjust the `freq` parameter based on your specific time series data."}
{"task_id": "BigCodeBench/120", "solution": "Below is a self-contained code snippet that fulfills the requirements you've described. This function generates a pandas Series of random dates within a specified date range, including both the start and end dates. It also allows for an optional seed parameter to ensure reproducibility.\n\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate input dates\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both 'start_date' and 'end_date' must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' cannot be later than 'end_date'.\")\n\n    # Set the seed for reproducibility\n    random_seed(seed)\n\n    # Calculate the number of days between start_date and end_date\n    delta = end_date - start_date\n    num_days = delta.days + 1  # +1 to make end_date inclusive\n\n    # Generate random dates within the specified range\n    random_dates = [start_date + timedelta(days=randint(0, num_days - 1)) for _ in range(num_days)]\n\n    # Create a pandas Series from the list of random dates\n    date_series = pd.Series(random_dates)\n\n    return date_series\n\n# Example usage\nif __name__ == \"__main__\":\n    random_dates_series = task_func()\n    print(random_dates_series)\n```\n\nThis code defines a function `task_func` that generates a pandas Series of random dates within the specified date range. It checks if the input dates are valid `datetime` instances and if the start date is not after the end date. It then uses the `randint` function from the `random` module to generate random dates within the range, ensuring reproducibility by allowing an optional seed parameter. Finally, it returns a pandas Series containing the generated random dates."}
{"task_id": "BigCodeBench/123", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This function, `task_func`, first checks if the input `my_list` is a list and raises a `TypeError` if not. It then adds the element '12' to the list, calculates the sum of the numbers in the list (assuming all elements can be converted to integers), and attempts to concatenate a number of CSV files from a specified directory into a single pandas DataFrame. The number of files concatenated is determined by the sum of the numbers in the list. If no files are found in the specified directory, it raises a `FileNotFoundError`.\n\n```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Calculate the sum of the numbers in the list\n    try:\n        total_files = sum(int(item) for item in my_list)\n    except ValueError:\n        # If there's an error converting list items to integers, raise a TypeError\n        raise TypeError(\"All elements in my_list must be convertible to integers\")\n    \n    # Generate the file pattern to search for CSV files in the directory\n    file_pattern = os.path.join(file_dir, '*' + file_ext)\n    files = glob.glob(file_pattern)\n    \n    # If no files are found, raise FileNotFoundError\n    if not files:\n        raise FileNotFoundError(f\"No files found in the directory {file_dir} with extension {file_ext}\")\n    \n    # Select the first 'total_files' number of files based on the sum of the list\n    selected_files = files[:total_files]\n    \n    # If the number of files to concatenate is more than the available files, raise an error\n    if len(selected_files) < total_files:\n        raise FileNotFoundError(f\"Insufficient files to match the sum {total_files}. Only {len(selected_files)} files found.\")\n    \n    # Concatenate the selected CSV files into a single DataFrame\n    df_list = [pd.read_csv(file) for file in selected_files]\n    concatenated_df = pd.concat(df_list, ignore_index=True)\n    \n    return concatenated_df\n\n# Example usage\n# Ensure you have a './data_files/' directory with some CSV files to test this function.\ntry:\n    result_df = task_func(['1', '3', '5'], './data_files/')\n    print(result_df)\nexcept Exception as e:\n    print(e)\n```\n\nThis code snippet includes error handling for the specified exceptions and attempts to concatenate the specified number of CSV files into a single DataFrame. Make sure the `./data_files/` directory exists and contains some CSV files to test this function effectively. Adjust the `file_dir` parameter as needed to point to the correct directory containing your CSV files."}
{"task_id": "BigCodeBench/124", "solution": "Below is a self-contained code snippet that fulfills the requirements you've described. This code enhances `my_list` by appending the number 12, generates a list of random integers based on the sum of elements in `my_list` (with a limit set by `size`), measures the time taken for this process, and plots a histogram of the generated random numbers. It also includes error handling for the specified exceptions.\n\n```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Setting the seed for reproducibility\n    random_seed(seed)\n    \n    # Validate input\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in my_list must be numeric (int or float).\")\n    \n    # Enhance my_list by appending 12\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    sum_of_elements = sum(my_list)\n    \n    # Determine the size of the list of random integers\n    num_elements = min(sum_of_elements, size)\n    \n    # Generate the list of random integers\n    start_time = time.time()\n    random_integers = [randint(1, 100) for _ in range(int(num_elements))]\n    end_time = time.time()\n    \n    # Measure the time taken to generate the list\n    time_taken = end_time - start_time\n    \n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.hist(random_integers, bins=range(1, 102), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    \n    # Show plot (commented out to not interrupt automation, but you can uncomment it to see the plot when running interactively)\n    # plt.show()\n    \n    return time_taken, ax\n\n# Example usage:\n# Ensure you have matplotlib installed to run this example\nif __name__ == \"__main__\":\n    my_list = [1, 2, 3, 4, 5]  # Example list\n    time_taken, ax = task_func(my_list, 100, 100)\n    print(f\"Time taken: {time_taken} seconds\")\n    plt.show()  # Display the plot\n```\n\nThis code snippet includes everything required to run the task as described. It generates a list of random integers based on the sum of elements in `my_list`, limited by `size`, and plots a histogram of these numbers. The function checks if `my_list` is indeed a list and if all elements are numeric, raising exceptions if these conditions are not met. The histogram's x-axis represents the range of random integers (1-100), and the y-axis represents the frequency of each integer in the generated list. The `plt.show()` command is commented out to prevent automatic plotting during script execution, but you can uncomment it for interactive use."}
{"task_id": "BigCodeBench/129", "solution": "Below is a self-contained code snippet that defines a function `task_func` to scrape the first table from a given web page URL and extract its data into a Pandas DataFrame. The function handles exceptions as specified and uses the `requests` library for HTTP requests, `BeautifulSoup` for parsing HTML content, and `pandas` for creating and manipulating the DataFrame.\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        # Raise an HTTPError exception for bad requests (4XX or 5XX status codes)\n        response.raise_for_status()\n    except requests.exceptions.ConnectionError as e:\n        raise ConnectionError(\"There was a problem connecting to the URL.\") from e\n    except requests.exceptions.HTTPError as e:\n        raise requests.HTTPError(\"The HTTP request failed.\") from e\n    \n    try:\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.text, 'html.parser')\n        # Find the first table\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page.\")\n        \n        # Extract table headers\n        headers = []\n        for th in table.find_all('th'):\n            headers.append(th.text.strip())\n        \n        # If the table lacks headers, the DataFrame's columns will remain unnamed\n        data = []\n        for row in table.find_all('tr'):\n            cols = row.find_all('td')\n            if not cols:\n                continue  # This skips rows without table data elements, like header rows\n            cols = [ele.text.strip() for ele in cols]\n            data.append(cols)\n        \n        if not data:\n            raise ValueError(\"No table data found on the page.\")\n        \n        # Create DataFrame\n        df = pd.DataFrame(data, columns=headers if headers else None)\n        return df\n    except Exception as e:\n        raise ValueError(\"Failed to parse the page content.\") from e\n\n# Example usage\nif __name__ == \"__main__\":\n    url = 'http://example.com'  # Replace with the actual URL\n    try:\n        df = task_func(url)\n        print(df)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n\nThis code defines `task_func` which takes a URL as input, scrapes the first table from the webpage at that URL, and returns the table data as a Pandas DataFrame. It includes error handling for connection issues, HTTP errors, and situations where no table data is found or the page content cannot be parsed."}
{"task_id": "BigCodeBench/139", "solution": "Below is a self-contained code snippet that fulfills the given problem statement. This code first checks if the input is a valid, non-empty pandas DataFrame and whether it contains any numeric columns. It then proceeds to draw histograms for each numeric column and returns a list of Matplotlib Axes objects corresponding to each histogram.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame.\")\n    \n    # Check if the DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    \n    # Check if there are any numeric columns in the DataFrame\n    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n    if not numeric_cols:\n        raise ValueError(\"No numeric columns in the DataFrame.\")\n    \n    # Initialize a list to store the Axes objects\n    axes_list = []\n    \n    # Loop through each numeric column and plot the histogram\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        df[col].hist(ax=ax)\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        # Append the Axes object to the list\n        axes_list.append(ax)\n    \n    # Return the list of Axes objects\n    return axes_list\n\n# Example usage:\n# Create a sample DataFrame\ndata = {\n    'A': np.random.randn(100),\n    'B': np.random.rand(100) * 100,\n    'C': ['text'] * 100  # Non-numeric column\n}\ndf = pd.DataFrame(data)\n\n# Call the function\naxes = task_func(df)\n\n# Show plots for demonstration purposes\nplt.show()\n```\n\nThis code snippet includes the necessary imports and defines the `task_func` function as requested. It checks for the validity of the input DataFrame, whether it's empty, and if it contains numeric columns. It then plots histograms for each numeric column and returns the Axes objects. The example usage at the end demonstrates how to call this function with a sample DataFrame, but you can replace `df` with any DataFrame you wish to analyze."}
{"task_id": "BigCodeBench/147", "solution": "Certainly! Below is a self-contained Python code snippet that scans a specified IP address range to check if a specified port is open on each IP. It utilizes threading to speed up the scanning process and returns a dictionary with IP addresses as keys and a boolean indicating the port's status (True if open, False otherwise).\n\n```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\nfrom queue import Queue\n\ndef scan_ip(ip, port, output):\n    \"\"\"\n    Scans a single IP address to check if the port is open.\n    Updates the output dictionary with the result.\n    \"\"\"\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(1)  # Timeout for the socket operation\n    try:\n        result = sock.connect_ex((ip, port))\n        if result == 0:\n            output[ip] = True\n        else:\n            output[ip] = False\n    except socket.error:\n        output[ip] = False\n    finally:\n        sock.close()\n\ndef worker_thread(ip_queue, port, output):\n    \"\"\"\n    Worker thread function that keeps processing IPs from the queue.\n    \"\"\"\n    while not ip_queue.empty():\n        ip = ip_queue.get()\n        scan_ip(ip, port, output)\n        ip_queue.task_done()\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans the specified IP range to check if the specified port is open on each IP.\n    Returns a dictionary with IP addresses as keys and a boolean indicating the port's status.\n    \"\"\"\n    ip_network = IPv4Network(ip_range)\n    output = {}  # Dictionary to hold the scan results\n    ip_queue = Queue()\n\n    # Fill the queue with all IPs in the range\n    for ip in ip_network:\n        ip_queue.put(str(ip))\n\n    # List to keep track of threads\n    threads = []\n\n    # Launch threads\n    for _ in range(10):  # Adjust the number of threads based on your needs\n        thread = Thread(target=worker_thread, args=(ip_queue, port, output))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return output\n\n# Example usage\nif __name__ == \"__main__\":\n    ip_range = \"192.168.1.0/24\"  # Example IP range\n    port = 80  # Example port\n    open_ports = task_func(ip_range, port)\n    print(open_ports)\n```\n\nThis script defines a `task_func` function that takes an IP range (in CIDR notation) and a port number as inputs. It uses threading to concurrently scan the IP addresses within the specified range for the specified port. The `scan_ip` function attempts to connect to the given port on each IP address and updates the `output` dictionary with the result of the scan (True if the port is open, False otherwise).\n\nPlease note that scanning networks without permission can be illegal or considered malicious activity. Always ensure you have authorization before scanning networks."}
{"task_id": "BigCodeBench/161", "solution": "Below is a self-contained code snippet that follows the given instructions. This code reads a log file, extracts logging information based on the specified format, validates the timestamp, and then stores the extracted data into a CSV file. If any log entry has an invalid timestamp or if no valid log entries are found, it raises a `ValueError`.\n\n```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\nimport os\n\ndef task_func(log_file):\n    # Regular expression to match the log format\n    log_pattern = re.compile(r'(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.*)')\n    \n    # List to store extracted data\n    extracted_data = []\n    \n    # Read and process the log file\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                log_data = match.groupdict()\n                # Validate timestamp\n                try:\n                    datetime.strptime(log_data['timestamp'], '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp in log entry: {line}\")\n                extracted_data.append(log_data)\n    \n    # Check if no valid log entries were found\n    if not extracted_data:\n        raise ValueError(\"No valid log entries found.\")\n    \n    # Convert extracted data to a DataFrame\n    df = pd.DataFrame(extracted_data)\n    \n    # Define the CSV file path\n    csv_file_path = 'extracted_logs.csv'\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file_path, index=False)\n    \n    return os.path.abspath(csv_file_path)\n\n# Example usage\n# Assuming you have a log file named 'example_log.txt' in the current directory\nif __name__ == \"__main__\":\n    log_file_path = 'example_log.txt'\n    try:\n        csv_file_path = task_func(log_file_path)\n        print(f\"CSV file created at: {csv_file_path}\")\n    except ValueError as e:\n        print(e)\n```\n\nThis code snippet includes:\n- A regular expression to match the log entries according to the specified format.\n- Validation of the timestamp to ensure it follows the `YYYY-MM-DD HH:MM:SS` format. If not, a `ValueError` is raised.\n- Conversion of the extracted data into a pandas DataFrame, which is then saved as a CSV file.\n- The function returns the absolute path to the created CSV file.\n\nMake sure you have the `pandas` library installed in your environment. You can install it using pip:\n\n```\npip install pandas\n```\n\nReplace `'example_log.txt'` with the path to your actual log file when using this code."}
{"task_id": "BigCodeBench/162", "solution": "Certainly! Below is a self-contained Python code snippet that defines a function `task_func`. This function takes a text input and an optional `rwidth` parameter for the histogram's relative bar width. It analyzes the distribution of word lengths in the given text, generates a histogram to visualize this distribution, and returns the `matplotlib.axes.Axes` object containing the histogram.\n\n```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Use regular expression to find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Check if word_lengths is empty\n    if not word_lengths:\n        print(\"No words found in the input text.\")\n        return None\n    \n    # Generate histogram data\n    max_length = max(word_lengths)\n    bins = np.arange(0, max_length + 2) - 0.5  # Shift bins to center integers\n    \n    # Create the histogram plot\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=bins, rwidth=rwidth, edgecolor='black')\n    \n    # Set the plot labels and title\n    ax.set_title('Distribution of Word Lengths')\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    \n    # Adjust x-axis ticks to match word lengths\n    ax.set_xticks(np.arange(1, max_length + 1))\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\n\n# Example usage\ntext = \"This is an example text to demonstrate the function. It includes various word lengths.\"\ntask_func(text)\n```\n\nThis code snippet starts by importing the necessary libraries: `re` for regular expressions to extract words from the input text, `matplotlib.pyplot` for plotting, and `numpy` for numerical operations.\n\nThe `task_func` function first extracts all words from the input text using a regular expression. It then calculates the length of each word and checks if the resulting list of word lengths is empty. If not, it proceeds to generate and configure a histogram that visualizes the distribution of word lengths. The histogram's bins are adjusted to center integer values, making it more intuitive for word length visualization. Finally, the function displays the histogram and returns the `matplotlib.axes.Axes` object for further manipulation or inspection if needed."}
{"task_id": "BigCodeBench/177", "solution": "To solve the problem described, we'll first need to filter the DataFrame based on the condition that the titles contain specific keywords (\"like\" or \"what\"), ignoring case sensitivity. After filtering, we'll analyze the frequency of each word in the content of these articles, excluding punctuation. We'll use the `re` module for regular expressions to handle text processing, `nltk` for natural language processing tasks, and `pandas` for DataFrame manipulation. If you haven't installed `nltk`, you can do so by running `pip install nltk`.\n\nHere's a self-contained code snippet that accomplishes the task:\n\n```python\nimport re\nimport nltk\nimport pandas as pd\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Ensure that necessary NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\n\ndef task_func(df):\n    # Check if DataFrame is empty or lacks necessary columns\n    if df.empty or not {'Title', 'Content'}.issubset(df.columns):\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Convert titles to lowercase and filter rows\n    keywords = ['like', 'what']\n    pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n    filtered_df = df[df['Title'].str.contains(pattern, na=False)]\n\n    # Process content: tokenize, remove punctuation and stopwords\n    stopwords_set = set(stopwords.words('english'))\n    words = []\n    for content in filtered_df['Content']:\n        # Remove punctuation\n        content = content.translate(str.maketrans('', '', punctuation))\n        # Tokenize and filter stopwords\n        tokens = word_tokenize(content.lower())\n        words.extend([word for word in tokens if word not in stopwords_set and word.isalpha()])\n\n    # Count frequency of each word\n    word_freq = Counter(words)\n\n    return dict(word_freq)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample DataFrame creation\n    data = {\n        'Title': ['What is Lorem Ipsum?', 'Why do we use it?', 'The standard Lorem Ipsum passage', 'Where does it come from?'],\n        'Content': ['Lorem Ipsum is simply dummy text of the printing and typesetting industry.', 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.', 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages.', 'Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text.']\n    }\n    df = pd.DataFrame(data)\n\n    # Function call\n    word_freq = task_func(df)\n    print(word_freq)\n```\n\nThis code snippet defines a function `task_func` that takes a DataFrame `df` as input. It first checks if the DataFrame is empty or lacks the necessary 'Title' and 'Content' columns, raising a `ValueError` if the check fails. Then, it filters the DataFrame for articles whose titles contain the keywords \"like\" or \"what\", ignoring case. After filtering, it processes the content of these articles to tokenize the text, remove punctuation and stopwords, and finally counts the frequency of each word. The result is returned as a dictionary of word frequencies."}
{"task_id": "BigCodeBench/184", "solution": "Certainly! Below is a self-contained Python code snippet that defines a function `task_func`. This function takes a pandas DataFrame and a column name as input. It preprocesses the text data in the specified column by removing stopwords, numbers, and punctuation. Then, it applies a vectorization process to convert the text into a numeric format, specifically using a bag-of-words model. The output is a DataFrame where each word (after preprocessing) is a column, and the rows represent their count in each document.\n\n```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Function to preprocess the text: remove stopwords, numbers, and punctuation\n    def preprocess_text(text):\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Convert text to lowercase and split into words\n        words = text.lower().split()\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n        # Join words back to string\n        return ' '.join(words)\n    \n    # Apply the preprocessing function to the specified text column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n    \n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n    # Fit and transform the text data to a bag-of-words model\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert the result to a DataFrame\n    result_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return result_df\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample DataFrame\n    data = {'text_column': ['This is a sample text, it contains numbers like 123 and stopwords.']}\n    df = pd.DataFrame(data)\n    \n    # Apply the function\n    transformed_df = task_func(df, 'text_column')\n    print(transformed_df)\n```\n\nThis code snippet includes an example usage part, where a sample DataFrame is created and passed to the `task_func` function along with the name of the column to preprocess and vectorize. The result is printed to the console."}
{"task_id": "BigCodeBench/187", "solution": "Certainly! Below is a self-contained code snippet that generates a GeoPandas DataFrame for a list of cities with randomly generated coordinates based on specified ranges. It includes error handling for missing or incorrectly formatted 'Lon' and 'Lat' keys in the dictionary.\n\n```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Check if 'Lon' and 'Lat' keys exist and their values are tuples\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"'Lon' and 'Lat' values must be tuples\")\n    \n    # Generate random coordinates for each city\n    lon_values = np.random.uniform(dic['Lon'][0], dic['Lon'][1], len(cities))\n    lat_values = np.random.uniform(dic['Lat'][0], dic['Lat'][1], len(cities))\n    \n    # Create a list of Point objects for GeoDataFrame\n    points = [Point(lon, lat) for lon, lat in zip(lon_values, lat_values)]\n    \n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': points})\n    \n    return gdf\n\n# Example usage\ngdf = task_func()\nprint(gdf)\n```\n\nThis code defines a function `task_func` that takes a dictionary `dic` with longitude ('Lon') and latitude ('Lat') ranges, and a list of city names `cities`. It generates random coordinates within the specified ranges for each city, creates Point objects from these coordinates, and then constructs a GeoPandas DataFrame with these points and the city names. The function checks for the presence and correct format of the 'Lon' and 'Lat' keys in the dictionary, raising a `ValueError` if the conditions are not met."}
{"task_id": "BigCodeBench/199", "solution": "Certainly! Below is a self-contained code snippet that generates a weather report for specified cities at a given UTC datetime. It includes error handling for input validation and uses the provided parameters to create a pandas DataFrame with the weather report.\n\n```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate utc_datetime\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    \n    # Validate cities, weather_conditions, and timezones\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions must be strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and values\")\n    \n    # Set seed for reproducibility\n    set_seed(seed)\n    \n    # Prepare data for DataFrame\n    data = []\n    for city in cities:\n        # Get timezone for the city\n        tz = pytz.timezone(timezones[city])\n        # Convert UTC datetime to local datetime\n        local_datetime = utc_datetime.astimezone(tz)\n        # Format local datetime\n        formatted_local_datetime = local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')\n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        # Append data\n        data.append([city, formatted_local_datetime, weather_condition])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, columns=['City', 'Local Time', 'Weather Condition'])\n    \n    return df\n\n# Example usage\nutc_now = datetime.utcnow().replace(tzinfo=pytz.utc)\nprint(task_func(utc_now))\n```\n\nThis code defines a function `task_func` that generates a weather report for a list of cities at a specified UTC datetime. It randomly selects a weather condition for each city and converts the UTC datetime to the local time of each city using their respective timezones. The result is returned as a pandas DataFrame. Make sure you have `pandas` and `pytz` installed in your environment to run this code."}
{"task_id": "BigCodeBench/208", "solution": "Below is a self-contained code snippet that fulfills the requirements you've outlined. This code generates a random sequence of steps for a specified number of elements, plots the random walk, and returns the descriptive statistics of the walk along with the plot.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Calculate the positions by cumulatively summing the steps\n    positions = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = pd.Series(positions).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    stats_dict = stats.to_dict()\n    # Add count, min, and max explicitly to ensure they are included\n    stats_dict['count'] = len(positions)\n    stats_dict['min'] = positions.min()\n    stats_dict['max'] = positions.max()\n    \n    # Plot the random walk\n    plt.figure(figsize=(10, 6))\n    plt.plot(positions)\n    plt.title(\"Random Walk\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Position\")\n    ax = plt.gca()  # Get the current Axes instance on the current figure\n    \n    # Return the descriptive statistics and the Axes object\n    return stats_dict, ax\n\n# Example usage\nstats, ax = task_func(1000)\nprint(stats)\nplt.show()\n```\n\nThis code defines a function `task_func` that takes the number of steps (`elements`) and an optional `seed` for the random number generator to ensure reproducibility. It generates a sequence of random steps, computes the cumulative sum to simulate a random walk, calculates descriptive statistics using pandas, and plots the walk using matplotlib. The function returns a dictionary containing the descriptive statistics and the matplotlib `Axes` object with the plotted random walk."}
{"task_id": "BigCodeBench/211", "solution": "Certainly! Below is a self-contained Python function that accomplishes the task described. This function downloads a ZIP file from the specified URL, extracts its contents to the given directory, and returns a list of filenames of the extracted files. It uses the `requests` library to handle the download, so make sure you have it installed in your environment (`pip install requests` if needed).\n\n```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n    \n    # Temporary file path for downloading the zip file\n    temp_zip_path = os.path.join(destination_directory, \"temp_download.zip\")\n    \n    # Download the zip file\n    with requests.get(url, headers=headers, stream=True) as r:\n        r.raise_for_status()  # Raises a HTTPError if the response status code is 4XX/5XX\n        with open(temp_zip_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    \n    # List to store the names of the extracted files\n    extracted_files = []\n    \n    # Extract the zip file\n    with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        extracted_files = zip_ref.namelist()\n    \n    # Optionally, remove the downloaded zip file after extraction\n    os.remove(temp_zip_path)\n    \n    return extracted_files\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"https://example.com/path/to/your/zipfile.zip\"  # Replace with the actual URL\n    destination_directory = \"./extracted_files\"  # Replace with your desired path\n    extracted_files = task_func(url, destination_directory)\n    print(\"Extracted files:\", extracted_files)\n```\n\nPlease replace `\"https://example.com/path/to/your/zipfile.zip\"` with the actual URL of the ZIP file you wish to download and extract. Also, adjust the `destination_directory` to your desired path where the contents of the ZIP file should be extracted.\n\nThis code does not handle specific edge cases, such as what to do if the ZIP file contains directories or if files with the same names already exist in the destination directory. Depending on your requirements, you might want to add additional logic to handle these situations."}
{"task_id": "BigCodeBench/214", "solution": "To complete the given task, we will follow these steps:\n\n1. **Initialize the random seed**: This ensures reproducibility of the random image generated.\n2. **Validate `range_low` and `range_high`**: Raise a `ValueError` if `range_low` is not less than `range_high`.\n3. **Generate a random RGB image**: Using the specified `image_size` and random values within the given range.\n4. **Display the image**: Utilize `matplotlib` to plot the generated image.\n5. **Return the `Axes` object and the image array**.\n\nHere's the self-contained code to accomplish the task:\n\n```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Initialize the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Validate range_low and range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n    \n    # Generate a random RGB image\n    image = np.random.randint(low=range_low, high=range_high, size=image_size, dtype=np.uint8)\n    \n    # Display the image\n    fig, ax = plt.subplots()\n    ax.imshow(image)\n    ax.axis('off')  # Hide the axes ticks\n    plt.show()\n    \n    # Return the Axes object and the image array\n    return ax, image\n\n# Example usage\nif __name__ == \"__main__\":\n    ax, image = task_func()\n```\n\nThis code snippet defines the `task_func` function that generates and displays a random RGB image based on the provided parameters. It uses `numpy` for array manipulation and `matplotlib` for displaying the image. The `np.random.randint` function is used to fill the array with random integers within the specified range, which are suitable for representing RGB pixel values."}
{"task_id": "BigCodeBench/227", "solution": "Below is a self-contained code snippet that follows the instructions provided. This code creates an MxN matrix from a list `L`, normalizes it based on the sound pressure level (SPL) of a specified audio file, and generates a spectrogram from the matrix. The SPL is calculated using the provided formula, and the spectrogram is displayed with a logarithmic scale for frequency and a linear scale for time. The SPL is used to adjust the amplitude displayed in the spectrogram. The function raises an exception if the specified audio file does not exist.\n\n```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file {audio_file} does not exist.\")\n    \n    # Read audio file\n    data, samplerate = sf.read(audio_file)\n    \n    # Calculate SPL\n    SPL = 20 * np.log10(np.sqrt(np.mean(data**2)))\n    \n    # Create MxN matrix from list L\n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize matrix based on SPL\n    normalized_matrix = matrix / np.max(matrix) * SPL\n    \n    # Generate spectrogram\n    fig, ax = plt.subplots()\n    S = librosa.amplitude_to_db(np.abs(librosa.stft(normalized_matrix.flatten())), ref=np.max)\n    img = librosa.display.specshow(S, sr=samplerate, x_axis='time', y_axis='log', ax=ax)\n    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n    ax.set(title='Spectrogram')\n    \n    return normalized_matrix, fig\n\n# Example usage\nL = np.random.rand(100)  # Example list\nM = 10  # Number of rows\nN = 10  # Number of columns\naudio_file = 'path/to/your/audiofile.wav'  # Specify the path to your audio file\n\ntry:\n    normalized_matrix, fig = task_func(L, M, N, audio_file)\n    plt.show()  # Display the spectrogram\nexcept FileNotFoundError as e:\n    print(e)\n```\n\nMake sure to replace `'path/to/your/audiofile.wav'` with the actual path to your audio file. This code uses `numpy` for numerical operations, `soundfile` to read the audio file, `librosa` for generating the spectrogram, and `matplotlib` for plotting. The `librosa.display.specshow` function is used to display the spectrogram with a logarithmic scale for frequency and a linear scale for time, and the SPL calculated from the audio file is used to adjust the amplitude displayed in the spectrogram."}
{"task_id": "BigCodeBench/239", "solution": "Certainly! Below is a self-contained Python code snippet that fulfills the requirements you've outlined. This code defines a function `task_func` that takes a list of tuples, extracts numeric values, computes basic statistics, and generates a histogram with an overlaid probability density function (PDF).\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([item for sublist in original for item in sublist if isinstance(item, (int, float))])\n    \n    # Compute basic statistics\n    statistics = {\n        'mean': np.mean(numeric_values),\n        'std': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n    \n    # Plotting the histogram with density=True and bins='auto'\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, bins='auto', alpha=0.6, density=True, label='Histogram')\n    \n    # Overlaying the PDF\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, statistics['mean'], statistics['std'])\n    ax.plot(x, p, 'k', linewidth=2, label='PDF')\n    \n    title = \"Histogram with PDF overlay\"\n    ax.set_title(title)\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    \n    # Show plot for interactive environments\n    plt.show()\n    \n    return numeric_values, statistics, ax\n\n# Example usage\noriginal = [(1, 2), (3, 4.5), ('text', 6), (7.1, 8)]\nnumeric_values, statistics, ax = task_func(original)\nprint(\"Numeric Values:\", numeric_values)\nprint(\"Statistics:\", statistics)\n```\n\nThis code does the following:\n- It first extracts all numeric values from the input list of tuples, ignoring non-numeric types.\n- It then computes basic statistics (mean, standard deviation, minimum, and maximum) for the extracted numeric values.\n- Finally, it generates a histogram of the numeric values with the density flag set to True, which normalizes the histogram. It also automatically selects the number of bins using the 'auto' option. The histogram is overlaid with a probability density function (PDF) that matches the mean and standard deviation of the data.\n- The function returns the numpy array of numeric values, a dictionary containing the computed statistics, and the `Axes` object for the plot. This allows for further customization of the plot outside the function if needed.\n\nNote: The `plt.show()` command is used to display the plot. In some environments, such as Jupyter notebooks, the plot may display automatically without needing this command."}
{"task_id": "BigCodeBench/241", "solution": "Below is a self-contained code snippet that completes the specified task. This code defines a function `task_func` that takes a list `original` as input, converts it into a numpy array, normalizes this array, and then plots both the original and normalized arrays using matplotlib. The normalization is done using the `MinMaxScaler` from scikit-learn's preprocessing module, which scales the data to a range between 0 and 1.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Reshape the array to fit the scaler\n    original_array_reshaped = original_array.reshape(-1, 1)\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the data to normalize it\n    normalized_array = scaler.fit_transform(original_array_reshaped)\n    \n    # Flatten the normalized array to its original shape\n    normalized_array = normalized_array.flatten()\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original')\n    ax.plot(normalized_array, label='Normalized')\n    ax.legend()\n    \n    # Show the plot\n    plt.show()\n    \n    return original_array, normalized_array, ax\n\n# Example usage\noriginal = [10, 20, 30, 40, 50]\noriginal_array, normalized_array, ax = task_func(original)\n```\n\nThis code does the following:\n- Converts the input list `original` into a numpy array.\n- Normalizes this array using `MinMaxScaler` from scikit-learn, which requires the data to be reshaped into a 2D array for the fit_transform method. After normalization, the data is reshaped back to its original shape.\n- Plots the original and normalized arrays on the same graph with a legend to differentiate them.\n- Returns the original numpy array, the normalized array, and the matplotlib `Axes` object containing the plot."}
{"task_id": "BigCodeBench/267", "solution": "To complete the given problem, we'll follow the steps outlined in the task description. We'll start by adding a new key \"a\" with the value 1 to the dictionary. Then, we'll generate a signal based on the values in \"data\". For simplicity, let's assume the signal is a combination of the values in the dictionary. We'll then run a Fast Fourier Transform (FFT) on this signal and plot the FFT. Finally, we'll return the FFT of the signal along with the plot.\n\nHere's how you can do it:\n\n```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\"\n    # For simplicity, let's create a signal that is a sum of sine waves based on the values in the dictionary\n    duration = 1  # seconds\n    t = np.linspace(0, duration, sample_rate, endpoint=False)\n    signal = np.zeros_like(t)\n    for value in data.values():\n        frequency = value  # Assuming each value in the dictionary represents a frequency\n        signal += np.sin(2 * np.pi * frequency * t)\n    \n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal\n    fft_signal = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal)) * sample_rate\n    \n    # Step 4: Plot and return the FFT of the signal\n    fig, ax = plt.subplots()\n    ax.stem(freqs, np.abs(fft_signal), 'b', markerfmt=\" \", basefmt=\"-b\")\n    ax.set_xlabel('Frequency in Hertz [Hz]')\n    ax.set_ylabel('Frequency Domain (Spectrum) Magnitude')\n    ax.set_xlim(0, sample_rate / 2)  # Only plot the positive frequencies\n    ax.set_ylim(-5, np.max(np.abs(fft_signal))+10)  # Adjust the y-axis limits\n    plt.grid(True)\n    plt.show()\n    \n    return fft_signal, ax\n\n# Example usage:\ndata = {'b': 2, 'c': 3}  # Example dictionary\nfft_result, plot = task_func(data)\n```\n\nThis code snippet defines a function `task_func` that takes a dictionary `data` and an optional `sample_rate` parameter. It adds a new key-value pair to the dictionary, generates a signal from the dictionary values, computes its FFT, and plots the FFT spectrum. The function returns the FFT of the signal and the plot axis. Note that the signal generation part is simplified for demonstration purposes, and the actual signal generation logic might vary based on your specific requirements."}
{"task_id": "BigCodeBench/273", "solution": "```python\nimport cgi\nimport http.server\nimport json\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass RequestHandler(BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers.get('Content-Type')\n        if content_type != 'application/json':\n            self.respond(400, 'Content-Type header is not application/json')\n            return\n        \n        content_length = int(self.headers.get('Content-Length', 0))\n        try:\n            post_data = self.rfile.read(content_length)\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.respond(400, 'Invalid JSON')\n            return\n        \n        if 'data' not in data:\n            self.respond(400, 'No data key in request')\n            return\n        \n        self.respond(200, json.dumps(SUCCESS_RESPONSE))\n    \n    def respond(self, status_code, message):\n        self.send_response(status_code)\n        if status_code == 200:\n            content = message.encode()\n            self.send_header('Content-Type', 'application/json')\n        else:\n            content = json.dumps({'status': 'error', 'message': message}).encode()\n            self.send_header('Content-Type', 'application/json')\n        self.send_header('Content-Length', str(len(content)))\n        self.end_headers()\n        self.wfile.write(content)\n\ndef run(server_class=HTTPServer, handler_class=RequestHandler, port=8080):\n    server_address = ('', port)\n    httpd = server_class(server_address, handler_class)\n    print(f'Starting httpd on port {port}...')\n    httpd.serve_forever()\n\nif __name__ == \"__main__\":\n    run()\n```\nThis code defines a simple HTTP server that listens for POST requests. It checks if the incoming request has the correct `Content-Type` header (`application/json`) and whether the JSON data contains a `data` key. It responds with a success message if the conditions are met or an appropriate error message otherwise. The server runs on port 8080 by default, but you can change the `port` parameter in the `run` function call to use a different port."}
{"task_id": "BigCodeBench/274", "solution": "Certainly! Below is a self-contained code snippet that defines a class to handle HTTP POST requests for processing incoming email data and sending emails based on the provided JSON object. This class will be used within a function that sets up an HTTP server and configures it to handle POST requests for sending emails. The code includes error handling for JSON decoding issues, missing keys in the email data, and SMTP authentication errors.\n\n```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nfrom socketserver import ThreadingMixIn\nimport ssl\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHTTPRequestHandler(BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Initialize response data\n            content_type = \"application/json\"\n            response_data = {\"status\": \"success\", \"message\": \"Email sent successfully\"}\n\n            try:\n                # Get the size of data\n                content_length = int(self.headers['Content-Length'])\n                # Read the JSON data\n                post_data = self.rfile.read(content_length)\n\n                # Decode JSON data\n                email_data = json.loads(post_data.decode('utf-8'))\n\n                # Check for required keys\n                if not all(key in email_data for key in ['subject', 'message', 'to']):\n                    raise ValueError(\"Missing 'subject', 'message', or 'to' in the email data\")\n\n                # Prepare email\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n\n                # Send email\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, [email_data['to']], msg.as_string())\n\n            except json.JSONDecodeError:\n                self.send_response(400)  # Bad Request\n                response_data = {\"status\": \"error\", \"message\": \"Invalid JSON data\"}\n            except ValueError as e:\n                self.send_response(400)  # Bad Request\n                response_data = {\"status\": \"error\", \"message\": str(e)}\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)  # Authentication Failed\n                response_data = {\"status\": \"error\", \"message\": \"SMTP Authentication failed\"}\n            except Exception as e:\n                self.send_response(500)  # Internal Server Error\n                response_data = {\"status\": \"error\", \"message\": str(e)}\n            else:\n                self.send_response(200)  # OK\n\n            # Send headers\n            self.send_header('Content-type', content_type)\n            self.send_header('Content-length', str(len(json.dumps(response_data))))\n            self.end_headers()\n\n            # Write the response\n            self.wfile.write(json.dumps(response_data).encode('utf-8'))\n\n    return EmailHTTPRequestHandler\n\n# Example usage\nif __name__ == \"__main__\":\n    smtp_server = \"smtp.example.com\"\n    smtp_port = 587\n    smtp_username = \"your_username@example.com\"\n    smtp_password = \"your_password\"\n\n    handler_class = task_func(smtp_server, smtp_port, smtp_username, smtp_password)\n\n    class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):\n        \"\"\"Handle requests in a separate thread.\"\"\"\n\n    server_address = ('', 8080)\n    httpd = ThreadedHTTPServer(server_address, handler_class)\n    print(\"Serving at port\", 8080)\n    httpd.serve_forever()\n```\n\nThis code defines a function `task_func` that returns a `EmailHTTPRequestHandler` class. This handler processes POST requests, expecting JSON data with 'subject', 'message', and 'to' keys, and sends an email using the provided SMTP server details. It handles various errors, including JSON decoding errors, missing keys, and SMTP authentication errors, by sending appropriate HTTP response codes and messages."}
{"task_id": "BigCodeBench/287", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This function, `task_func`, takes a filename for the JSON output and a directory to search for `.txt` files. It counts the number of words in each `.txt` file within the specified directory, exports these counts to a JSON file (with the given filename), and returns the total number of words found across all `.txt` files.\n\n```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    # Initialize a Counter object to keep track of word counts per file\n    word_counts = Counter()\n    total_words = 0\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if the file is a .txt file\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    # Read the file and count the words\n                    words = f.read().split()\n                    word_counts[file] = len(words)\n                    total_words += len(words)\n\n    # Export the counts to a JSON file\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(word_counts, f, indent=4)\n\n    # Return the total number of words\n    return total_words\n\n# Example usage\ndirectory = 'path/to/your/directory'  # Update this path to the directory you want to search\noutput_filename = 'word_counts.json'  # The name of the JSON file to output\ntotal_words = task_func(output_filename, directory)\nprint(f\"Total number of words: {total_words}\")\n```\n\nMake sure to replace `'path/to/your/directory'` with the actual path to the directory you want to search. This code counts the words in all `.txt` files within the specified directory (including its subdirectories) and outputs the counts to a JSON file named as per the `filename` parameter."}
{"task_id": "BigCodeBench/302", "solution": "Below is a self-contained code snippet that processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns, calculates the Pearson correlation coefficient between these columns, and optionally visualizes the correlation matrix using a heatmap. The function raises a `ValueError` if the DataFrame input is empty or has invalid 'Value'.\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nimport numpy as np\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Check if the DataFrame is empty or if 'Value' column contains non-list items\n    if df.empty or not all(df['Value'].apply(lambda x: isinstance(x, list))):\n        raise ValueError(\"DataFrame is empty or 'Value' column contains invalid data.\")\n    \n    # Split lists in the 'Value' column into separate columns\n    values_df = pd.DataFrame(df['Value'].to_list())\n    \n    # Check if the resulting DataFrame after splitting is empty\n    if values_df.empty:\n        raise ValueError(\"No data available after splitting 'Value' column into separate columns.\")\n    \n    # Calculate the Pearson correlation coefficient matrix\n    correlation_matrix = values_df.corr(method='pearson')\n    \n    if plot:\n        # Plotting the correlation matrix as a heatmap\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n    \n    # Return the correlation matrix and optionally the Axes object containing the heatmap plot\n    return correlation_matrix, plt.gca() if plot else None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample DataFrame\n    data = {\n        'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n        'Value': [[1, 2, 3], [2, 3, 4], [3, 4, 5]]\n    }\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Call the function without plotting\n    correlation_matrix, _ = task_func(df, plot=False)\n    print(correlation_matrix)\n    \n    # Call the function with plotting\n    _, ax = task_func(df, plot=True)\n```\n\nThis code defines a function `task_func` that takes a pandas DataFrame and an optional `plot` argument. It first validates the input DataFrame to ensure it contains lists in the 'Value' column and is not empty. Then, it splits these lists into separate DataFrame columns, calculates the Pearson correlation coefficient matrix among these columns, and optionally plots this matrix as a heatmap. The function returns the correlation matrix and, if plotting is enabled, the matplotlib Axes object containing the heatmap."}
{"task_id": "BigCodeBench/308", "solution": "To complete the given task, I'll write a Python function that generates a report on students' grades in different subjects, calculates the average grade for each student and each subject, and outputs the result as a pandas DataFrame. This function will use predefined constants for the subjects and students, and it can also handle additional subjects if specified. Here's the self-contained code to accomplish this:\n\n```python\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n    # Combine predefined fields with any additional fields\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    data = {student: [random.randint(0, 100) for _ in all_fields] for student in STUDENTS}\n    \n    # Create a DataFrame from the generated data\n    df = pd.DataFrame(data, index=all_fields)\n    \n    # Calculate the average grade for each student and add it as a new row\n    df.loc['Average'] = df.mean(axis=0)\n    \n    # Calculate the average grade for each subject and add it as a new column\n    df['Average Grade'] = df.mean(axis=1)\n    \n    return df\n\n# Example usage\nadditional_subjects = ['Art', 'Physical Education']\ndf_report = task_func(additional_subjects)\nprint(df_report)\n```\n\nThis code defines a function `task_func` that generates a report based on the constants `FIELDS` and `STUDENTS`, and can also include additional subjects if provided. It randomly assigns grades between 0 and 100 for each student in each subject, calculates the average grade per student and per subject, and returns the result as a pandas DataFrame. The DataFrame includes an 'Average' row for the average grade per subject and an 'Average Grade' column for the average grade per student."}
{"task_id": "BigCodeBench/310", "solution": "Certainly! Below is a self-contained Python code snippet that generates a CSV file with simulated data for 100 people, including their name, age, height, and weight. It then calculates the average age, height, and weight and appends these averages at the end of the file. Finally, it returns the path of the created CSV file.\n\n```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef generate_random_name():\n    \"\"\"Generates a random name.\"\"\"\n    first_names = ['John', 'Jane', 'Chris', 'Anna', 'Mike', 'Linda', 'James', 'Mary', 'Robert', 'Patricia']\n    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Miller', 'Davis', 'Garcia', 'Rodriguez', 'Wilson']\n    return random.choice(first_names) + \" \" + random.choice(last_names)\n\ndef generate_random_person():\n    \"\"\"Generates a random person's data.\"\"\"\n    name = generate_random_name()\n    age = random.randint(18, 80)  # Age between 18 and 80\n    height = round(random.uniform(1.5, 2.0), 2)  # Height between 1.5m and 2.0m\n    weight = round(random.uniform(50, 100), 2)  # Weight between 50kg and 100kg\n    return [name, age, height, weight]\n\ndef task_func(filename):\n    # Generate data for PEOPLE_COUNT people\n    people_data = [generate_random_person() for _ in range(PEOPLE_COUNT)]\n    \n    # Calculate averages\n    avg_age = mean([person[1] for person in people_data])\n    avg_height = mean([person[2] for person in people_data])\n    avg_weight = mean([person[3] for person in people_data])\n    \n    # Append averages to the data\n    people_data.append([\"Average\", round(avg_age, 2), round(avg_height, 2), round(avg_weight, 2)])\n    \n    # Write data to CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)  # Write header\n        writer.writerows(people_data)  # Write person data and averages\n    \n    return os.path.abspath(filename)\n\n# Example usage\nfilename = \"people_data.csv\"\nfile_path = task_func(filename)\nprint(f\"CSV file created at: {file_path}\")\n```\n\nThis code snippet includes functions to generate random names and random person data. It then calculates the averages for age, height, and weight, appends these averages to the data, and writes everything to a CSV file specified by the `filename` parameter. Finally, it returns the absolute path of the created file."}
{"task_id": "BigCodeBench/313", "solution": "Below is a self-contained Python code snippet that completes the specified problem. This code organizes files in a given directory based on the first text that is not enclosed in square brackets. Files are moved to subdirectories named after this text. If no matching text is found, the file is not moved. The function returns the directory path and a dictionary detailing the subdirectories created and the files moved to them.\n\n```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Regular expression to find the first text not enclosed in square brackets\n    pattern = re.compile(r'(?<!\\[)[a-zA-Z0-9]+(?![\\]])')\n\n    # Dictionary to keep track of moved files\n    moved_files = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        # Skip directories\n        if os.path.isdir(os.path.join(directory, filename)):\n            continue\n\n        # Search for the pattern in the filename\n        match = pattern.search(filename)\n        if match:\n            # Extract the matched text\n            subdir_name = match.group(0)\n            # Create the subdirectory path\n            subdir_path = os.path.join(directory, subdir_name)\n            # Create the subdirectory if it doesn't exist\n            os.makedirs(subdir_path, exist_ok=True)\n            # Move the file to the subdirectory\n            shutil.move(os.path.join(directory, filename), os.path.join(subdir_path, filename))\n            # Update the moved_files dictionary\n            if subdir_name in moved_files:\n                moved_files[subdir_name].append(filename)\n            else:\n                moved_files[subdir_name] = [filename]\n\n    return directory, moved_files\n\n# Example usage\nif __name__ == \"__main__\":\n    directory_path = \"/path/to/your/directory\"  # Change this to your directory path\n    result_directory, moved_files_dict = task_func(directory_path)\n    print(f\"Directory organized: {result_directory}\")\n    for subdir, files in moved_files_dict.items():\n        print(f\"Subdirectory '{subdir}' contains files: {files}\")\n```\n\nMake sure to replace `\"/path/to/your/directory\"` with the actual path of the directory you want to organize. This script uses regular expressions to identify the first text in a filename that is not enclosed in square brackets and moves the file into a subdirectory named after this text. If the subdirectory does not exist, it is created. The script then returns the path of the directory and a dictionary detailing the organization performed."}
{"task_id": "BigCodeBench/324", "solution": "Certainly! Below is a self-contained Python code snippet that runs files from a given list as subprocesses simultaneously. It uses threading to achieve concurrency, allowing each subprocess to run in its own thread. After all subprocesses have finished, it collects and returns their exit codes in a list.\n\n```python\nimport subprocess\nimport threading\n\ndef run_subprocess(file_path, exit_codes, index):\n    \"\"\"\n    Function to run a subprocess for a given file path and store its exit code.\n    \"\"\"\n    # Run the subprocess\n    process = subprocess.Popen(file_path, shell=True)\n    # Wait for the subprocess to finish\n    process.wait()\n    # Store the exit code in the corresponding index\n    exit_codes[index] = process.returncode\n\ndef task_func(file_list):\n    \"\"\"\n    Runs files from the given list as subprocesses simultaneously and returns their exit codes.\n    \"\"\"\n    # List to store exit codes, initialized with None for each file\n    exit_codes = [None] * len(file_list)\n    # List to keep track of threads\n    threads = []\n\n    # Loop through each file in the list\n    for index, file_path in enumerate(file_list):\n        # Create a thread for each subprocess\n        thread = threading.Thread(target=run_subprocess, args=(file_path, exit_codes, index))\n        threads.append(thread)\n        # Start the thread\n        thread.start()\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n\n# Example usage\nif __name__ == \"__main__\":\n    # List of files to run (replace these with actual file paths or commands)\n    file_list = [\"python script1.py\", \"python script2.py\", \"echo 'Hello, World!'\"]\n    exit_codes = task_func(file_list)\n    print(\"Exit codes:\", exit_codes)\n```\n\n### Notes:\n- Replace `\"python script1.py\"`, `\"python script2.py\"`, and `\"echo 'Hello, World!'\"` in `file_list` with the actual file paths or commands you wish to run.\n- This example assumes that the files you're running are executable commands that can be run directly by the shell. If they are not shell commands, you'll need to adjust the `Popen` call accordingly (e.g., by specifying the interpreter explicitly).\n- The `shell=True` parameter in `Popen` allows shell commands to be executed directly. Be cautious with this setting, especially with untrusted input, as it can pose a security risk.\n- This code is designed to work with Python 3. Make sure you're using a compatible Python version."}
{"task_id": "BigCodeBench/326", "solution": "Below is a self-contained Python code snippet that accomplishes the task described. This function, `task_func`, takes a directory path as input, finds all `.bat` files within that directory, attempts to run them, and then collects their file names along with their exit codes into a list of tuples. If a file cannot be executed, the exit code is recorded as `None`.\n\n```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    # List to store tuples of file names and exit codes\n    results = []\n    \n    # Construct the search pattern for .bat files\n    search_pattern = os.path.join(directory_path, '*.bat')\n    \n    # Find all .bat files in the given directory\n    bat_files = glob.glob(search_pattern)\n    \n    for bat_file in bat_files:\n        try:\n            # Attempt to run the .bat file and capture the exit code\n            result = subprocess.run(bat_file, capture_output=True, text=True, shell=True)\n            # Append the file name and exit code to the results list\n            results.append((os.path.basename(bat_file), result.returncode))\n        except Exception as e:\n            # If the file could not be executed, append the file name with a None exit code\n            results.append((os.path.basename(bat_file), None))\n    \n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace 'your_directory_path_here' with the actual directory path\n    directory_path = 'your_directory_path_here'\n    if not os.path.isdir(directory_path):\n        print(f\"Error: The directory '{directory_path}' does not exist.\")\n        sys.exit(1)\n    \n    result = task_func(directory_path)\n    for file_name, exit_code in result:\n        print(f\"File: {file_name}, Exit Code: {exit_code}\")\n```\n\nReplace `'your_directory_path_here'` with the path to the directory you want to search for `.bat` files. This script uses `subprocess.run` to execute each `.bat` file found. It captures the exit code of each execution, which is stored along with the file name in the `results` list. If an error occurs during the execution of a `.bat` file (e.g., the file does not have execute permissions, or it's not a valid executable), that file's exit code is recorded as `None`.\n\nNote: The use of `shell=True` in `subprocess.run` is necessary for executing `.bat` files on Windows but can introduce security risks if the script is used with untrusted input. Always ensure the input (in this case, the directory path and the `.bat` files within it) is from a trusted source."}
{"task_id": "BigCodeBench/341", "solution": "Below is a self-contained code snippet that fulfills the requirements you've specified. This function, `task_func`, takes a pandas DataFrame and a column name as input. It then generates a figure with two subplots: a histogram with a kernel density estimate (for numerical data) and a box plot, both representing the distribution of the values in the specified column. It includes the necessary error checking to ensure the input DataFrame is valid and contains the specified column.\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Check if the input is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"Input df must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The specified column '{col}' does not exist in the DataFrame.\")\n    \n    # Create a figure with two subplots\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    # Plot a histogram with a kernel density estimate on the first subplot\n    sns.histplot(data=df, x=col, kde=True, ax=axs[0])\n    axs[0].set_title(f'Histogram and KDE of {col}')\n    \n    # Plot a box plot on the second subplot\n    sns.boxplot(data=df, x=col, ax=axs[1])\n    axs[1].set_title(f'Box plot of {col}')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Return the matplotlib figure object\n    return fig\n\n# Example usage:\n# Assuming you have a pandas DataFrame `df` and a column name `col_name` to test.\n# df = pd.DataFrame({'col_name': [1, 2, 3, 4, 5]})\n# col_name = 'col_name'\n# fig = task_func(df, col_name)\n# plt.show()\n```\n\nThis code snippet includes the necessary imports for pandas, seaborn, and matplotlib at the beginning. It defines the `task_func` function, which performs the required checks on the input DataFrame and column name, generates the plots, and returns the matplotlib figure object containing these plots. The example usage at the end is commented out; you can uncomment and modify it according to your specific DataFrame and column name to see the function in action."}
{"task_id": "BigCodeBench/346", "solution": "Certainly! Below is a self-contained Python code snippet that fulfills the requirements you've outlined. This function, `task_func`, will run a Python script as a subprocess with predefined arguments. It checks if the script exists before execution, waits for the process to complete unless specified otherwise, and handles exceptions appropriately.\n\n```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    # Check if the script exists\n    if not os.path.isfile(script_path):\n        raise ValueError(\"The script does not exist.\")\n    \n    # Prepare the command to run the script with any additional arguments\n    command = [sys.executable, script_path] + list(args)\n    \n    try:\n        # Start the subprocess\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        if wait:\n            # Wait for the process to complete\n            stdout, stderr = process.communicate()\n            \n            # Check if the process has exited with a non-zero exit code\n            if process.returncode != 0:\n                # Raise an error with the stderr output if the script failed\n                raise subprocess.CalledProcessError(process.returncode, command, output=stdout, stderr=stderr)\n            \n            # Return the exit code of the process\n            return process.returncode\n        else:\n            # If not waiting, return None immediately\n            return None\n    except subprocess.CalledProcessError as e:\n        # Re-raise the exception if the script raises an exception\n        raise e\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace 'your_script.py' with the path to the script you want to run\n    # and add any arguments it needs after 'wait' parameter as needed\n    try:\n        result = task_func('your_script.py', True, 'arg1', 'arg2')\n        if result is not None:\n            print(f\"Script executed successfully with exit code {result}.\")\n        else:\n            print(\"Script execution started, not waiting for it to finish.\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Script execution failed with exit code {e.returncode}. Error message: {e.stderr}\")\n```\n\nThis script uses the `subprocess.Popen` method to start the script as a subprocess, allowing for both synchronous and asynchronous execution based on the `wait` parameter. It captures both the standard output and standard error streams, and it raises a `subprocess.CalledProcessError` if the subprocess exits with a non-zero status, indicating an error."}
{"task_id": "BigCodeBench/360", "solution": "Below is a self-contained code snippet that fulfills the specified requirements. This code loads data from an Excel spreadsheet, calculates the mean and standard deviation for each column, and then draws a bar chart displaying these statistics. The bar chart is returned as a `matplotlib.figure.Figure` object, and the function also returns a dictionary containing the calculated mean and standard deviation for each column. It includes error handling for the cases where the Excel file does not exist or the specified sheet is not found in the workbook.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(f\"The file {file_location} does not exist.\")\n    \n    try:\n        # Load data from Excel file\n        data = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        # Reraise the ValueError if the sheet does not exist in the workbook\n        raise ValueError(f\"The sheet {sheet_name} does not exist in the workbook.\") from e\n    \n    # Calculate mean and standard deviation for each column\n    stats = {'mean': data.mean(), 'std': data.std()}\n    \n    # Prepare data for the bar chart\n    labels = data.columns\n    mean_values = stats['mean']\n    std_values = stats['std']\n    x = np.arange(len(labels))  # the label locations\n    width = 0.35  # the width of the bars\n    \n    # Create matplotlib figure and axes\n    fig, ax = plt.subplots()\n    rects1 = ax.bar(x - width/2, mean_values, width, label='Mean')\n    rects2 = ax.bar(x + width/2, std_values, width, label='Std Dev')\n    \n    # Add some text for labels, title, and custom x-axis tick labels, etc.\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels)\n    ax.legend()\n    \n    # Function to auto-label the bars, showing their height for better readability\n    def autolabel(rects):\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('{}'.format(round(height, 2)),\n                        xy=(rect.get_x() + rect.get_width() / 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')\n    \n    autolabel(rects1)\n    autolabel(rects2)\n    \n    fig.tight_layout()\n    \n    # Return the dictionary and the figure\n    return stats, fig\n\n# Example usage (Uncomment and replace with actual file path and sheet name)\n# stats, fig = task_func('path_to_your_file.xlsx', 'Sheet1')\n# plt.show()  # To display the figure if running interactively\n```\n\nThis code snippet includes all necessary imports and defines a single function `task_func` that performs the required operations. Note that you need to replace `'path_to_your_file.xlsx'` and `'Sheet1'` with the actual path to your Excel file and the name of the sheet you wish to analyze. Uncomment the example usage and adjust the parameters accordingly to run the code in your environment."}
{"task_id": "BigCodeBench/367", "solution": "Certainly! Below is a self-contained code snippet that completes the described problem. This code will check if each item in the `activities` list is a `datetime` object. If not, it will raise a `TypeError`. If all items are `datetime` objects, it will then count the number of activities performed on each day of the week and generate a bar chart using `matplotlib`.\n\n```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all items in activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All items in the activities list must be datetime objects.\")\n    \n    # Initialize a defaultdict to count activities per day of the week\n    activities_per_day = defaultdict(int)\n    \n    # Populate the defaultdict with counts\n    for activity in activities:\n        activities_per_day[activity.strftime(\"%A\")] += 1\n    \n    # Sort the dictionary by day of the week\n    days_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    sorted_activities = {day: activities_per_day[day] for day in days_of_week if day in activities_per_day}\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(sorted_activities.keys(), sorted_activities.values())\n    \n    # Setting labels and title\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    # Return the Axes object\n    return ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example list of datetime objects\n    activities = [datetime(2023, 10, 1, 14, 30), datetime(2023, 10, 2, 15, 0),\n                  datetime(2023, 10, 2, 18, 0), datetime(2023, 10, 3, 12, 0),\n                  datetime(2023, 10, 4, 9, 30), datetime(2023, 10, 5, 16, 0),\n                  datetime(2023, 10, 6, 17, 30), datetime(2023, 10, 7, 10, 0),\n                  datetime(2023, 10, 8, 11, 0)]\n    \n    ax = task_func(activities)\n    plt.show()\n```\n\nThis code defines a function `task_func` that takes a list of `datetime` objects (`activities`), counts how many activities occurred on each day of the week, and then generates a bar chart displaying this information. The bar chart is customized with labels for the x-axis, y-axis, and a title as specified."}
{"task_id": "BigCodeBench/368", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This function will move a random file from the source directory to the specified destination directory and return the name of the file moved. It uses the `os`, `shutil`, and `random` modules to achieve this. The `seed` parameter is used to ensure reproducibility in the selection of the random file, which can be useful for testing or debugging.\n\n```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the seed for the random number generator for reproducibility\n    random.seed(seed)\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files to move\n    if not files:\n        raise FileNotFoundError(\"No files found in the source directory.\")\n    \n    # Select a random file\n    selected_file = random.choice(files)\n    \n    # Construct the full path of the source and destination\n    src_file_path = os.path.join(src_dir, selected_file)\n    dest_file_path = os.path.join(dest_dir, selected_file)\n    \n    # Move the file\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the file moved\n    return selected_file\n\n# Example usage:\n# src_directory = \"/path/to/source/directory\"\n# dest_directory = \"/path/to/destination/directory\"\n# print(task_func(src_directory, dest_directory))\n```\n\nPlease replace `\"/path/to/source/directory\"` and `\"/path/to/destination/directory\"` with the actual paths to your source and destination directories before running the example. This code will raise a `FileNotFoundError` if the source directory does not contain any files, ensuring that the function behaves predictably in such cases."}
{"task_id": "BigCodeBench/374", "solution": "Below is a self-contained Python code snippet that completes the specified task. This code will search for all Excel (.xlsx) files in the given directory, open each file, iterate through all cells in all sheets, and replace any double quotes (`\"`) with a double backslash followed by a double quote (`\\\\\"`). After processing all files, it will return the number of Excel files it has processed.\n\n```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Compile a regex pattern for finding double quotes\n    double_quote_pattern = re.compile(r'(?<!\\\\)\"')  # Look for \" not preceded by a backslash\n    \n    # Counter for the number of Excel files processed\n    files_processed = 0\n    \n    # Construct the search pattern for all .xlsx files in the directory\n    search_pattern = os.path.join(directory_path, '*.xlsx')\n    \n    # Iterate over all .xlsx files in the directory\n    for excel_file in glob.glob(search_pattern):\n        # Load the workbook\n        workbook = load_workbook(filename=excel_file)\n        modified = False  # Flag to track if the file needs to be saved\n        \n        # Iterate through all sheets in the workbook\n        for sheet in workbook:\n            # Iterate through all rows in the sheet\n            for row in sheet.iter_rows(values_only=False):\n                # Iterate through all cells in the row\n                for cell in row:\n                    if cell.value and isinstance(cell.value, str):  # Check if the cell contains text\n                        # Replace \" with \\\\\" in the cell's text\n                        new_value = double_quote_pattern.sub(r'\\\\\"', cell.value)\n                        if new_value != cell.value:\n                            cell.value = new_value\n                            modified = True\n        \n        # If any modifications were made, save the workbook\n        if modified:\n            workbook.save(excel_file)\n            files_processed += 1\n    \n    # Return the number of Excel files processed\n    return files_processed\n\n# Example usage\nif __name__ == \"__main__\":\n    directory_path = './xlsx_files/'  # Specify the directory path\n    num_processed = task_func(directory_path)\n    print(f\"Number of Excel files processed: {num_processed}\")\n```\n\nThis code uses the `regex` module for its advanced lookbehind capabilities, allowing it to replace double quotes not preceded by a backslash. It iterates over all `.xlsx` files in the specified directory, checks each cell in every sheet, and applies the necessary replacement. After processing all files, it saves the changes to the files that were modified and returns the count of files processed."}
{"task_id": "BigCodeBench/399", "solution": "Certainly! Below is a self-contained code snippet that generates a diagram of a sine wave and a cosine wave given a frequency. It also includes the necessary exception handling for negative frequencies and non-positive sample sizes as per your request.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    # Validate input parameters\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n    \n    # Generate the time points\n    t = np.linspace(0, 1, sample_size)\n    \n    # Calculate the sine and cosine values\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave', linestyle='--')\n    \n    # Adding legend\n    ax.legend()\n    \n    # Adding labels and title\n    ax.set(xlabel='Time (s)', ylabel='Amplitude',\n           title='Sine and Cosine Waves')\n    \n    # Grid\n    ax.grid()\n    \n    # Show the plot (commented out to return the figure and axes objects instead)\n    # plt.show()\n    \n    return fig, ax\n\n# Example usage\nfrequency = 5  # Frequency in Hz\nsample_size = 10000  # Number of samples\ntry:\n    fig, ax = task_func(frequency, sample_size)\n    plt.show()  # Uncomment this line if you want to display the plot outside the function\nexcept ValueError as e:\n    print(e)\n```\n\nThis code defines a function `task_func` that takes a frequency and an optional sample size (defaulting to 10000). It first checks if the frequency and sample size are valid, raising a `ValueError` if they are not. Then, it generates a sine wave and a cosine wave based on the given frequency and plots them on the same graph. The function returns the figure and axes objects of the plot. Note that the `plt.show()` call is commented out inside the function to allow the function to return the plot objects; you can uncomment it if you wish to display the plot immediately after calling the function."}
{"task_id": "BigCodeBench/401", "solution": "Certainly! Below is a self-contained code snippet that initializes a Flask application with the specified `app_name`, configures it for sending emails using Flask-Mail with configurations retrieved from environment variables (or defaults if the variables are not set), and returns a tuple containing the Flask-Mail instance and the app's mail configurations.\n\n```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create a Flask application\n    app = Flask(app_name)\n    \n    # Set default mail configurations\n    default_mail_config = {\n        'MAIL_SERVER': 'localhost',\n        'MAIL_PORT': 25,\n        'MAIL_USE_TLS': False,\n        'MAIL_USERNAME': None,\n        'MAIL_PASSWORD': None\n    }\n    \n    # Update mail configurations with environment variables if they exist\n    mail_config = {key: os.getenv(key, default) for key, default in default_mail_config.items()}\n    \n    # Update the app's configuration with the mail configuration\n    app.config.update(mail_config)\n    \n    # Initialize Flask-Mail with the app\n    mail = Mail(app)\n    \n    # Return the Flask-Mail instance and the mail configurations\n    return (mail, app.config)\n\n# Example usage\nif __name__ == \"__main__\":\n    mail_instance, mail_config = task_func(\"my_flask_app\")\n    print(\"Mail Configurations:\", mail_config)\n```\n\nThis code snippet starts by importing the necessary modules and then defines `task_func`, which takes `app_name` as an argument. It sets up default mail configurations and then attempts to update these with values from environment variables. The Flask app's configuration is updated with these mail settings, and a `Mail` instance is initialized with this app. Finally, the function returns a tuple containing the `Mail` instance and the app's mail configurations.\n\nTo use this code, ensure you have Flask and Flask-Mail installed in your environment. You can install them using pip:\n\n```sh\npip install Flask Flask-Mail\n```\n\nRemember, for this code to function correctly in a real-world scenario, you might need to set the environment variables (`MAIL_SERVER`, `MAIL_PORT`, `MAIL_USE_TLS`, `MAIL_USERNAME`, `MAIL_PASSWORD`) according to your email server's details."}
{"task_id": "BigCodeBench/409", "solution": "Certainly! Below is a self-contained code snippet that completes the specified task. This code assumes you have `pandas` and `numpy` installed in your environment. If not, you can install them using pip (`pip install pandas numpy`). This function reads an Excel file, checks for the existence of the specified column, and then calculates the mean, median, and standard deviation of the data in that column. It raises exceptions as specified for a missing file or column.\n\n```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the Excel file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path: {excel_file_path}\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise e  # Reraise any exception caused by pd.read_excel\n    \n    # Check if the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The specified column '{column_name}' is not found in the Excel file.\")\n    \n    # Extract the column data\n    column_data = df[column_name]\n    \n    # Remove NaN or null values to avoid errors in calculations\n    column_data_clean = column_data.dropna()\n    \n    # Calculate mean, median, and standard deviation\n    mean_val = column_data_clean.mean()\n    median_val = column_data_clean.median()\n    std_dev = column_data_clean.std()\n    \n    # Return the results as a dictionary\n    return {\n        \"mean\": mean_val,\n        \"median\": median_val,\n        \"std_dev\": std_dev\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    excel_file_path = \"path/to/your/excel\"\n    file_name = \"your_excel_file.xlsx\"\n    column_name = \"your_column_name\"\n    try:\n        results = task_func(excel_file_path, file_name, column_name)\n        print(results)\n    except Exception as e:\n        print(e)\n```\n\nReplace `\"path/to/your/excel\"`, `\"your_excel_file.xlsx\"`, and `\"your_column_name\"` with the actual path to your Excel file, the name of your Excel file, and the column name you want to analyze, respectively. This script will print the mean, median, and standard deviation of the specified column or an error message if the file or column does not exist."}
{"task_id": "BigCodeBench/417", "solution": "Below is a self-contained code snippet that follows the instructions provided. This code defines a function `task_func` that takes input data `X` and target labels `Y`, then trains a simple neural network according to the specified requirements. It splits the data, constructs and compiles the model, fits the model to the training data, evaluates it on the test set, and plots the training and validation loss over epochs.\n\n```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD  # Updated import for optimizer\nimport numpy as np\n\ndef task_func(X, Y):\n    # Splitting the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Constructing the Sequential model\n    model = Sequential()\n    model.add(Dense(10, input_dim=2, activation='sigmoid'))  # One dense hidden layer\n    model.add(Dense(1, activation='sigmoid'))  # Output layer\n\n    # Compiling the model\n    model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy')\n\n    # Fitting the model to the training data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n\n    # Plotting the model's training and validation loss over epochs\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Returning the Sequential model and the Axes object of the plot\n    return model, plt.gca()\n\n# Example usage (Uncomment and replace with actual data)\n# X = np.random.rand(100, 2)  # Example input data\n# Y = np.random.randint(2, size=(100, 1))  # Example target labels (binary)\n# model, ax = task_func(X, Y)\n# plt.show()\n```\n\nThis code includes everything needed to run the task as described. It uses `train_test_split` from `sklearn` to split the data, constructs a `Sequential` model with Keras, and plots the training and validation loss using `matplotlib`. Note that you need to provide actual data for `X` and `Y` to use this function. The example usage is commented out; you can uncomment it and replace `X` and `Y` with your data to see how it works."}
{"task_id": "BigCodeBench/418", "solution": "Below is a self-contained code snippet that follows the instructions provided. This code assumes that `X` and `Y` are your features and labels, respectively. It divides the data into training and test sets, creates a Keras Sequential model with one hidden layer using a sigmoid activation function, compiles the model with binary cross-entropy loss and an SGD optimizer with a specified learning rate, fits the model to the training data in a non-verbose mode, and finally plots the ROC curve for the model on the test set, including the AUC score in the plot legend.\n\n```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer\n    model = keras.Sequential([\n        keras.layers.Dense(10, input_dim=X.shape[1], activation='sigmoid'),  # Adjust the input_dim based on your features\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    # Fit the model to the training data\n    model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=0)\n\n    # Predict probabilities for the test set\n    Y_probs = model.predict(X_test)\n    Y_pred = np.round(Y_probs).flatten()\n\n    # Compute ROC curve and AUC\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_probs)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    ax = plt.gca()  # Get current Axes object for the ROC curve plot\n\n    return model, ax\n\n# Example usage (assuming you have X and Y as your dataset):\n# model, ax = task_func(X, Y)\n# plt.show()  # To display the plot if running interactively\n```\n\nPlease ensure that your dataset (`X` and `Y`) is correctly formatted before using this function. `X` should be a 2D array-like structure with samples as rows and features as columns, and `Y` should be a 1D array-like structure with binary labels. Adjust the `input_dim` in the `Dense` layer if your feature set size differs."}
{"task_id": "BigCodeBench/424", "solution": "Below is a self-contained code snippet that follows the given instructions. This code reads an RGB image from a specified path, applies K-means clustering to segment the image into a specified number of regions, saves each region as a separate image, and returns numpy arrays of the original and segmented images. It includes error handling for file not found and invalid `n_clusters` values.\n\n```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise FileNotFoundError(f\"The file at {image_path} could not be read as an image.\")\n    \n    # If n_clusters is 1, return the original image for both original and segmented images\n    if n_clusters == 1:\n        return image, image\n    \n    # Convert image to RGB (OpenCV uses BGR by default)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = image_rgb.reshape((-1, 3))\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n    \n    # Replace each pixel value with its corresponding cluster centroid\n    segmented_img_flat = kmeans.cluster_centers_[kmeans.labels_]\n    segmented_img = segmented_img_flat.reshape(image_rgb.shape).astype(np.uint8)\n    \n    # Convert segmented image back to BGR for saving\n    segmented_img_bgr = cv2.cvtColor(segmented_img, cv2.COLOR_RGB2BGR)\n    \n    # Save each cluster as a separate image (optional)\n    for i in range(n_clusters):\n        cluster_img = np.where(kmeans.labels_.reshape(image_rgb.shape[:-1]) == i, segmented_img_bgr, 0)\n        cv2.imwrite(f\"cluster_{i}.jpg\", cluster_img)\n    \n    return image_rgb, segmented_img\n\n# Example usage\nif __name__ == \"__main__\":\n    original_img, segmented_img = task_func(image_path='path_to_your_image.jpg', n_clusters=3)\n    # Optionally, you can display the images using cv2.imshow or any other method you prefer\n```\n\nThis code includes the following steps:\n- It checks if the specified image file exists and if `n_clusters` is a valid positive integer.\n- It reads the image using OpenCV and checks if the image is successfully loaded.\n- If `n_clusters` is 1, it returns the original image for both the original and segmented images.\n- It applies K-means clustering to the image pixels and replaces each pixel's color with the centroid of the cluster it belongs to.\n- It saves each cluster as a separate image. This step is optional and can be removed if not needed.\n- It returns the original and segmented images as numpy arrays.\n\nMake sure to replace `'path_to_your_image.jpg'` with the actual path to your image when using this function."}
{"task_id": "BigCodeBench/443", "solution": "Below is a self-contained code snippet that follows your instructions. This code defines a function `task_func` that takes a matrix `P`, a 3D tensor `T`, and parameters for KMeans clustering (`n_clusters`, `random_state`, and `n_init`). It calculates the product of `P` and `T`, flattens the result, applies KMeans clustering to the flattened data, and visualizes the clustering result. The function returns the clustering result and the Axes object containing the visualization.\n\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Check if the multiplication is possible\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(\"The number of columns in P must be equal to the number of rows in T for the first dimension.\")\n    \n    # Calculate the product of P and T\n    # The result is a 3D tensor\n    result = np.einsum('ij,jkl->ikl', P, T)\n    \n    # Flatten the result\n    flattened_result = result.reshape(-1, result.shape[-1])\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(flattened_result)\n    cluster_result = kmeans.labels_\n    \n    # Visualization\n    fig, ax = plt.subplots()\n    # For visualization, we need to reduce the dimensionality. Here, we simply use the first two dimensions.\n    ax.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis', marker='o')\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5, marker='x')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate random data for demonstration\n    P = np.random.rand(2, 3)  # Matrix of shape (2, 3)\n    T = np.random.rand(3, 4, 5)  # 3D Tensor of shape (3, 4, 5)\n    \n    cluster_result, ax = task_func(P, T)\n    plt.show()\n```\n\nThis code snippet includes an example usage that generates random data for `P` and `T` to demonstrate how the function works. The `task_func` function first checks if the multiplication between `P` and `T` is possible based on their shapes. It then performs the multiplication using `np.einsum` for more flexibility with tensors, flattens the result, applies KMeans clustering, and finally visualizes the clusters. The visualization uses the first two dimensions of the flattened result for simplicity."}
{"task_id": "BigCodeBench/445", "solution": "Certainly! Below is a self-contained code snippet that calculates the Voronoi diagram for a set of 2D points, applies jittering to these points before plotting, and ensures input validation. It returns a tuple containing the Voronoi object and the axes of the plotted diagram.\n\n```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Input validation\n    if not isinstance(points, np.ndarray):\n        raise ValueError(\"Input points must be a numpy array.\")\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points array must be of shape (n,2) where n is the number of points.\")\n    \n    # Apply jittering to the points to avoid degenerate cases\n    np.random.seed(seed)\n    jitter = np.random.normal(0, 0.0001, points.shape)\n    points_jittered = points + jitter\n    \n    # Calculate Voronoi diagram\n    vor = Voronoi(points_jittered)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    # Adjust plot limits\n    min_x, max_x = points_jittered[:,0].min(), points_jittered[:,0].max()\n    min_y, max_y = points_jittered[:,1].min(), points_jittered[:,1].max()\n    ax.set_xlim(min_x - 1, max_x + 1)\n    ax.set_ylim(min_y - 1, max_y + 1)\n    \n    plt.show()\n    \n    return vor, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random points\n    points = np.random.rand(10, 2) * 50\n    vor, ax = task_func(points)\n```\n\nThis code defines `task_func` which takes an array of points and an optional `seed` for the random number generator used in jittering. It first validates the input to ensure it's a 2D numpy array with the correct shape. Then, it applies a small jitter to each point to avoid issues with degenerate cases in the Voronoi diagram calculation. After calculating the Voronoi diagram using `scipy.spatial.Voronoi`, it plots the diagram using `matplotlib` and adjusts the plot limits based on the jittered points. Finally, it returns the Voronoi object and the plot axes."}
{"task_id": "BigCodeBench/454", "solution": "Below is a self-contained Python code snippet that fulfills the requirements you've described. This function, `task_func`, will move files with a specified extension from a source directory to a destination directory, handling exceptions and providing the desired output.\n\n```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n    \n    # Prepare the pattern for matching files with the given extension\n    pattern = os.path.join(src_dir, f\"*.{ext}\")\n    \n    # List to hold the full paths of successfully moved files\n    moved_files = []\n    \n    # Search for files matching the pattern\n    for file_path in glob.glob(pattern):\n        # Extract the file name from the path\n        file_name = os.path.basename(file_path)\n        # Prepare the destination file path\n        dest_file_path = os.path.join(dest_dir, file_name)\n        \n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            # Move the file\n            shutil.move(file_path, dest_file_path)\n            # Add the destination file path to the list of moved files\n            moved_files.append(dest_file_path)\n    \n    return moved_files\n\n# Example usage\nif __name__ == \"__main__\":\n    src_directory = \"path/to/source/directory\"\n    dest_directory = \"path/to/destination/directory\"\n    extension = \"txt\"  # Example extension\n    try:\n        moved_files = task_func(src_directory, dest_directory, extension)\n        print(f\"Files moved: {moved_files}\")\n    except FileNotFoundError as e:\n        print(e)\n```\n\nReplace `\"path/to/source/directory\"` and `\"path/to/destination/directory\"` with the actual paths to your source and destination directories, and change `\"txt\"` to the extension of the files you want to move. This script will then move all files with the specified extension from the source to the destination directory, unless a file with the same name already exists in the destination directory. It will print the paths of the files that were successfully moved."}
{"task_id": "BigCodeBench/458", "solution": "Below is a self-contained code snippet that fulfills the requirements you've described. This code will load a JSON string into a dictionary, normalize the dictionary by doubling the numerical values (including those within lists and represented as strings), and then create a Pandas DataFrame from the normalized dictionary.\n\n```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Load JSON string into a dictionary\n    try:\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        # Return an empty DataFrame in case of JSON decode error\n        return pd.DataFrame()\n\n    # Function to double numerical values, including those in strings\n    def double_value(value):\n        if isinstance(value, list):\n            return [double_value(v) for v in value]\n        elif isinstance(value, str):\n            # Check if the string can be interpreted as a number\n            if re.match(r'^-?\\d+(\\.\\d+)?$', value):\n                return float(value) * 2\n            else:\n                return value\n        elif isinstance(value, (int, float)):\n            return value * 2\n        else:\n            return value\n\n    # Normalize the dictionary by doubling the numerical values\n    normalized_dict = {k: double_value(v) for k, v in data_dict.items()}\n\n    # Create a Pandas DataFrame from the normalized dictionary\n    df = pd.DataFrame([normalized_dict])\n\n    # Ensure numerical values are stored as floats\n    for col in df.columns:\n        try:\n            df[col] = df[col].astype(float)\n        except ValueError:\n            pass  # Leave non-numeric columns as-is\n\n    return df\n\n# Example usage\njson_str = '{\"age\": \"30\", \"height\": 175, \"scores\": [10, 20, 30], \"name\": \"John\"}'\ndf = task_func(json_str)\nprint(df)\n```\n\nThis code snippet starts by attempting to load the JSON string into a dictionary. It then defines a helper function `double_value` to handle the doubling of numerical values, including those represented as strings. This function is applied to all values in the dictionary. After normalizing the dictionary, it creates a Pandas DataFrame from this dictionary. The code attempts to convert all columns to float where possible, leaving non-numeric columns unchanged. Finally, it returns the resulting DataFrame. If the input JSON string is empty or cannot be decoded, an empty DataFrame is returned instead."}
{"task_id": "BigCodeBench/461", "solution": "Below is a self-contained Python code snippet that fulfills the requirements you've described. This code defines a function `task_func` that executes a given bash script and monitors its CPU and memory usage using the `psutil` library. It checks if the script exists, executes it, and aggregates CPU and memory usage data until the script completes or the specified timeout is reached. The function returns a dictionary containing the cumulative CPU usage in percentage and the total memory usage in bytes.\n\n```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script file exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script {script_path} does not exist.\")\n    \n    # Start the script in a subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Use psutil to monitor the process\n    try:\n        ps_process = psutil.Process(process.pid)\n    except psutil.NoSuchProcess:\n        raise Exception(\"Failed to monitor the process.\")\n    \n    start_time = time.time()\n    cpu_usage = 0\n    memory_usage = 0\n    \n    while True:\n        if ps_process.is_running() and not ps_process.status() == psutil.STATUS_ZOMBIE:\n            try:\n                # Accumulate CPU and memory usage\n                cpu_usage += ps_process.cpu_percent(interval=1)\n                memory_info = ps_process.memory_info()\n                memory_usage += memory_info.rss  # rss is the Resident Set Size\n            except psutil.NoSuchProcess:\n                break  # Process has finished\n        else:\n            break  # Process is not running or is a zombie\n        \n        # Check for timeout\n        if time.time() - start_time > timeout:\n            ps_process.terminate()  # Attempt to terminate the process\n            break\n    \n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    script_path = \"path/to/your/script.sh\"\n    result = task_func(script_path)\n    print(result)\n```\n\nPlease replace `\"path/to/your/script.sh\"` with the actual path to your bash script. This code assumes that you have the `psutil` library installed. If not, you can install it using pip:\n\n```bash\npip install psutil\n```\n\nThis function starts the specified script in a subprocess, then uses `psutil` to monitor its CPU and memory usage. It accumulates these metrics until the script completes or the specified timeout is reached. If the process becomes a zombie or is not found, or if it runs beyond the timeout, the function handles these cases appropriately."}
{"task_id": "BigCodeBench/477", "solution": "Below is a self-contained code snippet that fulfills the requirements you've described. This code generates a DataFrame with N rows and 3 columns: \"x\" and \"y\" containing random values, and \"category\" containing random categories from a given list. It ensures that each category appears at least once if N is greater than or equal to the number of categories. It then draws a scatter plot of \"x\" vs \"y,\" colored by \"category\".\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Ensure each category appears at least once if N >= len(CATEGORIES)\n    min_categories = min(N, len(CATEGORIES))\n    guaranteed_categories = np.random.choice(CATEGORIES, size=min_categories, replace=False)\n    \n    # Fill the remaining categories randomly\n    remaining_categories = np.random.choice(CATEGORIES, size=N - min_categories)\n    \n    # Combine guaranteed and remaining categories\n    categories = np.concatenate((guaranteed_categories, remaining_categories))\n    \n    # Shuffle the categories to avoid any order bias\n    np.random.shuffle(categories)\n    \n    # Generate random values for x and y columns\n    x_values = np.random.rand(N)\n    y_values = np.random.rand(N)\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\n        'x': x_values,\n        'y': y_values,\n        'category': categories\n    })\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    for category in CATEGORIES:\n        # Filter data for each category\n        filtered_data = df[df['category'] == category]\n        ax.scatter(filtered_data['x'], filtered_data['y'], label=category)\n    \n    ax.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Scatter plot of x vs y, colored by category')\n    plt.show()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func(N=100)\n```\n\nThis code starts by setting a random seed for reproducibility, then ensures that if the number of rows `N` is greater than or equal to the number of categories, each category appears at least once. It fills the rest of the category assignments randomly. The `x` and `y` columns are filled with random values using `numpy.random.rand`. Finally, it creates a scatter plot using `matplotlib`, with points colored by their respective category. The function returns the DataFrame and the Axes object of the plot."}
{"task_id": "BigCodeBench/486", "solution": "Certainly! Below is a self-contained code snippet that generates a time series from a given epoch start time to end time with a specified step and trend. The values are generated from a normal distribution, and a linear trend is added based on the provided trend value. The time series is then plotted with timestamps on the x-axis ('Time') and values on the y-axis ('Value').\n\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generates and plots a time series with a specified step and trend from a given start time to end time.\n    \n    Parameters:\n    - start_time (str): The start time in 'YYYY-MM-DD HH:MM:SS' format.\n    - end_time (str): The end time in 'YYYY-MM-DD HH:MM:SS' format.\n    - step (int): The step in seconds between each timestamp.\n    - trend (float): The value to be added to each subsequent value to create a linear trend.\n    - seed (int): The seed for the random number generator.\n    \n    Returns:\n    - ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    # Convert start and end time to datetime objects\n    start = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Generate timestamps\n    timestamps = pd.date_range(start=start, end=end, freq=f'{step}s')\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random values from a normal distribution\n    values = np.random.randn(len(timestamps))\n    \n    # Add a linear trend to the values\n    values += np.arange(len(timestamps)) * trend\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data={'Time': timestamps, 'Value': values})\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'])\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n    \n    # Formatting the x-axis to show more readable dates\n    plt.gcf().autofmt_xdate()  # Auto format the date on the x-axis for better readability\n    \n    return ax\n\n# Example usage\nstart_time = '2023-01-01 00:00:00'\nend_time = '2023-01-02 00:00:00'\nstep = 3600  # 1 hour\ntrend = 0.1\nax = task_func(start_time, end_time, step, trend)\nplt.show()\n```\n\nThis code defines a function `task_func` that takes a start time, end time, step in seconds, trend value, and an optional seed for the random number generator. It generates a series of timestamps between the start and end times with the specified step, creates values from a normal distribution, adds a linear trend to these values, and then plots the resulting time series. The x-axis of the plot is labeled 'Time', and the y-axis is labeled 'Value'."}
{"task_id": "BigCodeBench/492", "solution": "Below is a self-contained code snippet that fulfills the described requirements. This code generates sales data for five products from a given epoch time up to the current time. It checks the input validity, generates random sales data for each day within the specified range, and outputs the data as a pandas DataFrame.\n\n```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Convert epoch milliseconds to datetime\n    start_date = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    \n    # Get the current datetime\n    end_date = datetime.now()\n    \n    # Initialize a list to store the sales data\n    sales_data = []\n    \n    # Generate sales data for each day between the start and end dates\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            # Generate random sales quantity\n            sales_quantity = random.randint(10, 50)\n            \n            # Append the data to the sales_data list\n            sales_data.append({\n                \"Product\": product,\n                \"Date\": current_date,\n                \"Sales\": sales_quantity\n            })\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n    \n    # Convert the sales data into a pandas DataFrame\n    sales_df = pd.DataFrame(sales_data)\n    \n    # Ensure the 'Date' column is in datetime format\n    sales_df['Date'] = pd.to_datetime(sales_df['Date']).dt.date\n    \n    return sales_df\n\n# Example usage\nepoch_milliseconds = 1609459200000  # This represents January 1, 2021\nsales_df = task_func(epoch_milliseconds)\nprint(sales_df.head())\n```\n\nThis code starts by setting a random seed for reproducibility, then converts the given epoch time in milliseconds to a Python `datetime` object. It iterates over each day from the start date to the current date, generating random sales data for each of the specified products. The sales data is collected in a list of dictionaries, which is then converted into a pandas DataFrame. The 'Date' column of the DataFrame is formatted to ensure it contains `datetime.date` objects. Finally, the DataFrame is returned."}
{"task_id": "BigCodeBench/501", "solution": "```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is of valid type\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Attempt to parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except ValueError as e:\n        raise ValueError(\"Invalid JSON string\") from e\n    \n    # Ensure the parsed data is a list (as expected for an array in JSON)\n    if not isinstance(data, list):\n        raise ValueError(\"JSON string must represent a list/array\")\n    \n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(data)\n    \n    # Define the Excel writer and the file path\n    file_path = os.path.abspath(filename)\n    writer = pd.ExcelWriter(file_path, engine='xlsxwriter')\n    \n    # Handle empty JSON arrays by not adding any data rows\n    if not df.empty:\n        df.to_excel(writer, sheet_name=sheet_name, index=False)\n    else:\n        # Create an empty sheet\n        workbook = writer.book\n        workbook.add_worksheet(sheet_name)\n    \n    # Save the Excel file\n    try:\n        writer.save()\n    except Exception as e:\n        raise Exception(\"Error writing Excel file\") from e\n    \n    return file_path\n\n# Example usage\nif __name__ == \"__main__\":\n    json_str = '[{\"name\": \"John Doe\", \"age\": 30}, {\"name\": \"Jane Doe\", \"age\": 25}]'\n    filename = \"output.xlsx\"\n    try:\n        excel_file_path = task_func(json_str, filename)\n        print(f\"Excel file created at: {excel_file_path}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```\nThis code snippet defines a function `task_func` that converts a JSON string into an Excel file. It uses the `pandas` library for data manipulation and the `xlsxwriter` engine for writing to an Excel file. The function checks for the correct type of the input JSON string, attempts to parse it, and then writes it to an Excel file, handling empty arrays as specified. It raises exceptions for invalid input types, invalid JSON, and other file writing errors."}
{"task_id": "BigCodeBench/502", "solution": "To fulfill your request, I'll provide a self-contained Python code snippet that generates a DataFrame with randomly generated activity durations for a specified number of days in the past. It will then plot this data using Seaborn, showing daily activity durations for activities like Running, Swimming, Cycling, Yoga, and Weight Training. The function `task_func` will return a tuple containing the matplotlib Axes object from the plot and the DataFrame used for plotting.\n\n```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Seed the random number generator for reproducibility\n    random.seed(random_seed)\n    \n    # Define the activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate dates for the specified number of days in the past\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    date_range = pd.date_range(start=start_date, end=end_date, closed='left')\n    \n    # Initialize an empty list to store the data\n    data = []\n    \n    # Generate random activity durations for each day and activity\n    for date in date_range:\n        for activity in activities:\n            duration = random.randint(0, 120)  # Random duration between 0 and 120\n            data.append({\"Date\": date, \"Activity\": activity, \"Duration\": duration})\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x=\"Date\", y=\"Duration\", hue=\"Activity\", marker=\"o\")\n    plt.xticks(rotation=45)\n    plt.title(\"Daily Activity Durations\")\n    \n    # Return the Axes object and the DataFrame\n    return ax, df\n\n# Example usage\nax, df = task_func(days_in_past=7, random_seed=42)\nplt.show()\n```\n\nThis code snippet starts by importing the necessary libraries: `datetime` for handling dates, `pandas` for data manipulation, `random` for generating random numbers, `seaborn` for plotting, and `matplotlib.pyplot` for additional plotting options.\n\nThe `task_func` function generates a list of dates for the specified number of days in the past. For each date and each activity, it generates a random duration between 0 and 120 minutes. This data is then used to create a pandas DataFrame. Finally, it plots this data using Seaborn's `lineplot` function, with dates on the x-axis, activity durations on the y-axis, and different colors for each activity. The function returns the matplotlib Axes object for the plot and the DataFrame containing the generated data."}
{"task_id": "BigCodeBench/503", "solution": "Certainly! Below is a self-contained code snippet that generates a DataFrame of stock prices for a specified number of days in the past using random data. The prices are floats in the range [0.0, 1.0), and the DataFrame includes a date index for the specified number of days back from today.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate a date range for the specified number of days in the past\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    dates = pd.date_range(start=start_date, end=end_date, closed='left')\n    \n    # Generate random stock prices\n    prices = np.random.rand(len(dates), len(stock_names))\n    \n    # Create the DataFrame\n    df = pd.DataFrame(prices, index=dates, columns=stock_names)\n    \n    return df\n\n# Example usage\ndf = task_func()\nprint(df)\n```\n\nThis code does the following:\n- It imports the necessary libraries: `numpy` for generating random numbers, `pandas` for creating and handling the DataFrame, and `datetime` and `timedelta` for date manipulation.\n- Defines a function `task_func` with default parameters for the number of days in the past, stock names, and a random seed to ensure reproducibility.\n- Calculates the start and end dates based on the current date and the specified number of days in the past.\n- Generates random stock prices using `numpy.random.rand`, which produces values in the range [0.0, 1.0).\n- Creates a pandas DataFrame with the generated dates as the index and the stock names as the column headers.\n- Finally, it prints the resulting DataFrame when the function is called.\n\nThis code snippet can be easily modified to adjust the number of days, the stock names, or the random seed as needed."}
{"task_id": "BigCodeBench/509", "solution": "Certainly! Below is a self-contained Python code snippet that compares two CSV files and generates a difference report as specified. This code uses the `pandas` library for creating the DataFrame and `difflib.ndiff` for comparing the files. It also handles the exceptions as requested.\n\n```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        # Check if files exist and are not empty\n        for file_path in [file_path1, file_path2]:\n            with open(file_path, 'r', newline='') as csvfile:\n                if csvfile.read().strip() == '':\n                    raise ValueError(f\"File {file_path} is empty.\")\n        \n        # Read files and compare\n        with open(file_path1, 'r', newline='') as file1, open(file_path2, 'r', newline='') as file2:\n            file1_reader = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            file2_reader = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            \n            file1_lines = [''.join(row) for row in file1_reader]\n            file2_lines = [''.join(row) for row in file2_reader]\n            \n            diff = list(ndiff(file1_lines, file2_lines))\n            \n        # Process differences\n        differences = []\n        line_number = 1\n        for line in diff:\n            if line.startswith(' '):\n                # No change, increment line_number\n                line_number += 1\n            elif line.startswith('-'):\n                differences.append({'Line Number': line_number, 'Status': '-', 'Content': line[2:]})\n                line_number += 1\n            elif line.startswith('+'):\n                differences.append({'Line Number': line_number, 'Status': '+', 'Content': line[2:]})\n            else:\n                # This case should not happen with ndiff but is here for completeness\n                continue\n        \n        # Create DataFrame\n        df_differences = pd.DataFrame(differences)\n        \n        return df_differences\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\")\n    except ValueError as e:\n        raise ValueError(e)\n    except Exception as e:\n        raise Exception(f\"An error occurred: {str(e)}\")\n\n# Example usage\nfile_path1 = 'path/to/your/first_file.csv'\nfile_path2 = 'path/to/your/second_file.csv'\ntry:\n    diff_report = task_func(file_path1, file_path2)\n    print(diff_report)\nexcept Exception as e:\n    print(e)\n```\n\nThis code snippet defines the `task_func` function that takes two file paths as input, along with optional `delimiter` and `quotechar` parameters for reading the CSV files. It then compares these files line by line, generates a report of the differences, and returns this report as a pandas DataFrame. The function also handles various exceptions, including file not found, empty files, and other I/O related errors."}
{"task_id": "BigCodeBench/511", "solution": "Below is a self-contained code snippet that defines the `task_func` function. This function takes a column name and a list of employee data, calculates statistics for the specified column, and visualizes the data with a pie chart using the Age column as labels. The function returns a tuple containing a dictionary with the 'sum', 'mean', 'min', and 'max' of the column, and an Axes object showing the pie chart.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Convert the list of dictionaries to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Initialize the statistics dictionary\n    stats = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n    \n    # Check if the DataFrame is not empty and the column exists\n    if not df.empty and column in df.columns:\n        # Calculate statistics\n        stats['sum'] = df[column].sum()\n        stats['mean'] = df[column].mean()\n        stats['min'] = df[column].min()\n        stats['max'] = df[column].max()\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    if 'Age' in df.columns and not df.empty:\n        # Use the Age column as labels if it exists\n        labels = df['Age'].astype(str).tolist()\n        # Ensure the column to visualize is numeric and has no missing values\n        if df[column].dtype in [np.float64, np.int64] and not df[column].isnull().values.any():\n            sizes = df[column].tolist()\n            ax.pie(sizes, labels=labels, autopct='%1.1f%%')\n            ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n        else:\n            print(f\"The column '{column}' is not numeric or contains missing values. Cannot generate pie chart.\")\n    else:\n        print(\"The 'Age' column is missing or the data is empty. Cannot generate pie chart.\")\n    \n    plt.show()\n    \n    return stats, ax\n\n# Example usage:\ndata = [\n    {'Name': 'John', 'Age': 28, 'Salary': 5000},\n    {'Name': 'Jane', 'Age': 32, 'Salary': 5500},\n    {'Name': 'Doe', 'Age': 45, 'Salary': 6000},\n    {'Name': 'Smith', 'Age': 36, 'Salary': 5200}\n]\n\ncolumn = 'Salary'\nstats, ax = task_func(column, data)\nprint(stats)\n```\n\nThis code snippet starts by importing the necessary libraries: `pandas` for data manipulation, `numpy` for numerical operations, and `matplotlib.pyplot` for plotting. The `task_func` function then processes the input data to calculate the required statistics and generate a pie chart visualization based on the Age column. Note that the pie chart is only generated if the specified column is numeric and contains no missing values, and if the Age column is present in the data."}
{"task_id": "BigCodeBench/513", "solution": "Below is a self-contained code snippet that fulfills the requirements you've outlined. This code defines a function `task_func` that takes a column name and a list of dictionaries (representing fitness data) as input. It validates the input data, calculates statistical measures (sum, mean, minimum, maximum) for the specified column, and plots a line chart for the column values over dates. It raises exceptions if any of the specified conditions are not met.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Validate input data\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    # Convert list of dictionaries to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Validate column\n    if column not in df.columns:\n        raise KeyError(f\"The specified column {column} is not valid.\")\n    \n    # Validate non-negative values for specific columns\n    for col in ['steps', 'calories burned', 'distance walked']:\n        if col in df.columns and df[col].apply(lambda x: x < 0).any():\n            raise ValueError(f\"Negative values found in {col}.\")\n    \n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Date'], df[column], marker='o', linestyle='-')\n    plt.title(f\"Line Chart of {column}\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(column)\n    ax = plt.gca()  # Get the current Axes instance\n    \n    return stats, ax\n\n# Example usage\ndata = [\n    {'Date': '2023-01-01', 'steps': 1000, 'calories burned': 300, 'distance walked': 0.8},\n    {'Date': '2023-01-02', 'steps': 1500, 'calories burned': 350, 'distance walked': 1.1},\n    {'Date': '2023-01-03', 'steps': 2000, 'calories burned': 400, 'distance walked': 1.5},\n    # Add more data as needed\n]\n\ntry:\n    column = 'steps'  # Specify the column you want to analyze\n    stats, ax = task_func(column, data)\n    print(stats)\n    plt.show()\nexcept KeyError as e:\n    print(f\"Error: {e}\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n```\n\nThis code snippet includes error handling for the specified exceptions and uses Pandas for data manipulation and Matplotlib for plotting. Make sure you have these libraries installed in your Python environment (`pandas` and `matplotlib`) to run this code successfully."}
{"task_id": "BigCodeBench/526", "solution": "Certainly! Below is a self-contained code snippet that reads a list of dictionaries from a JSON file, calculates the mean and median for each key (ignoring non-numeric or missing values), and converts the results into a Pandas DataFrame. The function is defined as `task_func` and takes an input file name with a default value of `\"data.json\"`.\n\n```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Read data from JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize a defaultdict to store lists of values for each key\n    values_dict = defaultdict(list)\n    \n    # Iterate over each dictionary in the list\n    for entry in data:\n        for key, value in entry.items():\n            # Check if the value is numeric (int or float)\n            if isinstance(value, (int, float)):\n                values_dict[key].append(value)\n    \n    # Initialize a dictionary to store the mean and median\n    stats_dict = {'mean': [], 'median': []}\n    keys_sorted = sorted(values_dict.keys())  # Sort keys for consistent order\n    \n    # Calculate mean and median for each key\n    for key in keys_sorted:\n        numeric_values = np.array(values_dict[key])\n        mean_value = np.mean(numeric_values)\n        median_value = np.median(numeric_values)\n        \n        # Append the results to the stats dictionary\n        stats_dict['mean'].append(mean_value)\n        stats_dict['median'].append(median_value)\n    \n    # Convert the stats dictionary to a Pandas DataFrame\n    df = pd.DataFrame(stats_dict, index=keys_sorted)\n    \n    return df\n\n# Example usage\n# Ensure you have a file named 'data.json' in the same directory with the appropriate content\n# df = task_func(\"data.json\")\n# print(df)\n```\n\nThis code snippet starts by importing the necessary libraries: `json` for reading JSON files, `pandas` for data manipulation and analysis, `numpy` for numerical operations, and `collections.defaultdict` for easily accumulating lists of values. It defines the `task_func` function, which reads the JSON file, processes the data to calculate mean and median values while ignoring non-numeric or missing values, and finally organizes the results into a Pandas DataFrame sorted by the keys from the input data. The DataFrame contains two columns: 'mean' and 'median', indexed by the variable names (keys) from the input data."}
{"task_id": "BigCodeBench/528", "solution": "Below is a self-contained code snippet that follows your instructions. This code defines a function `task_func` that takes a file path as input, checks if the file is a CSV, identifies duplicate rows, converts them into a pandas DataFrame, and then plots the counts of these duplicates using matplotlib. It returns a dictionary of duplicate rows with their counts and a matplotlib Axes object showing a bar chart of these counts.\n\n```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check if the file is a CSV\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File is not a CSV\")\n\n    # Read the CSV file and count duplicates\n    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        rows = [tuple(row) for row in reader]\n        row_counts = Counter(rows)\n\n    # Filter out unique rows, keeping only duplicates\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to a pandas DataFrame\n    duplicates_df = pd.DataFrame(duplicates.keys(), columns=next(csv.reader(open(file_path))))\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    duplicates_df.apply(lambda x: x.value_counts()).sum(axis=1).plot(kind='bar', ax=ax)\n    ax.set_title('Counts of Duplicate Rows')\n    ax.set_xlabel('Rows')\n    ax.set_ylabel('Counts')\n\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    # Return the dictionary of duplicates and the Axes object\n    return duplicates, ax\n\n# Example usage (Uncomment and replace 'your_file_path.csv' with your actual file path)\n# duplicates_dict, ax = task_func('your_file_path.csv')\n# plt.show()\n```\n\nThis code does the following:\n- It starts by checking if the provided file path ends with `.csv`, raising a `ValueError` if not.\n- It reads the CSV file, counts occurrences of each row, and identifies duplicates.\n- It filters out unique rows to keep only the duplicates and converts this information into a pandas DataFrame.\n- It plots a bar chart showing the counts of each duplicate row.\n- Finally, it returns a dictionary of the duplicate rows with their counts and the matplotlib Axes object for the plot.\n\nRemember to replace `'your_file_path.csv'` with the actual path to your CSV file when calling `task_func`. Also, ensure that your environment has the required libraries (`csv`, `collections`, `pandas`, and `matplotlib`) installed."}
{"task_id": "BigCodeBench/530", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check for empty DataFrame or negative ages\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Round down ages to the nearest integer\n    df['age'] = df['age'].apply(np.floor).astype(int)\n    \n    # Identify duplicate names\n    duplicate_names = df[df.duplicated('name', keep=False)]\n    \n    # If there are no duplicates, return None for the plot\n    if duplicate_names.empty:\n        return Counter(), None\n    \n    # Calculate age distribution among duplicates\n    age_distribution = Counter(duplicate_names['age'])\n    \n    # Plotting\n    min_age, max_age = min(age_distribution.keys()), max(age_distribution.keys())\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)  # Adjust bins to ensure ages fall within bins\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(duplicate_names['age'], bins=bins, kde=False)\n    ax.set_title('Age Distribution Among Duplicate Names')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    \n    return age_distribution, ax\n\n# Example usage:\n# Create a sample DataFrame\ndata = {\n    'name': ['John', 'Jane', 'John', 'Mike', 'Jane', 'Mike', 'Anna'],\n    'age': [23, 34, 23.5, 40, 34.9, 40.2, 28]\n}\ndf = pd.DataFrame(data)\n\n# Call the function\nage_distribution, plot = task_func(df)\nprint(age_distribution)\nif plot is not None:\n    plt.show()\n```"}
{"task_id": "BigCodeBench/532", "solution": "To complete the given task, you'll need to write a function that takes a pandas DataFrame and an optional `bins` parameter for the histogram. The function will identify duplicate values in the 'value' column, count them using `Counter`, plot a histogram of the values, and overlay a normal distribution curve. Here's how you can do it:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Count duplicates in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates_counter = Counter({k: v for k, v in value_counts.items() if v > 1})\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    # Histogram for the 'value' column\n    n, bins, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6, label='Histogram')\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(df['value'])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    \n    # Overlay a normal distribution curve\n    ax.plot(x, p * max(n) / max(p), 'k', linewidth=2, label='Normal distribution')\n    \n    # Setting titles and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Show legend\n    ax.legend()\n    \n    # Return the Counter object and the Axes object\n    return duplicates_counter, ax\n\n# Example usage:\n# Assuming you have a DataFrame 'df' with a 'value' column\n# df = pd.DataFrame({'value': np.random.randn(100)})\n# duplicates_counter, ax = task_func(df, bins=10)\n# plt.show()\n```\n\nThis code snippet defines the `task_func` function, which first counts the duplicate values in the 'value' column of the provided DataFrame. It then plots a histogram of the values in this column, fits a normal distribution to these values, and overlays this distribution curve on the histogram. The histogram bars are green with 60% opacity, and the normal distribution curve is black with a linewidth of 2. The plot is titled \"Distribution\" and includes labels for both axes. Finally, the function returns a tuple containing the `Counter` object for duplicate values and the `Axes` object for the plot. \n\nRemember to replace the example usage with your actual DataFrame or adjust the parameters as needed."}
{"task_id": "BigCodeBench/553", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Ensure the number of columns does not exceed the length of COLUMNS\n    num_columns = min(len(b), len(COLUMNS))\n    \n    # Generate random data\n    data = np.random.rand(len(a), num_columns)\n    \n    # Create DataFrame using 'a' for index and the appropriate number of 'COLUMNS' for columns\n    df = pd.DataFrame(data, index=a, columns=COLUMNS[:num_columns])\n    \n    # Plotting the DataFrame as a bar chart\n    ax = df.plot(kind='bar')\n    \n    # Display the plot\n    plt.show()\n    \n    return ax\n\n# Example usage\na = ['Row1', 'Row2', 'Row3']\nb = [1, 2, 3, 4]  # This example will create a DataFrame with 4 columns since len(b) = 4 but will be limited by len(COLUMNS)\n\ntask_func(a, b)\n```\nThis code snippet defines a function `task_func` that takes two lists, `a` and `b`, as input. It generates a pandas DataFrame with random values where `a` sets the DataFrame's row indices, and the length of `b` determines the number of columns, which are named according to the predefined `COLUMNS` list but limited by the length of `b` and the maximum available columns in `COLUMNS`. The DataFrame is then plotted as a bar chart, and the function returns the `matplotlib.axes.Axes` object of the plotted chart."}
{"task_id": "BigCodeBench/560", "solution": "Certainly! Below is a self-contained code snippet that defines a function `task_func` to plot a bar chart of monthly data values for a single year. The function takes a dictionary `data` where keys are month numbers (1 for January, 2 for February, etc.) and values are the data values for those months. It then plots these values with the appropriate labels and title.\n\n```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Convert the input data to a pandas DataFrame\n    df = pd.DataFrame(list(data.items()), columns=['Month', 'Value'])\n    # Convert month numbers to month names for better readability\n    df['Month'] = df['Month'].apply(lambda x: datetime.strptime(str(x), \"%m\").strftime(\"%B\"))\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', x='Month', y='Value', ax=ax, legend=False)\n    \n    # Setting the title and labels\n    ax.set_title('Monthly Data for yyyy')  # Assuming 'yyyy' is static; replace as needed\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Rotating x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Show plot (optional in Jupyter Notebooks, required in other environments)\n    plt.show()\n    \n    return ax\n\n# Example usage\ndata = {\n    1: 10,\n    2: 15,\n    3: 12,\n    4: 18,\n    5: 20,\n    6: 25,\n    7: 22,\n    8: 30,\n    9: 28,\n    10: 27,\n    11: 24,\n    12: 30\n}\n\n# Call the function with the example data\ntask_func(data)\n```\n\nThis code snippet starts by importing the necessary libraries: `datetime` for handling dates, `matplotlib.pyplot` for plotting, and `pandas` for data manipulation. The `task_func` function first converts the input dictionary into a pandas DataFrame for easier manipulation and plotting. It then converts the month numbers into month names for readability on the x-axis. After setting up the plot with the title and labels, it displays the bar chart. Note that the title is set to 'Monthly Data for yyyy'; you might want to dynamically insert the year based on your data or requirements."}
{"task_id": "BigCodeBench/567", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input string data to a numpy array of integers\n    data_array = np.array(list(map(int, data.split(','))))\n    \n    # Calculate bins for the histogram\n    bins = np.arange(data_array.min(), data_array.max() + 2) - 0.5\n    \n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram\n    ax.hist(data_array, bins=bins, edgecolor='black')\n    \n    # Set the title and labels\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\n\n# Example usage:\ndata = \"1,2,2,3,3,3,4,4,4,4,5,5,5,5,5\"\ntask_func(data)\n```\nThis code snippet defines a function `task_func` that takes a string of comma-separated numeric values as input. It first converts this string into a numpy array of integers. Then, it calculates the bins for the histogram using the provided formula. A histogram is plotted with these bins, and the axes are labeled accordingly. The function finally displays the histogram and returns the `Axes` object for further customization or inspection if needed."}
{"task_id": "BigCodeBench/574", "solution": "Certainly! Below is a self-contained Python code snippet that creates a noisy sine wave of a specified length and adjusts a curve to the data using `curve_fit` from `scipy.optimize`. The function `task_func` will plot the noisy sine wave along with its adjusted curve and return the `Axes` object of the plot.\n\n```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sine_wave(x, amplitude, frequency, phase, offset):\n    \"\"\"\n    Sine wave function.\n    \n    Parameters:\n    - x: Array of x values.\n    - amplitude: Amplitude of the sine wave.\n    - frequency: Frequency of the sine wave.\n    - phase: Phase shift of the sine wave.\n    - offset: Vertical offset of the sine wave.\n    \n    Returns:\n    - Array of y values corresponding to the sine wave.\n    \"\"\"\n    return amplitude * np.sin(frequency * x + phase) + offset\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    Generates a noisy sine wave, fits a curve to it, and plots both.\n    \n    Parameters:\n    - array_length: Number of points in the sine wave.\n    - noise_level: Standard deviation of the noise.\n    \n    Returns:\n    - ax: Axes object of the plot.\n    \"\"\"\n    # Generate x values\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # Parameters for the true sine wave\n    amplitude = 1\n    frequency = 1\n    phase = 0\n    offset = 0\n    \n    # Generate clean sine wave\n    y_true = sine_wave(x, amplitude, frequency, phase, offset)\n    \n    # Add noise\n    y_noisy = y_true + np.random.normal(0, noise_level, array_length)\n    \n    # Fit curve\n    popt, _ = curve_fit(sine_wave, x, y_noisy, p0=[1, 1, 0, 0])\n    \n    # Plot\n    fig, ax = plt.subplots()\n    ax.plot(x, y_noisy, 'b.', label='Noisy data')  # Noisy data\n    ax.plot(x, sine_wave(x, *popt), 'r-', label='Fitted curve')  # Fitted curve\n    ax.legend()\n    \n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.title('Noisy Sine Wave and Fitted Curve')\n    \n    return ax\n\n# Example usage\nax = task_func(100, 0.2)\nplt.show()\n```\n\nThis code defines a sine wave function and a task function. The task function generates a noisy sine wave based on the specified `array_length` and `noise_level`. It then uses `curve_fit` to find the best-fitting sine wave parameters and plots both the noisy data and the fitted curve. The plot is displayed, and the `Axes` object is returned for further manipulation or inspection if needed."}
{"task_id": "BigCodeBench/579", "solution": "Below is a self-contained code snippet that fulfills the requirements you've outlined. This code reads a CSV file, normalizes the text within it to ASCII, counts the words, and then returns both a matplotlib bar plot of the 10 most common words with their frequencies and a list of these words with their counts as tuples. It also includes error handling for file not found and IO errors.\n\n```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    try:\n        # Initialize a Counter object to count word occurrences\n        word_counts = Counter()\n        \n        # Open the CSV file\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                for cell in row:\n                    # Normalize text to ASCII\n                    normalized_text = unicodedata.normalize('NFKD', cell).encode('ascii', 'ignore').decode('ascii')\n                    # Split the text into words and update counts\n                    words = normalized_text.split()\n                    word_counts.update(words)\n        \n        # Get the 10 most common words and their counts\n        most_common_words = word_counts.most_common(10)\n        \n        # Prepare data for plotting\n        words, frequencies = zip(*most_common_words)\n        \n        # Create a bar plot\n        fig, ax = plt.subplots()\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        \n        # Show the plot\n        plt.show()\n        \n        # Return the Axes object and the list of tuples\n        return (ax, most_common_words)\n    \n    except FileNotFoundError as e:\n        raise FileNotFoundError(\"The CSV file could not be found.\") from e\n    except IOError as e:\n        raise IOError(\"There was an error reading the file.\") from e\n\n# Example usage (Uncomment and replace 'path/to/your/csvfile.csv' with the actual file path)\n# result = task_func('path/to/your/csvfile.csv')\n# print(result[1])\n```\n\nThis code snippet includes:\n- Reading a CSV file and handling potential errors.\n- Normalizing text data to ASCII to ensure consistency in word counting.\n- Counting words using a `Counter` object from the `collections` module.\n- Generating a bar plot with matplotlib to visualize the 10 most common words and their frequencies.\n- Returning both the plot and the list of tuples as specified.\n\nPlease ensure you have matplotlib installed (`pip install matplotlib`) to run this code successfully. Also, replace `'path/to/your/csvfile.csv'` with the actual path to your CSV file when you're ready to test it."}
{"task_id": "BigCodeBench/582", "solution": "Certainly! Below is a self-contained Python code snippet that generates a list of normally distributed random numbers, then plots their histogram alongside their probability density function (PDF) using `matplotlib` and `scipy`. The function `task_func` takes an optional parameter `size` to specify the number of random numbers to generate and returns a `matplotlib.figure.Figure` object containing the plots.\n\n```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.randn(size)\n    \n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the data\n    count, bins, ignored = ax.hist(data, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n    \n    # Best fit line (PDF)\n    mu, std = stats.norm.fit(data)  # Get the parameters of the fitted normal distribution\n    best_fit_line = stats.norm.pdf(bins, mu, std)\n    \n    # Plot the PDF\n    ax.plot(bins, best_fit_line, '--', color='r')\n    \n    # Set titles and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    # Show the plot (optional, depending on your environment)\n    # plt.show()\n    \n    return fig\n\n# Example usage:\nfig = task_func(1000)\n# If running in an interactive environment like Jupyter, display the figure with:\n# fig.show()\n```\n\nThis code defines `task_func` which does the following:\n- Generates `size` normally distributed random numbers using `numpy`.\n- Creates a histogram of these numbers.\n- Fits a normal distribution to the data to find the mean (`mu`) and standard deviation (`std`).\n- Plots the PDF of the fitted normal distribution over the histogram.\n- Returns the figure object containing the plot.\n\nYou can adjust the `size` parameter to generate more or fewer random numbers as needed. If you're running this in a script, the plot might not show up automatically; you might need to call `plt.show()` after `fig.show()` or instead of it, depending on your setup."}
{"task_id": "BigCodeBench/583", "solution": "To complete the given task, we'll follow these steps:\n\n1. Generate an RSA public and private key pair using the `rsa` library.\n2. Generate a random password and nonce for AES encryption.\n3. Encrypt the RSA private key using AES encryption with the generated password.\n4. Save the encrypted private key to a file with the specified naming convention.\n5. Return the RSA public key, the filename of the encrypted private key, the encryption password, and the nonce.\n\nHere's how you can do it:\n\n```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Step 1: Generate RSA keys\n    (pub_key, priv_key) = rsa.newkeys(2048)\n    \n    # Step 2: Generate a random password and nonce for AES encryption\n    password = get_random_bytes(16)  # AES key size can be 16, 24, or 32 bytes\n    nonce = get_random_bytes(16)  # AES block size for nonce\n    \n    # Convert the private key to PEM format for encryption\n    priv_key_pem = priv_key.save_pkcs1().decode('utf-8')\n    \n    # Step 3: Encrypt the RSA private key using AES\n    cipher = AES.new(password, AES.MODE_EAX, nonce=nonce)\n    ciphertext, tag = cipher.encrypt_and_digest(priv_key_pem.encode('utf-8'))\n    \n    # Step 4: Save the encrypted private key to a file\n    number = os.urandom(8).hex()  # Generate 8 random bytes and convert to hex\n    filename = f\"private_key_{number}.txt\"\n    with open(filename, 'wb') as file_out:\n        # Store the nonce, tag, and ciphertext in a single file\n        for item in (nonce, tag, ciphertext):\n            file_out.write(item)\n    \n    # Step 5: Return the required information\n    return pub_key, filename, password, nonce\n\n# Example usage\npub_key, filename, password, nonce = task_func()\nprint(f\"Public Key: {pub_key}\")\nprint(f\"Encrypted Private Key Filename: {filename}\")\nprint(f\"Encryption Password: {password.hex()}\")\nprint(f\"Encryption Nonce: {nonce.hex()}\")\n```\n\nThis code snippet does the following:\n- Generates an RSA key pair.\n- Encrypts the private key using AES with EAX mode for confidentiality and authenticity.\n- Saves the encrypted private key along with the nonce and tag to ensure integrity when decrypting.\n- Returns the public key, the filename of the encrypted private key, the encryption password, and the nonce for testing purposes.\n\nRemember to install the required packages (`rsa` and `pycryptodome`) if you haven't already. You can install them using pip:\n\n```sh\npip install rsa pycryptodome\n```\n\nThis will ensure you have all the necessary modules to run the script."}
{"task_id": "BigCodeBench/587", "solution": "Certainly! Below is a self-contained Python code snippet that demonstrates the requested functionality. This code generates RSA public and private keys, uses AES to encrypt a file, and then encrypts the AES key with the RSA public key. The encrypted file and the encrypted AES key are saved in separate files. This is a basic example of a hybrid encryption system, combining the convenience of symmetric encryption (AES) for data with the security of asymmetric encryption (RSA) for the encryption key.\n\n```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Generate a random AES key\n    aes_key = os.urandom(32)  # AES-256 key\n    iv = os.urandom(16)  # Initialization vector for AES\n    \n    # Encrypt the file with AES\n    backend = default_backend()\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=backend)\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    \n    encrypted_file_path = file_path + '.enc'\n    with open(file_path, 'rb') as infile:\n        with open(encrypted_file_path, 'wb') as outfile:\n            outfile.write(iv)  # Prepend the IV to the file for use in decryption\n            while True:\n                chunk = infile.read(1024)\n                if len(chunk) == 0:\n                    break\n                padded_data = padder.update(chunk)\n                outfile.write(encryptor.update(padded_data))\n            outfile.write(encryptor.update(padder.finalize()))\n            outfile.write(encryptor.finalize())\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n    \n    # Save the encrypted AES key to a file\n    aes_key_file_path = file_path + '.key'\n    with open(aes_key_file_path, 'wb') as keyfile:\n        keyfile.write(encrypted_aes_key)\n    \n    return public_key, encrypted_file_path, aes_key_file_path\n\n# Example usage\nif __name__ == \"__main__\":\n    public_key, encrypted_file_path, aes_key_file_path = task_func('example.txt')\n    print(f\"RSA Public Key: {public_key}\")\n    print(f\"Encrypted file saved as: {encrypted_file_path}\")\n    print(f\"Encrypted AES key saved as: {aes_key_file_path}\")\n```\n\nThis code does the following:\n- Generates a pair of RSA keys for asymmetric encryption of the AES key.\n- Generates a random AES key for symmetric encryption of the file's content.\n- Encrypts the file using AES-CBC mode, padding the file's content to ensure it's a multiple of the block size.\n- Encrypts the AES key using the RSA public key.\n- Saves the encrypted file content and the encrypted AES key to separate files.\n\nPlease ensure you have the `rsa` and `cryptography` packages installed in your Python environment. You can install them using pip if necessary:\n\n```sh\npip install rsa cryptography\n```\n\nReplace `'example.txt'` with the path to the file you wish to encrypt. The encrypted file and AES key will be saved with `.enc` and `.key` extensions, respectively."}
{"task_id": "BigCodeBench/590", "solution": "To complete the given problem, we'll first need to ensure that the required libraries are available. If not, you might need to install them using pip (e.g., `pip install pandas pyquery`). The following code snippet fulfills the task requirements by fetching HTML content from the provided URL, parsing it to extract the text and href attributes of all anchor tags, and then compiling this information into a pandas DataFrame along with the fetch time. It also includes error handling for invalid URLs and network issues.\n\n```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\nimport socket\n\ndef task_func(url):\n    # Validate the URL\n    if not url:\n        raise ValueError(\"The provided URL is invalid or empty.\")\n    \n    try:\n        # Fetch the HTML content from the URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(\"There is an issue with network connectivity or the server.\") from e\n    except ValueError as e:\n        raise ValueError(\"The provided URL is invalid or empty.\") from e\n    except socket.timeout as e:\n        raise urllib.error.URLError(\"The request timed out.\") from e\n    \n    # Parse the HTML content\n    pq_obj = pq(html_content)\n    anchor_tags = pq_obj('a')\n    \n    # Extract the text and href of each anchor tag\n    data = []\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    for tag in anchor_tags:\n        text = pq(tag).text()\n        href = pq(tag).attr('href')\n        data.append({'text': text, 'href': href, 'fetch_time': fetch_time})\n    \n    # Convert the list of dictionaries into a pandas DataFrame\n    df = pd.DataFrame(data, columns=['text', 'href', 'fetch_time'])\n    \n    return df\n\n# Example usage\n# Replace 'http://example.com' with a valid URL\n# print(task_func('http://example.com'))\n```\n\nThis code defines a function `task_func` that takes a URL as input and returns a pandas DataFrame containing the text and href attributes of all anchor tags found in the HTML content of the URL, along with the timestamp of when the data was fetched. It includes error handling for invalid URLs and network connectivity issues. Remember to replace `'http://example.com'` with the URL you wish to fetch data from when testing the function."}
{"task_id": "BigCodeBench/592", "solution": "Certainly! Below is a self-contained Python code snippet that generates sensor data for a specified number of hours and saves it in a CSV file. The file includes columns for 'Time', 'Temperature', 'Humidity', and 'Pressure'. The data for 'Temperature', 'Humidity', and 'Pressure' are generated using random values within reasonable ranges for each type of measurement.\n\n```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef generate_sensor_data(hours):\n    \"\"\"\n    Generate sensor data for each hour.\n    \"\"\"\n    start_time = datetime.now()\n    data = []\n    for _ in range(hours):\n        temperature = randint(-10, 35)  # Temperature in Celsius\n        humidity = randint(20, 90)  # Humidity in %\n        pressure = randint(980, 1050)  # Pressure in hPa\n        data.append((start_time, temperature, humidity, pressure))\n        start_time += timedelta(hours=1)\n    return data\n\ndef save_to_csv(data, output_dir, filename=\"sensor_data.csv\"):\n    \"\"\"\n    Save the generated data to a CSV file.\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        for row in data:\n            writer.writerow([row[0].strftime('%Y-%m-%d %H:%M:%S'), row[1], row[2], row[3]])\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Main function to generate and save sensor data.\n    \"\"\"\n    data = generate_sensor_data(hours)\n    save_to_csv(data, output_dir)\n\n# Example usage\nif __name__ == \"__main__\":\n    task_func(24)  # Generate data for 24 hours\n```\n\nThis script starts by defining a function to generate random sensor data for a specified number of hours. The `generate_sensor_data` function creates a list of tuples, each representing an hour's data with a timestamp, temperature, humidity, and pressure. The `save_to_csv` function then writes this data to a CSV file, creating the output directory if it doesn't exist. Finally, the `task_func` function ties everything together, generating the data and saving it to the specified directory. The example usage at the bottom of the script calls `task_func` to generate data for 24 hours."}
{"task_id": "BigCodeBench/593", "solution": "Certainly! Below is a self-contained Python code snippet that generates traffic data for different vehicle types over a specified number of hours, saves the data to a CSV file, and plots the data in a line chart. The function `task_func` will return the path to the generated CSV file and the matplotlib axes object of the line plot.\n\n```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef generate_traffic_data(hours):\n    \"\"\"Generates random traffic data for the specified number of hours.\"\"\"\n    start_time = datetime.now()\n    data = []\n    for i in range(hours):\n        current_time = (start_time + timedelta(hours=i)).strftime('%Y-%m-%d %H:%M:%S')\n        data.append([current_time] + [randint(0, 100) for _ in VEHICLE_TYPES])\n    return data\n\ndef save_to_csv(data, file_path):\n    \"\"\"Saves the generated data to a CSV file.\"\"\"\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time'] + VEHICLE_TYPES)\n        writer.writerows(data)\n\ndef plot_data(file_path):\n    \"\"\"Plots the data from the CSV file.\"\"\"\n    df = pd.read_csv(file_path)\n    plt.figure(figsize=(10, 6))\n    for vehicle in VEHICLE_TYPES:\n        plt.plot(df['Time'], df[vehicle], label=vehicle)\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data')\n    plt.xticks(rotation=45)\n    plt.legend()\n    return plt.gca()\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate traffic data\n    data = generate_traffic_data(hours)\n    \n    # Define the CSV file path\n    file_name = f\"traffic_data_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\"\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Save data to CSV\n    save_to_csv(data, file_path)\n    \n    # Plot data\n    ax = plot_data(file_path)\n    \n    # Show plot for demonstration purposes (comment out if running in a non-interactive environment)\n    plt.show()\n    \n    return file_path, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    file_path, ax = task_func(24)  # Generate data for 24 hours\n    print(f\"Data saved to {file_path}\")\n```\n\nThis code does the following:\n- Generates random traffic data for the specified number of hours.\n- Saves this data to a CSV file in the specified output directory.\n- Plots the data in a line chart with 'Time' on the x-axis and 'Vehicle Count' on the y-axis for each vehicle type.\n- Returns the path to the CSV file and the matplotlib axes object of the line plot.\n\nMake sure you have `matplotlib` and `pandas` installed in your environment to run this code successfully. You can install them using pip:\n\n```sh\npip install matplotlib pandas\n```"}
{"task_id": "BigCodeBench/594", "solution": "Below is a self-contained Python code snippet that generates weather data for a specified number of hours, saves it in a CSV file with columns 'Time' and 'Condition', and backs up the file to a backup directory. The function `task_func` will output the path of the generated CSV file.\n\n```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import choice\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\nBACKUP_DIR = './backup'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output and backup directories exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if not os.path.exists(BACKUP_DIR):\n        os.makedirs(BACKUP_DIR)\n    \n    # Define the CSV file name based on the current timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    file_name = f\"weather_data_{timestamp}.csv\"\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Generate weather data\n    with open(file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])  # Write the header\n        \n        current_time = datetime.now()\n        for _ in range(hours):\n            # Generate a random weather condition\n            condition = choice(WEATHER_CONDITIONS)\n            writer.writerow([current_time.strftime(\"%Y-%m-%d %H:%M:%S\"), condition])\n            current_time += timedelta(hours=1)\n    \n    # Backup the file\n    shutil.copy(file_path, os.path.join(BACKUP_DIR, file_name))\n    \n    return file_path\n\n# Example usage\nif __name__ == \"__main__\":\n    hours = 24  # Generate weather data for 24 hours\n    generated_file_path = task_func(hours)\n    print(f\"Weather data generated and saved to: {generated_file_path}\")\n```\n\nThis code does the following:\n- Ensures the output and backup directories exist, creating them if they do not.\n- Generates a CSV file named with a timestamp to ensure uniqueness.\n- Writes weather data for the specified number of hours, with each hour getting a random weather condition from the predefined list.\n- Backs up the generated CSV file to a separate directory.\n- Returns the path to the generated CSV file.\n\nYou can adjust the `hours` variable in the example usage to generate data for a different number of hours."}
{"task_id": "BigCodeBench/618", "solution": "Certainly! Below is a self-contained Python code snippet that generates a Pandas DataFrame of football match results for multiple teams, including goals and penalties. It also visualizes the goals and penalty costs using seaborn plots.\n\n```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generate match results data\n    data = {\n        'Team': [],\n        'Goals': [],\n        'Penalties': [],\n        'Penalty Cost': []\n    }\n    \n    for team in TEAMS:\n        for _ in range(goals):\n            data['Team'].append(team)\n            data['Goals'].append(randint(0, 5))  # Random goals between 0 and 5\n            penalty_count = randint(0, penalties)  # Random penalties between 0 and penalties\n            data['Penalties'].append(penalty_count)\n            data['Penalty Cost'].append(penalty_count * PENALTY_COST)\n    \n    df = pd.DataFrame(data)\n    \n    # Visualization\n    plt.figure(figsize=(12, 6))\n    \n    # Plot for Goals\n    plt.subplot(1, 2, 1)\n    sns_plot_goals = sns.barplot(x='Team', y='Goals', data=df, estimator=sum, ci=None)\n    plt.title('Total Goals by Team')\n    \n    # Plot for Penalty Costs\n    plt.subplot(1, 2, 2)\n    sns_plot_penalty_costs = sns.barplot(x='Team', y='Penalty Cost', data=df, estimator=sum, ci=None)\n    plt.title('Total Penalty Costs by Team')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return df, [sns_plot_goals, sns_plot_penalty_costs]\n\n# Example usage\ndf, plots = task_func(10, 3)  # Assuming each team played 10 matches and could have up to 3 penalties per match\nprint(df)\n```\n\nThis code does the following:\n- It generates random match results for each team, including the number of goals (between 0 and 5) and penalties (up to a specified maximum).\n- It calculates the penalty cost by multiplying the number of penalties by a fixed cost.\n- It creates a Pandas DataFrame to hold these results.\n- It visualizes the total goals and total penalty costs for each team using seaborn bar plots.\n- Finally, it returns the DataFrame and a list containing the plot objects for further manipulation or inspection if needed."}
{"task_id": "BigCodeBench/636", "solution": "Certainly! Below is a self-contained code snippet that generates a DataFrame with random integer values between 0 and 9 for a specified number of rows, counts the non-zero values in each column, and visualizes this information using a bar plot. The function returns a tuple containing the DataFrame and the matplotlib Axes object containing the bar plot.\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Generate DataFrame with random integers between 0 and 9\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.astype(bool).sum(axis=0)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Values Count in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-Zero Values Count')\n    \n    # Show plot\n    plt.show()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func(10)\nprint(df)\n```\n\nThis code defines a function `task_func` that takes the number of rows as an input. It first creates a DataFrame `df` with random integers between 0 and 9 for the specified number of rows and columns named 'Column1' through 'Column5'. It then counts the non-zero values in each column using `astype(bool).sum(axis=0)`, which converts the DataFrame to boolean values (where 0 becomes `False` and all other values become `True`) and sums these boolean values across rows for each column, effectively counting non-zero values.\n\nThe function proceeds to plot these counts using `matplotlib`'s bar plot functionality, setting appropriate titles and labels for the plot. Finally, it displays the plot and returns a tuple containing the DataFrame and the Axes object of the plot. Note that `plt.show()` is called within the function to display the plot when the function is executed."}
{"task_id": "BigCodeBench/637", "solution": "Certainly! Below is a self-contained code snippet that generates a Pandas DataFrame with the grades of a randomly selected group of students across multiple courses. It calculates the average grade in each course, the number of students with a passing grade (>= 60), and visualizes this information using a bar plot. The function `task_func` takes the number of students as an input and returns a tuple containing the DataFrame and the bar plot's Axes object.\n\n```python\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Define course names and number of courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate random grades for each student in each course\n    grades_data = {course: np.random.randint(0, 101, num_students) for course in courses}\n    \n    # Create a DataFrame from the grades data\n    df = pd.DataFrame(grades_data)\n    \n    # Calculate the average grade for each course\n    average_grades = df.mean()\n    \n    # Count the number of students with a passing grade (>= 60) in each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create a bar plot\n    fig, ax = plt.subplots()\n    index = np.arange(len(courses))\n    bar_width = 0.35\n    \n    ax.bar(index, average_grades, bar_width, label='Average Grade')\n    ax.bar(index + bar_width, passing_counts, bar_width, label='Passing Students')\n    \n    ax.set_xlabel('Course')\n    ax.set_ylabel('Scores / Counts')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    \n    plt.tight_layout()\n    \n    # Return the DataFrame and the Axes object for the plot\n    return df, ax\n\n# Example usage:\nnum_students = 100\ndf, ax = task_func(num_students)\nplt.show()\n```\n\nThis code defines a function `task_func` that:\n- Generates a DataFrame with random grades for a specified number of students across multiple courses.\n- Calculates the average grade and the number of passing students for each course.\n- Creates a bar plot to visualize these statistics.\n- Returns the DataFrame and the Axes object of the plot for further manipulation or inspection."}
{"task_id": "BigCodeBench/654", "solution": "Certainly! Below is a self-contained Python code snippet that fits an exponential decay function to the indices in the array where the first column matches the target value. The function `task_func` will return a tuple containing the optimized parameters of the fitting function and the matplotlib Axes object used for plotting the fitting curve against the data points.\n\n```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Filter the rows where the first column matches the target value\n    filtered_data = array[array[:, 0] == target_value]\n    \n    # Check if filtered_data is not empty\n    if filtered_data.size == 0:\n        print(\"No matching data found for the target value.\")\n        return None\n    \n    # Extract indices (x) and values (y) for fitting\n    x_data = np.arange(len(filtered_data))\n    y_data = filtered_data[:, 1]  # Assuming we want to fit to the second column values\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Fit the exponential decay function to the data\n    popt, pcov = optimize.curve_fit(exp_decay, x_data, y_data, p0=(1, 1e-6, 1))\n    \n    # Plotting\n    plt.figure(figsize=(8, 5))\n    plt.scatter(x_data, y_data, label='Data', color='blue')\n    plt.plot(x_data, exp_decay(x_data, *popt), label='Fitted curve', color='red')\n    \n    plt.title('Exponential Decay Fit')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n    ax = plt.gca()  # Get the current Axes instance on the current figure\n    \n    return (popt, ax)\n\n# Example usage:\n# Create a sample array with two columns, where the first column is the target value\narray = np.array([[1, 2], [1, 1.8], [1, 1.5], [1, 1.2], [2, 3], [2, 2.8]])\ntarget_value = 1\n\n# Call the function\nresult = task_func(array, target_value)\n\nif result:\n    popt, ax = result\n    print(\"Optimized parameters:\", popt)\n    plt.show()\nelse:\n    print(\"An error occurred.\")\n```\n\nThis code snippet starts by filtering the array for rows where the first column matches the target value. It then fits an exponential decay function to the indices of these rows, plotting both the original data points and the fitted curve. The function `exp_decay` is defined within `task_func` and used for curve fitting with `optimize.curve_fit`. Finally, it plots the results using `matplotlib` and returns the optimized parameters along with the `Axes` object for further customization or display."}
{"task_id": "BigCodeBench/655", "solution": "Certainly! Below is a self-contained Python code snippet that performs topic extraction from a collection of text documents using Non-Negative Matrix Factorization (NMF) following the instructions provided:\n\n```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(texts, num_topics):\n    # Preprocess the texts\n    def preprocess(text):\n        text = ALPHANUMERIC.sub(' ', text)  # Remove non-alphanumeric characters\n        text = text.lower()  # Convert to lowercase\n        return ' '.join([word for word in text.split() if word not in STOPWORDS])  # Remove stopwords\n\n    processed_texts = [preprocess(text) for text in texts]\n\n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=1)\n    nmf.fit(tfidf)\n\n    # Extract the most significant words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features_ind = topic.argsort()[:-10 - 1:-1]  # Indices of top features\n        top_features = [feature_names[i] for i in top_features_ind]\n        topics.append(top_features)\n\n    return topics\n\n# Example usage\ntexts = [\n    \"Natural language processing enables computers to understand human language.\",\n    \"Machine learning provides systems the ability to automatically learn from data.\",\n    \"Deep learning is a subset of machine learning with networks capable of learning unsupervised from data that is unstructured or unlabeled.\",\n    \"Artificial intelligence is a broad field that includes machine learning and deep learning.\",\n    \"Data science involves the extraction of knowledge from large volumes of data.\"\n]\n\nnum_topics = 3\ntopics = task_func(texts, num_topics)\nfor i, topic in enumerate(topics):\n    print(f\"Topic {i+1}: {topic}\")\n```\n\nThis code snippet defines a function `task_func` that takes a list of text documents and the number of topics to extract. It preprocesses the texts by removing non-alphanumeric characters (excluding spaces), converting all characters to lowercase, and removing stopwords. Then, it vectorizes the processed texts using TF-IDF and applies NMF to extract the specified number of topics. Each topic is represented as a list of its most significant words based on the NMF component weights. The example usage at the end demonstrates how to call this function and print the extracted topics."}
{"task_id": "BigCodeBench/657", "solution": "Certainly! Below is a self-contained code snippet that completes the specified task. This code first cleans the input texts by removing all non-alphanumeric characters except spaces, lowercases the text, and removes stop words. Then, it uses the cleaned texts to train a Word2Vec model from the Gensim library. Note that you need to have `gensim` and `nltk` installed in your Python environment. If you haven't installed these packages yet, you can do so by running `pip install gensim nltk`.\n\n```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords as nltk_stopwords\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # If no custom stopwords are provided, use NLTK's English stopwords\n    if stopwords is None:\n        stopwords = set(nltk_stopwords.words('english'))\n    \n    # Function to clean and tokenize text\n    def clean_and_tokenize(text):\n        # Remove all non-alphanumeric characters (except spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase\n        text = text.lower()\n        # Tokenize by splitting the text into words\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in stopwords]\n        return words\n    \n    # Clean and tokenize each text\n    tokenized_texts = [clean_and_tokenize(text) for text in texts]\n    \n    # Train a Word2Vec model\n    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model\n\n# Example usage\ntexts = [\n    \"Hello, world! This is a test sentence.\",\n    \"Another example sentence goes here.\"\n]\n\nmodel = task_func(texts)\n\n# To test the model, let's check the vector of a word\nword = 'hello'\nif word in model.wv:\n    print(f\"Vector for the word '{word}':\\n{model.wv[word]}\")\nelse:\n    print(f\"The word '{word}' is not in the model's vocabulary.\")\n```\n\nThis code snippet includes everything needed to perform the task, including downloading the necessary NLTK stopwords data. The `task_func` function cleans the input texts, trains a Word2Vec model using these cleaned texts, and returns the trained model. The example usage at the end demonstrates how to use this function and how to access a word vector from the trained model."}
{"task_id": "BigCodeBench/678", "solution": "Certainly! Below is a self-contained code snippet that accomplishes the described task. This function will process JSON files found in the specified directory, add a \"Source\" column to each DataFrame to indicate the source file, combine them into a single DataFrame, and then move the processed files to a \"processed\" subdirectory.\n\n```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Ensure the processed directory exists\n    processed_dir = os.path.join(path, 'processed')\n    os.makedirs(processed_dir, exist_ok=True)\n    \n    # Initialize an empty DataFrame for collecting data\n    combined_df = pd.DataFrame()\n    \n    # List all JSON files in the directory, sorted alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n    \n    for file in json_files:\n        # Construct full file path\n        file_path = os.path.join(path, file)\n        \n        # Read JSON file into a DataFrame\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n            df = pd.DataFrame(data)\n        \n        # Add a 'Source' column to the DataFrame\n        df['Source'] = file\n        \n        # Append this DataFrame to the combined DataFrame\n        combined_df = pd.concat([combined_df, df], ignore_index=True)\n        \n        # Move the processed file to the 'processed' directory\n        shutil.move(file_path, os.path.join(processed_dir, file))\n    \n    return combined_df\n\n# Example usage\npath = 'path/to/your/directory'\ndf = task_func(path)\nprint(df)\n```\n\nThis code assumes that each JSON file contains data that can be directly converted into a DataFrame. If the JSON structure is nested or more complex, additional parsing logic might be needed. Also, ensure that the path you provide to the function exists or adjust the code to handle such cases as needed."}
{"task_id": "BigCodeBench/720", "solution": "Below is a self-contained Python code snippet that creates a CSV file named \"Output.txt\" within the directory \"task_func_data\". It generates random sensor data for temperature and humidity, writes this data to the file in append mode, and then deletes the file after its path has been returned. The function `task_func` encapsulates this entire process.\n\n```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Ensure the directory exists\n    dir_path = \"task_func_data\"\n    os.makedirs(dir_path, exist_ok=True)\n    \n    # Define the file path\n    file_path = os.path.join(dir_path, \"Output.txt\")\n    \n    # Generate and write random sensor data\n    try:\n        with open(file_path, mode='a', newline='') as file:\n            writer = csv.writer(file)\n            # Assuming you want a header, remove if not needed\n            writer.writerow([\"Timestamp\", \"Temperature(C)\", \"Humidity(%)\"])\n            # Generate 10 random data entries for demonstration\n            for _ in range(10):\n                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                temperature = round(random.uniform(20, 30), 2)  # Random temperature between 20 and 30\n                humidity = round(random.uniform(40, 60), 2)  # Random humidity between 40 and 60\n                writer.writerow([timestamp, temperature, humidity])\n    except Exception as e:\n        print(f\"An error occurred while writing to the file: {e}\")\n        return None\n    \n    # Return the path before deletion\n    print(f\"File path before deletion: {file_path}\")\n    \n    # Ensure the file is deleted after its path has been returned\n    try:\n        os.remove(file_path)\n        print(\"File successfully deleted.\")\n    except Exception as e:\n        print(f\"An error occurred while deleting the file: {e}\")\n    \n    return file_path\n\n# Call the function\ntask_func_path = task_func()\nprint(f\"The file was located at: {task_func_path}\")\n```\n\nThis code does the following:\n- Ensures the `task_func_data` directory exists or creates it if it doesn't.\n- Opens (and creates if necessary) the `Output.txt` file in append mode.\n- Writes a header row followed by 10 rows of random temperature and humidity data, each with a timestamp.\n- Closes the file and prints its path.\n- Deletes the file and confirms deletion.\n- Returns the path of the file for reference.\n\nPlease note, the file is deleted using `os.remove()` after its path is printed, so it won't be accessible for further operations beyond this script."}
{"task_id": "BigCodeBench/723", "solution": "Certainly! Below is a self-contained Python code snippet that demonstrates how to scrape data from a given URL and save the scraped data to a CSV file. This example assumes that you want to scrape simple text data from a webpage, such as paragraphs (`<p>` tags). You might need to adjust the BeautifulSoup part to fit the specific data you're interested in scraping (e.g., tables, lists, etc.).\n\n```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Attempt to open the URL and read the content\n    try:\n        # Open the URL\n        response = urllib.request.urlopen(url)\n        # Read the content\n        webContent = response.read()\n        \n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(webContent, 'html.parser')\n        \n        # Find the data you're interested in, for this example, let's scrape all paragraphs\n        paragraphs = soup.find_all('p')\n        \n        # Open a CSV file for writing\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            # Create a CSV writer object\n            csvwriter = csv.writer(csvfile)\n            \n            # Write a header row, if needed\n            csvwriter.writerow(['Paragraph'])\n            \n            # Write the scraped data\n            for paragraph in paragraphs:\n                # Extract text from each paragraph\n                text = paragraph.get_text()\n                # Write the text to the CSV file\n                csvwriter.writerow([text])\n                \n        print(f\"Data has been scraped and saved to {CSV_FILE_PATH}\")\n        \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nurl = 'http://example.com'\ntask_func(url)\n\n# This will save the path of the CSV file where the scraped data is saved\nprint(CSV_FILE_PATH)\n```\n\nThis code does the following:\n1. Opens the given URL using `urllib.request`.\n2. Reads the content of the page.\n3. Parses the HTML content using BeautifulSoup.\n4. Finds all paragraph (`<p>`) elements (you can modify this to suit your needs).\n5. Opens a CSV file for writing and writes the text of each paragraph to the file.\n6. Prints the path of the CSV file where the data is saved.\n\nPlease ensure you have BeautifulSoup installed (`pip install beautifulsoup4`) and adjust the scraping logic (`soup.find_all('p')`) to fit the specific data you're interested in scraping from the webpage."}
{"task_id": "BigCodeBench/752", "solution": "Certainly! Below is the self-contained code that fulfills the requirements you've outlined. This code snippet includes the necessary imports, checks for the specified exceptions, and trains a linear regression model to return the model score on the test set.\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data is not a DataFrame\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"data is empty\")\n    \n    # Check if target_column is a column in data\n    if target_column not in data.columns:\n        raise ValueError(f\"{target_column} is not a column of data\")\n    \n    # Check if data contains only numeric values\n    if not np.issubdtype(data.drop(columns=[target_column]).select_dtypes(exclude=['number']).shape[1], np.integer):\n        raise ValueError(\"data contains values that are not numeric\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state is not an integer\")\n    \n    # Check if test_size is between 0 and 1\n    if not 0 < test_size < 1:\n        raise ValueError(\"test_size is not between 0 and 1\")\n    \n    # Splitting the data into training and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Training the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Returning the model score on the test set\n    return model.score(X_test, y_test)\n\n# Example usage\nif __name__ == \"__main__\":\n    rng = np.random.default_rng(42)  # Initialize random number generator for reproducibility\n    data = pd.DataFrame({\n        'x1': rng.random(500),\n    })\n    data['y'] = data['x1'] * 2 + 1  # Creating a target column 'y'\n    try:\n        result = task_func(data, 'y', random_state=9, test_size=0.1)\n        print(result)  # Expected output: 1.0 or very close to 1.0, depending on random state and data\n    except ValueError as e:\n        print(e)\n```\n\nThis code snippet includes validation checks for the input parameters and the data itself, ensuring that the function behaves as expected under various conditions. The linear regression model is trained using the `LinearRegression` class from `sklearn.linear_model`, and the model's performance is evaluated using the `.score()` method, which returns the coefficient of determination \\(R^2\\) of the prediction."}
{"task_id": "BigCodeBench/760", "solution": "Below is a self-contained code snippet that follows the given instructions. This code generates a DataFrame with 100 records, including ID, Name, Date of Birth, and Email fields. The names are randomly selected from the provided lists of Latin and other names, dates of birth are randomly generated between specified years, and emails are constructed using the name, year of birth, and a provided email domain. It also corrects improperly encoded Latin characters in names.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n              latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n              other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n              rng_seed=None):\n    # Seed the random number generator for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Combine latin and other names into a single list\n    all_names = latin_names + other_names\n    \n    # Generate random IDs\n    ids = np.arange(1, 101)\n    \n    # Randomly select names for each record\n    names = np.random.choice(all_names, size=100)\n    \n    # Generate random dates of birth within the specified range\n    start_date = datetime.date(start_year, 1, 1)\n    end_date = datetime.date(end_year, 12, 31)\n    days_between_dates = (end_date - start_date).days\n    random_days = np.random.randint(0, days_between_dates, size=100)\n    dob = [start_date + datetime.timedelta(days=days) for days in random_days]\n    \n    # Correct improperly encoded Latin characters in names\n    corrected_names = [codecs.encode(name, 'latin-1', 'ignore').decode('latin-1') for name in names]\n    \n    # Construct emails\n    emails = [re.sub(r'[\\W_]+', '', name.lower()) + str(d.year) + '@' + email_domain for name, d in zip(corrected_names, dob)]\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': corrected_names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n    \n    return df\n\n# Example usage\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\nprint(df)\n```\n\nThis code snippet includes the necessary imports and defines the `task_func` function as requested. It generates a DataFrame with the specified characteristics, including handling of Latin character encoding and construction of email addresses. Note that the correction of improperly encoded Latin characters is done through a simple encode-decode operation, which may not handle all edge cases perfectly but should work for the provided examples."}
{"task_id": "BigCodeBench/763", "solution": "Certainly! Below is a self-contained Python code snippet that reads a list of dictionaries from a JSON file, calculates the mean and median for each key, and writes the results to a CSV file. Additionally, it returns a dictionary with the calculated mean and median for each field.\n\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Read data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a defaultdict to store lists of values for each key\n    values_by_key = defaultdict(list)\n    \n    # Populate values_by_key with data from the JSON file\n    for entry in data:\n        for key, value in entry.items():\n            values_by_key[key].append(value)\n    \n    # Calculate mean and median for each key\n    results = {}\n    for key, values in values_by_key.items():\n        mean_val = np.mean(values)\n        median_val = np.median(values)\n        results[key] = {'mean': mean_val, 'median': median_val}\n    \n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['key', 'mean', 'median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for key, stats in results.items():\n            writer.writerow({'key': key, 'mean': stats['mean'], 'median': stats['median']})\n    \n    return results\n\n# Example usage\ninput_file = 'input.json'  # Path to your input JSON file\noutput_file = 'output.csv'  # Path to your output CSV file\nresult = task_func(input_file, output_file)\nprint(result)\n```\n\nThis code does the following:\n1. Reads a list of dictionaries from the specified JSON file.\n2. Iterates through each dictionary, collecting values for each key across all dictionaries.\n3. Calculates the mean and median for the values associated with each key using NumPy.\n4. Writes the results to a CSV file, where each row contains the key, its mean, and its median.\n5. Returns a dictionary with the calculated mean and median for each key.\n\nMake sure to have the `numpy` library installed in your Python environment. You can install it using pip if you haven't already:\n\n```sh\npip install numpy\n```\n\nAlso, ensure that the input JSON file (`input.json` in the example) is in the correct format and accessible at the specified path."}
{"task_id": "BigCodeBench/765", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This function takes a dictionary `kwargs` where keys are filenames (including paths) and values are irrelevant for this task, and a `target_dir` where the non-empty files will be copied to. It checks if each file exists and has content before copying it to the target directory, and then returns a list of the full paths of the files that were successfully copied.\n\n```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    # Ensure the target directory exists\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n    \n    copied_files = []  # List to store paths of copied files\n    \n    for file_path in kwargs.keys():\n        # Check if the file exists and has content\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n            # Construct the full target path\n            target_path = os.path.join(target_dir, os.path.basename(file_path))\n            # Copy the file\n            shutil.copyfile(file_path, target_path)\n            # Add the target path to the list of copied files\n            copied_files.append(target_path)\n    \n    return copied_files\n\n# Example usage:\n# Assuming you have a dictionary of files to process\nfiles_to_process = {\n    \"/path/to/file1.txt\": None,\n    \"/path/to/file2.txt\": None,\n    # Add more files as needed\n}\n\n# Call the function with the dictionary and optional target directory\ncopied_files = task_func(files_to_process, \"desired_target_directory\")\nprint(\"Copied files:\", copied_files)\n```\n\nMake sure to replace `\"/path/to/file1.txt\"` and `\"/path/to/file2.txt\"` with actual paths to files you want to process, and `\"desired_target_directory\"` with the actual target directory where you want the non-empty files to be copied. This script checks each file listed in the `files_to_process` dictionary, verifies that it exists and is not empty, and then copies it to the specified target directory, creating the directory if it doesn't exist."}
{"task_id": "BigCodeBench/771", "solution": "Certainly! Below is a self-contained Python code snippet that processes CSV files in a specified directory. It matches the filenames against a given pattern, and for each file that matches, it creates a new file with an altered name while preserving the content. The altered name is derived from the original name by removing the numeric part that matches the pattern. The function returns a list of new filenames created.\n\n```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    # Compile the regex pattern for efficiency\n    compiled_pattern = re.compile(pattern)\n    \n    # Ensure the directory path is a Path object for easier manipulation\n    directory_path = Path(directory)\n    \n    # Iterate over all CSV files in the directory\n    for file in directory_path.glob('*.csv'):\n        # Check if the filename matches the pattern\n        match = compiled_pattern.match(file.name)\n        if match:\n            # Extract the base name from the match for the new filename\n            base_name = match.group(1)\n            new_filename = f\"{base_name}_altered.csv\"\n            new_file_path = directory_path / new_filename\n            \n            # Copy the content of the original file to the new file\n            with open(file, 'r', newline='') as infile, open(new_file_path, 'w', newline='') as outfile:\n                reader = csv.reader(infile)\n                writer = csv.writer(outfile)\n                for row in reader:\n                    writer.writerow(row)\n            \n            # Add the new filename to the list\n            new_files.append(new_filename)\n    \n    return new_files\n\n# Example usage\ndirectory = 'path/to/your/directory'\nnew_files_created = task_func(directory)\nprint(new_files_created)\n```\n\nReplace `'path/to/your/directory'` with the actual path to the directory containing your CSV files. This script uses regular expressions to identify files that match the given pattern, copies their contents to a new file with an altered name (removing the numeric part and appending `_altered` to the base name), and returns a list of the new filenames."}
{"task_id": "BigCodeBench/777", "solution": "Below is a self-contained Python code snippet that accomplishes the task described. This function searches for zip files in the specified directory that match a given pattern. The pattern is designed to capture filenames that have a prefix followed by a \"-\" and then some digits before the \".zip\" extension. It then extracts each matching zip file into a directory named after the prefix part of the filename. Finally, it returns a list of directories where the files were extracted.\n\n```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []  # List to store directories where files are extracted\n    \n    # Compile the regex pattern for efficiency\n    compiled_pattern = re.compile(pattern)\n    \n    # Walk through the directory\n    for filename in os.listdir(directory):\n        # Check if the filename matches the pattern\n        match = compiled_pattern.match(filename)\n        if match:\n            # Extract the prefix as the directory name\n            dir_name = match.group(1)\n            full_dir_path = os.path.join(directory, dir_name)\n            \n            # Create the directory if it doesn't exist\n            if not os.path.exists(full_dir_path):\n                os.makedirs(full_dir_path)\n            \n            # Full path to the zip file\n            zip_path = os.path.join(directory, filename)\n            \n            # Extract the zip file\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(full_dir_path)\n            \n            # Add the directory to the list if not already added\n            if full_dir_path not in extracted_dirs:\n                extracted_dirs.append(full_dir_path)\n    \n    return extracted_dirs\n\n# Example usage\n# Assuming there's a directory named 'zip_files' in the current working directory\n# containing zip files that match the pattern.\nif __name__ == \"__main__\":\n    directory = 'zip_files'  # Change this to the path of your directory\n    extracted_dirs = task_func(directory)\n    print(\"Extracted to directories:\", extracted_dirs)\n```\n\nThis code snippet defines the `task_func` function, which takes two parameters: `directory`, the path to the directory containing the zip files, and `pattern`, a regular expression pattern to match the filenames. The default pattern looks for filenames that have a prefix followed by a dash and some digits before the \".zip\" extension. The function then extracts each matching zip file into a new directory named after the prefix part of the filename and returns a list of these directories."}
{"task_id": "BigCodeBench/785", "solution": "Certainly! Below is a self-contained Python code snippet that matches your requirements. This function will search for files matching a given pattern, archive them into a zip file located in a specified directory, and then delete the original files. The function will return the path to the created archive file.\n\n```python\nimport subprocess\nimport os\nimport glob\nimport zipfile\nfrom datetime import datetime\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Ensure the archive directory exists\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n    \n    # Generate a unique filename for the archive\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    archive_filename = f\"archive_{timestamp}.zip\"\n    archive_filepath = os.path.join(ARCHIVE_DIR, archive_filename)\n    \n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n    \n    # Create a zip file and add files\n    with zipfile.ZipFile(archive_filepath, 'w') as archive:\n        for file in files_to_archive:\n            archive.write(file, os.path.basename(file))\n            # Delete the original file\n            os.remove(file)\n    \n    return archive_filepath\n\n# Example usage\nif __name__ == \"__main__\":\n    pattern = '/path/to/your/files/*.txt'  # Update this pattern to match your files\n    archive_file = task_func(pattern)\n    print(f\"Archive created at: {archive_file}\")\n```\n\n### How it works:\n1. **Directory Creation**: It first checks if the `ARCHIVE_DIR` exists, and if not, it creates it.\n2. **Unique Archive Name**: It generates a unique name for the archive file using the current timestamp to avoid overwriting previous archives.\n3. **File Matching**: It uses `glob.glob(pattern)` to find all files matching the given pattern.\n4. **Archiving**: It creates a zip file in the specified archive directory and adds all found files to this archive.\n5. **File Deletion**: After adding a file to the archive, it deletes the original file from the filesystem.\n6. **Return Value**: Finally, it returns the path to the created archive file.\n\n### Note:\n- Make sure to replace `'/path/to/your/files/*.txt'` with the actual pattern you want to match.\n- This script requires write permissions to the `ARCHIVE_DIR` and permissions to delete the files matching the pattern."}
{"task_id": "BigCodeBench/800", "solution": "To complete the given problem, we'll first create a function `task_func` that reads data from a CSV file, counts the total number of goals and penalties, updates these totals with the given goals and penalties, and then returns a `Counter` object with these totals. We'll also ensure that the code is self-contained by including the necessary imports and helper functions within the code snippet.\n\nHere's how you can do it:\n\n```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n    \n    # Check if the CSV file exists, if not, create it using setup_csv\n    if not os.path.exists(csv_file_path):\n        setup_csv()\n    \n    # Initialize a Counter object to count goals and penalties\n    counts = Counter({'goals': 0, 'penalties': 0})\n    \n    # Read the CSV file and update the counts\n    with open(csv_file_path, mode='r', newline='') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            counts['goals'] += int(row['goals'])\n            counts['penalties'] += int(row['penalties'])\n    \n    # Update the counts with the given goals and penalties\n    counts['goals'] += goals\n    counts['penalties'] += penalties\n    \n    return counts\n\n# Example usage\nif __name__ == \"__main__\":\n    # Setup the CSV for demonstration purposes\n    setup_csv()\n    \n    # Call the function with additional goals and penalties\n    result = task_func(2, 3, 'test_data/test_case_2.csv')\n    print(result)\n```\n\nThis code snippet includes everything you need to run the task as described. It starts by checking if the CSV file exists and creates it using the `setup_csv` function if it doesn't. Then, it reads the existing data from the CSV file, updates the counts of goals and penalties, and finally returns a `Counter` object with the total counts."}
{"task_id": "BigCodeBench/826", "solution": "Certainly! Below is a self-contained Python code snippet that defines a function `task_func`. This function moves files from a source directory to a target directory based on a specified pattern. It uses regular expressions to match file names against the provided pattern and counts the number of files successfully moved.\n\n```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    # Compile the regular expression pattern for efficiency\n    pattern = re.compile(file_pattern)\n    \n    # Initialize a counter for moved files\n    moved_files_count = 0\n    \n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Construct the full path of the file\n        file_path = os.path.join(source_dir, filename)\n        \n        # Check if the current item is a file and matches the pattern\n        if os.path.isfile(file_path) and pattern.match(filename):\n            # Construct the target file path\n            target_file_path = os.path.join(target_dir, filename)\n            \n            # Move the file\n            shutil.move(file_path, target_file_path)\n            \n            # Increment the moved files counter\n            moved_files_count += 1\n    \n    # Output the number of files moved\n    print(f\"Moved {moved_files_count} files.\")\n    return moved_files_count\n\n# Example usage\nsource_dir = 'path/to/source'\ntarget_dir = 'path/to/target'\nfile_pattern = r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'\ntask_func(source_dir, target_dir, file_pattern)\n```\n\nMake sure to replace `'path/to/source'` and `'path/to/target'` with the actual paths to your source and target directories. The `file_pattern` is set to match files with extensions `.txt`, `.doc`, or `.docx`, but you can adjust it according to your needs."}
{"task_id": "BigCodeBench/845", "solution": "Certainly! Below is a self-contained Python code snippet that completes the described task. This code first cleans the input texts by removing all non-alphanumeric characters except spaces and converting them to lowercase. It then calculates the cosine similarity based on term frequency and the Levenshtein ratio using the `ratio` function from the `python-Levenshtein` library.\n\n```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef clean_text(text):\n    \"\"\"Clean the input text by removing non-alphanumeric characters (except spaces) and converting to lowercase.\"\"\"\n    return ALPHANUMERIC.sub('', text).lower()\n\ndef get_cosine_similarity(counter1, counter2):\n    \"\"\"Calculate cosine similarity between two term frequency counters.\"\"\"\n    intersection = set(counter1.keys()) & set(counter2.keys())\n    numerator = sum([counter1[x] * counter2[x] for x in intersection])\n\n    sum1 = sum([counter1[x]**2 for x in counter1.keys()])\n    sum2 = sum([counter2[x]**2 for x in counter2.keys()])\n    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef task_func(text1, text2):\n    # Clean texts\n    clean_text1 = clean_text(text1)\n    clean_text2 = clean_text(text2)\n\n    # Calculate term frequencies\n    counter1 = Counter(clean_text1.split())\n    counter2 = Counter(clean_text2.split())\n\n    # Calculate cosine similarity\n    cosine_similarity = get_cosine_similarity(counter1, counter2)\n\n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(clean_text1, clean_text2)\n\n    return (cosine_similarity, levenshtein_ratio)\n\n# Example usage\ntext1 = \"Hello world!\"\ntext2 = \"Hello, world.\"\nresult = task_func(text1, text2)\nprint(f\"Cosine Similarity: {result[0]}, Levenshtein Ratio: {result[1]}\")\n```\n\nThis code defines a `task_func` function that takes two text inputs, cleans them, computes their term frequencies, calculates the cosine similarity based on these frequencies, and computes the Levenshtein ratio. The function finally returns a tuple containing both similarity measures.\n\nPlease ensure you have the `numpy` and `python-Levenshtein` packages installed in your environment to run this code. You can install them using pip:\n\n```sh\npip install numpy python-Levenshtein\n```\n\nThis example demonstrates basic usage, but you can replace `text1` and `text2` with any strings you wish to compare."}
{"task_id": "BigCodeBench/854", "solution": "```python\nfrom functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Check if input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of integers.\")\n    # Check if all elements in the list are integers and non-negative\n    if not all(isinstance(x, int) and x >= 0 for x in numbers):\n        raise ValueError(\"All numbers must be non-negative integers.\")\n    \n    # Function to calculate the sum of factorials of a tuple\n    def sum_of_factorials(tup):\n        return sum(math.factorial(num) for num in tup)\n    \n    # Generate all permutations of the input list\n    perms = list(permutations(numbers))\n    # Calculate the sum of factorials for each permutation\n    fac_sums = [sum_of_factorials(perm) for perm in perms]\n    \n    # Convert permutations from tuples to lists\n    perms_as_lists = [list(perm) for perm in perms]\n    \n    return fac_sums, perms_as_lists\n\n# Example usage\nfac, perm = task_func([0, 4])\nprint(fac)  # [25, 25]\nprint(perm)  # [(0, 4), (4, 0)]\n```"}
{"task_id": "BigCodeBench/857", "solution": "Below is a self-contained Python code snippet that accomplishes the task described. This function, `task_func`, transfers files from a source directory to a destination directory based on specified file extensions. It also handles and issues warnings for files that could not be transferred due to any issues, such as permission errors or non-existent directories.\n\n```python\nimport warnings\nimport os\nimport glob\nimport shutil\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []  # List to keep track of successfully transferred files\n    \n    # Ensure DEST_DIR exists, if not, create it\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n    \n    # Loop through each extension and transfer matching files\n    for ext in EXTENSIONS:\n        # Create a pattern for glob to match all files with the current extension in SOURCE_DIR\n        pattern = os.path.join(SOURCE_DIR, f'*.{ext}')\n        \n        # Use glob to find all files in SOURCE_DIR with the current extension\n        for file_path in glob.glob(pattern):\n            try:\n                # Extract the file name from the file_path\n                file_name = os.path.basename(file_path)\n                \n                # Define the destination file path\n                dest_file_path = os.path.join(DEST_DIR, file_name)\n                \n                # Transfer the file\n                shutil.move(file_path, dest_file_path)\n                \n                # Add the file name to the list of transferred files\n                transferred_files.append(file_name)\n            except Exception as e:\n                # Issue a warning for the file that could not be transferred\n                warnings.warn(f'Could not transfer {file_path} to {DEST_DIR}. Reason: {e}')\n    \n    return transferred_files\n\n# Example usage:\nSOURCE_DIR = 'path/to/source_dir'\nDEST_DIR = 'path/to/dest_dir'\nEXTENSIONS = ['txt', 'jpg', 'png']  # Example file extensions\n\ntransferred_files = task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS)\nprint(\"Transferred Files:\", transferred_files)\n```\n\nThis code snippet includes error handling to catch and warn about any issues that occur during the file transfer process, such as permission errors or if the source file does not exist. It uses the `shutil.move` function to move files, which will overwrite files in the destination directory if they have the same name. Make sure to replace `'path/to/source_dir'` and `'path/to/dest_dir'` with your actual source and destination directory paths, and adjust the `EXTENSIONS` list to include the file extensions you want to transfer."}
{"task_id": "BigCodeBench/865", "solution": "Certainly! Below is a self-contained Python code snippet that defines the `task_func` function. This function takes a list of tuples as input, where each tuple contains an item, its count, and its weight. It then normalizes the counts using z-score normalization and the weights using min-max scaling. Finally, it returns a pandas DataFrame with the items, normalized counts, and normalized weights.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Convert input data to a pandas DataFrame\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n    \n    # Normalize the 'Count' column using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n    \n    # Normalize the 'Weight' column using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n    \n    # Select and return the required columns\n    return df[['Item', 'Normalized Count', 'Normalized Weight']]\n\n# Example usage\ndata = [('Apple', 10, 0.5), ('Banana', 15, 0.3), ('Cherry', 7, 0.8)]\nresult = task_func(data)\nprint(result)\n```\n\nThis code starts by importing the necessary libraries: `pandas` for data manipulation, `numpy` for numerical operations, `scipy.stats` for the z-score normalization, and `sklearn.preprocessing` for min-max scaling.\n\nThe `task_func` function first converts the input list of tuples into a pandas DataFrame for easier data manipulation. It then applies z-score normalization to the 'Count' column and min-max scaling to the 'Weight' column. Finally, it returns a new DataFrame containing the original items along with their normalized counts and weights.\n\nThe example usage at the end demonstrates how to call this function with a sample list of tuples and prints the resulting DataFrame."}
{"task_id": "BigCodeBench/870", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize a list to hold the lists of numeric values for each position\n    numeric_values_by_position = []\n\n    # Determine the maximum length of tuples in the list\n    max_length = max(len(item) for item in data_list) if data_list else 0\n\n    # Initialize lists to hold numeric values for each position\n    for _ in range(max_length):\n        numeric_values_by_position.append([])\n\n    # Iterate over each item in the list\n    for item in data_list:\n        for position, value in enumerate(item):\n            # Check if the value is numeric and add it to the corresponding list\n            if isinstance(value, (int, float)):\n                numeric_values_by_position[position].append(value)\n\n    # Calculate the mean for each position, ignoring empty lists\n    means = [np.mean(values) if values else np.nan for values in numeric_values_by_position]\n\n    # Create a DataFrame from the means\n    df = pd.DataFrame(means, columns=['Mean Value'])\n\n    # Adjust the index to match the required format\n    df.index = ['Position ' + str(i) for i in range(len(means))]\n\n    return df\n\n# Example usage\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)\nprint(df)\n```\nThis code defines a function `task_func` that takes a list of tuples as input. It processes the list to calculate the mean of numerical values at each position across the tuples, ignoring non-numeric values. The result is returned as a pandas DataFrame with a single column named 'Mean Value', where each row corresponds to a position in the tuples, formatted as 'Position i'. If the input list is empty, the function returns an empty DataFrame."}
{"task_id": "BigCodeBench/879", "solution": "Below is a self-contained code snippet that defines the `task_func` function. This function performs a chi-square test of independence on two categorical columns from a given DataFrame. It checks for various conditions to ensure the validity of the chi-square test, including non-empty data, column existence, multiple categories, and sufficient observations per category.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the specified columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both of the specified columns '{col1}' and '{col2}' are not in the DataFrame.\")\n    \n    # Check if the columns contain non-categorical data\n    if data[col1].dtype not in [object, \"category\"] or data[col2].dtype not in [object, \"category\"]:\n        raise TypeError(\"One or both of the columns contain non-categorical data.\")\n    \n    # Check if both columns have multiple categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Construct the contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check if some categories have less than 5 observations\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations, violating the chi-square test assumptions.\")\n    \n    # Perform the chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Return the p-value of the test\n    return p\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    data = pd.DataFrame({\n        'a': np.random.choice(['A', 'B'], size=100),\n        'b': np.random.choice(['X', 'Y'], size=100)\n    })\n    try:\n        p_value = task_func(data, 'a', 'b')\n        print(f\"Chi-square test p-value: {p_value}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```\n\nThis code snippet includes error handling for various conditions that could invalidate the chi-square test, such as non-categorical data, insufficient categories, or too few observations per category. It uses the `chi2_contingency` function from `scipy.stats` to perform the chi-square test and returns the p-value, which indicates the probability of observing the given data if the null hypothesis (that the variables are independent) is true."}
{"task_id": "BigCodeBench/897", "solution": "Certainly! Below is a self-contained Python code snippet that simulates a number of dice rolls, calculates the frequency of each result, and then generates both the frequency array and a histogram of the results. The function `task_func` takes in the number of rolls and an optional seed for reproducibility of the random results.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)  # Set the seed if provided for reproducibility\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequencies\n    frequencies = np.zeros(len(NUMBERS))\n    for number in NUMBERS:\n        frequencies[number-1] = results.count(number)\n    \n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.hist(results, bins=np.arange(0.5, 7.5, 1), rwidth=0.8, color='blue', alpha=0.7)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(range(1, 7))\n    \n    plt.show()\n    \n    return frequencies, ax\n\n# Example usage\nrolls = 1000\nseed = 42  # Optional seed for reproducibility\nfrequencies, ax = task_func(rolls, seed)\nprint(frequencies)\n```\n\nThis code defines the `task_func` function, which simulates the dice rolls using a simple random choice from the predefined `NUMBERS` list for each roll. It then calculates the frequency of each dice value appearing in the simulation. After calculating the frequencies, it plots a histogram of the results using Matplotlib, setting appropriate titles and labels as requested. The function finally returns a tuple containing the numpy array of frequencies and the Axes object representing the histogram plot. \n\nRemember, to see the plot, you need to run this code in an environment that supports displaying Matplotlib figures, such as Jupyter notebooks or a Python script executed in a GUI environment."}
{"task_id": "BigCodeBench/906", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This function will search for files in the `source_dir` directory that have the '_processed' suffix in their filename. It then archives these files into a zip file named `archive_name` and places this archive in the `target_dir` directory. Finally, it returns the path to the created archive.\n\n```python\nimport zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # The full path to the archive that will be created\n    archive_path = os.path.join(target_dir, archive_name)\n    \n    # Create a zipfile object in write mode\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        # Walk through the source directory\n        for folderName, subfolders, filenames in os.walk(source_dir):\n            for filename in filenames:\n                # Check if the file has '_processed' in its name\n                if '_processed' in filename:\n                    # Create the full file path\n                    file_path = os.path.join(folderName, filename)\n                    # Add the file to the archive, use arcname to exclude the path\n                    archive.write(file_path, arcname=filename)\n    \n    # Return the path to the created archive\n    return archive_path\n\n# Example usage\nif __name__ == \"__main__\":\n    source_directory = 'path/to/source_directory'\n    target_directory = 'path/to/target_directory'\n    archive_name = 'processed_files_archive.zip'\n    \n    archive_path = task_func(source_directory, target_directory, archive_name)\n    print(f\"Archive created at: {archive_path}\")\n```\n\nMake sure to replace `'path/to/source_directory'` and `'path/to/target_directory'` with the actual paths to your source and target directories, respectively. This script will create an archive of all files ending with '_processed' from the source directory and save it in the target directory with the name provided in `archive_name`."}
{"task_id": "BigCodeBench/914", "solution": "Certainly! Below is a self-contained code snippet that takes a DataFrame `df` with a column named `'Close'` representing the closing prices of a stock. It uses simple linear regression to predict the stock closing prices for the next 7 days and plots the original closing prices along with the predicted prices. The function returns a tuple containing the list of predicted prices and the matplotlib Axes object containing the plot.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom datetime import datetime, timedelta\n\ndef task_func(df):\n    # Ensure the DataFrame is sorted by date if it has a date column\n    if 'Date' in df.columns:\n        df.sort_values('Date', inplace=True)\n    \n    # Generate a sequence of integers as our feature variable\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Close'].values\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Predict the closing prices for the next 7 days\n    future_days = 7\n    future_X = np.arange(len(df), len(df) + future_days).reshape(-1, 1)\n    predicted_prices = model.predict(future_X)\n    \n    # Generate future timestamps for plotting\n    if 'Date' in df.columns:\n        last_date = pd.to_datetime(df['Date'].iloc[-1])\n    else:\n        # Assuming the last index is the latest date if no Date column is present\n        last_date = datetime.now()\n    future_dates = [last_date + timedelta(seconds=24*60*60*i) for i in range(1, future_days + 1)]\n    \n    # Plotting the original and predicted prices\n    plt.figure(figsize=(10, 6))\n    plt.plot(df.index, df['Close'], label='Original Prices')\n    plt.plot(range(len(df), len(df) + future_days), predicted_prices, label='Predicted Prices', linestyle='--')\n    plt.xlabel('Time')\n    plt.ylabel('Stock Closing Price')\n    plt.title('Stock Closing Prices and Predictions')\n    plt.legend()\n    \n    # If the DataFrame has a Date column, adjust the x-axis to show dates\n    if 'Date' in df.columns:\n        all_dates = pd.to_datetime(df['Date']).tolist() + future_dates\n        plt.xticks(range(len(all_dates)), [date.strftime('%Y-%m-%d') for date in all_dates], rotation=45)\n        plt.tight_layout()\n    \n    return predicted_prices, plt.gca()\n\n# Example usage:\n# Assuming df is your DataFrame with a 'Close' column and optionally a 'Date' column\n# predicted_prices, ax = task_func(df)\n# plt.show()\n```\n\nThis code assumes that the DataFrame `df` either contains a 'Date' column for plotting purposes or will simply use the index for plotting if no 'Date' column is present. The future dates are generated based on the last date in the DataFrame or the current date if no 'Date' column is available, using a constant time step of 24*60*60 seconds (1 day) to generate future timestamps for the next 7 days."}
{"task_id": "BigCodeBench/915", "solution": "Below is a self-contained code snippet that defines the function `task_func` as per your requirements. This function takes a pandas DataFrame and a Z-Score threshold as input. It identifies outliers in the 'closing_price' column based on the Z-Score method, returns these outliers in a DataFrame, and plots them. The plot will have 'Index' as the x-axis label, 'Closing Price' as the y-axis label, and 'Outliers in Closing Prices' as the title.\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Scores of closing prices\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the z_threshold\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Drop the 'z_score' column as it's no longer needed in the output\n    outliers = outliers.drop(columns=['z_score'])\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df.index, df['closing_price'], label='Data Points')\n    plt.scatter(outliers.index, outliers['closing_price'], color='r', label='Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Closing Price')\n    plt.title('Outliers in Closing Prices')\n    plt.legend()\n    plot = plt.gca()  # Get the current Axes instance on the current figure matching the given keyword args, or create one.\n    \n    return outliers, plot\n\n# Example usage\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\n\n# Display the outliers DataFrame\nprint(outliers2)\n\n# Show the plot\nplt.show()\n```\n\nThis code snippet starts by importing the necessary libraries: `numpy`, `pandas`, `matplotlib.pyplot`, and `zscore` from `scipy.stats`. The `task_func` function calculates the Z-Scores for the 'closing_price' column and identifies outliers based on the provided `z_threshold`. It then plots all data points and highlights the outliers in red. The function returns a DataFrame containing only the outliers and the plot object for further customization or display."}
{"task_id": "BigCodeBench/916", "solution": "Certainly! Below is a self-contained code snippet that defines a function `task_func`. This function takes a pandas DataFrame `df` with stock closing prices and visualizes the distribution using both a box plot and a histogram within a single figure. The function returns a tuple containing the Axes objects for both plots.\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Set up the matplotlib figure\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    # Box plot for the distribution of stock closing prices\n    sns.boxplot(data=df, ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    \n    # Histogram for the distribution of stock closing prices\n    sns.histplot(data=df, ax=axes[1], kde=True)  # kde for a smooth curve\n    axes[1].set_title('Histogram of Closing Prices')\n    \n    plt.tight_layout()  # Adjust the layout to make room for the titles\n    \n    return (axes[0], axes[1])\n\n# Example usage:\n# Assuming 'df' is your DataFrame containing the stock closing prices.\n# df = pd.DataFrame({'Closing Prices': [100, 101, 102, 98, 95, 105, 110, 108, 107, 105, 103, 99]})\n# axes = task_func(df)\n# plt.show()\n```\n\nThis code snippet starts by importing the necessary libraries: `pandas` for data manipulation, `matplotlib.pyplot` for plotting, and `seaborn` for enhanced visualization features. The `task_func` function is defined to accept a DataFrame `df` as input. It then creates a figure with two subplots side by side: the first for a box plot and the second for a histogram. The `sns.boxplot` and `sns.histplot` functions from the seaborn library are used to create these plots, respectively. The titles for each plot are set as specified. Finally, the function returns a tuple containing the Axes objects for both plots, allowing further customization if needed.\n\nTo use this function, you would need to have a DataFrame `df` with stock closing prices. You can uncomment the example usage section and modify it according to your DataFrame to visualize the distribution of your stock closing prices."}
{"task_id": "BigCodeBench/917", "solution": "Certainly! Below is a self-contained code snippet that forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast. This code assumes that the input DataFrame `df` has a column named 'Close' which contains the closing prices of the shares. It uses the ARIMA model for forecasting and matplotlib for plotting the forecast.\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Ensure the DataFrame index is of datetime type and it's sorted\n    df.index = pd.to_datetime(df.index)\n    df = df.sort_index()\n    \n    # Fit the ARIMA model\n    # Note: You might need to adjust the order (p,d,q) based on your data's characteristics\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Plotting the forecast\n    fig, ax = plt.subplots()\n    plt.plot(df.index, df['Close'], label='Historical Daily Closing Price')\n    forecast_dates = [df.index[-1] + pd.Timedelta(days=i) for i in range(1, 8)]\n    plt.plot(forecast_dates, forecast, color='red', linestyle='--', label='Forecasted Closing Price')\n    plt.title('Share Closing Price Forecast')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.legend()\n    \n    return forecast.tolist(), ax\n\n# Example usage:\n# Assuming you have a DataFrame 'df' with a 'Close' column\n# df = pd.read_csv('your_data.csv')  # Make sure your data has a 'Close' column\n# forecast_prices, ax = task_func(df)\n# plt.show()\n```\n\nThis function does the following:\n- It first ensures that the DataFrame's index is of datetime type and sorted, which is crucial for time series forecasting.\n- It then fits an ARIMA model to the 'Close' column of the DataFrame. The ARIMA model's order (p, d, q) is set to (5, 1, 0) as a starting point, but you might need to adjust these parameters based on the characteristics of your data or use model selection techniques to find the best order.\n- It forecasts the next 7 days of closing prices.\n- It plots the historical closing prices along with the forecasted prices for the next 7 days.\n\nPlease note that the performance and accuracy of the ARIMA model highly depend on the nature of your data and the chosen parameters (p, d, q). You might need to experiment with different orders or even consider different models based on the characteristics of your data set."}
{"task_id": "BigCodeBench/928", "solution": "Certainly! Below is a self-contained Python code snippet that creates a dictionary containing all possible two-letter combinations of the lowercase English alphabets, with the dictionary values representing the frequency of these two-letter combinations in the given word. If a combination does not appear in the word, its value will be 0.\n\n```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of lowercase English letters\n    all_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize the dictionary with all combinations set to 0\n    combinations_dict = {combination: 0 for combination in all_combinations}\n    \n    # Count the frequency of each two-letter combination in the word\n    word_combinations = [word[i:i+2] for i in range(len(word)-1)]\n    word_combinations_count = Counter(word_combinations)\n    \n    # Update the dictionary with the counts from the word\n    for combination, count in word_combinations_count.items():\n        if combination in combinations_dict:\n            combinations_dict[combination] = count\n    \n    return combinations_dict\n\n# Example usage\nword = \"hello\"\nresult = task_func(word)\nprint(result)\n```\n\nThis code defines a function `task_func` that takes a word as input and returns a dictionary with keys as two-letter alphabet combinations and values as their counts in the word. It first generates all possible two-letter combinations of lowercase English letters using `itertools.product` and initializes a dictionary with these combinations as keys and 0 as their initial values. Then, it creates a list of all two-letter combinations found in the input word and counts their frequency using `Counter`. Finally, it updates the initial dictionary with these counts and returns it."}
{"task_id": "BigCodeBench/942", "solution": "Certainly! Below is a self-contained Python code snippet that generates a sales report for different categories over a period of time. The code creates a DataFrame with sales data and visualizes this data using Matplotlib. The sales data is randomly generated for the sake of this example.\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create empty DataFrame\n    sales_data = pd.DataFrame()\n    \n    # Populate DataFrame\n    for category in categories:\n        # Generate random sales numbers for each category\n        sales = np.random.randint(100, 5000, size=len(dates))\n        \n        # Create a temporary DataFrame\n        temp_df = pd.DataFrame({\n            'Date': dates,\n            'Category': category,\n            'Sales': sales\n        })\n        \n        # Append to the main DataFrame\n        sales_data = pd.concat([sales_data, temp_df], ignore_index=True)\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    for category in categories:\n        # Filter data for each category\n        category_data = sales_data[sales_data['Category'] == category]\n        \n        # Plot\n        ax.plot(category_data['Date'], category_data['Sales'], marker='o', label=category)\n    \n    # Formatting the plot\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Show plot\n    plt.show()\n    \n    return sales_data, ax\n\n# Call the function\nsales_data, ax = task_func()\n```\n\nThis code does the following:\n- Generates a date range based on the specified start date, periods, and frequency.\n- Creates a DataFrame to hold sales data for different categories.\n- Populates the DataFrame with random sales data for each category across the generated dates.\n- Plots the sales data for each category over time using Matplotlib.\n- Returns the populated DataFrame and the Matplotlib Axes object for further manipulation or inspection."}
{"task_id": "BigCodeBench/945", "solution": "Certainly! Below is a self-contained code snippet that generates a time series of sales data starting from a specified date. If sales data is provided, it uses that data; otherwise, it generates sales data. Then, it applies linear regression to forecast future sales for the same number of periods as the input or generated data. The function outputs a numpy array containing the forecasted future sales.\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Check if sales_data is provided\n    if sales_data is None:\n        # Generate synthetic sales data (for example purposes)\n        np.random.seed(0)  # For reproducibility\n        sales_data = np.random.randint(100, 500, size=len(dates))\n    else:\n        # Ensure sales_data has the correct length\n        if len(sales_data) != periods:\n            raise ValueError(\"sales_data length must match the number of periods\")\n    \n    # Prepare the data for linear regression\n    X = np.arange(len(sales_data)).reshape(-1, 1)  # Reshape for sklearn\n    y = sales_data\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Predict future sales for the same number of periods\n    future_periods = np.arange(len(sales_data), len(sales_data) * 2).reshape(-1, 1)\n    future_sales = model.predict(future_periods)\n    \n    return future_sales\n\n# Example usage\nstart_date = '2016-01-01'\nperiods = 13\nfreq = 'WOM-2FRI'\n# Example: No sales_data provided, it will generate and use synthetic sales data\nforecasted_sales = task_func(start_date, periods, freq)\nprint(forecasted_sales)\n```\n\nThis code starts by generating a range of dates based on the specified start date, number of periods, and frequency. If no sales data is provided, it generates synthetic sales data for demonstration purposes. It then fits a linear regression model to the sales data, with time as the independent variable and sales as the dependent variable. Finally, it uses the model to forecast future sales for the same number of future periods and returns these forecasts as a numpy array."}
{"task_id": "BigCodeBench/952", "solution": "Certainly! Below is a self-contained code snippet that fulfills the requirements you've outlined. This code defines a function `task_func` that randomly assigns a specified number of tasks to employees, sanitizes task names by replacing spaces with underscores, sets due dates to the current system date, and returns the assignments as a pandas DataFrame. It also includes error handling for a negative `n_tasks` value.\n\n```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Set the seed for reproducibility if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Raise ValueError if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n    \n    # Ensure we do not exceed the number of available tasks\n    n_tasks = min(n_tasks, len(task_list))\n    \n    # Randomly select tasks and employees\n    selected_tasks = random.sample(task_list, n_tasks)\n    assigned_employees = [random.choice(employees) for _ in range(n_tasks)]\n    \n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in selected_tasks]\n    \n    # Get the current system date\n    due_date = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    # Create a DataFrame with the assignments\n    assignments_df = pd.DataFrame({\n        \"Task Name\": sanitized_tasks,\n        \"Assigned To\": assigned_employees,\n        \"Due Date\": due_date\n    })\n    \n    return assignments_df\n\n# Example usage\nif __name__ == \"__main__\":\n    task_list = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\", \"Task 5\"]\n    n_tasks = 3\n    seed = 42  # Optional seed for reproducibility\n    assignments_df = task_func(task_list, n_tasks, seed=seed)\n    print(assignments_df)\n```\n\nThis code snippet includes everything needed to run the function, including example usage at the bottom. You can adjust the `task_list`, `n_tasks`, and `seed` variables as needed to test different scenarios."}
{"task_id": "BigCodeBench/955", "solution": "Certainly! Below is a self-contained code snippet that fulfills the requirements you've outlined. This code replaces spaces in given words with underscores, plots the frequency of each unique word (case-insensitive), and raises a `ValueError` if the input text is empty. It uses `matplotlib` for plotting and `Counter` from the `collections` module to count the frequency of each unique word after modification.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Convert the text to lowercase to ensure case-insensitivity\n    text = text.lower()\n    \n    # Replace spaces with underscores in the given words\n    for word in mystrings:\n        # Use re to ensure case-insensitive replacement\n        pattern = re.compile(re.escape(word), re.IGNORECASE)\n        text = pattern.sub(word.replace(\" \", \"_\"), text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Prepare data for plotting\n    labels, values = zip(*word_counts.items())\n    \n    # Create a new figure and axis for the plot\n    fig, ax = plt.subplots()\n    \n    # Plot the frequencies\n    ax.bar(labels, values)\n    \n    # Set the title and labels\n    ax.set_title('Word Frequency')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Rotate the x-axis labels for better readability\n    plt.xticks(rotation=45, ha=\"right\")\n    \n    # Adjust layout to make room for the rotated x-axis labels\n    plt.tight_layout()\n    \n    # Return the Axes object\n    return ax\n\n# Example usage\nmystrings = [\"example text\", \"test case\"]\ntext = \"This is an example text and another Example Text. Also, a test case and another Test Case.\"\nax = task_func(mystrings, text)\nplt.show()\n```\n\nThis code snippet starts by defining the `task_func` function, which takes two parameters: `mystrings`, a list of words to replace spaces with underscores, and `text`, the input text to process. It then checks if the input text is empty and raises a `ValueError` if it is. After that, it replaces spaces in the specified words with underscores, counts the frequency of each unique word, and plots these frequencies using `matplotlib`. Finally, it returns the `Axes` object of the plot for further customization or inspection if needed."}
{"task_id": "BigCodeBench/963", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This function will zip files with certain extensions from a source directory and save the zip file to a target directory. It checks if the source directory exists to avoid an `OSError` and only includes files with the specified extensions.\n\n```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Validate if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n    \n    # Ensure target directory exists, if not, create it\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # Define the full path for the zip file to be created\n    zip_file_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    \n    # List of valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create a zip file and add files with valid extensions\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        # Walk through the source directory\n        for root, dirs, files in os.walk(source_directory):\n            for file in files:\n                # Check if the file has a valid extension before adding\n                if any(file.endswith(ext) for ext in valid_extensions):\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, source_directory))\n    \n    return zip_file_path\n\n# Example usage\n# Assuming the directories and zip_name are correctly defined\n# source_directory = \"/path/to/source_directory\"\n# target_directory = \"/path/to/target_directory\"\n# zip_name = \"my_zipfile\"\n# print(task_func(source_directory, target_directory, zip_name))\n```\n\nThis code snippet defines the `task_func` function, which takes `source_directory`, `target_directory`, and `zip_name` as arguments. It first checks if the source directory exists to prevent an `OSError`. Then, it ensures the target directory exists (creating it if necessary) and proceeds to create a zip file within the target directory, including only files with the specified valid extensions. Finally, it returns the full path to the created zip file."}
{"task_id": "BigCodeBench/964", "solution": "Certainly! Below is a self-contained Python code snippet that completes the described task. This code converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files and saves them in a target directory. It captures each file's text content and stores it in a CSV file with a single 'Text' column. It also handles the specified exception and overwrites existing files in the target directory if necessary.\n\n```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl  # For .xlsx files\nimport csv\n\ndef read_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.read()\n\ndef read_docx_file(file_path):\n    doc = docx.Document(file_path)\n    fullText = []\n    for para in doc.paragraphs:\n        fullText.append(para.text)\n    return '\\n'.join(fullText)\n\ndef read_xlsx_file(file_path):\n    workbook = openpyxl.load_workbook(file_path)\n    sheet = workbook.active\n    data = []\n    for row in sheet.iter_rows(values_only=True):\n        data.append(' '.join([str(cell) for cell in row if cell is not None]))\n    return '\\n'.join(data)\n\ndef read_csv_file(file_path):\n    with open(file_path, mode='r', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        data = [' '.join(row) for row in reader]\n    return '\\n'.join(data)\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The source directory {source_directory} does not exist.\")\n    \n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    \n    file_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    converted_files_count = 0\n    \n    for file in os.listdir(source_directory):\n        file_path = os.path.join(source_directory, file)\n        if os.path.isfile(file_path) and Path(file_path).suffix in file_extensions:\n            if Path(file_path).suffix == '.txt':\n                file_content = read_txt_file(file_path)\n            elif Path(file_path).suffix == '.docx':\n                file_content = read_docx_file(file_path)\n            elif Path(file_path).suffix == '.xlsx':\n                file_content = read_xlsx_file(file_path)\n            elif Path(file_path).suffix == '.csv':\n                file_content = read_csv_file(file_path)\n            \n            # Convert to CSV\n            df = pd.DataFrame([file_content], columns=['Text'])\n            target_file_path = os.path.join(target_directory, Path(file).stem + '.csv')\n            df.to_csv(target_file_path, index=False)\n            converted_files_count += 1\n    \n    return converted_files_count\n\n# Example usage\n# source_dir = 'path/to/source/directory'\n# target_dir = 'path/to/target/directory'\n# print(task_func(source_dir, target_dir))\n```\n\nThis code defines helper functions to read the content of files with different extensions and then uses these functions within the `task_func` to convert the files to CSV format. It counts and returns the number of files successfully converted. Note that for `.xlsx` files, it concatenates all cell values in a row with a space and reads only the active sheet. Adjustments may be needed based on specific requirements for handling Excel files."}
{"task_id": "BigCodeBench/969", "solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check for NaN values in the DataFrame\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Check for non-numeric data types in the DataFrame\n    if not all([np.issubdtype(dtype, np.number) for dtype in df.dtypes]):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n    \n    # Compute the cumulative sum for each column\n    cumsum_df = df.cumsum()\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Scale the cumulative sum DataFrame using MinMaxScaler\n    scaled_cumsum = scaler.fit_transform(cumsum_df)\n    \n    # Convert the scaled cumulative sum back to a DataFrame with original column names\n    scaled_cumsum_df = pd.DataFrame(scaled_cumsum, columns=df.columns)\n    \n    return scaled_cumsum_df\n\n# Example usage\nif __name__ == \"__main__\":\n    # Creating a sample DataFrame\n    data = {\n        'A': [1, 2, 3, 4, 5],\n        'B': [5, 4, 3, 2, 1],\n        'C': [2, 3, 4, 5, 6]\n    }\n    df = pd.DataFrame(data)\n    \n    # Applying the function\n    result_df = task_func(df)\n    print(result_df)\n```\nThis code snippet defines a function `task_func` that takes a pandas DataFrame as input and returns a new DataFrame where each numeric column contains the MinMax-normalized cumulative sum of the respective column in the input DataFrame. It checks for empty DataFrames, NaN values, and non-numeric data types, raising exceptions as specified. The example usage at the end demonstrates how to apply this function to a sample DataFrame."}
{"task_id": "BigCodeBench/971", "solution": "Certainly! Below is a self-contained Python code snippet that fulfills the specified requirements. This code will analyze a given directory (without recursing into subdirectories) and list each file it contains along with its size, creation time, and last modification time. The times are reported in UTC and in ISO format. It raises a `ValueError` if the directory does not exist.\n\n```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Check if the directory exists\n    if not Path(directory_path).is_dir():\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # Initialize an empty list to store file details\n    file_details = []\n    \n    # Iterate over each item in the directory\n    for item in os.scandir(directory_path):\n        if item.is_file():  # Ensure we are only processing files\n            # Get file size in bytes\n            file_size = os.path.getsize(item.path)\n            \n            # Get creation time and modification time, convert them to UTC and ISO format\n            # Note: os.path.getctime() and os.path.getmtime() return a float timestamp; we convert it to a datetime object in UTC\n            creation_time = datetime.fromtimestamp(os.path.getctime(item.path), tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(os.path.getmtime(item.path), tz=timezone.utc).isoformat()\n            \n            # Append the details as a tuple to the list\n            file_details.append((item.name, file_size, creation_time, modification_time))\n    \n    return file_details\n\n# Example usage (Uncomment and replace '/path/to/directory' with a real path)\n# result = task_func('/path/to/directory')\n# print(result)\n```\n\nThis code snippet starts by checking if the provided directory exists. If it does not, a `ValueError` is raised. It then proceeds to list each file in the directory, ignoring subdirectories, and collects the required details: file name, size, creation time, and last modification time. The times are converted to UTC and formatted in ISO 8601 format. Finally, it returns a list of tuples containing these details for each file.\n\nRemember to replace `'/path/to/directory'` with the actual path to the directory you want to analyze when using this function."}
{"task_id": "BigCodeBench/985", "solution": "Below is a self-contained code snippet that fulfills the requirements you've outlined. This code defines a function `task_func` that takes JSON data representing country-population pairs, validates this data, and then generates both a pandas DataFrame and a CSV file with the validated data. It handles rounding of float populations, checks for malformed or invalid data, and ensures the output directory exists and is writable.\n\n```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Attempt to parse the JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError:\n        raise ValueError(\"Malformed JSON data\")\n    \n    # Check if the data is empty\n    if not data:\n        raise ValueError(\"JSON data is empty\")\n    \n    # Prepare a list to hold valid country-population pairs\n    valid_data = []\n    \n    for country, population in data.items():\n        # Check for non-string country names and non-numeric or negative populations\n        if not isinstance(country, str) or not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Invalid country name or population value\")\n        \n        # Round down float populations to the nearest integer\n        if isinstance(population, float):\n            population = math.floor(population)\n        \n        valid_data.append({\"Country\": country, \"Population\": population})\n    \n    # Convert the valid data into a pandas DataFrame\n    df = pd.DataFrame(valid_data, columns=[\"Country\", \"Population\"])\n    \n    # Ensure the output directory exists\n    if not os.path.isdir(output_dir):\n        raise IOError(\"Output directory does not exist\")\n    \n    # Check if the output directory is writable\n    if not os.access(output_dir, os.W_OK):\n        raise IOError(\"Output directory is not writable\")\n    \n    # Construct the full file path\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Save the DataFrame to a CSV file without an index column\n    df.to_csv(file_path, index=False)\n    \n    # Return the file path and the DataFrame\n    return file_path, df\n\n# Example usage:\njson_data = '{\"USA\": 331002651, \"Canada\": 37742154, \"Australia\": 25499884, \"New Zealand\": 4822233.5}'\noutput_dir = \".\"  # Current directory\nfile_name = \"country_population_report.csv\"\n\ntry:\n    file_path, df = task_func(json_data, output_dir, file_name)\n    print(f\"CSV report generated at: {file_path}\")\n    print(df)\nexcept ValueError as ve:\n    print(f\"Error: {ve}\")\nexcept IOError as ioe:\n    print(f\"Error: {ioe}\")\n```\n\nThis code snippet includes error handling for various types of invalid input data and issues related to file writing. It uses the `json.loads` function to parse the JSON data, validates the parsed data, and then uses pandas to create a DataFrame and save it as a CSV file. The function returns the path to the generated CSV file and the DataFrame itself."}
{"task_id": "BigCodeBench/988", "solution": "Certainly! Below is a self-contained Python code snippet that fulfills the specified requirements. This function, `task_func`, takes a directory path and a list of predicates (conditions) as arguments. It evaluates each item (files and directories) within the given directory against the specified conditions. The function outputs a dictionary with directory items as keys and the results of condition checks as values. It also includes the necessary error handling as described.\n\n```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Validate the predicates list\n    if not predicates or not all(callable(p) for p in predicates):\n        raise ValueError(\"No valid predicates are provided.\")\n    \n    # Ensure the directory exists and is a directory\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"The specified directory does not exist or is not a directory: {dir_path}\")\n    \n    # Deduplicate predicates based on their __code__ attribute to ensure uniqueness\n    unique_predicates = list(set(predicates))\n    \n    # Initialize the result dictionary\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in os.listdir(dir_path):\n        # Initialize a list to store the result of each predicate for the current item\n        item_results = []\n        \n        # Evaluate each predicate against the item\n        for predicate in unique_predicates:\n            try:\n                # Apply the predicate to the item name (not the full path)\n                result = predicate(item)\n            except Exception as e:\n                # In case of any error during predicate evaluation, consider the result as False\n                result = False\n            item_results.append(result)\n        \n        # Store the combined results for the current item\n        results[item] = item_results\n    \n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a directory path (adjust this path to a valid one on your system)\n    directory_path = \"/path/to/directory\"\n    \n    # Define predicates\n    predicates = [\n        lambda name: name.endswith('.txt'),  # Checks if the item is a .txt file\n        lambda name: 'sample' in name,       # Checks if the word 'sample' is in the item name\n    ]\n    \n    try:\n        result = task_func(directory_path, predicates)\n        for item, checks in result.items():\n            print(f\"{item}: {checks}\")\n    except Exception as e:\n        print(e)\n```\n\nThis code snippet includes error handling for invalid predicates and non-existent directories. It also deduplicates the predicates based on their code objects to ensure that each unique predicate is only applied once. Adjust the `directory_path` variable to point to a valid directory on your system before running the example."}
{"task_id": "BigCodeBench/990", "solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Decode the hex string to bytes\n    bytes_data = binascii.unhexlify(hex_string)\n    \n    try:\n        # Convert bytes to UTF-8 string\n        utf8_str = bytes_data.decode('utf-8')\n    except UnicodeDecodeError:\n        return \"Error decoding UTF-8\"\n    \n    # Prepare the dictionary with various encodings\n    encoded_dict = {}\n    encoded_dict['hex'] = hex_string\n    encoded_dict['base64'] = base64.b64encode(bytes_data).decode('utf-8')\n    encoded_dict['utf-8'] = utf8_str\n    \n    # UTF-16, UTF-32, and ASCII (with check)\n    encoded_dict['utf-16'] = bytes_data.decode('utf-8').encode('utf-16').decode('utf-16')\n    encoded_dict['utf-32'] = bytes_data.decode('utf-8').encode('utf-32').decode('utf-32')\n    \n    try:\n        ascii_str = bytes_data.decode('ascii')\n        encoded_dict['ASCII'] = ascii_str\n    except UnicodeDecodeError:\n        encoded_dict['ASCII'] = 'Not representable in ASCII'\n    \n    # URL encoding\n    encoded_dict['URL'] = urllib.parse.quote(utf8_str)\n    \n    # ROT13 encoding\n    encoded_dict['ROT13'] = codecs.encode(utf8_str, 'rot_13')\n    \n    return encoded_dict\n\n# Example usage\nhex_string = \"68656c6c6f\"\nprint(task_func(hex_string))\n```\nThis code snippet defines a function `task_func` that takes a hexadecimal string as input and returns a dictionary with the string encoded in various formats. It uses the `binascii`, `base64`, `urllib.parse`, and `codecs` modules to perform the encoding operations. The function handles UTF-8 decoding and checks if the string can be represented in ASCII, returning a specific message if it cannot. It also encodes the string into hexadecimal, base64, UTF-8, UTF-16, UTF-32, ASCII (with a check), URL encoding, and ROT13, as required."}
{"task_id": "BigCodeBench/998", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task as described. This code will download a `.tar.gz` file from a specified URL, validate its MD5 checksum against a predefined expected value, and if the checksum matches, it will extract the contents of the `.tar.gz` file. If the checksum does not match, it will delete the downloaded file. The function returns `True` if the file is successfully downloaded, validated, and extracted, and `False` otherwise.\n\n```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the file from `url` and save it locally under `TARGET_TAR_FILE`\n        with urllib.request.urlopen(url) as response, open(TARGET_TAR_FILE, 'wb') as out_file:\n            data = response.read()  # a `bytes` object\n            out_file.write(data)\n    except Exception as e:\n        print(f\"Failed to download the file: {e}\")\n        return False\n\n    # Calculate MD5 checksum of the downloaded file\n    md5_hash = hashlib.md5()\n    try:\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        downloaded_file_md5 = md5_hash.hexdigest()\n    except Exception as e:\n        print(f\"Failed to calculate MD5 checksum: {e}\")\n        return False\n\n    # Validate MD5 checksum\n    if downloaded_file_md5 != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match. Deleting the downloaded file.\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n        print(\"File extracted successfully.\")\n    except tarfile.TarError as e:\n        print(f\"Failed to extract the tar.gz file: {e}\")\n        return False\n    finally:\n        # Cleanup: Remove the tar.gz file after extraction\n        os.remove(TARGET_TAR_FILE)\n\n    return True\n\n# Example usage\nurl = \"http://example.com/path/to/your/file.tar.gz\"\nresult = task_func(url)\nprint(f\"Task completed: {result}\")\n```\n\nPlease replace `\"http://example.com/path/to/your/file.tar.gz\"` with the actual URL you intend to download the `.tar.gz` file from. Also, ensure that the `EXPECTED_MD5_CHECKSUM` constant is set to the correct MD5 checksum value for your file. This script uses basic exception handling to manage errors such as download failures, issues calculating the MD5 checksum, and problems extracting the `.tar.gz` file."}
{"task_id": "BigCodeBench/999", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task as described. This function downloads a CSV file from a given URL, saves it to a specified path, counts the occurrences of each value in a particular column, and handles various error scenarios including missing columns and file download errors. After processing, the downloaded CSV file is deleted.\n\n```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a dictionary to count occurrences\n        occurrences = collections.defaultdict(int)\n        \n        # Open the downloaded CSV file\n        with open(csv_file_path, mode='r', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                occurrences[row[column_name]] += 1\n                \n        # Return the occurrences dictionary\n        return dict(occurrences)\n    \n    except urllib.error.URLError as e:\n        # Handle URL errors (e.g., connection issues, invalid URLs)\n        raise urllib.error.URLError(f\"Failed to download the CSV file from the URL: {url}. Error: {e}\")\n    \n    finally:\n        # Ensure the downloaded CSV file is deleted after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"http://example.com/path/to/your/csvfile.csv\"  # Replace with the actual URL\n    column_name = \"YourColumnName\"  # Replace with the actual column name you want to count occurrences for\n    csv_file_path = \"downloaded_file.csv\"  # Path where the CSV file will be temporarily saved\n    \n    try:\n        occurrences = task_func(url, column_name, csv_file_path)\n        print(occurrences)\n    except ValueError as e:\n        print(e)\n    except urllib.error.URLError as e:\n        print(e)\n```\n\nThis code snippet includes error handling for scenarios such as the specified column not existing in the CSV file and issues downloading the file from the provided URL. After processing the CSV file for the occurrences of each value in the specified column, it ensures the file is deleted, even if an error occurs, by using a `finally` block."}
{"task_id": "BigCodeBench/1003", "solution": "Below is a self-contained Python code snippet that fetches and parses an XML file from a specified URL, then converts it into a Pandas DataFrame. The function `task_func` will raise a `ValueError` in the scenarios described, such as when the URL is invalid, the XML file cannot be fetched, the XML has invalid syntax, or the XML structure does not conform to the expected format.\n\n```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\nimport io\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n    except Exception:\n        raise ValueError(\"The URL is invalid or the XML file cannot be fetched from the URL.\")\n    \n    try:\n        # Parse the XML data\n        xml_root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"The XML file has invalid syntax.\")\n    \n    # Check if the XML structure conforms to the expected format\n    # Assuming the expected format is a root element containing multiple 'item' elements\n    if not xml_root.tag == 'root' or not all(child.tag == 'item' for child in xml_root):\n        raise ValueError(\"XML structure does not match expected format.\")\n    \n    # Extract data into a list of dictionaries\n    data = []\n    for item in xml_root:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n        data.append(item_data)\n    \n    if not data:\n        raise ValueError(\"No data found in the XML file.\")\n    \n    # Convert the list of dictionaries into a DataFrame\n    df = pd.DataFrame(data)\n    \n    return df\n\n# Example usage (this URL won't work as it's just an example; replace with a valid URL for actual use)\n# df = task_func('http://example.com/valid_structure.xml')\n# print(df)\n```\n\nThis code does the following:\n- Tries to fetch the XML file from the provided URL. If it fails (e.g., due to a network issue or the URL not pointing to an XML file), it raises a `ValueError`.\n- Parses the fetched XML data. If the XML has syntax errors, it raises a `ValueError`.\n- Checks if the XML structure conforms to the expected format, which is assumed to be a root element containing multiple 'item' elements. If not, it raises a `ValueError`.\n- Extracts data from each 'item' element into a list of dictionaries, where each dictionary represents a row in the resulting DataFrame, and each key-value pair in the dictionary represents a column name and cell value, respectively.\n- Converts the list of dictionaries into a Pandas DataFrame and returns it.\n\nPlease replace `'http://example.com/valid_structure.xml'` with the actual URL you intend to fetch the XML data from."}
{"task_id": "BigCodeBench/1004", "solution": "Certainly! Below is a self-contained Python code snippet that fulfills the specified requirements. This code downloads a text file from a given URL, counts the frequency of each word using a basic regular expression to identify words, and plots a bar chart showing the ten most frequently occurring words. It returns a tuple containing a `Counter` object with word frequencies and a `matplotlib` Axes object representing the plotted bar chart.\n\n```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file from the specified URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n    \n    # Use a basic regular expression to find words (case-sensitive)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Select the ten most common words\n    most_common_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, frequencies = zip(*most_common_words)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Frequent Words')\n    plt.xticks(rotation=45)\n    \n    # Show the plot (optional, depending on your environment)\n    # plt.show()\n    \n    # Return the Counter object and the Axes object for the bar chart\n    return word_counts, ax\n\n# Example usage (Replace 'your_url_here' with the actual URL)\n# url = 'http://example.com/your_text_file.txt'\n# word_counts, ax = task_func(url)\n```\n\nPlease replace `'http://example.com/your_text_file.txt'` with the actual URL of the text file you wish to process. Note that this code assumes the URL points to a plain text file and may not handle binary files or non-text content correctly. Words are identified using a basic regular expression and are case-sensitive. The function does not remove common stopwords; all words are counted as is. This code requires internet access to download the file from the URL and `matplotlib` for plotting the bar chart."}
{"task_id": "BigCodeBench/1006", "solution": "Below is a self-contained Python code snippet that fulfills the requirements you've outlined. This code attempts to download a ZIP file from a specified URL and extract its contents to a given directory. It handles various exceptions as specified, including network issues, incorrect file type, and corrupt ZIP files.\n\n```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    # Ensure the download path exists\n    if not os.path.exists(download_path):\n        os.makedirs(download_path)\n    \n    try:\n        # Attempt to download the file\n        response = requests.get(url)\n        \n        # Check if the request was successful\n        if response.status_code != 200:\n            return \"Error: Unable to download the file from the provided URL.\"\n        \n        # Check if the content type is a ZIP file\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Save the downloaded ZIP to a temporary file\n        temp_zip_path = os.path.join(download_path, \"temp_download.zip\")\n        with open(temp_zip_path, 'wb') as temp_file:\n            temp_file.write(response.content)\n        \n        # Attempt to extract the ZIP file\n        try:\n            with ZipFile(temp_zip_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n        \n        # Cleanup the temporary ZIP file\n        os.remove(temp_zip_path)\n        \n        return download_path\n    \n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage\nurl = \"https://example.com/path/to/your/zipfile.zip\"\ndownload_path = \"mnt/data/downloads/\"\nprint(task_func(url, download_path))\n```\n\nThis code does the following:\n- It first checks if the download path exists and creates it if it doesn't.\n- It then attempts to download the file from the provided URL. If the download fails or the server returns a status code other than 200, it reports an error.\n- It checks the `Content-Type` of the downloaded file to ensure it is a ZIP file. If not, it reports an error.\n- It saves the downloaded content to a temporary ZIP file and attempts to extract it. If the file is corrupt or not a valid ZIP file, it reports an error.\n- If any other exceptions occur during the process, it catches them and returns a generic error message.\n- Finally, it cleans up the temporary ZIP file and returns the path to the directory containing the extracted contents if everything succeeds.\n\nPlease replace `\"https://example.com/path/to/your/zipfile.zip\"` and `\"mnt/data/downloads/\"` with your actual URL and desired download path."}
{"task_id": "BigCodeBench/1008", "solution": "Below is a self-contained Python code snippet that defines the `task_func` function. This function fetches an HTML page from a given URL, searches for a table with a specified `table_id`, and then extracts the data into a Pandas DataFrame. It handles errors as specified in your requirements, including HTTP errors and the case where the specified table is not found or contains no data rows.\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        # Raise an HTTPError for bad responses\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        # Re-raise the HTTPError exception if the request was unsuccessful\n        raise e\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Find the table with the specified ID\n    table = soup.find('table', id=table_id)\n\n    # Raise a ValueError if the table with the specified ID is not found\n    if table is None:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    # Initialize a list to store the rows of the DataFrame\n    rows = []\n    # Find all the table rows (<tr> tags) in the table\n    for tr in table.find_all('tr'):\n        # Extract the text from each cell in the row\n        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n        # Append the row to the list\n        rows.append(cells)\n\n    # Check if the table has no data rows\n    if len(rows) <= 1:  # Considering the header row, if present\n        return pd.DataFrame()  # Return an empty DataFrame\n\n    # Convert the list of rows into a DataFrame\n    # The first row of the list is considered as the header\n    df = pd.DataFrame(rows[1:], columns=rows[0])\n\n    return df\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        df = task_func('https://example.com/data.html', 'data_table_id')\n        print(df)\n    except Exception as e:\n        print(e)\n```\n\nThis code includes error handling for HTTP errors and the case where the specified table is not found on the webpage. It uses BeautifulSoup to parse the HTML content and extract the table data, which is then converted into a Pandas DataFrame. If the table is found but contains no data rows, an empty DataFrame is returned."}
{"task_id": "BigCodeBench/1012", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This code will attempt to download a ZIP file from the specified URL, save it to a directory, and then extract its contents to another directory. It handles network-related and file-related exceptions by returning an appropriate status message and an empty list if the operation fails at any point.\n\n```python\nimport requests\nfrom pathlib import Path\nimport zipfile\nimport os\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure the download and unzip directories exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    # Path to save the downloaded zip file\n    zip_path = DOWNLOAD_DIR / filename\n    \n    try:\n        # Download the zip file\n        print(f\"Downloading {filename}...\")\n        response = requests.get(url)\n        response.raise_for_status()  # Raises a HTTPError if the response was an error\n        \n        # Save the zip file\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n        \n        # Extract the zip file\n        print(f\"Extracting {filename}...\")\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        \n        # List the filenames in the unzipped directory\n        filenames = [f.name for f in ZIP_DIR.iterdir()]\n        \n        return (\"Success\", filenames)\n    \n    except requests.exceptions.RequestException as e:\n        # Handle network-related exceptions\n        print(f\"Error: Network-related exception occurred - {e}\")\n        return (\"Error: Network-related exception occurred\", [])\n    \n    except zipfile.BadZipFile as e:\n        # Handle file-related exceptions specific to zip files\n        print(f\"Error: Bad zip file - {e}\")\n        return (\"Error: Bad zip file\", [])\n    \n    except Exception as e:\n        # Handle other file-related exceptions\n        print(f\"Error: File-related exception occurred - {e}\")\n        return (\"Error: File-related exception occurred\", [])\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"https://example.com/path/to/your/zipfile.zip\"  # Replace with the actual URL\n    filename = \"your_zipfile.zip\"  # Replace with the actual filename\n    status, files = task_func(url, filename)\n    print(status)\n    if files:\n        print(\"Extracted files:\")\n        for file in files:\n            print(file)\n```\n\nThis script defines a function `task_func` that takes a URL and a filename as arguments. It attempts to download a ZIP file from the given URL, save it to a `downloads` directory, and then extract its contents to an `unzipped_files` directory. The function returns a tuple containing a status message and a list of filenames in the unzipped directory. In case of any exceptions, it catches them and returns an appropriate error message with an empty list."}
{"task_id": "BigCodeBench/1013", "solution": "Certainly! Below is a self-contained Python code snippet that defines a function `task_func`. This function scrapes all hyperlinks from the specified webpage, converts them to absolute URLs using the provided base URL if necessary, saves these URLs to a CSV file, and returns the count of unique absolute URLs found.\n\n```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    # Initialize BeautifulSoup to parse the response content\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find all hyperlinks in the webpage\n    links = soup.find_all('a')\n    \n    # Initialize a set to store unique absolute URLs\n    unique_links = set()\n    \n    # Convert each link to absolute URL and add to the set of unique links\n    for link in links:\n        href = link.get('href')\n        if href:  # Ensure href is not None\n            absolute_url = urljoin(base_url, href)\n            unique_links.add(absolute_url)\n    \n    # Write the unique absolute URLs to a CSV file\n    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        for link in unique_links:\n            writer.writerow([link])\n    \n    # Return the number of unique absolute links\n    return len(unique_links)\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"https://www.example.com\"\n    print(task_func(url))\n```\n\nThis code does the following:\n1. Sends a GET request to the specified URL.\n2. Parses the HTML content of the page using BeautifulSoup.\n3. Finds all `<a>` tags to extract hyperlinks.\n4. Converts each hyperlink to an absolute URL using the specified base URL if necessary.\n5. Saves these absolute URLs to a CSV file, ensuring each URL is unique.\n6. Returns the count of unique absolute URLs found.\n\nPlease ensure you have the required packages (`requests`, `beautifulsoup4`, and `csv` which is part of the standard library) installed in your Python environment. You can install `requests` and `beautifulsoup4` using pip if you haven't already:\n\n```sh\npip install requests beautifulsoup4\n```"}
{"task_id": "BigCodeBench/1015", "solution": "Below is a self-contained code snippet that defines the `task_func` function. This function takes a webpage URL or a local file path and a database name as inputs. It fetches HTML table data from the specified source, parses it, and stores it into an SQLite database, replacing the table \"my_table\" each time it is called. The function handles exceptions related to network issues and database errors, and returns the number of rows parsed and stored in the database.\n\n```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Check if the input URL is a local file path or a webpage URL\n        if webpage_url.startswith('http'):\n            # Fetch the webpage content\n            response = requests.get(webpage_url)\n            # Raise an exception for bad responses\n            response.raise_for_status()\n            content = response.text\n        else:\n            # Read the content of a local file\n            with open(webpage_url, 'r') as file:\n                content = file.read()\n        \n        # Parse the HTML content\n        tree = html.fromstring(content)\n        # Extract the first table found\n        table = tree.xpath('//table[1]')\n        if not table:\n            return 0  # Return 0 if no table is found\n        \n        # Convert the HTML table to a pandas DataFrame\n        df = pd.read_html(html.tostring(table[0]), flavor='lxml')[0]\n        if df.empty:\n            return 0  # Return 0 if the table is empty\n        \n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        cursor = conn.cursor()\n        \n        # Replace the \"my_table\" with new data\n        cursor.execute(\"DROP TABLE IF EXISTS my_table\")\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        \n        # Commit changes and close the connection\n        conn.commit()\n        conn.close()\n        \n        # Return the number of rows parsed and stored\n        return len(df)\n    \n    except requests.RequestException as e:\n        # Handle exceptions related to network issues\n        raise requests.RequestException(f\"Network issue encountered: {e}\")\n    \n    except sqlite3.DatabaseError as e:\n        # Handle exceptions related to the SQLite database\n        raise sqlite3.DatabaseError(f\"Database error encountered: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    webpage_url = \"http://example.com/table.html\"  # Replace with the actual URL or local file path\n    database_name = \"my_database.db\"\n    try:\n        rows_count = task_func(webpage_url, database_name)\n        print(f\"Number of rows parsed and stored: {rows_count}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```\n\nThis code snippet includes error handling for network issues and database errors as specified. It uses the `requests` library to fetch webpage content, `lxml` to parse HTML, `pandas` to convert HTML tables into a DataFrame, and `sqlite3` to interact with the SQLite database. The function replaces the \"my_table\" in the SQLite database with the new data each time it is called and returns the number of rows parsed and stored."}
{"task_id": "BigCodeBench/1019", "solution": "To accomplish the task described, we'll need to use the `PIL` (Python Imaging Library, known as Pillow in its maintained version), `pytesseract` for OCR (Optical Character Recognition), and `codecs` for encoding conversion. The function `task_func` will try to extract text from an image using OCR. If OCR fails or doesn't find any text, it will attempt to read any comments embedded in the image file. It will then convert the extracted text or comment from one encoding to another, handling specific exceptions as required.\n\nFirst, ensure you have the necessary libraries installed:\n- Pillow: `pip install Pillow`\n- pytesseract: `pip install pytesseract`\n- Additionally, you need to have Tesseract-OCR installed on your system. Installation instructions can be found at https://github.com/tesseract-ocr/tesseract\n\nHere's the complete code:\n\n```python\nfrom PIL import Image\nimport pytesseract\nimport codecs\n\n# Define the path to your image\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Attempt to extract text using OCR\n        text = pytesseract.image_to_string(Image.open(filename))\n        \n        # If text is extracted, try to convert its encoding\n        if text:\n            try:\n                text = text.encode(from_encoding).decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(e)\n        else:\n            # If OCR fails to extract text, attempt to read the image comment\n            with Image.open(filename) as img:\n                comment = img.info.get(\"comment\", b\"\")\n                if comment:\n                    try:\n                        comment = comment.decode(from_encoding).encode(to_encoding).decode(to_encoding)\n                        text = comment  # Use the comment as the text\n                    except (UnicodeDecodeError, LookupError) as e:\n                        raise ValueError(e)\n                else:\n                    # If there's no comment, return an empty string\n                    text = \"\"\n    except Exception as e:\n        # Handle unexpected errors\n        print(f\"An error occurred: {e}\")\n        text = \"\"  # Ensure text is set to an empty string on failure\n\n    return text\n\n# Example usage\nif __name__ == \"__main__\":\n    extracted_text = task_func()\n    print(\"Extracted Text:\", extracted_text)\n```\n\nThis code tries to extract text from an image using OCR. If successful, it converts the text's encoding. If OCR fails or finds no text, it looks for an image comment and attempts to convert its encoding. It handles `UnicodeDecodeError` and `LookupError` by raising a `ValueError` with the original exception's message. If both OCR and comment extraction fail, or if any other exception occurs, it returns an empty string."}
{"task_id": "BigCodeBench/1020", "solution": "Certainly! Below is a self-contained Python code snippet that fulfills the requirements you've outlined. This code fetches data from a specified REST API URL, handles different encoding scenarios, and processes the data for JSON parsing. It includes error handling for undetectable encoding and empty content scenarios.\n\n```python\nimport json\nimport requests\nimport chardet  # For detecting character encoding\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Initiate an HTTP GET request with a 5-second timeout\n        response = requests.get(url, timeout=5)\n        # Check if the response content is empty\n        if not response.content:\n            return {}\n        \n        # Detect encoding if not specified\n        if from_encoding is None:\n            detected = chardet.detect(response.content)\n            from_encoding = detected.get('encoding')\n            if from_encoding is None:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n        \n        # Decode the content using the detected or specified encoding\n        decoded_content = response.content.decode(from_encoding)\n        \n        # Re-encode the content to the specified encoding (e.g., utf8)\n        re_encoded_content = decoded_content.encode(to_encoding)\n        \n        # Parse the re-encoded content as JSON\n        json_data = json.loads(re_encoded_content)\n        \n        return json_data\n    \n    except requests.RequestException as e:\n        print(f\"Request error: {e}\")\n        return {}\n    except ValueError as e:\n        print(f\"Value error: {e}\")\n        return {}\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return {}\n\n# Example usage\nif __name__ == \"__main__\":\n    result = task_func()\n    print(result)\n```\n\nThis code snippet includes the following functionalities:\n- It makes an HTTP GET request to the specified URL with a 5-second timeout.\n- It checks if the response content is empty and returns an empty dictionary if so.\n- It uses the `chardet` library to detect the encoding of the response content if not specified.\n- It raises a `ValueError` if it fails to detect the encoding for non-empty response content.\n- It decodes the content using the detected or specified encoding, then re-encodes it to UTF-8.\n- It parses the re-encoded content as JSON and returns the resulting dictionary.\n- It includes error handling for request errors, value errors, and other unexpected errors, printing the error message and returning an empty dictionary in such cases."}
{"task_id": "BigCodeBench/1022", "solution": "Certainly! Below is a self-contained code snippet that fulfills the requirements you've outlined. This function reads a CSV file, checks for its existence, validates the presence of a specified date column, converts date values to datetime objects, filters rows based on the current date, sorts the resulting data, and handles exceptions as specified.\n\n```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the CSV file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        # Attempt to read the CSV file\n        df = pd.read_csv(csv_file_path)\n        \n        # Check if the DataFrame is empty (which also means the CSV was empty)\n        if df.empty:\n            return pd.DataFrame()  # Return an empty DataFrame\n        \n        # Validate the presence of the specified date column\n        if column_name not in df.columns:\n            raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n        \n        # Convert date values in the specified column to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n        \n        # Filter rows based on the current date (keep rows with dates up to today)\n        today = datetime.now()\n        filtered_df = df[df[column_name] <= today]\n        \n        # Sort the resulting data by the date column\n        sorted_df = filtered_df.sort_values(by=column_name)\n        \n        return sorted_df\n    \n    except EmptyDataError:\n        # Handle the case where the CSV file is empty\n        return pd.DataFrame()  # Return an empty DataFrame\n\n# Example usage\nif __name__ == \"__main__\":\n    csv_file_path = 'example.csv'  # Specify the path to your CSV file\n    column_name = 'date_column'    # Specify the name of the date column in your CSV file\n    try:\n        result_df = task_func(csv_file_path, column_name)\n        print(result_df)\n    except FileNotFoundError as e:\n        print(e)\n    except ValueError as e:\n        print(e)\n```\n\nThis code snippet includes error handling for a missing file (`FileNotFoundError`), a missing column (`ValueError`), and an empty CSV file (handled by returning an empty `DataFrame`). It uses `pandas` for data manipulation, `os` to check file existence, `datetime` for date operations, and handles `pandas.errors.EmptyDataError` in case the CSV file is empty."}
{"task_id": "BigCodeBench/1039", "solution": "Certainly! Below is a self-contained Python function that secures a client socket using SSL/TLS, reads a file path sent by the client, computes the SHA256 hash of the requested file, and sends the hash back to the client. If the file does not exist, it sends back a 'File not found' message. It also includes basic error handling.\n\n```python\nimport ssl\nimport os\nimport hashlib\nimport socket\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    Secures a client socket using SSL/TLS, reads a file path from the client,\n    computes the SHA256 hash of the requested file, and sends the hash back to the client.\n    If the file does not exist, sends back 'File not found'.\n    In case of an exception during processing, an error message is returned.\n\n    Parameters:\n    - client_socket: The client socket object (before SSL wrapping).\n    - cert_file: Path to the SSL certificate file.\n    - key_file: Path to the SSL key file.\n    - buffer_size: Size of the buffer for reading from the socket and the file.\n\n    Returns:\n    - str: The SHA256 hash of the requested file, 'File not found', or an error message.\n    \"\"\"\n    try:\n        # Wrap the client socket in SSL\n        context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_socket.recv(buffer_size).decode('utf-8')\n\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            secure_socket.sendall(b'File not found')\n            return 'File not found'\n\n        # Compute the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(buffer_size), b''):\n                sha256_hash.update(byte_block)\n        hash_hex = sha256_hash.hexdigest()\n\n        # Send the hash back to the client\n        secure_socket.sendall(hash_hex.encode('utf-8'))\n\n        return hash_hex\n    except Exception as e:\n        error_message = f\"An error occurred: {str(e)}\"\n        try:\n            secure_socket.sendall(error_message.encode('utf-8'))\n        except:\n            pass  # In case the socket is in a bad state\n        return error_message\n\n# Example usage (This part is not meant to be run as-is since it requires a client and SSL setup)\nif __name__ == \"__main__\":\n    # Create a socket and bind to an address\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.bind(('localhost', 12345))\n    server_socket.listen(5)\n\n    print(\"Server listening...\")\n\n    while True:\n        client_socket, address = server_socket.accept()\n        print(f\"Connection from {address} has been established.\")\n\n        # Call the function with the client socket and paths to your SSL certificate and key\n        result = task_func(client_socket, 'path/to/cert.pem', 'path/to/key.pem')\n        print(result)\n\n        client_socket.close()\n```\n\nThis code snippet includes a basic server setup for demonstration purposes. However, the `task_func` function is the primary focus, designed to be integrated into a larger application where the server setup, SSL certificate, and key management are appropriately handled. Remember to replace `'path/to/cert.pem'` and `'path/to/key.pem'` with the actual paths to your SSL certificate and key files."}
{"task_id": "BigCodeBench/1040", "solution": "Certainly! Below is a self-contained Python code snippet that implements a non-blocking echo server. This server appends the server's current time to received data and sends it back to the client. It also handles exceptional conditions for each socket and outputs a status message indicating the server's operation and run duration.\n\n```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)  # Set the socket to non-blocking\n\n    # Bind the socket to the port\n    server.bind((server_address, server_port))\n\n    # Listen for incoming connections\n    server.listen(5)\n    print(f\"Listening on {server_address}:{server_port}\")\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Outgoing message queues (socket:Queue)\n    message_queues = {}\n\n    # Time tracking\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    while inputs and datetime.now() < end_time:\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        # Handle inputs\n        for s in readable:\n            if s is server:\n                # A \"readable\" server socket is ready to accept a connection\n                connection, client_address = s.accept()\n                print(f\"New connection from {client_address}\")\n                connection.setblocking(0)\n                inputs.append(connection)\n\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # A readable client socket has data\n                    print(f\"Received {data} from {s.getpeername()}\")\n                    message_queues[s].put(data + b\" \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").encode())\n                    # Add output channel for response\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # Interpret empty result as closed connection\n                    print(f\"Closing {client_address} after reading no data\")\n                    # Stop listening for input on the connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n\n                    # Remove message queue\n                    del message_queues[s]\n\n        # Handle outputs\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                # No messages waiting so stop checking for writability.\n                print(f\"Output queue for {s.getpeername()} is empty\")\n                outputs.remove(s)\n            else:\n                print(f\"Sending {next_msg} to {s.getpeername()}\")\n                s.send(next_msg)\n\n        # Handle \"exceptional conditions\"\n        for s in exceptional:\n            print(f\"Handling exceptional condition for {s.getpeername()}\")\n            # Stop listening for input on the connection\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n\n            # Remove message queue\n            del message_queues[s]\n\n    # Clean up\n    server.close()\n    run_time = datetime.now() - start_time\n    return f\"Server operation completed. Run duration was {run_time.seconds} seconds.\"\n\n# Example usage\nif __name__ == \"__main__\":\n    status_message = task_func()\n    print(status_message)\n```\n\nThis code sets up a non-blocking echo server that listens for incoming connections, reads data from clients, appends the current server time to the data, and sends it back. It handles new connections, reads from existing connections, and manages writable sockets to ensure data is sent back efficiently. The server runs for a specified duration (`run_duration` in seconds) and then returns a status message indicating how long it operated. Exceptional conditions on sockets are also handled gracefully, ensuring the server remains robust."}
{"task_id": "BigCodeBench/1042", "solution": "Certainly! Below is a self-contained Python code snippet that completes the specified task. This code will receive a message from a client socket, then send it as an email via an SMTP server. It uses `smtplib` for SMTP operations and `email.message.EmailMessage` for creating the email message. It also uses `getpass` to securely input the sender's email password. Note that for Gmail, you might need to enable \"Less secure app access\" or use an App Password if two-factor authentication is enabled.\n\n```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 1025  # Adjusted for a typical testing scenario; change as needed\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Receive message from client\n    received_message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    print(f\"Message received from client: {received_message}\")\n\n    # Prompt for sender's email, recipient's email, and sender's email password\n    sender_email = input(\"Sender's Email: \")\n    recipient_email = input(\"Recipient's Email: \")\n    sender_password = getpass.getpass(\"Sender's Email Password: \")\n\n    # Create the email message\n    email_message = EmailMessage()\n    email_message.set_content(received_message)\n    email_message['Subject'] = 'Message from Client Socket'\n    email_message['From'] = sender_email\n    email_message['To'] = recipient_email\n\n    # Send the email via SMTP\n    try:\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp_server:\n            smtp_server.ehlo()\n            smtp_server.starttls()  # Secure the connection\n            smtp_server.ehlo()\n            smtp_server.login(sender_email, sender_password)\n            smtp_server.send_message(email_message)\n            print(\"Email sent successfully!\")\n    except Exception as e:\n        print(f\"Failed to send email: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Setup a server socket for demonstration purposes (normally, you'd have a client connect to this)\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:\n        server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n        server_socket.listen(1)\n        print(f\"Server listening on {SERVER_ADDRESS}:{SERVER_PORT}\")\n\n        # Accept a connection for demonstration\n        client_socket, addr = server_socket.accept()\n        with client_socket:\n            print(f\"Connected by {addr}\")\n            task_func(client_socket)\n```\n\n**Important Notes:**\n- This code includes a basic setup for receiving a message from a client socket for demonstration purposes. In a real-world scenario, you would have a separate client program or device sending data to this server.\n- The SMTP server details (`SMTP_SERVER` and `SMTP_PORT`) are set for Gmail's SMTP server. If you're using a different email provider, you'll need to adjust these settings accordingly.\n- For Gmail, you might need to enable \"Less secure app access\" in your Google account settings or use an App Password if two-factor authentication is enabled. Google is phasing out \"Less secure app access,\" so using OAuth 2.0 is recommended for new applications.\n- The server and client parts are combined here for demonstration. In practice, they would be separate entities, possibly on different machines."}
{"task_id": "BigCodeBench/1053", "solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n    \n    try:\n        # Load the CSV file\n        data = pd.read_csv(file_path)\n        \n        # Check if the CSV has the expected 'Text' column or use the first column\n        if 'Text' not in data.columns:\n            # Assuming the first column contains the text data if 'Text' column is missing\n            text_data = data.iloc[:, 0]\n        else:\n            text_data = data['Text']\n        \n        # Initialize CountVectorizer with stopwords\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        \n        # Fit and transform the text data to count frequency\n        word_count = vectorizer.fit_transform(text_data)\n        \n        # Summing up the word frequency\n        sum_words = word_count.sum(axis=0)\n        \n        # Mapping word to its frequency\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        \n        # Sorting words by frequency\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n        \n        # Selecting top 10 most frequent words\n        top_words = words_freq[:10]\n        \n        # Unzipping for plotting\n        words, freqs = zip(*top_words)\n        \n        # Plotting\n        plt.figure(figsize=(10, 8))\n        plt.bar(words, freqs, color='skyblue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        \n        if save_path:\n            plt.savefig(save_path)\n            plt.close()  # Close the plot explicitly after saving\n            return None\n        else:\n            return plt.gca()  # Return the Axes object for further customization or display\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage\n# task_func('path/to/your/csvfile.csv')\n# task_func('path/to/your/csvfile.csv', 'path/to/save/plot.png')\n```\nThis code snippet defines a function `task_func` that processes a CSV file to generate a histogram of the ten most common words, excluding a predefined set of stopwords. It uses pandas for reading the CSV, sklearn's `CountVectorizer` for text processing, and matplotlib for plotting. The function is designed to handle errors gracefully, including file not found errors and other exceptions during execution."}
{"task_id": "BigCodeBench/1057", "solution": "Certainly! Below is a self-contained code snippet that fulfills the requirements you've outlined. This code defines a function `task_func` that takes optional `animals` and `foods` lists, generates all possible combinations of these, shuffles them, and then formats them into a pandas DataFrame as specified.\n\n```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nfrom random import shuffle\n\ndef task_func(animals=None, foods=None):\n    # Predefined lists in case one or both arguments are empty or not provided\n    default_animals = ['cat', 'dog', 'bird']\n    default_foods = ['fish', 'meat', 'seeds']\n    \n    # Use default lists if arguments are None or empty\n    if not animals:\n        animals = default_animals\n    if not foods:\n        foods = default_foods\n    \n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations to ensure variety\n    shuffle(combinations)\n    \n    # Format combinations into 'animal:food' strings\n    formatted_combinations = [f'{animal}:{food}' for animal, food in combinations]\n    \n    # Calculate the dimensions of the DataFrame\n    num_rows = len(animals)\n    num_cols = len(foods)\n    \n    # Ensure the list is not empty to avoid reshaping errors\n    if formatted_combinations:\n        # Reshape the list into a matrix for the DataFrame\n        matrix = np.array(formatted_combinations).reshape(num_rows, num_cols)\n    else:\n        # If the list is empty, create an empty DataFrame\n        matrix = []\n    \n    # Create the DataFrame\n    df = pd.DataFrame(matrix, columns=foods, index=animals)\n    \n    return df\n\n# Example usage\ndf = task_func(['lion', 'tiger'], ['beef', 'chicken'])\nprint(df)\n```\n\nThis code snippet starts by importing the necessary libraries (`pandas`, `itertools`, `numpy`, and `shuffle` from `random`). It then defines the `task_func` function, which checks if the `animals` and `foods` lists are provided and non-empty; otherwise, it uses default lists. It generates all combinations of the provided lists, shuffles them for variety, formats them into the 'animal:food' string format, and finally reshapes the list into a matrix that fits the desired DataFrame structure. The DataFrame is then created with animals as row indices and foods as column names, and each cell containing the corresponding 'animal:food' string."}
{"task_id": "BigCodeBench/1077", "solution": "Below is a self-contained code snippet that follows the given instructions. This code defines a function `task_func` that takes a list of timestamp strings (`time_strings`) and a string representing a timezone (`timezone`). It converts each timestamp to the specified timezone, calculates the absolute time differences in seconds between each consecutive pair of timestamps, and returns the average of these differences using numpy's mean function. If the list contains less than two timestamps, it returns 0.0.\n\n```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Check if the list has less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n    \n    # Convert the timezone string to a timezone object\n    tz = pytz.timezone(timezone)\n    \n    # Convert each timestamp string to a datetime object in the specified timezone\n    timestamps = [datetime.fromisoformat(ts).astimezone(tz) for ts in time_strings]\n    \n    # Calculate the absolute time differences in seconds between each consecutive pair of timestamps\n    time_diffs = [abs((timestamps[i] - timestamps[i-1]).total_seconds()) for i in range(1, len(timestamps))]\n    \n    # Calculate the mean of the time differences\n    mean_diff = np.mean(time_diffs)\n    \n    return mean_diff\n\n# Example usage\ntime_strings = ['2023-01-01T12:00:00+00:00', '2023-01-01T15:00:00+00:00', '2023-01-02T12:00:00+00:00']\ntimezone = 'America/New_York'\nprint(task_func(time_strings, timezone))\n```\n\nThis code does the following:\n- It first checks if there are less than two timestamps in the input list. If so, it returns 0.0 immediately.\n- It converts the input timezone string into a timezone object using `pytz.timezone`.\n- It then converts each timestamp string in the input list to a datetime object, adjusts it to the specified timezone, and calculates the absolute differences in seconds between each consecutive pair of these timestamps.\n- Finally, it calculates and returns the mean of these differences using `np.mean`.\n\nPlease ensure you have the `pytz` and `numpy` packages installed in your Python environment to run this code successfully."}
{"task_id": "BigCodeBench/1085", "solution": "Certainly! Below is a self-contained Python code snippet that accomplishes the task described. This code first processes the input text by lowercasing it, removing punctuation, splitting it into words, and then uses `Counter` to find the most common words. It then plots the top 10 most common words using `matplotlib`.\n\n```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text to make the analysis case-insensitive\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the 10 most common words and their counts\n    most_common_words = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, frequencies = zip(*most_common_words)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    \n    # Show the plot\n    plt.show()\n    \n    return most_common_words, ax\n\n# Example usage:\ntext = \"This is a test text. It's meant to test how well the function can identify the most common words, including words like 'test', 'is', and 'the'.\"\nmost_common_words, ax = task_func(text)\nprint(most_common_words)\n```\n\nThis code defines `task_func` which takes a string `text` as input and performs the following steps:\n1. Lowercases the entire text for case-insensitive analysis.\n2. Removes punctuation using a regular expression that keeps only word characters and spaces.\n3. Splits the text into a list of words.\n4. Counts the frequency of each word using `Counter` from the `collections` module.\n5. Extracts the 10 most common words and their counts.\n6. Plots these words and their frequencies as a bar chart using `matplotlib`.\n7. Returns the list of tuples containing the 10 most common words and their counts, along with the `Axes` object for the plot.\n\nRemember to have `matplotlib` installed in your environment to run this code (`pip install matplotlib`)."}
{"task_id": "BigCodeBench/1124", "solution": "Certainly! Below is a self-contained Python code snippet that extracts a URL from a given string, retrieves the title of the web page from that URL, and handles errors as specified:\n\n```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs in the given string\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    url_match = re.search(url_pattern, myString)\n    \n    # Check if a URL was found\n    if not url_match:\n        return \"No valid URL found in the provided string.\"\n    \n    url = url_match.group(0)\n    \n    # Attempt to fetch the content of the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError if the response status code is 4XX/5XX\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    # Parse the content of the webpage\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Attempt to extract the title of the webpage\n    title_tag = soup.find('title')\n    \n    if title_tag:\n        return title_tag.text.strip()\n    else:\n        return \"No title tag found in the webpage.\"\n\n# Example usage\nmyString = \"Check out this website: https://www.example.com It's really cool!\"\nprint(task_func(myString))\n```\n\nThis code defines a function `task_func` that takes a string as input. It uses a regular expression to search for URLs within the string. If a URL is found, it attempts to fetch the webpage using the `requests` library. If the fetch is successful, it parses the HTML content with `BeautifulSoup` to find the `<title>` tag and returns its text content. If any step fails, it returns an appropriate error message as specified."}
{"task_id": "BigCodeBench/1129", "solution": "Below is a self-contained code snippet that accomplishes the task described. This code parses a JSON string to find a URL associated with a specified key, downloads the file from the URL, and saves it with a timestamped filename in the specified directory or the current working directory by default. It then returns the absolute path of the downloaded file.\n\n```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract the URL associated with the specified key\n    url = data.get(unknown_key)\n    if not url:\n        raise ValueError(f\"Key '{unknown_key}' not found in the provided JSON data.\")\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download the file from URL: {url}\")\n    \n    # Generate a timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir is None:\n        save_dir = os.getcwd()  # Use the current working directory if no directory is specified\n    else:\n        # Create the directory if it does not exist\n        os.makedirs(save_dir, exist_ok=True)\n    \n    # Save the file\n    file_path = os.path.join(save_dir, filename)\n    with open(file_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the downloaded file\n    return os.path.abspath(file_path)\n\n# Example usage:\njson_data = '{\"example_key\": \"https://example.com/path/to/file.txt\"}'\nunknown_key = \"example_key\"\nsave_dir = \"./downloads\"  # Specify your directory or leave as None for current directory\n\ntry:\n    downloaded_file_path = task_func(json_data, unknown_key, save_dir)\n    print(f\"File downloaded and saved at: {downloaded_file_path}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n```\n\nThis code snippet includes error handling for cases where the key is not found in the JSON data or the file cannot be downloaded from the URL. It also ensures that the specified directory is created if it does not already exist. Remember to replace the `json_data`, `unknown_key`, and `save_dir` variables with your actual data when using this function."}
{"task_id": "BigCodeBench/1137", "solution": "Below is a self-contained Python code snippet that extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format. The function `task_func` takes a URL or a local file path as input and an output file path where the extracted phone numbers will be saved in JSON format. It returns a list of found phone numbers.\n\n```python\nimport bs4\nimport requests\nimport re\nimport json\nfrom urllib.parse import urlparse\n\ndef task_func(url: str, output_path: str) -> list:\n    # Determine if the input is a URL or a local file path\n    try:\n        # Check if input is a URL\n        result = urlparse(url)\n        if all([result.scheme, result.netloc]):\n            is_url = True\n        else:\n            is_url = False\n    except:\n        is_url = False\n\n    # Initialize an empty list to store phone numbers\n    phone_numbers = []\n\n    # Define a regular expression pattern for phone numbers\n    # This pattern is quite general; you might need to adjust it based on the expected phone number formats\n    phone_pattern = re.compile(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    if is_url:\n        # Fetch the content from the URL\n        response = requests.get(url)\n        content = response.text\n    else:\n        # Read the content from the local file\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the HTML or text content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    text = soup.get_text()\n\n    # Find all phone numbers in the text\n    phone_numbers = phone_pattern.findall(text)\n\n    # Save the phone numbers to the specified output file in JSON format\n    with open(output_path, 'w') as json_file:\n        json.dump(phone_numbers, json_file)\n\n    return phone_numbers\n\n# Example usage\nif __name__ == \"__main__\":\n    url = 'example.html'  # This can be a URL or a local file path\n    output_path = 'phone_numbers.json'\n    found_phone_numbers = task_func(url, output_path)\n    print(found_phone_numbers)\n```\n\nThis code uses BeautifulSoup to parse HTML content and extract text where phone numbers are likely to be found. It uses a regular expression to find phone numbers in the text. The regular expression `\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b` is designed to match U.S. phone numbers in various formats, but you might need to adjust it based on the specific formats of phone numbers you expect to encounter.\n\nPlease note that the effectiveness of this script in extracting phone numbers accurately depends on the complexity of the webpage or document's structure and the variability of phone number formats. You may need to customize the regular expression to better suit your needs."}
