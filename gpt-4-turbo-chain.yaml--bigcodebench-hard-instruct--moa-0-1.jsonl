{"task_id": "BigCodeBench/13", "solution": "```python\nimport subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Login to the FTP server\n        ftp.login(ftp_user, ftp_password)\n    except Exception as e:\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    try:\n        # List files in the directory\n        filenames = ftp.nlst()\n    except Exception as e:\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n\n    attempted_downloads = []\n    for filename in filenames:\n        # Construct the file URL\n        file_url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        try:\n            # Use wget to download the file\n            subprocess.run([\"wget\", file_url], check=True)\n            attempted_downloads.append(filename)\n        except subprocess.CalledProcessError as e:\n            # In case wget command fails, log the filename but continue with the next files\n            attempted_downloads.append(filename)\n    \n    # Close the FTP connection\n    ftp.quit()\n    \n    return attempted_downloads\n```"}
{"task_id": "BigCodeBench/15", "solution": "```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the specified commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Ensure the output directory exists, create it if it does not\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    output_paths = []  # To store paths to the output files\n    \n    # Open and read the CSV file containing commands\n    with open(commands_file_path, newline='') as csvfile:\n        command_reader = csv.reader(csvfile)\n        for index, row in enumerate(command_reader):\n            command = row[0]  # Assuming the command is in the first column\n            \n            # Initialize output variable\n            output = \"\"\n            \n            # Try executing the command\n            try:\n                # Execute the command and capture its output\n                result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                output = result.stdout  # On success, use stdout as output\n            except subprocess.CalledProcessError as e:\n                # On failure, format the error message and exit code\n                output = f\"Error executing command: {command}\\nExit code: {e.returncode}\\nError output: {e.stderr}\"\n            \n            # Define the path for the output file\n            output_file_path = os.path.join(output_dir_path, f'command_{index + 1}_output.txt')\n            \n            # Write the command's output or error message to the file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(output)\n            \n            # Append the path of the output file to the list\n            output_paths.append(output_file_path)\n    \n    return output_paths\n```"}
{"task_id": "BigCodeBench/17", "solution": "```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Iterate through all running processes to check if the process is running\n    for proc in psutil.process_iter(attrs=['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # If the process is found, terminate it\n            process = psutil.Process(proc.info['pid'])\n            process.terminate()\n            process.wait()  # Wait for the process to terminate\n            time.sleep(1)  # Ensure the process has fully terminated\n            \n            # Restart the process\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # If the process is not found running, start it\n    subprocess.Popen(process_name)\n    return f\"Process not_found. Starting {process_name}.\"\n\n# Example of using the function\n# print(task_func(\"notepad.exe\"))\n```"}
{"task_id": "BigCodeBench/19", "solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(\"The specified directory does not exist\")\n    \n    # Find all files directly inside the specified directory\n    files = glob.glob(os.path.join(directory, \"*.*\"))\n    \n    # Filter out directories, keep only files\n    files = [f for f in files if os.path.isfile(f)]\n    \n    # If no files are found, return None\n    if not files:\n        return None\n    \n    # Create a zip file named 'files.zip' in the specified directory\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            # Add each file to the zip file\n            zipf.write(file, os.path.basename(file))\n    \n    # Return the path to the created zip file\n    return zip_path\n```"}
{"task_id": "BigCodeBench/34", "solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove all URLs from the text using regular expressions\n    text_without_urls = re.sub(r'http\\S+|www\\.\\S+', '', text)\n    \n    # Check if there are any words left in the text. If not, raise ValueError\n    if not text_without_urls.strip():\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate a word cloud using the WordCloud class\n    word_cloud = WordCloud().generate(text_without_urls)\n    \n    # Display the generated word cloud using matplotlib.pyplot\n    plt.imshow(word_cloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n    \n    # Return the WordCloud object created\n    return word_cloud\n```"}
{"task_id": "BigCodeBench/37", "solution": "```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, target_column):\n    # Separate the features and the target variable\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    # Instantiate and train the RandomForestClassifier\n    model = RandomForestClassifier()\n    model.fit(X, y)\n    \n    # Get feature importance scores\n    feature_importances = model.feature_importances_\n    \n    # Map feature importances to the corresponding feature names\n    feature_names = X.columns\n    feature_importance_dict = dict(zip(feature_names, feature_importances))\n    \n    # Sort features by importance\n    sorted_features = sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True)\n    sorted_feature_names = [item[0] for item in sorted_features]\n    sorted_importances = [item[1] for item in sorted_features]\n    \n    # Plotting the feature importances\n    plt.figure(figsize=(10, 8))\n    sns.barplot(x=sorted_importances, y=sorted_feature_names)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    ax = plt.gca()  # Get the current Axes instance on the current figure\n    \n    return model, ax\n```"}
{"task_id": "BigCodeBench/82", "solution": "```python\nfrom flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    \n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n    \n    # Assuming a simple in-memory 'database' for demonstration purposes\n    users = {'user1': {'password': generate_password_hash('example')}}\n    \n    class User(UserMixin):\n        def __init__(self, username):\n            self.id = username\n        \n        @staticmethod\n        def authenticate(username, password):\n            user = users.get(username)\n            if user and check_password_hash(user['password'], password):\n                return User(username)\n            return None\n        \n    @login_manager.user_loader\n    def load_user(user_id):\n        if user_id not in users:\n            return None\n        return User(user_id)\n    \n    @app.route('/')\n    def index():\n        return 'Home Page'\n    \n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            username = form.username.data\n            password = form.password.data\n            user = User.authenticate(username, password)\n            if user:\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                return 'Invalid username or password'\n        return render_template('login.html', form=form)\n    \n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Protected Page. Current User: {current_user.id}'\n    \n    @app.route('/logout')\n    def logout():\n        logout_user()\n        return redirect(url_for('index'))\n    \n    return app\n\n# To run the application outside of a Python script, you can use:\n# app = task_func('your_secret_key', 'path/to/your/templates')\n# app.run(debug=True)\n```\nThis script defines a Flask application with user authentication using Flask-Login. It includes routes for login, logout, and accessing a protected page that requires authentication. User authentication is demonstrated using a simple `User` class, and a login form is implemented with Flask-WTF. The application is designed to be configured dynamically with a secret key and template folder path passed as arguments to the `task_func` function."}
{"task_id": "BigCodeBench/89", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    # Convert input data to numpy array if it's not already, to ensure compatibility\n    data = np.asarray(data)\n\n    # Extract the specified column for analysis\n    column_data = data[:, column].reshape(-1, 1)\n\n    # Standardize the column data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(column_data)\n\n    # Calculate Z-scores of the standardized data\n    z_scores = np.abs(stats.zscore(standardized_data, axis=0))\n\n    # Identify outliers based on the Z-score threshold\n    outliers = np.where(z_scores > outlier_z_score)\n\n    # Remove outliers to create a new dataset\n    data_without_outliers = np.delete(data, outliers[0], axis=0)\n\n    # Plotting\n    plt.figure(figsize=(12, 6))\n\n    # Plot original data with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(range(len(column_data)), column_data, color='blue', label='Data with Outliers')\n    plt.title('Data with Outliers')\n    plt.legend()\n\n    # Plot data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(range(len(data_without_outliers[:, column])), data_without_outliers[:, column], color='red', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.legend()\n\n    plt.show()\n\n    # Return the original data, the data without outliers, and the indices of the outliers\n    return (data, data_without_outliers, outliers[0])\n\n# Example usage:\n# data = np.random.normal(size=(100, 1))  # Generating random data for demonstration\n# column = 0  # Column index to analyze for outliers\n# outlier_z_score = 2  # Z-score threshold to identify outliers\n# original_data, data_without_outliers, outlier_indices = task_func(data, column, outlier_z_score)\n```"}
{"task_id": "BigCodeBench/92", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    # Error checking\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pd.DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters < 2:\n        raise ValueError(\"`n_clusters` must be an integer greater than 1\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Scatter plot for clusters\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis', label='Data Points')\n    \n    # Overlay the cluster centroids\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=50, alpha=0.75, label='Centroids')\n    \n    # Enhance plot with legend and title\n    plt.legend()\n    plt.title('K-means Clustering')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    \n    # Show plot for interactive environments\n    plt.show()\n    \n    # Return cluster labels and the Axes object for the scatter plot\n    return cluster_labels, ax\n```"}
{"task_id": "BigCodeBench/93", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    # Ensure n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    \n    # Perform PCA on the dataset\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Convert the transformed data into a DataFrame\n    columns = [f\"PC{i+1}\" for i in range(n_components)]\n    pca_df = pd.DataFrame(transformed_data, columns=columns)\n    \n    # Generate a scatter plot for the first two principal components\n    fig, ax = plt.subplots()\n    if n_components >= 2:\n        ax.scatter(pca_df['PC1'], pca_df['PC2'])\n        ax.set_xlabel('Principal Component 1')\n        ax.set_ylabel('Principal Component 2')\n        ax.set_title('PCA - First two principal components')\n    else:\n        # Provide feedback if not enough components for a 2D scatter plot\n        print(\"Note: Only one principal component available for plotting.\")\n    \n    # Return the DataFrame and Axes object\n    return pca_df, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data for demonstration\n    np.random.seed(0)\n    data = np.random.rand(100, 5)  # 100 samples, 5 features\n    \n    # Perform PCA\n    pca_df, pca_plot = task_func(data, n_components=2)\n    \n    # Display the transformed data and the plot\n    print(pca_df.head())\n    plt.show()\n```\nThis code snippet defines a function `task_func` that takes a dataset and the number of principal components (`n_components`) as input to perform Principal Component Analysis (PCA) on the dataset. It checks if `n_components` is a positive integer and raises a `ValueError` if it is not. The transformed data is returned as a pandas DataFrame, and the function also returns a `matplotlib` Axes object containing the scatter plot of the first two principal components, if there are at least two components."}
{"task_id": "BigCodeBench/99", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Set global font to Arial for better readability\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Load the iris dataset\n    iris = load_iris()\n\n    # Convert the loaded iris dataset into a pandas DataFrame, including feature names\n    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n    # Map the species names to the numerical values in the 'target' attribute\n    species_map = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n    iris_df['species'] = [species_map[target] for target in iris.target]\n\n    # Generate a pair plot from the iris DataFrame, colored by species\n    pair_plot = sns.pairplot(iris_df, hue='species')\n\n    # Set the title of the plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot', y=1.02)\n\n    # Return the matplotlib Figure object containing the seaborn pair plot\n    return pair_plot.fig\n\n# To display the plot when running this function\nfig = task_func()\nplt.show()\n```"}
{"task_id": "BigCodeBench/100", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef task_func(seed=42):\n    # Set the seed for reproducibility if specified\n    np.random.seed(seed)\n\n    # Generate dates for the last 30 days\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=30)\n    dates = pd.date_range(start=start_date, end=end_date)\n\n    # Generate random data for each date\n    try:\n        values = np.random.randint(0, 100, size=len(dates))\n    except Exception as e:\n        raise ValueError(\"An issue occurred while generating the data.\") from e\n\n    # Create a DataFrame to hold the generated data\n    data = pd.DataFrame({'Date': dates, 'Value': values})\n\n    # Plotting\n    try:\n        fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n        ax.plot(data['Date'], data['Value'], label='Random Data')\n        ax.set_title('Random Time Series Data', fontname='Arial', fontsize=14)\n        ax.set_xlabel('Date', fontname='Arial', fontsize=12)\n        ax.set_ylabel('Value', fontname='Arial', fontsize=12)\n        ax.tick_params(axis='x', labelrotation=45, labelsize=10)  # Rotate x-axis labels for better readability\n        ax.tick_params(axis='y', labelsize=10)\n        plt.tight_layout()  # Adjust layout to not overlap items\n        ax.legend()\n    except Exception as e:\n        raise ValueError(\"An issue occurred while plotting the data.\") from e\n\n    return ax\n```"}
{"task_id": "BigCodeBench/101", "solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        # Load dataset\n        raw_data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None).dropna(axis=1, how='all')\n        raw_data.columns = range(raw_data.shape[1])\n        data = pd.DataFrame(np.hstack([raw_data.values[::2, :], raw_data.values[1::2, :2]]))\n        columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \n                   \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n        data.columns = columns\n\n        # Compute the correlation matrix\n        corr = data.corr()\n\n        # Generate a mask for the upper triangle\n        mask = np.triu(np.ones_like(corr, dtype=bool))\n\n        # Set up the matplotlib figure\n        f, ax = plt.subplots(figsize=(11, 9))\n\n        # Generate a custom diverging colormap\n        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n        # Draw the heatmap with the mask and correct aspect ratio\n        sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\n        # Check if a save path is provided to save the plot\n        if save_path:\n            plt.savefig(save_path)\n            \n        return ax\n    except Exception as e:\n        raise ValueError(\"An error occurred in generating or saving the plot\") from e\n\n# Example of calling the function without saving the plot\nax = task_func()\nplt.show()\n\n# Example of calling the function and saving the plot\n# ax = task_func(save_path='heatmap.png')\n```"}
{"task_id": "BigCodeBench/108", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' is not a pandas DataFrame.\")\n        \n    # Check if 'value' column exists\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame lacks required 'value' column.\")\n        \n    # Check if all entries in 'value' column are numeric\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"Invalid data type in 'value' column. It must be numeric.\")\n    \n    # Validate frequency string by converting it to a pandas offset alias, if possible\n    try:\n        freq_period = pd.tseries.frequencies.to_offset(freq).n\n    except Exception:\n        raise ValueError(f\"Invalid frequency string '{freq}'. Please use a valid pandas frequency string.\")\n        \n    # Validate decomposition model\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Decomposition model must be either 'additive' or 'multiplicative'.\")\n    \n    # Setting the period for the seasonal decomposition based on the frequency\n    if freq in ['D', 'M', 'Y']:  # Assuming these are the most common, but the freq to period conversion covers more cases\n        period = 1 if freq == 'D' else 30 if freq == 'M' else 365\n    else:\n        period = freq_period # For any other valid pandas frequency strings, use the converted period\n    \n    # Apply seasonal decompose\n    decomposed = seasonal_decompose(df['value'], model=decomposition_model, period=period)\n    \n    # Plotting\n    fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n    \n    ax[0].plot(decomposed.observed)\n    ax[0].set_title('Observed')\n    ax[1].plot(decomposed.trend)\n    ax[1].set_title('Trend')\n    ax[2].plot(decomposed.seasonal)\n    ax[2].set_title('Seasonality')\n    ax[3].plot(decomposed.resid)\n    ax[3].set_title('Residuals')\n    \n    plt.tight_layout()\n    \n    return decomposed, plt.gca()\n```"}
{"task_id": "BigCodeBench/120", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Validate the input dates\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances\")\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n\n    # Initialize the random number generator\n    random_seed(seed)\n\n    # Calculate the total number of days in the range\n    total_days = (end_date - start_date).days + 1  # +1 to include end_date\n\n    # Generate a series of random dates within the specified range\n    random_dates = [start_date + timedelta(days=randint(0, total_days - 1)) for _ in range(total_days)]\n\n    # Convert list of dates to pandas Series\n    dates_series = pd.Series(random_dates)\n\n    # Return the pandas Series\n    return dates_series\n\n# Example usage\nprint(task_func())\n```"}
{"task_id": "BigCodeBench/123", "solution": "```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Add '12' to my_list\n    my_list.append(12)\n    \n    # Calculate sum of the numbers in my_list\n    total_files_to_concat = sum(my_list)\n    \n    # Use glob to find all files in file_dir with the file extension file_ext\n    files = glob.glob(os.path.join(file_dir, '*' + file_ext))\n    \n    # Check if no files were found\n    if len(files) == 0:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n    \n    # If there are more files to concatenate than available, adjust the number\n    total_files_to_concat = min(total_files_to_concat, len(files))\n    \n    # Read the first N files into a DataFrame, where N is total_files_to_concat\n    df_list = [pd.read_csv(file) for file in files[:total_files_to_concat]]\n    concatenated_df = pd.concat(df_list, ignore_index=True)\n    \n    return concatenated_df\n```"}
{"task_id": "BigCodeBench/124", "solution": "```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Check if 'my_list' is of the list type\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    \n    # Check if all elements in 'my_list' are either int or float\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"All elements in 'my_list' must be numeric (int or float).\")\n    \n    # Append 12 to 'my_list'\n    my_list.append(12)\n    \n    # Calculate the sum of 'my_list', limit it by 'size'\n    total_sum = sum(my_list)\n    num_integers = min(total_sum, size)\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Generate random integers and measure the time taken\n    start_time = time.time()\n    random_integers = [randint(1, 100) for _ in range(int(num_integers))]\n    end_time = time.time()\n    \n    # Measure the time taken to generate the list\n    time_taken = end_time - start_time\n    \n    # Plot a histogram of the generated random integers\n    fig, ax = plt.subplots()\n    ax.hist(random_integers, bins=range(1, 102), edgecolor='black')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    \n    # Return the time taken and the Axes object for the histogram\n    return time_taken, ax\n```"}
{"task_id": "BigCodeBench/129", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        # Attempt to get the content of the URL\n        response = requests.get(url)\n        # Raise an HTTPError if the response status code indicates failure\n        response.raise_for_status()\n    except requests.ConnectionError as e:\n        raise ConnectionError(f\"Failed to connect to URL: {url}\") from e\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP request to URL: {url} failed with status code: {response.status_code}\") from e\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the first table in the parsed HTML\n    table = soup.find('table')\n    if not table:\n        raise ValueError(\"No table found on the webpage\")\n\n    # Attempt to extract headers (<th> elements) if present\n    headers = [th.text.strip() for th in table.find_all('th')]\n\n    # Initialize a list to hold all row data\n    data = []\n    # Extract all rows (<tr> elements) from the table\n    rows = table.find_all('tr')\n    for row in rows:\n        # For each row, find all table data (<td>) elements and strip the text\n        cols = row.find_all('td')\n        if not cols:  # If there are no <td> elements, look for <th> elements (i.e., the header row)\n            cols = row.find_all('th')\n        cols = [ele.text.strip() for ele in cols]\n        data.append(cols)\n\n    if not data:\n        raise ValueError(\"No data found in the table\")\n\n    # Create a DataFrame. If headers were found, use them as column names\n    if headers:\n        df = pd.DataFrame(data[1:], columns=headers)  # Exclude header row from data if headers were extracted\n    else:\n        df = pd.DataFrame(data)\n\n    return df\n```"}
{"task_id": "BigCodeBench/139", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    # Check for numeric columns in the DataFrame\n    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n    if len(numeric_cols) == 0:\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    axes_list = []  # Initialize a list to store Matplotlib Axes objects\n    \n    # Generate histograms for each numeric column\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        df[col].plot(kind='hist', ax=ax)  # Plot histogram\n        ax.set_title(col)  # Set the column name as the title\n        ax.set_xlabel('Value')  # Label for the x-axis\n        ax.set_ylabel('Frequency')  # Label for the y-axis\n        \n        axes_list.append(ax)  # Add the Axes object to the list\n    \n    return axes_list  # Return the list of Axes objects\n```"}
{"task_id": "BigCodeBench/147", "solution": "```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    # Initialize an empty dictionary to store the scan results\n    result = {}\n\n    # Function to scan a single IP address\n    def scan(ip):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(1)  # Set a timeout for socket operations\n            try:\n                s.connect((str(ip), port))\n                result[str(ip)] = True\n            except (socket.timeout, socket.error):\n                result[str(ip)] = False\n\n    # List to store all thread objects\n    threads = []\n\n    # Loop through each IP in the specified range and create a thread for scanning\n    for ip in IPv4Network(ip_range):\n        t = Thread(target=scan, args=(ip,))\n        threads.append(t)\n        t.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return result\n```"}
{"task_id": "BigCodeBench/161", "solution": "```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Regular expression to parse the log entries\n    log_pattern = re.compile(r'(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.*)')\n    \n    # List to store parsed log entries\n    logs_data = []\n    \n    # Open and read the log file\n    with open(log_file, 'r') as file:\n        for line in file:\n            match = log_pattern.match(line)\n            if match:\n                log_data = match.groupdict()\n                try:\n                    # Validate the timestamp format\n                    datetime.strptime(log_data['timestamp'], '%Y-%m-%d %H:%M:%S')\n                    logs_data.append(log_data)\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp format: {log_data['timestamp']}\")\n    \n    # Check for the case when no valid log entries exist\n    if not logs_data:\n        raise ValueError(\"No valid log entries found.\")\n    \n    # Convert the list of dictionaries to a pandas DataFrame\n    df_logs = pd.DataFrame(logs_data)\n    \n    # Define the output CSV file name\n    output_csv = 'structured_log_data.csv'\n    \n    # Save the DataFrame to a CSV file\n    df_logs.to_csv(output_csv, index=False)\n    \n    # Return the path to the output CSV file\n    return output_csv\n```"}
{"task_id": "BigCodeBench/162", "solution": "```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Extract words from the input text using regex, considering words as sequences of alphabetic characters\n    words = re.findall(r'\\b[A-Za-z]+\\b', text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Check if the list of word lengths is empty (no words or all words filtered out)\n    if not word_lengths:\n        # If there are no words, create an empty plot to signify no bins/data\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.text(0.5, 0.5, 'No words to display in histogram', horizontalalignment='center',\n                verticalalignment='center', transform=ax.transAxes)\n        plt.xlabel('Word Lengths')\n        plt.ylabel('Frequency')\n        plt.title('Distribution of Word Lengths in Text')\n        return ax\n    \n    # Generate the histogram of word lengths\n    plt.figure(figsize=(10, 6))  # Optional: Adjust figure size\n    n, bins, patches = plt.hist(word_lengths, bins=range(1, max(word_lengths) + 2), rwidth=rwidth, edgecolor='black')\n    \n    # Set plot labels and title for clarity\n    plt.xlabel('Word Lengths')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Word Lengths in Text')\n    plt.xticks(np.arange(1, max(word_lengths) + 1, 1))  # Ensure x-axis ticks cover all word lengths\n    \n    # Return the Axes object containing the histogram\n    return plt.gca()\n```"}
{"task_id": "BigCodeBench/177", "solution": "```python\nimport re\nimport nltk\nfrom string import punctuation\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\n# Ensure necessary nltk resources are available\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\n\ndef task_func(df):\n    # Validate input DataFrame\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or missing necessary columns.\")\n    \n    # Filter DataFrame based on case-insensitive title keywords\n    df_filtered = df[df['Title'].str.contains('like|what', case=False, regex=True)]\n    \n    # Initialize dictionary for word frequencies\n    word_frequencies = {}\n    \n    # Compile regex pattern for word extraction, ignoring punctuation\n    word_pattern = re.compile(r'\\b\\w+\\b')\n    \n    # Loop through filtered DataFrame content\n    for content in df_filtered['Content']:\n        # Normalize content to lowercase and remove punctuation\n        content_normalized = content.lower().translate(str.maketrans('', '', punctuation))\n        \n        # Tokenize the normalized content\n        words = nltk.word_tokenize(content_normalized)\n        \n        # Filter out stopwords\n        filtered_words = [word for word in words if word not in stopwords.words('english') and word.isalpha()]\n        \n        # Update word frequencies\n        for word in filtered_words:\n            if word in word_frequencies:\n                word_frequencies[word] += 1\n            else:\n                word_frequencies[word] = 1\n    \n    return word_frequencies\n```\n\nThis code snippet defines a function `task_func` that takes a pandas DataFrame `df` as input. It first checks for empty DataFrame or missing required columns ('Title' and 'Content'). It then filters the DataFrame based on titles containing \"like\" or \"what\" in a case-insensitive manner. For each article's content in the filtered DataFrame, it counts the frequency of each word, excluding stopwords and punctuation, and returns a dictionary of word frequencies. The nltk library is used for tokenization and stopwords filtering, and the `re` and `string` libraries are used for handling regular expressions and punctuation, respectively."}
{"task_id": "BigCodeBench/184", "solution": "```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Remove numbers, punctuation, and convert to lowercase\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'\\d+', '', x))  # Remove numbers\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))  # Remove punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: x.lower())  # Convert to lowercase\n    \n    # Remove stopwords\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n    \n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the data into a document-term matrix\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    # Convert to DataFrame\n    result_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return result_df\n```"}
{"task_id": "BigCodeBench/187", "solution": "```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate 'Lon' and 'Lat' keys and their tuple values in the dictionary\n    if 'Lon' not in dic or 'Lat' not in dic or not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"If 'Lon' or 'Lat' keys are missing in the dictionary, or if their values are not tuples.\")\n    \n    # Lists to hold city names and their corresponding randomly generated coordinates\n    city_list = []\n    coord_list = []\n    \n    # Generate random coordinates for each city based on the given ranges\n    for city in cities:\n        lon_range = dic['Lon']\n        lat_range = dic['Lat']\n        random_lon = np.random.uniform(lon_range[0], lon_range[1])\n        random_lat = np.random.uniform(lat_range[0], lat_range[1])\n        coord = Point(random_lon, random_lat)\n        \n        city_list.append(city)\n        coord_list.append(coord)\n    \n    # Creating the GeoDataFrame with cities and their coordinates\n    gdf = gpd.GeoDataFrame({'City': city_list, 'Coordinates': coord_list}, geometry='Coordinates')\n    \n    return gdf\n```"}
{"task_id": "BigCodeBench/199", "solution": "```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate utc_datetime is a datetime object\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime is not a datetime object\")\n    \n    # Validate the format of cities, weather_conditions, and timezones\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not isinstance(weather_conditions, list) or not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary of city-timezone pairs where both keys and values are strings\")\n    \n    # Seed the random number generator\n    set_seed(seed)\n    \n    # Initialize an empty list to store the weather report data\n    weather_report_data = []\n    \n    # Generate weather report\n    for city in cities:\n        if city not in timezones:\n            continue  # Skip city if its timezone is not defined\n        \n        # Convert UTC datetime to local city time\n        city_timezone = pytz.timezone(timezones[city])\n        local_datetime = utc_datetime.astimezone(city_timezone)\n        \n        # Format local_datetime as 'YYYY-MM-DD HH:MM:SS ZZZ'\n        formatted_local_datetime = local_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')\n        \n        # Randomly select a weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        # Append the data to the list\n        weather_report_data.append({\n            'City': city,\n            'Local Time': formatted_local_datetime,\n            'Weather Condition': weather_condition\n        })\n    \n    # Convert the list of dictionaries into a pandas DataFrame\n    weather_report_df = pd.DataFrame(weather_report_data)\n    \n    return weather_report_df\n```\n\nThis code defines a function `task_func` that generates a weather report for a given list of cities at a specified UTC datetime. It validates the parameters, seeds the random number generator, converts UTC datetime to local city times, randomly selects weather conditions, and creates a pandas DataFrame with the results."}
{"task_id": "BigCodeBench/208", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    # Validate that 'elements' is a positive integer\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"Elements must be a positive integer\")\n    \n    # Initialize random number generator with seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a sequence of random steps (-1 or 1)\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Calculate the cumulative sum to obtain the position at each step\n    positions = np.cumsum(steps)\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(positions)\n    ax.set_title(\"Random Walk\")\n    ax.set_xlabel(\"Steps\")\n    ax.set_ylabel(\"Position\")\n    \n    # Calculate descriptive statistics of the positions\n    stats = pd.Series(positions).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    # Convert statistics to dictionary\n    stats_dict = stats.to_dict()\n    # Adjust the dictionary to include exact 'count' and rename '50%' to 'median'\n    stats_dict['count'] = elements\n    stats_dict['median'] = stats_dict.pop('50%')\n    \n    return stats_dict, ax\n```"}
{"task_id": "BigCodeBench/211", "solution": "```python\nimport requests\nimport os\nimport zipfile\nimport tempfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n    \n    # Create a temporary file to store the downloaded zip file\n    temp_zip_file = tempfile.NamedTemporaryFile(delete=False)\n    \n    try:\n        # Download the zip file from the URL\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Write the content of the response to the temporary file\n        temp_zip_file.write(response.content)\n        temp_zip_file.close()  # Close to ensure all data is written before opening with zipfile\n        \n        # Extract the zip file\n        with zipfile.ZipFile(temp_zip_file.name, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n            extracted_files = zip_ref.namelist()\n        \n        return extracted_files\n    finally:\n        # Ensure the temporary file is removed after processing\n        os.remove(temp_zip_file.name)\n\n# Example usage\n# url = \"http://example.com/somefile.zip\"\n# destination_directory = \"./extracted_files\"\n# headers = {\"User-Agent\": \"MyApp\"}\n# extracted_files = task_func(url, destination_directory, headers)\n# print(extracted_files)\n```\nThis code snippet correctly implements the task of downloading a zip file from a specified URL, extracting its contents into a specified directory, and returning the list of filenames of the extracted files. It includes error handling for HTTP request errors and ensures the temporary file is deleted after use to avoid leaving residual files on the system."}
{"task_id": "BigCodeBench/214", "solution": "```python\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    # Check if range_low is not less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n    \n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a random RGB image\n    image = np.random.randint(low=range_low, high=range_high, size=image_size, dtype=np.uint8)\n    \n    # Display the generated image\n    plt.figure(figsize=(5,5))\n    ax = plt.imshow(image, interpolation='nearest')\n    plt.axis('off')  # Hide axes ticks\n    plt.show()\n    \n    return plt.gca(), image  # Return Axes object and the generated image array\n\n# Example usage\nax, image = task_func()\n```"}
{"task_id": "BigCodeBench/227", "solution": "```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\nfrom numpy import log10, sqrt, mean\nimport librosa.display\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(\"The specified audio file does not exist.\")\n\n    # Convert the list L into an MxN matrix\n    if len(L) < M*N:\n        raise ValueError(\"List L does not contain enough elements to reshape into the desired MxN matrix.\")\n    matrix = np.reshape(L[:M*N], (M, N))  # Ensure matrix is exactly MxN even if L is longer\n\n    # Read the audio data from the audio file\n    data, samplerate = sf.read(audio_file)\n\n    # Calculate the Sound Pressure Level (SPL) of the audio data\n    SPL = 20 * log10(sqrt(mean(data**2)))\n\n    # Normalize the matrix based on the SPL\n    # Assuming direct proportionality for simplicity; normalization specifics can vary\n    normalized_matrix = matrix / np.max(matrix) * SPL\n\n    # Generate a spectrogram from the normalized matrix\n    plt.figure(figsize=(10, 4))\n    # Flatten the matrix to 1D since librosa.stft expects a 1D array and we're simulating a time signal\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(normalized_matrix.flatten())), ref=np.max)\n    librosa.display.specshow(D, sr=samplerate, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n    plt.tight_layout()\n    fig = plt.gcf()  # Get the current figure object for the spectrogram\n\n    # Return the normalized MxN matrix and the figure object for the spectrogram\n    return normalized_matrix, fig\n```"}
{"task_id": "BigCodeBench/239", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    \"\"\"\n    Given a list of tuples, this function extracts numeric values, computes basic statistics,\n    and generates a histogram with an overlaid probability density function (PDF).\n\n    Parameters:\n    original (list of tuples): Input list of tuples to process.\n\n    Returns:\n    np.array: Numpy array of the extracted numeric values.\n    dict: Dictionary containing basic statistics (mean, standard deviation, minimum, maximum).\n    Axes: Matplotlib Axes object showing the histogram with overlaid PDF.\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    numeric_values = [value for tup in original for value in tup if isinstance(value, (int, float))]\n    \n    # Convert list of numeric values into numpy array\n    numeric_array = np.array(numeric_values)\n    \n    # Calculate basic statistics\n    mean = np.mean(numeric_array)\n    std = np.std(numeric_array)\n    min_val = np.min(numeric_array)\n    max_val = np.max(numeric_array)\n    stats_dict = {'mean': mean, 'std': std, 'min': min_val, 'max': max_val}\n    \n    # Plot histogram with density=True, alpha=0.6, and automatic bin selection\n    fig, ax = plt.subplots()\n    ax.hist(numeric_array, bins='auto', density=True, alpha=0.6, color='blue')\n    \n    # Overlay PDF on the histogram\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mean, std)\n    ax.plot(x, p, 'k', linewidth=2)\n    title = \"Fit results: mean = %.2f,  std = %.2f\" % (mean, std)\n    ax.set_title(title)\n    \n    # Return numpy array, dictionary of stats, and Axes object\n    return numeric_array, stats_dict, ax\n```"}
{"task_id": "BigCodeBench/241", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(original):\n    # Convert the original list to a numpy array\n    original_array = np.array(original).reshape(-1, 1)\n    \n    # Normalize the original array\n    scaler = MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array).flatten()\n    \n    # Plotting the original and normalized arrays\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(original_array, label='Original Array')\n    ax.plot(normalized_array, label='Normalized Array')\n    ax.set_title('Original and Normalized Arrays')\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Return the numpy array of the original data, the normalized array, and the Axes object.\n    return original_array.flatten(), normalized_array, ax\n```"}
{"task_id": "BigCodeBench/267", "solution": "```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Performs operations on the provided data dictionary, including adding a key,\n    generating a signal from the data, running an FFT on this signal, and plotting the FFT.\n\n    Parameters:\n    - data (dict): The input data dictionary.\n    - sample_rate (int, optional): The sample rate for the signal. Defaults to 8000.\n\n    Returns:\n    - tuple: A tuple containing the FFT of the signal as an ndarray and the Axes object of the plot.\n    \"\"\"\n    # Step 1: Add a new key \"a\" with the value 1 to the dictionary.\n    data[\"a\"] = 1\n    \n    # Step 2: Generate a signal based on the values in \"data\".\n    # Assuming the signal generation process involves creating a sum of sine waves \n    # based on the frequencies specified in the values of `data`.\n    t = np.arange(0, 1.0, 1.0 / sample_rate)\n    signal = np.sum([np.sin(2 * np.pi * freq * t) for freq in data.values()], axis=0)\n    \n    # Step 3: Run a Fast Fourier Transform (FFT) on the signal.\n    fft_signal = fftpack.fft(signal)\n    \n    # Step 4: Plot the FFT of the signal.\n    freqs = fftpack.fftfreq(signal.size, d=1./sample_rate)\n    fig, ax = plt.subplots()\n    ax.stem(freqs, np.abs(fft_signal), 'b', markerfmt=\" \", basefmt=\"-b\")\n    ax.set_xlabel('Frequency in Hertz [Hz]')\n    ax.set_ylabel('Frequency Domain (Spectrum) Magnitude')\n    ax.set_xlim([0, sample_rate / 2]) # Show only positive frequencies\n    ax.set_title('FFT of signal')\n    plt.show()\n    \n    # Return the FFT of the signal and the Axes object of the plot.\n    return (fft_signal, ax)\n\n# Example usage:\ndata = {\"freq1\": 100, \"freq2\": 200, \"freq3\": 300}\nfft_result, plot = task_func(data)\n```"}
{"task_id": "BigCodeBench/273", "solution": "```python\nimport http.server\nimport json\n\n# Define responses for success and different error scenarios\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nclass JSONRequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        # Check if the Content-Type of the request is application/json\n        if self.headers.get('Content-Type') != 'application/json':\n            self.send_error(400, \"Content-Type header is not application/json\")\n            return\n\n        # Attempt to read and decode the request body\n        try:\n            content_length = int(self.headers.get('Content-Length', 0))\n            post_data = self.rfile.read(content_length)\n            data = json.loads(post_data.decode('utf-8'))\n        except json.JSONDecodeError:\n            self.send_error(400, \"Invalid JSON\")\n            return\n\n        # Check if the 'data' key exists in the JSON object\n        if 'data' not in data:\n            self.send_error(400, \"No data key in request\")\n            return\n\n        # If all checks pass, send a success response\n        self.send_response(200)\n        self.send_header('Content-type', 'application/json')\n        response_bytes = json.dumps(SUCCESS_RESPONSE).encode('utf-8')\n        self.send_header('Content-length', str(len(response_bytes)))\n        self.end_headers()\n        self.wfile.write(response_bytes)\n\ndef task_func():\n    # This function is a placeholder to illustrate where the JSONRequestHandler class would be utilized.\n    # In a real application, you would set up an HTTP server here and pass JSONRequestHandler as the handler class.\n    # Example:\n    # server_address = ('', 8000)  # Listen on all interfaces on port 8000\n    # httpd = http.server.HTTPServer(server_address, JSONRequestHandler)\n    # httpd.serve_forever()\n    pass\n```"}
{"task_id": "BigCodeBench/274", "solution": "```python\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHTTPRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Attempt to read the content length and read the data from the request\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n\n                # Convert the post data from bytes to a string and then parse it as JSON\n                email_data = json.loads(post_data.decode('utf-8'))\n\n                # Check for the presence of all required keys\n                if not all(key in email_data for key in ('subject', 'message', 'to')):\n                    raise ValueError(\"Missing one of the required keys: 'subject', 'message', 'to'.\")\n\n                # Send email using the parsed data\n                self.send_email(smtp_server, smtp_port, smtp_username, smtp_password, email_data)\n\n                # Construct a success response\n                self.send_response(200)\n                self.send_header('Content-type', 'text/plain')\n                response_content = \"Email sent successfully.\"\n                self.send_header('Content-length', str(len(response_content)))\n                self.end_headers()\n                self.wfile.write(response_content.encode('utf-8'))\n\n            except json.JSONDecodeError:\n                self.send_error(400, \"Bad Request: Invalid JSON format.\")\n            except ValueError as e:\n                self.send_error(400, f\"Bad Request: {str(e)}\")\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, \"Authentication Failed\")\n\n        def send_email(self, smtp_server, smtp_port, smtp_username, smtp_password, email_data):\n            # Setup the email message\n            msg = MIMEText(email_data['message'])\n            msg['Subject'] = email_data['subject']\n            msg['From'] = smtp_username\n            msg['To'] = email_data['to']\n\n            # Send the email via the SMTP server\n            with smtplib.SMTP(smtp_server, smtp_port) as server:\n                server.starttls()\n                server.login(smtp_username, smtp_password)\n                server.send_message(msg)\n\n    # Return the request handler class\n    return EmailHTTPRequestHandler\n```"}
{"task_id": "BigCodeBench/287", "solution": "```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    # Dictionary to store file names and their respective word counts\n    word_counts = {}\n    \n    # Variable to keep track of the total word count\n    total_word_count = 0\n    \n    # Iterate through each file in the specified directory\n    for file in os.listdir(directory):\n        # Check if the file is a .txt file\n        if file.endswith(\".txt\"):\n            # Construct full file path\n            file_path = os.path.join(directory, file)\n            \n            # Open and read the file\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                \n                # Count the words in the file\n                words = content.split()\n                num_words = len(words)\n                \n                # Update the word counts dictionary\n                word_counts[file] = num_words\n                \n                # Update the total word count\n                total_word_count += num_words\n                \n    # Export the word counts to a JSON file\n    with open(filename, 'w', encoding='utf-8') as json_file:\n        json.dump(word_counts, json_file)\n        \n    # Return the total word count\n    return total_word_count\n```"}
{"task_id": "BigCodeBench/302", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    # Check if DataFrame is empty or 'Value' column contains invalid data\n    if df.empty or not all(isinstance(x, list) for x in df['Value']):\n        raise ValueError(\"DataFrame is empty or 'Value' does not contain valid lists.\")\n    \n    # Expanding the lists in the 'Value' column into separate columns\n    expanded_df = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    expanded_df.columns = [f'Value_{i}' for i in range(expanded_df.shape[1])]\n    \n    # Calculating Pearson correlation coefficient\n    correlation_matrix = expanded_df.corr()\n    \n    # Plotting heatmap if plot=True\n    if plot:\n        plt.figure(figsize=(10, 8))\n        ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n        plt.title(\"Correlation Heatmap\")\n        plt.show()\n        return correlation_matrix, ax\n    else:\n        return correlation_matrix\n```"}
{"task_id": "BigCodeBench/308", "solution": "```python\nimport pandas as pd\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields=[]):\n    # Extend FIELDS with any additional fields provided\n    all_fields = FIELDS + additional_fields\n    \n    # Generate random grades for each student in each subject\n    grades_data = {field: [random.randint(0, 100) for _ in STUDENTS] for field in all_fields}\n    \n    # Create a DataFrame with the generated grades data\n    df = pd.DataFrame(grades_data, index=STUDENTS)\n    \n    # Calculate the average grade for each student and add it as a new column\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject and add it as a new row\n    df.loc['Average'] = df.mean()\n    \n    return df\n\n# Example usage with no additional fields provided for simplicity in the task description\nreport = task_func()  # Utilize the function without additional subjects\nprint(report)\n```\nThis self-contained code snippet starts with the required import statements and constant definitions, then defines a function `task_func` in line with the task's specifications. The function generates a DataFrame reporting students' grades across predefined and optionally additional subjects, calculates average grades for each student and for each subject, and then displays this information in the returned DataFrame."}
{"task_id": "BigCodeBench/310", "solution": "```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Step 1: Generate simulated data for each person\n    people_data = []\n    for i in range(PEOPLE_COUNT):\n        person = {\n            'Name': f'Person {i + 1}',\n            'Age': random.randint(18, 100),  # Age between 18 and 100\n            'Height': round(random.uniform(150.0, 200.0), 2),  # Height between 150cm and 200cm\n            'Weight': round(random.uniform(50.0, 100.0), 2)  # Weight between 50kg and 100kg\n        }\n        people_data.append(person)\n    \n    # Step 2: Calculate the average age, height, and weight\n    avg_age = mean([person['Age'] for person in people_data])\n    avg_height = mean([person['Height'] for person in people_data])\n    avg_weight = mean([person['Weight'] for person in people_data])\n    \n    # Adding the averages to the people_data\n    people_data.append({\n        'Name': 'Average',\n        'Age': round(avg_age, 2),\n        'Height': round(avg_height, 2),\n        'Weight': round(avg_weight, 2)\n    })\n    \n    # Step 3: Write the data to a CSV file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=COLUMNS)\n        writer.writeheader()\n        writer.writerows(people_data)\n    \n    # Returning the path of the created CSV file\n    return os.path.abspath(filename)\n\n# Example usage:\nfilename = 'simulated_people_data.csv'\nfile_path = task_func(filename)\nprint(f'CSV file created at: {file_path}')\n```"}
{"task_id": "BigCodeBench/313", "solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(directory):\n    # Initialize a dictionary to hold the subdirectory names and the files moved into them\n    organized_files = {}\n\n    # Verify the directory exists\n    if not os.path.isdir(directory):\n        return \"Directory does not exist\", {}\n\n    # List all files in the directory\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    for file_name in files:\n        # Extract the first sequence of text not within square brackets using regex\n        match = re.search(r'(?<!\\[)\\b[a-zA-Z0-9_]+\\b(?!\\])', file_name)\n        \n        if match:\n            # The text that will be used as the subdirectory name\n            subdirectory_name = match.group(0)\n            \n            # Create the subdirectory path\n            subdirectory_path = os.path.join(directory, subdirectory_name)\n            \n            # Check if the subdirectory does not exist, and create it\n            if not os.path.exists(subdirectory_path):\n                os.makedirs(subdirectory_path)\n            \n            # Move the file to the new subdirectory\n            shutil.move(os.path.join(directory, file_name), os.path.join(subdirectory_path, file_name))\n            \n            # Update the organized_files dictionary\n            if subdirectory_name in organized_files:\n                organized_files[subdirectory_name].append(file_name)\n            else:\n                organized_files[subdirectory_name] = [file_name]\n\n    return directory, organized_files\n```"}
{"task_id": "BigCodeBench/324", "solution": "```python\nimport subprocess\nimport threading\n\ndef task_func(file_list):\n    def run_subprocess(file, exit_codes, index):\n        \"\"\"\n        Runs a file as a subprocess and stores its exit code in the shared list.\n        Args:\n        - file: The file path to execute.\n        - exit_codes: The shared list to store exit codes.\n        - index: The position in the list where the exit code should be stored.\n        \"\"\"\n        process = subprocess.Popen(file, shell=True)\n        process.wait()\n        exit_codes[index] = process.returncode\n\n    # Initialize a list to store exit codes with the size of file_list\n    exit_codes = [None] * len(file_list)\n    threads = []\n\n    # Create and start a thread for each file in the list\n    for index, file in enumerate(file_list):\n        thread = threading.Thread(target=run_subprocess, args=(file, exit_codes, index))\n        threads.append(thread)\n        thread.start()\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n\n# Example usage\nfile_list = [\"path/to/your/script1.sh\", \"path/to/your/script2.sh\"]\nprint(task_func(file_list))\n```\n\nThis code snippet defines the `task_func` function to execute a list of files as subprocesses simultaneously using threading. It captures and returns the exit codes of these subprocesses in a list. Each subprocess is run in its own thread, and the exit code is stored in a pre-allocated list at the corresponding index. This ensures that the main thread waits for all subprocesses to complete before returning the list of exit codes."}
{"task_id": "BigCodeBench/326", "solution": "```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    pattern = os.path.join(directory_path, '*.bat')\n    bat_files = glob.glob(pattern)\n    results = []\n    \n    for bat_file in bat_files:\n        try:\n            process = subprocess.run(bat_file, shell=True, capture_output=True)\n            exit_code = process.returncode\n        except Exception as e:\n            exit_code = None\n        results.append((os.path.basename(bat_file), exit_code))\n    \n    return results\n```"}
{"task_id": "BigCodeBench/341", "solution": "```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    # Check if input is a pandas DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input df must be a pandas DataFrame.\")\n    \n    # Check if DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if DataFrame contains the specified column\n    if col not in df.columns:\n        raise ValueError(f\"The DataFrame does not contain the column '{col}'.\")\n    \n    # Creating a matplotlib figure object with two subplots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    \n    # Plotting the histogram and boxplot\n    # If the data is numerical, add a KDE plot on top of the histogram\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0])\n    else:\n        sns.histplot(df[col], ax=axes[0])\n        \n    sns.boxplot(x=df[col], ax=axes[1])\n    \n    # Adjusting layout\n    plt.tight_layout()\n    \n    # Return the matplotlib figure object\n    return fig\n```"}
{"task_id": "BigCodeBench/346", "solution": "```python\nimport subprocess\nimport os\nimport sys\n\ndef task_func(script_path, wait=True, *args):\n    # Verify if the script file exists\n    if not os.path.exists(script_path):\n        raise ValueError(f\"The script {script_path} does not exist.\")\n    \n    # Prepare the command to execute the Python script along with any arguments\n    command = [sys.executable, script_path] + list(args)\n    \n    try:\n        # Run the script as a separate process\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        if wait:\n            # Wait for the process to complete and get the return code\n            stdout, stderr = process.communicate()\n            return_code = process.returncode\n            if return_code != 0:\n                # If there was an error in the subprocess, raise an exception with the error output\n                raise subprocess.CalledProcessError(return_code, command, output=stdout, stderr=stderr)\n            return return_code\n        else:\n            # Do not wait for the process to complete; return None\n            return None\n    except subprocess.CalledProcessError as e:\n        # If the script raises an exception, re-raise it\n        raise\n```"}
{"task_id": "BigCodeBench/360", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the Excel file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The Excel file does not exist at the specified path.\")\n    \n    # Attempt to load the specified sheet\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as e:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\") from e\n    \n    # Calculate mean and standard deviation for each numeric column\n    stats_dict = {}\n    for column in df.select_dtypes(include=[np.number]).columns:\n        stats_dict[column] = {\n            'mean': df[column].mean(),\n            'std': df[column].std()\n        }\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    columns = list(stats_dict.keys())\n    means = [stats_dict[column]['mean'] for column in columns]\n    stds = [stats_dict[column]['std'] for column in columns]\n    x = np.arange(len(columns))  # the label locations\n    \n    # Plotting bars for mean and standard deviation\n    ax.bar(x - 0.2, means, 0.4, label='Mean', color='blue')\n    ax.bar(x + 0.2, stds, 0.4, label='Standard Deviation', color='orange')\n    \n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns, rotation=45, ha=\"right\")\n    ax.legend()\n    \n    plt.tight_layout() # Adjust layout to not cut off labels\n    \n    # Return the dictionary and the figure\n    return stats_dict, fig\n```"}
{"task_id": "BigCodeBench/367", "solution": "```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all items are datetime instances\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"If the activities are not datetime objects.\")\n    \n    # Initialize a defaultdict to count occurrences of each day\n    day_counts = defaultdict(int)\n    \n    # Iterate through activities and count occurrences per day of the week\n    for activity in activities:\n        day_counts[activity.strftime('%A')] += 1\n    \n    # Prepare data for plotting\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    activity_counts = [day_counts[day] for day in days_of_week]\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(days_of_week, activity_counts)\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    # Display the plot\n    plt.show()\n    \n    # Return the Axes object\n    return ax\n```"}
{"task_id": "BigCodeBench/368", "solution": "```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    random.seed(seed)\n    \n    # Ensure the source directory exists\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    \n    # Ensure the destination directory exists; if not, create it\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # List all files in the source directory\n    files_in_src = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n    \n    # Ensure there's at least one file to move\n    if not files_in_src:\n        raise ValueError(\"No files found in the source directory to move.\")\n    \n    # Randomly select a file\n    selected_file = random.choice(files_in_src)\n    \n    # Full path of the selected file in the source and destination directories\n    src_file_path = os.path.join(src_dir, selected_file)\n    dest_file_path = os.path.join(dest_dir, selected_file)\n    \n    # Move the selected file\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the moved file\n    return selected_file\n```"}
{"task_id": "BigCodeBench/374", "solution": "```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Set a pattern to match double quotes and the replacement pattern\n    pattern = re.compile(r'\"')\n    replacement = r'\\\\\"'\n    \n    # Initialize a counter for the number of files processed\n    files_processed = 0\n    \n    # Create a search pattern for .xlsx files\n    search_pattern = os.path.join(directory_path, '*.xlsx')\n    \n    # Use glob to find all .xlsx files in the directory\n    for xlsx_file in glob.glob(search_pattern):\n        # Load the workbook\n        workbook = load_workbook(filename=xlsx_file)\n        \n        # Flag to indicate if the current workbook has been modified\n        modified = False\n\n        # Iterate through all the sheets in the workbook\n        for sheet in workbook:\n            for row in sheet.iter_rows():\n                for cell in row:\n                    if cell.value and isinstance(cell.value, str):\n                        # Check if the cell contains a double quote\n                        if '\"' in cell.value:\n                            # Replace double quotes with escaped version\n                            cell.value = pattern.sub(replacement, cell.value)\n                            modified = True\n\n        # If the workbook was modified, save it\n        if modified:\n            workbook.save(xlsx_file)\n            files_processed += 1\n\n    # Return the total number of Excel files processed\n    return files_processed\n```"}
{"task_id": "BigCodeBench/399", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    # Validate inputs\n    if frequency < 0:\n        raise ValueError(\"Frequency must not be negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Generate time points\n    time = np.linspace(0, 2 * np.pi, sample_size)\n    \n    # Calculate sine and cosine waves\n    sine_wave = np.sin(frequency * time)\n    cosine_wave = np.cos(frequency * time)\n    \n    # Create the plot\n    fig, ax = plt.subplots()\n    ax.plot(time, sine_wave, label='Sine Wave')\n    ax.plot(time, cosine_wave, label='Cosine Wave', linestyle='--')\n    \n    # Enhance the plot with grid, legend, and labels\n    ax.grid(True)\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves at {frequency} Hz Frequency')\n\n    return fig, ax\n```"}
{"task_id": "BigCodeBench/401", "solution": "```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create Flask application instance\n    app = Flask(app_name)\n    \n    # Set up email configurations with environment variables or default values\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', 'localhost')\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', 25))\n    app.config['MAIL_USE_TLS'] = os.getenv('MAIL_USE_TLS', 'False').lower() in ['true', '1', 't']\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', None)\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', None)\n    \n    # Initialize Flask-Mail instance\n    mail = Mail(app)\n    \n    # Prepare the mail configurations for output\n    mail_configurations = {\n        'MAIL_SERVER': app.config['MAIL_SERVER'],\n        'MAIL_PORT': app.config['MAIL_PORT'],\n        'MAIL_USE_TLS': app.config['MAIL_USE_TLS'],\n        'MAIL_USERNAME': app.config['MAIL_USERNAME'],\n        'MAIL_PASSWORD': app.config['MAIL_PASSWORD']\n    }\n    \n    # Return Flask-Mail instance and the mail configurations\n    return mail, mail_configurations\n```"}
{"task_id": "BigCodeBench/409", "solution": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the Excel file exists at the specified path\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the path {excel_file_path}.\")\n    \n    # Read the Excel file into a DataFrame\n    df = pd.read_excel(full_path)\n    \n    # Verify the specified column exists in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' is not found in the Excel file.\")\n    \n    # Calculate the mean, median, and standard deviation of the specified column\n    mean_val = df[column_name].mean()\n    median_val = df[column_name].median()\n    std_dev_val = df[column_name].std()\n    \n    # Package the results into a dictionary and return\n    results = {\n        \"mean\": mean_val,\n        \"median\": median_val,\n        \"standard deviation\": std_dev_val\n    }\n    \n    return results\n```"}
{"task_id": "BigCodeBench/417", "solution": "```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD  # Updated import for TensorFlow 2.x compatibility\n\ndef task_func(X, Y):\n    if X.shape[1] != 2:\n        raise ValueError(\"Input dimension of X must always be 2.\")\n    \n    # Splitting the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n    \n    # Constructing the Sequential model\n    model = Sequential([\n        Dense(1, input_dim=2, activation='sigmoid')\n    ])\n    \n    # Compiling the model with binary cross-entropy loss and SGD optimizer\n    model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy')\n    \n    # Fitting the model to the training data and using the test set as validation data\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=0)\n    \n    # Plotting the training and validation loss over epochs\n    plt.figure(figsize=(10, 6))  # Setting a figure size for better visibility\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Returning the trained model and the Axes object of the plot\n    return model, plt.gca()\n```"}
{"task_id": "BigCodeBench/418", "solution": "```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Create Keras Sequential model with one hidden layer using sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))\n    ])\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer with a learning rate\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    \n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n    \n    # Generate ROC curve data points\n    Y_pred = model.predict(X_test).ravel()\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n    \n    # Calculate the AUC score\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.legend(loc=\"lower right\")\n    \n    # Correct the return statement to match the given task output requirements\n    # Return model and the Axes object for the ROC curve plot\n    return model, ax\n```"}
{"task_id": "BigCodeBench/424", "solution": "```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"No file found at {image_path}\")\n    \n    # Validate n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer\")\n    \n    # Read the RGB image\n    original_image = cv2.imread(image_path)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n    \n    # Handle n_clusters == 1 case early\n    if n_clusters == 1:\n        return original_image, original_image\n\n    # Prepare image data for clustering\n    img_data = original_image.reshape((-1, 3))\n    img_data = np.float32(img_data)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(img_data)\n    \n    # Assign each pixel to the closest cluster centroid\n    center_colors = np.uint8(kmeans.cluster_centers_)\n    labels = kmeans.labels_\n    segmented_image = center_colors[labels.flatten()]\n    \n    # Reshape back to the original image shape\n    segmented_image = segmented_image.reshape(original_image.shape)\n    \n    # Save each cluster as a separate image\n    for i in range(n_clusters):\n        mask = labels.reshape(original_image.shape[:2]) == i\n        cluster_image = np.zeros_like(original_image)\n        cluster_image[mask] = original_image[mask]\n        \n        # Define the path for saving each cluster image\n        cluster_image_path = f\"cluster_{i}_{image_path}\"\n        cv2.imwrite(cluster_image_path, cv2.cvtColor(cluster_image, cv2.COLOR_RGB2BGR))\n\n    return original_image, segmented_image\n```"}
{"task_id": "BigCodeBench/443", "solution": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    # Calculate the product of P and T\n    product = np.tensordot(P, T, axes=([1], [0]))\n    \n    # Flatten the resulting product\n    flattened_product = product.flatten()\n    \n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_product.reshape(-1, 1))\n    \n    # Visualize the clustering result\n    plt.figure(figsize=(10, 6))\n    ax = plt.gca()\n    scatter = ax.scatter(np.arange(len(flattened_product)), flattened_product, c=cluster_result, cmap='viridis', alpha=0.6)\n    plt.title('KMeans Clustering Visualization')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.colorbar(scatter, label='Cluster')\n    \n    return cluster_result, ax\n```"}
{"task_id": "BigCodeBench/445", "solution": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    # Check if input points is a numpy array or can be converted into one\n    try:\n        points = np.asarray(points)\n    except:\n        raise ValueError(\"Input points could not be converted into a numpy array.\")\n    \n    # Ensure points array is 2-dimensional\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points must be a 2-dimensional array with shape (n,2).\")\n    \n    # Apply jittering to avoid issues with points being too close to each other\n    np.random.seed(seed)\n    jitter = np.random.normal(0, 0.0001, points.shape)\n    jittered_points = points + jitter\n    \n    # Compute Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    plt.show()  # Display the plot\n\n    # Return Voronoi object and Axes object for further manipulation\n    return (vor, ax)\n```"}
{"task_id": "BigCodeBench/454", "solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if both source and destination directories exist\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"The source directory {src_dir} does not exist.\")\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"The destination directory {dest_dir} does not exist.\")\n    \n    # Create a list to store the full paths of successfully moved files\n    moved_files = []\n    \n    # Construct the search pattern for the specified file extension\n    search_pattern = os.path.join(src_dir, '*' + ext)\n    \n    # Find all files in the source directory with the specified extension\n    for file_path in glob.glob(search_pattern):\n        # Extract the file name from the file path\n        file_name = os.path.basename(file_path)\n        \n        # Construct the full path for the file in the destination directory\n        dest_file_path = os.path.join(dest_dir, file_name)\n        \n        # Check if a file with the same name already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            # Move the file to the destination directory\n            shutil.move(file_path, dest_file_path)\n            # Add the destination file path to the list of moved files\n            moved_files.append(dest_file_path)\n    \n    # Return the list of moved files\n    return moved_files\n```"}
{"task_id": "BigCodeBench/458", "solution": "```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Check if the json_str is empty or not valid JSON\n    if not json_str:\n        return pd.DataFrame()\n\n    try:\n        # Parse json_str into a dictionary\n        data_dict = json.loads(json_str)\n    except json.JSONDecodeError:\n        # Return an empty DataFrame if json_str is not valid JSON\n        return pd.DataFrame()\n\n    # Function to double numerical values\n    def double_value(value):\n        # If value is a list, apply double_value recursively and return list\n        if isinstance(value, list):\n            return [double_value(v) for v in value]\n        # If value is an int or float, double it\n        elif isinstance(value, (int, float)):\n            return value * 2\n        # If value is a string that can be converted to a float, double it\n        elif isinstance(value, str):\n            if re.match(r'^-?\\d+(?:\\.\\d+)?$', value):\n                return float(value) * 2\n            else:\n                return value\n        else:\n            return value\n\n    # Apply double_value function to each item in the dictionary\n    for key in data_dict:\n        data_dict[key] = double_value(data_dict[key])\n\n    # Convert the normalized dictionary to a DataFrame\n    df = pd.DataFrame([data_dict])\n\n    # Ensure numerical values are stored as floats\n    for col in df.select_dtypes(include=['number']).columns:\n        df[col] = df[col].astype(float)\n\n    return df\n```"}
{"task_id": "BigCodeBench/461", "solution": "```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script file exists\n    if not os.path.exists(script_path):\n        print(f\"Script {script_path} does not exist.\")\n        return {}\n\n    # Execute the script in a subprocess\n    try:\n        process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except Exception as e:\n        print(f\"Failed to start script {script_path}: {e}\")\n        return {}\n\n    # Initialize variables to track CPU and memory usage\n    total_cpu_time = 0\n    total_memory = 0\n    start_time = time.time()\n\n    # Attempt to create a psutil process object for monitoring\n    try:\n        ps_process = psutil.Process(process.pid)\n    except psutil.NoSuchProcess:\n        print(f\"Process {process.pid} does not exist.\")\n        return {}\n\n    while time.time() - start_time < timeout:\n        # Check if the process has completed\n        if process.poll() is not None:\n            break\n\n        try:\n            # Collect CPU and memory usage\n            with ps_process.oneshot():\n                cpu_times = ps_process.cpu_times()\n                total_cpu_time += sum(cpu_times)  # Sum all CPU times\n                total_memory += ps_process.memory_info().rss\n        except (psutil.NoSuchProcess, psutil.ZombieProcess):\n            # Handle cases where the process is no longer accessible\n            break\n\n        time.sleep(0.1)  # Sleep briefly to avoid overwhelming the CPU\n\n    # If the timeout is reached, terminate the process\n    if time.time() - start_time >= timeout:\n        try:\n            ps_process.terminate()\n        except psutil.NoSuchProcess:\n            pass  # Process already terminated\n\n    # Calculate the total CPU usage in percentage\n    try:\n        # Get the total number of CPUs for normalization\n        cpu_count = psutil.cpu_count()\n        # Adjusting total CPU time using the number of CPUs and execution time\n        execution_time = time.time() - start_time\n        cpu_usage_percentage = (total_cpu_time / cpu_count) / execution_time * 100\n    except Exception as e:\n        print(f\"Error calculating CPU usage: {e}\")\n        cpu_usage_percentage = 0\n\n    return {\n        'CPU Usage': cpu_usage_percentage,\n        'Memory Usage': total_memory\n    }\n\n# Example usage\nresult = task_func('path/to/your/script.sh')\nprint(result)\n```"}
{"task_id": "BigCodeBench/477", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)  # Set the seed for reproducibility\n    \n    # Generate random values for \"x\" and \"y\" columns\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Ensure each category appears at least once if N >= len(CATEGORIES)\n    # or sample without replacement if N < len(CATEGORIES)\n    if N >= len(CATEGORIES):\n        # Fill the rest of the categories randomly\n        rest_categories = np.random.choice(CATEGORIES, N - len(CATEGORIES))\n        categories = np.concatenate((CATEGORIES, rest_categories))\n        np.random.shuffle(categories)\n    else:\n        categories = np.random.choice(CATEGORIES, N, replace=False)\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\"x\": x, \"y\": y, \"category\": categories})\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    colors = plt.cm.jet(np.linspace(0, 1, len(CATEGORIES)))\n    for category, color in zip(CATEGORIES, colors):\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category, color=color)\n    \n    ax.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Scatter Plot of x vs y by Category')\n    \n    # Return the DataFrame and the Axes object\n    return df, ax\n\n# Example of using the function\ndf, ax = task_func(100, [\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42)\nplt.show()\n```"}
{"task_id": "BigCodeBench/486", "solution": "```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate timestamps\n    timestamps = pd.date_range(start=datetime.fromtimestamp(start_time),\n                               end=datetime.fromtimestamp(end_time),\n                               freq=f'{step}S')\n    \n    # Generate values from a normal distribution\n    values = np.random.randn(len(timestamps))\n    \n    # Apply a linear trend to the values\n    values = values + np.arange(len(timestamps)) * trend\n    \n    # Create a DataFrame for easier plotting\n    df = pd.DataFrame({'Time': timestamps, 'Value': values})\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    df.plot(x='Time', y='Value', ax=ax, legend=False)\n    \n    # Labeling the axes\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    \n    return ax\n```"}
{"task_id": "BigCodeBench/492", "solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Ensure epoch_milliseconds is a positive integer\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds <= 0:\n        raise ValueError(\"epoch_milliseconds must be a positive integer\")\n    \n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Convert the epoch milliseconds to a datetime object for the start date\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    \n    # Get the current datetime for the end date\n    end_date = datetime.now()\n    \n    # Generate a list of dates from the start to the end date\n    date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n    \n    # Initialize an empty list to store the sales data\n    sales_data = []\n    \n    # Generate sales data for each product on each date in the range\n    for date in date_range:\n        for product in products:\n            sales_number = random.randint(10, 50)  # Random sales number between 10 and 50\n            sales_data.append({\n                \"Product\": product,\n                \"Date\": date,\n                \"Sales\": sales_number\n            })\n    \n    # Convert the list of sales data into a DataFrame\n    sales_df = pd.DataFrame(sales_data)\n    \n    # Ensure the 'Date' column is in datetime format (should already be but this is to confirm)\n    sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n    \n    # Return the DataFrame\n    return sales_df\n\n# Example usage\n# result = task_func(1609459200000)  # Epoch milliseconds for January 1, 2021\n# print(result.head())\n```\nThis script defines a function `task_func` that generates a DataFrame of sales data for a given list of products from a specified start date (in epoch milliseconds) up to the current date. Each product will have a randomly generated sales number between 10 and 50 for each day in the range. The function includes checks for the validity of the input parameters and sets the random seed to ensure reproducibility of the sales data."}
{"task_id": "BigCodeBench/501", "solution": "```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is of type string, bytes, or bytearray\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n\n    # Attempt to parse the JSON string\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:  # Corrected exception type for invalid JSON format\n        raise ValueError(\"Invalid JSON format\")\n\n    # Ensure the working directory exists or create it\n    if not os.path.exists('output'):\n        os.makedirs('output')\n\n    # Define the full path for the output file\n    output_path = os.path.join('output', filename + '.xlsx')  # Updated extension to xlsx for compatibility\n\n    # Create a Pandas DataFrame from the JSON data\n    if data:  # Checks if data is not an empty list\n        df = pd.DataFrame(data)\n    else:  # If data is an empty list, create an empty DataFrame\n        df = pd.DataFrame()\n\n    # Try to write the DataFrame to an Excel file\n    try:\n        df.to_excel(output_path, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(f\"Failed to write to Excel file. Error: {e}\")\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(output_path)\n```"}
{"task_id": "BigCodeBench/502", "solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Setting the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the activities list\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Create a list to hold the data\n    data = []\n    \n    # Calculate the start date based on the number of days in the past\n    start_date = datetime.now() - timedelta(days=days_in_past)\n    \n    # Generate random activity durations for each day and each activity\n    for day in range(days_in_past):\n        current_date = (start_date + timedelta(days=day)).strftime('%Y-%m-%d')\n        for activity in activities:\n            duration = random.randint(0, 120)  # Random duration between 0 and 120 minutes\n            data.append([current_date, activity, duration])\n    \n    # Convert the list to a DataFrame\n    df = pd.DataFrame(data, columns=['Date', 'Activity', 'Duration'])\n    \n    # Convert the 'Date' column to datetime format for plotting\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Plotting using seaborn\n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Enhancing the plot\n    plt.title('Daily Activity Durations Over the Past {} Days'.format(days_in_past))\n    plt.xlabel('Date')\n    plt.ylabel('Duration (minutes)')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.legend(title='Activity')\n    \n    # Return the Axes object for the plot and the DataFrame\n    return ax, df\n\n# Example of function usage\nax, df = task_func(7, 42)\nplt.show()\n```"}
{"task_id": "BigCodeBench/503", "solution": "```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0):\n    np.random.seed(random_seed)  # Ensure reproducibility\n    \n    # Calculate the range of dates for which to generate stock prices\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    dates = pd.date_range(start=start_date, end=end_date, closed='left')  # 'left' excludes the end_date\n    \n    # Generate random stock prices\n    prices = np.random.rand(len(dates), len(stock_names))\n    \n    # Create and return the DataFrame\n    df = pd.DataFrame(data=prices, index=dates, columns=stock_names)\n    \n    return df\n\n# Example usage of the function\ndf = task_func()\nprint(df)\n```"}
{"task_id": "BigCodeBench/509", "solution": "```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\nimport os\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    # Verify if files exist\n    if not os.path.exists(file_path1):\n        raise FileNotFoundError(f\"File not found: {file_path1}\")\n    if not os.path.exists(file_path2):\n        raise FileNotFoundError(f\"File not found: {file_path2}\")\n    \n    # Attempt to read and compare files\n    try:\n        with open(file_path1, 'r', encoding='utf-8') as f1, open(file_path2, 'r', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            \n            # Convert readers to lists of strings\n            lines1 = [','.join(row) for row in reader1]\n            lines2 = [','.join(row) for row in reader2]\n            \n            # Verify if files are not empty\n            if not lines1:\n                raise ValueError(f\"File {file_path1} is empty.\")\n            if not lines2:\n                raise ValueError(f\"File {file_path2} is empty.\")\n    \n    except Exception as e:\n        raise Exception(f\"An error occurred while reading the files: {e}\")\n    \n    # Process differences\n    diff = list(ndiff(lines1, lines2))\n    data = []\n    line_number = 0\n    for line in diff:\n        if line[0] == ' ':  # Unchanged line\n            line_number += 1\n            data.append({'Line Number': line_number, 'Status': ' ', 'Content': line[2:]})\n        elif line[0] == '-':  # Line present in first file but not in the second\n            line_number += 1\n            data.append({'Line Number': line_number, 'Status': '-', 'Content': line[2:]})\n        elif line[0] == '+':  # Line present in the second file but not in the first\n            # Do not increment line number for added lines to match the line number from the first file\n            data.append({'Line Number': line_number + 1, 'Status': '+', 'Content': line[2:]})\n    \n    # Creating pandas DataFrame\n    df = pd.DataFrame(data, columns=['Line Number', 'Status', 'Content'])\n    return df\n```"}
{"task_id": "BigCodeBench/511", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Check if data list is empty\n    if not data:\n        statistics = {'sum': 0, 'mean': np.nan, 'min': np.nan, 'max': np.nan}\n        fig, ax = plt.subplots()  # Prepare a plot for the case of empty data\n        return statistics, ax\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n\n    # Ensure the specified column exists in the DataFrame to avoid KeyError\n    if column not in df.columns:\n        raise KeyError(f\"Column '{column}' not found in data.\")\n\n    # Calculate statistics for the specified column\n    statistics = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Generate a pie chart for the Age column if it exists, else create an empty chart\n    fig, ax = plt.subplots()\n    if 'Age' in df.columns:\n        age_counts = df['Age'].value_counts()\n        ax.pie(age_counts, labels=age_counts.index, autopct='%1.1f%%')\n    else:\n        ax.text(0.5, 0.5, \"Age column not present\", ha='center', va='center')\n\n    # Return the dictionary of statistics and the pie chart Axes object\n    return statistics, ax\n```"}
{"task_id": "BigCodeBench/513", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Validate input data\n    if not data:\n        raise ValueError(\"Data list is empty.\")\n    \n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Validate numeric values are non-negative for specified columns\n    for col in ['steps', 'calories burned', 'distance walked']:\n        if col in df.columns and df[col].min() < 0:\n            raise ValueError(f\"Negative values found in {col}.\")\n    \n    # Check if specified column exists\n    if column not in df.columns:\n        raise KeyError(f\"Column {column} does not exist.\")\n    \n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Plotting\n    plt.figure()\n    plt.plot(df['Date'], df[column], label=column)\n    plt.xlabel('Date')\n    plt.ylabel(column)\n    plt.title(f'Line Chart of {column}')\n    plt.legend()\n    ax = plt.gca()  # Get the current Axes instance on the current figure\n    \n    return stats, ax\n```"}
{"task_id": "BigCodeBench/526", "solution": "```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Reading the data from the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n\n    # Initialize a defaultdict for collecting numeric values for each key\n    numeric_values = defaultdict(list)\n\n    # Iterate through each dictionary in the list, collecting numeric values for each key\n    for entry in data:\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):  # Check if value is numeric\n                numeric_values[key].append(value)\n\n    # Preparing a DataFrame from the defaultdict\n    # Calculating mean and median for each key\n    df = pd.DataFrame({\n        key: {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n        for key, values in numeric_values.items()\n    }).T  # Transpose to get keys as index and 'mean', 'median' as columns\n\n    # Sorting the DataFrame by index (key names)\n    df.sort_index(inplace=True)\n\n    return df\n```"}
{"task_id": "BigCodeBench/528", "solution": "```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File format is not supported. Please provide a .csv file\")\n    \n    # Initialize a list to store rows from the CSV file\n    rows = []\n    \n    # Read the CSV file and append rows to the list\n    with open(file_path, mode='r') as file:\n        csv_reader = csv.reader(file)\n        header = next(csv_reader)  # Skip the header\n        for row in csv_reader:\n            rows.append(tuple(row))  # Convert row to tuple to make it hashable\n\n    # Count occurrences of each row using Counter, filtering out unique rows\n    row_counts = Counter(rows)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates dictionary to pandas DataFrame for plotting\n    duplicates_df = pd.DataFrame(list(duplicates.items()), columns=['Row', 'Count'])\n\n    # Adjusting the plot for better readability, especially if rows are long\n    fig, ax = plt.subplots(figsize=(10, 8))\n    duplicates_df.plot(kind='barh', x='Row', y='Count', legend=False, ax=ax)\n    ax.set_xlabel('Counts')\n    ax.set_ylabel('Row')\n    ax.set_title('Duplicate Rows in CSV File')\n    plt.tight_layout()\n\n    # Return the dictionary of duplicates and the Axes object of the plot\n    return duplicates, ax\n```"}
{"task_id": "BigCodeBench/530", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Ensure that age values are integers; if floats, round them down\n    df['age'] = df['age'].apply(lambda x: np.floor(x) if isinstance(x, float) else x).astype(int)\n    \n    # Check if any age is negative\n    if any(df['age'] < 0):\n        raise ValueError(\"Age must not be negative\")\n    \n    # Find duplicate names\n    duplicate_names = df[df.duplicated('name', keep=False)]\n    \n    # Check if there are no duplicate names\n    if duplicate_names.empty:\n        return Counter(), None\n    \n    # Record the age distribution for duplicate names\n    age_distribution = Counter(duplicate_names['age'])\n    \n    # Plotting\n    # Determine the bins for the histogram based on min and max age, adjusted by 0.5\n    min_age, max_age = int(duplicate_names['age'].min()), int(duplicate_names['age'].max())\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1) # To ensure that integer ages align within bins\n    \n    # Plot the histogram\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=duplicate_names, x='age', bins=bins, kde=False)\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution Among Duplicate Names')\n    plt.grid(True)\n    \n    return age_distribution, plt.gca()\n```"}
{"task_id": "BigCodeBench/532", "solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(df, bins=4):\n    # Step 1: Identify and count duplicate values in the 'value' column\n    value_counts = Counter(df['value'])\n    \n    # Step 2: Plot a histogram for all values in 'value' column\n    n, bins, patches = plt.hist(df['value'], bins=bins, color='green', alpha=0.6, density=True, label='Histogram')\n    \n    # Step 3: Overlay a normal distribution curve\n    mu, std = norm.fit(df['value'])  # Fit a normal distribution to the data: get mean and standard deviation\n    xmin, xmax = plt.xlim()  # Get the range of x values\n    x = np.linspace(xmin, xmax, 100)  # Generate a smooth line plot\n    p = norm.pdf(x, mu, std)  # Calculate the PDF on the range of x\n    plt.plot(x, p, 'k', linewidth=2, label='Normal distribution')\n    \n    # Step 4: Title and labels for the plot\n    plt.title('Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n    \n    # Note: plt.show() is commented out to not display the plot during function calls in scripts or notebooks\n    # plt.show()\n    \n    # Return the tuple (Counter object, Axes object)\n    return value_counts, plt.gca()\n```"}
{"task_id": "BigCodeBench/553", "solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    # Initialize the DataFrame with indices from list 'a'\n    # and columns based on the length of list 'b', using the COLUMNS list.\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=COLUMNS[:len(b)])\n    \n    # Plot the DataFrame as a bar chart.\n    ax = df.plot(kind='bar')\n    \n    # Return the Axes object for the plot.\n    return ax\n\n# Example usage:\n# a = ['Row1', 'Row2', 'Row3']\n# b = [1, 2, 3]\n# ax = task_func(a, b)\n# plt.show()  # Display the plot in environments that require plt.show() to visualize the plot.\n```"}
{"task_id": "BigCodeBench/560", "solution": "```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    # Convert data into a DataFrame\n    df = pd.DataFrame(data, columns=['Date', 'Value'])\n    \n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Extract year from the first date (assuming all dates are within the same year)\n    year = df['Date'].dt.year.iloc[0]\n    \n    # Extract month and group by it, calculating mean value for each month (if needed)\n    df['Month'] = df['Date'].dt.month\n    monthly_data = df.groupby('Month')['Value'].mean().reset_index()\n    \n    # Create a bar chart\n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(monthly_data['Month'], monthly_data['Value'], color='skyblue')\n    \n    # Set the title and labels\n    plt.title(f'Monthly Data for {year}')\n    plt.xlabel('Month')\n    plt.ylabel('Value')\n    \n    # Customize the x-axis to show month names\n    plt.xticks(monthly_data['Month'], [datetime(1900, int(month), 1).strftime('%B') for month in monthly_data['Month']])\n    \n    # Rotate the x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Return the Axes object\n    return plt.gca()\n```"}
{"task_id": "BigCodeBench/567", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input data string to a numeric format\n    numeric_data = pd.to_numeric(pd.Series(data.split(',')))\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(numeric_data.min(), numeric_data.max() + 2) - 0.5\n    \n    # Plot the histogram\n    fig, ax = plt.subplots()\n    ax.hist(numeric_data, bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Return the Axes object\n    return ax\n```"}
{"task_id": "BigCodeBench/574", "solution": "```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x values\n    x = np.linspace(0, 4 * np.pi, array_length)\n    # Generate noisy sine wave\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n\n    # Define the model function for curve fitting\n    def model_func(x, a, b, c):\n        return a * np.sin(b * x + c)\n\n    # Perform curve fitting\n    params, params_covariance = curve_fit(model_func, x, y, p0=[1, 1, 0])\n\n    # Plot the original noisy data\n    plt.scatter(x, y, label='Data')\n    # Plot the fitted curve\n    plt.plot(x, model_func(x, *params), label='Fitted function', color='red')\n\n    # Labeling the plot\n    plt.xlabel('X axis')\n    plt.ylabel('Y axis')\n    plt.legend()\n    \n    # Return Axes object\n    return plt.gca()\n\n# Example usage\nax = task_func(150, 0.1)\nplt.show()  # Displays the plot with the noisy sine wave and its adjusted curve.\n```"}
{"task_id": "BigCodeBench/579", "solution": "```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(csv_file):\n    # Check if the file exists to handle FileNotFoundError\n    if not os.path.exists(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist\")\n    \n    try:\n        words = []\n        # Normalize text and count words\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                for cell in row:\n                    normalized_text = unicodedata.normalize('NFKD', cell).encode('ascii', 'ignore').decode('ascii')\n                    words.extend(normalized_text.lower().split())\n\n        word_counts = Counter(words)\n        common_words = word_counts.most_common(10)\n\n        # Plotting\n        words, frequencies = zip(*common_words)\n        fig, ax = plt.subplots()\n        ax.bar(words, frequencies)\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequencies')\n        ax.set_title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        \n        plt.show() # Display the plot\n        \n        return (ax, common_words)\n    except IOError:\n        raise IOError(\"An error occurred reading the file.\")\n```"}
{"task_id": "BigCodeBench/582", "solution": "```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.randn(size)\n    \n    # Calculate the kernel density estimate (KDE) for the data, which serves as an estimate of the PDF\n    density = stats.gaussian_kde(data)\n    \n    # Create a range of values for which we want to estimate the PDF\n    xs = np.linspace(min(data), max(data), 300)\n    \n    # Evaluate the density function for each xs\n    pdf = density(xs)\n    \n    # Create a figure object\n    fig, ax = plt.subplots()\n    \n    # Plot histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.5, color='r', label='Histogram')\n    \n    # Plot the PDF on the same axis\n    ax.plot(xs, pdf, color='k', label='PDF')\n    \n    # Add legend to the plot\n    ax.legend(loc='upper left')\n    \n    # Add titles and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    return fig\n\n# Example usage:\n# fig = task_func()\n# plt.show()\n```"}
{"task_id": "BigCodeBench/583", "solution": "```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Step 1: Generate RSA public and private key pair\n    public_key, private_key = rsa.newkeys(2048)\n    \n    # Step 2: Encrypt the private key using AES\n    password = get_random_bytes(16)  # AES key must be either 16, 24, or 32 bytes long\n    cipher = AES.new(password, AES.MODE_GCM)\n    nonce = cipher.nonce\n    encrypted_private_key = cipher.encrypt(private_key.save_pkcs1(format='PEM'))\n    \n    # Step 3: Save the encrypted private key into a file\n    filename_number = os.urandom(8).hex()  # Generate 8 random bytes and convert to hex\n    filename = f\"private_key_{filename_number}.txt\"\n    with open(filename, 'wb') as file:\n        file.write(encrypted_private_key)\n    \n    # Step 4: Return the necessary information\n    return public_key, filename, password, nonce\n\n# Example usage\npublic_key, filename, password, nonce = task_func()\nprint(f\"Public Key: {public_key}\")\nprint(f\"Filename: {filename}\")\nprint(f\"Password (base64 encoded for readability): {b64encode(password).decode()}\")\nprint(f\"Nonce (base64 encoded for readability): {b64encode(nonce).decode()}\")\n```"}
{"task_id": "BigCodeBench/587", "solution": "```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\n\ndef task_func(file_path):\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Read the content of the file\n    with open(file_path, 'rb') as file_to_encrypt:\n        file_data = file_to_encrypt.read()\n\n    # AES encryption setup\n    aes_key = os.urandom(32)  # Generate a 256-bit random AES key\n    iv = os.urandom(16)  # Generate a random 128-bit IV (initialization vector)\n    cipher = Cipher(algorithms.AES(aes_key), modes.CFB(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Padding the file content to be AES block size compliant\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n\n    # Encrypt the file data\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt the AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Save the encrypted file\n    encrypted_file_name = file_path + '.enc'\n    with open(encrypted_file_name, 'wb') as encrypted_file:\n        encrypted_file.write(iv + encrypted_data)  # Prepend the IV to the encrypted data\n\n    # Save the encrypted AES key\n    aes_key_file_name = file_path + '_aes_key.enc'\n    with open(aes_key_file_name, 'wb') as aes_key_file:\n        aes_key_file.write(encrypted_aes_key)\n\n    # Return the RSA public key, encrypted file name, and encrypted AES key file name\n    return pubkey, encrypted_file_name, aes_key_file_name\n```"}
{"task_id": "BigCodeBench/590", "solution": "```python\nimport urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    # Check if the URL is empty or invalid\n    if not url:\n        raise ValueError(\"The provided URL is invalid or empty.\")\n    \n    try:\n        # Fetch the HTML content of the given URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except urllib.error.URLError as e:\n        # Handle issues with network connectivity or the server\n        raise urllib.error.URLError(f\"Failed to fetch the HTML content from the URL due to: {e.reason}\")\n    \n    # Parse the HTML content for anchor tags\n    document = pq(html_content)\n    anchor_tags = document(\"a\")\n    \n    # Record data extraction time\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    # Organize data into a list of dictionaries\n    data = [{\n        'text': tag.text_content(),\n        'href': tag.attrib.get('href', ''),\n        'fetch_time': fetch_time\n    } for tag in anchor_tags]\n    \n    # Convert the list of dictionaries into a pandas DataFrame\n    df = pd.DataFrame(data, columns=['text', 'href', 'fetch_time'])\n    \n    # Return the DataFrame\n    return df\n\n# Example usage (Replace 'http://example.com' with a valid URL)\n# print(task_func('http://example.com'))\n```\nIn this improved version, the extraction of text from anchor tags is done using `tag.text_content()` instead of `tag.text` to ensure that all text within an anchor tag, including nested elements, is captured accurately. This addresses any potential issues with the original approach, which might have failed to retrieve text from more complex anchor tag structures."}
{"task_id": "BigCodeBench/592", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define the CSV file path\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n    csv_file_path = os.path.join(output_dir, f\"sensor_data_{timestamp}.csv\")\n    \n    # Generate sensor data and write to CSV\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        \n        # Current time as the starting point\n        current_time = datetime.now()\n        \n        # Generate and write data for each hour\n        for hour in range(hours):\n            # Calculate timestamp for each hour\n            time_stamp = (current_time + timedelta(hours=hour)).strftime('%Y-%m-%d %H:%M:%S')\n            # Generate random sensor values\n            temperature = randint(-20, 40)  # Temperature in degrees Celsius\n            humidity = randint(0, 100)  # Humidity in percentage\n            pressure = randint(960, 1060)  # Pressure in hPa\n            # Write the data row to the CSV file\n            writer.writerow([time_stamp, temperature, humidity, pressure])\n    \n    print(f\"Sensor data generated and saved to {csv_file_path}\")\n\n# Example call to the function\n# task_func(24)\n```"}
{"task_id": "BigCodeBench/593", "solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Initialize data structure to hold traffic data\n    data = {'Time': [datetime.now() + timedelta(hours=i) for i in range(hours)]}\n    for vehicle in VEHICLE_TYPES:\n        data[vehicle] = [randint(0, 100) for _ in range(hours)]\n    \n    # Convert the data into a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Define the path for the CSV file including a timestamp to avoid overwriting\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    filename = f\"traffic_data_{timestamp}.csv\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(filepath, index=False)\n    \n    # Plot the traffic data\n    plt.figure(figsize=(10, 6))\n    for vehicle in VEHICLE_TYPES:\n        plt.plot(df['Time'], df[vehicle], label=vehicle)\n    plt.xlabel('Time')\n    plt.ylabel('Vehicle Count')\n    plt.title('Traffic Data Over Time')\n    plt.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the path to the CSV file and the axes object of the plot\n    return filepath, plt.gca()\n```"}
{"task_id": "BigCodeBench/594", "solution": "```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Ensure the output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create a unique filename with current date and time\n    filename = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_weather_data.csv\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Generate and save weather data to CSV\n    with open(filepath, 'w', newline='') as csvfile:\n        fieldnames = ['Time', 'Condition']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for hour in range(hours):\n            time = datetime.now() + timedelta(hours=hour)\n            condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n            writer.writerow({'Time': time.strftime(\"%Y-%m-%d %H:%M:%S\"), 'Condition': condition})\n    \n    # Backup the file\n    backup_dir = os.path.join(output_dir, \"backup\")\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    backup_path = os.path.join(backup_dir, filename)\n    shutil.copy(filepath, backup_path)\n    \n    return filepath\n\n# Example usage\n# print(task_func(24))\n```"}
{"task_id": "BigCodeBench/618", "solution": "```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    # Generating random goals and penalties for each team\n    match_results = {\n        'Team': TEAMS,\n        'Goals': [randint(0, goals) for _ in TEAMS],\n        'Penalties': [randint(0, penalties) for _ in TEAMS]\n    }\n    \n    # Calculate Penalty Cost\n    match_results['Penalty Cost'] = [x * PENALTY_COST for x in match_results['Penalties']]\n    \n    # Create DataFrame\n    df = pd.DataFrame(match_results)\n    \n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Goals plot\n    goals_plot = sns.barplot(data=df, x='Team', y='Goals', ax=axes[0])\n    axes[0].set_title('Goals by Team')\n    \n    # Penalty Cost plot\n    penalties_plot = sns.barplot(data=df, x='Team', y='Penalty Cost', ax=axes[1])\n    axes[1].set_title('Penalty Costs by Team')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return df, [goals_plot, penalties_plot]\n\n# Example usage\ndf, plots = task_func(5, 3)\nprint(df)\nplt.show()\n```"}
{"task_id": "BigCodeBench/636", "solution": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    # Generate a DataFrame with random integers between 0 and 9\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = df.astype(bool).sum(axis=0)\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_title('Non-Zero Values Count in Each Column')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-Zero Values Count')\n    plt.show()\n    \n    # Return the DataFrame and the Axes object containing the bar plot\n    return df, ax\n```"}
{"task_id": "BigCodeBench/637", "solution": "```python\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    # Setting a seed for reproducibility\n    np.random.seed(0)\n    \n    # Define course names\n    courses = ['Math', 'Science', 'English', 'History', 'Art']\n    \n    # Generate student IDs\n    student_ids = [f'Student_{i+1}' for i in range(num_students)]\n    \n    # Generate random grades for each course\n    grades_data = {course: np.random.randint(0, 101, size=num_students) for course in courses}\n    grades_data['StudentID'] = student_ids\n    \n    # Create DataFrame\n    grades_df = pd.DataFrame(grades_data)\n    \n    # Calculate average grade for each course\n    avg_grades = grades_df.drop(columns='StudentID').mean()\n    \n    # Calculate number of students with passing grades for each course\n    passing_counts = (grades_df.drop(columns='StudentID') >= 60).sum()\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    index = np.arange(len(courses))\n    bar_width = 0.35\n    opacity = 0.8\n    \n    # Average Grades Bar\n    ax.bar(index, avg_grades, bar_width, alpha=opacity, color='b', label='Average Grade')\n    \n    # Passing Students Bar\n    ax.bar(index + bar_width, passing_counts, bar_width, alpha=opacity, color='g', label='Passing Students')\n    \n    ax.set_xlabel('Course')\n    ax.set_ylabel('Scores')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xticks(index + bar_width / 2)\n    ax.set_xticklabels(courses)\n    ax.legend()\n    \n    plt.tight_layout()\n    \n    return grades_df, ax\n```"}
{"task_id": "BigCodeBench/654", "solution": "```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    # Filter rows where the first column matches the target value\n    filtered_data = array[array[:, 0] == target_value]\n\n    # Check if filtered data is not empty\n    if filtered_data.size == 0:\n        raise ValueError(\"No matching data found for the target value.\")\n\n    # Define the indices (x) and values (y) for fitting, focusing on the second column for values\n    x = np.arange(len(filtered_data))\n    y = filtered_data[:, 1]  # Assuming we're fitting to the second column\n\n    # Define the exponential decay function to fit\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Fit the exponential decay function to the data\n    popt, pcov = optimize.curve_fit(exp_decay, x, y)\n\n    # Plot the original data and the fitted curve\n    plt.figure(figsize=(10, 6))\n    plt.scatter(x, y, label='Data', color='blue')\n    plt.plot(x, exp_decay(x, *popt), label='Fitted Curve', color='red')\n\n    # Labeling the plot\n    plt.title('Exponential Decay Fit')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.legend()\n\n    # Return the optimized parameters (popt) and the Axes object for further manipulation if needed\n    return popt, plt.gca()\n```"}
{"task_id": "BigCodeBench/655", "solution": "```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    Performs topic extraction from a collection of text documents using\n    Non-Negative Matrix Factorization (NMF).\n\n    Parameters:\n    - texts: list of str, the text documents from which to extract topics.\n    - num_topics: int, the number of topics to extract.\n\n    Returns:\n    - list of list of str: A list where each element is a list of words\n      representing a topic.\n    \"\"\"\n\n    def preprocess(text):\n        \"\"\"\n        Preprocesses the input text by removing non-alphanumeric characters\n        (excluding spaces), converting all characters to lowercase, and removing\n        stopwords.\n\n        Parameters:\n        - text: str, the text to preprocess.\n\n        Returns:\n        - str: The preprocessed text.\n        \"\"\"\n        text = ALPHANUMERIC.sub(' ', text)  # Remove non-alphanumeric characters\n        text = text.lower()  # Convert to lowercase\n        return ' '.join([word for word in text.split() if word not in STOPWORDS])  # Remove stopwords\n\n    processed_texts = [preprocess(text) for text in texts]\n\n    # Vectorize texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, init='random', random_state=0)\n    nmf.fit(tfidf)\n\n    # Extract topics\n    topics = []\n    for idx, topic in enumerate(nmf.components_):\n        top_words_idx = topic.argsort()[-10:][::-1]  # Get indices of the top words in this topic\n        top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n        topics.append(top_words)\n\n    return topics\n```"}
{"task_id": "BigCodeBench/657", "solution": "```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\n\n# Ensure necessary NLTK data is downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    # If stopwords are not provided, use the NLTK corpus\n    if stopwords is None:\n        stopwords = set(nltk.corpus.stopwords.words('english'))\n    \n    cleaned_texts = []\n    for text in texts:\n        # Remove all non-alphanumeric characters except spaces\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Split into words and remove stopwords\n        words = [word for word in text.split() if word not in stopwords]\n        cleaned_texts.append(words)\n    \n    # Train Word2Vec model\n    model = Word2Vec(sentences=cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model\n```"}
{"task_id": "BigCodeBench/678", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Check if the path exists\n    if not os.path.exists(path):\n        print(f\"Path {path} does not exist.\")\n        return pd.DataFrame()  # Returning an empty DataFrame if path doesn't exist\n    \n    # List all JSON files in the directory, sorted alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.endswith('.json')])\n    \n    # Initialize an empty DataFrame\n    combined_df = pd.DataFrame()\n    \n    # Process each JSON file\n    for json_file in json_files:\n        # Construct the full file path\n        file_path = os.path.join(path, json_file)\n        \n        # Read the JSON file into a DataFrame\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        temp_df = pd.json_normalize(data)  # Improved to handle nested JSON structures\n        \n        # Add a 'Source' column to the DataFrame\n        temp_df['Source'] = json_file\n        \n        # Append the DataFrame to the combined DataFrame\n        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n    \n    # Create a 'processed' subdirectory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # Move the processed files to the 'processed' directory\n    for json_file in json_files:\n        original_file_path = os.path.join(path, json_file)\n        processed_file_path = os.path.join(processed_path, json_file)\n        shutil.move(original_file_path, processed_file_path)\n    \n    # Return the combined DataFrame\n    return combined_df\n```"}
{"task_id": "BigCodeBench/720", "solution": "```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    dir_name = \"task_func_data\"\n    file_name = \"Output.txt\"\n    file_path = os.path.join(dir_name, file_name)\n\n    # Ensure the directory exists\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n\n    # Generate random sensor data\n    temperature = round(random.uniform(20.0, 30.0), 2)  # Temperature range between 20 and 30\n    humidity = round(random.uniform(40.0, 60.0), 2)  # Humidity range between 40 and 60\n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # Current timestamp\n\n    # Write the data to the CSV file in append mode\n    with open(file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([timestamp, temperature, humidity])\n\n    # Return the path to the CSV file before deleting it\n    print(file_path)  # Print the path for confirmation\n\n    # Delete the CSV file\n    os.remove(file_path)\n    \n    return file_path\n\n# Call the function\ntask_func()\n```"}
{"task_id": "BigCodeBench/723", "solution": "```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Open the URL and read the content\n    response = urllib.request.urlopen(url)\n    webContent = response.read()\n    \n    # Parse the content using BeautifulSoup\n    soup = BeautifulSoup(webContent, 'html.parser')\n    \n    # Define the data structure to hold scraped data\n    scraped_data = []\n    \n    # Example: Scraping for specific data like articles, products, etc.\n    # This is where you'll adjust the scraping logic based on the structure of your target webpage\n    # For demonstration, let's assume we are scraping articles, where each article is in a 'div' with class 'article'\n    articles = soup.find_all('div', class_='article')\n    \n    for article in articles:\n        # Assuming each article has a title in an 'h2' tag and a description in a 'p' tag\n        title = article.find('h2').text.strip() if article.find('h2') else 'No Title'\n        description = article.find('p').text.strip() if article.find('p') else 'No Description'\n        scraped_data.append([title, description])\n    \n    # Write the scraped data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Title', 'Description'])  # Writing header row\n        writer.writerows(scraped_data)\n    \n    # Return the full path of the saved CSV file\n    return os.path.abspath(CSV_FILE_PATH)\n\n# Example usage:\n# Replace 'your_url_here' with the actual URL you intend to scrape\n# csv_file_path = task_func('your_url_here')\n# print(f'Data saved to {csv_file_path}')\n```"}
{"task_id": "BigCodeBench/752", "solution": "```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validation checks\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data is not a DataFrame\")\n    if data.empty:\n        raise ValueError(\"data is empty\")\n    if target_column not in data.columns:\n        raise ValueError(\"target_column is not a column of data\")\n    if not all(isinstance(val, (int, float, np.number)) for val in data.drop(target_column, axis=1).values.ravel()):\n        raise ValueError(\"data contains values that are not numeric\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state is not an integer\")\n    if not isinstance(test_size, float) or not 0 < test_size < 1:\n        raise ValueError(\"test_size is not between 0 and 1\")\n    \n    # Splitting the dataset\n    X = data.drop(columns=target_column)\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    # Model training\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Model evaluation\n    score = model.score(X_test, y_test)\n    \n    return score\n\n# Example usage\nif __name__ == \"__main__\":\n    rng = np.random.default_rng(seed=42)  # Initialize random number generator for reproducibility\n    data = pd.DataFrame({\n        'x1': rng.random(500),\n    })\n    data['y'] = data['x1'] * 2 + 1  # Creating a linear relationship for demonstration\n\n    result = task_func(data, 'y', random_state=9, test_size=0.1)\n    print(result)\n```\nThis script defines the `task_func` function according to the specifications, including all the necessary validations. It then demonstrates how to use the function with a simple generated dataset, where the relationship between `x1` and `y` is linear, hence expecting a model score close to 1.0, indicating a perfect prediction."}
{"task_id": "BigCodeBench/760", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    # Set the seed for reproducibility\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Combine the names lists and initialize the DataFrame\n    all_names = latin_names + other_names\n    df = pd.DataFrame({\n        'ID': range(1, 101),\n        'Name': np.random.choice(all_names, 100),\n        'Date of Birth': [datetime.date(np.random.randint(start_year, end_year+1),\n                                        np.random.randint(1, 13),\n                                        np.random.randint(1, 29)) for _ in range(100)]\n    })\n\n    # Function to correct improperly encoded Latin characters and create email\n    def correct_and_create_email(name, dob, domain):\n        # Correct improperly encoded Latin characters\n        name_corrected = codecs.encode(name, 'latin-1').decode('utf-8')\n        # Replace spaces with underscores and convert to lowercase for the email\n        name_for_email = re.sub(r'\\s+', '_', name_corrected).lower()\n        # Construct the email\n        email = f\"{name_for_email}{dob.year}@{domain}\"\n        return email\n\n    # Apply the function to each row to create the 'Email' column\n    df['Email'] = df.apply(lambda row: correct_and_create_email(row['Name'], row['Date of Birth'], email_domain), axis=1)\n\n    # Ensure 'Date of Birth' is formatted as a date\n    df['Date of Birth'] = pd.to_datetime(df['Date of Birth']).dt.date\n    \n    return df\n```"}
{"task_id": "BigCodeBench/763", "solution": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\ndef task_func(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize a defaultdict(list) to hold all values of each key for calculation\n    values_by_key = defaultdict(list)\n\n    # Collect all values for each key\n    for item in data:\n        for key, value in item.items():\n            if isinstance(value, (int, float)):  # Ensure the value is numeric\n                values_by_key[key].append(value)\n\n    # Initialize a dictionary to hold the means and medians\n    results = {}\n\n    # Calculate mean and median for each key and store in results\n    for key, values in values_by_key.items():\n        results[key] = {\n            'mean': np.mean(values),\n            'median': np.median(values)\n        }\n\n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, stats in results.items():\n            writer.writerow([key, stats['mean'], stats['median']])\n\n    # Return the dictionary of means and medians\n    return results\n\n# Example of function usage:\n# Assuming 'input.json' is your input file and 'output.csv' is your desired output file.\n# results = task_func('input.json', 'output.csv')\n# print(results)\n```\n\nThis updated code performs the requested functionality, including the correct return of a dictionary where each key is a field from the input JSON and each value is another dictionary with the mean and median of that field. The code reads from a JSON file, computes the mean and median for numerical fields, writes these statistics to a CSV file, and returns the computed statistics as a dictionary."}
{"task_id": "BigCodeBench/765", "solution": "```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    # Create the target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n    \n    # List to store full paths of copied files\n    copied_files = []\n    \n    # Iterate over each file in the kwargs dictionary\n    for file_name, file_path in kwargs.items():\n        # Create a Path object for the file\n        file_path_obj = Path(file_path)\n        \n        # Check if the file exists and is not empty\n        if file_path_obj.exists() and file_path_obj.stat().st_size > 0:\n            # Construct the full path for the destination file\n            dest_file_path = Path(target_dir) / file_name\n            \n            # Copy the file to the target directory\n            shutil.copy(file_path, dest_file_path)\n            \n            # Add the full path of the copied file to the list\n            copied_files.append(str(dest_file_path))\n    \n    # Return the list of copied files\n    return copied_files\n```"}
{"task_id": "BigCodeBench/771", "solution": "```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    pattern_compiled = re.compile(pattern)\n    \n    for file in Path(directory).glob('*.csv'):\n        match = pattern_compiled.match(file.name)\n        if match:\n            new_filename = f\"{match.group(1)}_modified.csv\"\n            new_filepath = file.parent / new_filename\n            \n            with file.open('r', newline='') as infile, new_filepath.open('w', newline='') as outfile:\n                reader = csv.reader(infile)\n                writer = csv.writer(outfile)\n                \n                for row in reader:\n                    writer.writerow(row)\n                    \n            new_files.append(new_filename)\n    \n    return new_files\n```"}
{"task_id": "BigCodeBench/777", "solution": "```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    # Initialize an empty list to keep track of directories where files are extracted\n    extracted_dirs = []\n    \n    # Loop through each file in the specified directory\n    for filename in os.listdir(directory):\n        # Check if the filename matches the given pattern\n        if re.match(pattern, filename):\n            # Split the filename at the last occurrence of \"-\" to separate it into a prefix and a suffix\n            prefix = filename.rsplit('-', 1)[0]\n            # Construct the path for the directory where the contents are to be extracted\n            extract_path = os.path.join(directory, prefix)\n            \n            # Check if the directory exists; if not, create it\n            if not os.path.exists(extract_path):\n                os.makedirs(extract_path)\n                \n            # Construct the full path to the zip file\n            zip_path = os.path.join(directory, filename)\n            # Open the zip file and extract its contents\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(extract_path)\n                \n            # Add the directory name to the list of directories where files were extracted\n            if extract_path not in extracted_dirs:\n                extracted_dirs.append(extract_path)\n                \n    # Return the list of directories where files have been extracted\n    return extracted_dirs\n```"}
{"task_id": "BigCodeBench/785", "solution": "```python\nimport subprocess\nimport os\nimport glob\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Ensure the ARCHIVE_DIR exists\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern in the current directory\n    files_to_archive = glob.glob(pattern)\n    \n    # Generate a unique archive file name to prevent overwriting\n    archive_file_name = f\"archive_{int(os.path.getmtime(max(files_to_archive, key=os.path.getmtime)))}.tar.gz\" if files_to_archive else \"archive.tar.gz\"\n    archive_file_path = os.path.join(ARCHIVE_DIR, archive_file_name)\n    \n    # Create the archive with the matched files\n    if files_to_archive:  # Only proceed if there are files to archive\n        with open(archive_file_path, \"wb\") as archive_file:\n            subprocess.run([\"tar\", \"-czf\", \"-\", *files_to_archive], stdout=archive_file)\n\n        # Delete the original files\n        for file_path in files_to_archive:\n            os.remove(file_path)\n    \n    return archive_file_path\n```"}
{"task_id": "BigCodeBench/800", "solution": "```python\nimport csv\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    counts = Counter({'goals': 0, 'penalties': 0})\n    try:\n        with open(csv_file_path, 'r') as file:\n            reader = csv.DictReader(file)\n            for row in reader:\n                counts['goals'] += int(row['goals'])\n                counts['penalties'] += int(row['penalties'])\n    except FileNotFoundError:\n        print(f\"File {csv_file_path} not found.\")\n        return counts\n\n    counts['goals'] += goals\n    counts['penalties'] += penalties\n\n    return counts\n\n# Example test setup and usage\nsetup_csv()\nresult = task_func(5, 3, 'test_data/test_case_2.csv')\nprint(result)\n```"}
{"task_id": "BigCodeBench/826", "solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    moved_files_count = 0  # Initialize a counter for the moved files\n    \n    pattern = re.compile(file_pattern)  # Compile the regular expression pattern for efficiency\n    \n    # Check if the target directory exists, if not, create it\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Iterate through each file in the source directory\n    for filename in os.listdir(source_dir):\n        # Full path of the file\n        full_path = os.path.join(source_dir, filename)\n        \n        # Check if the current item is a file and matches the pattern\n        if os.path.isfile(full_path) and pattern.match(filename):\n            # Move the file to the target directory\n            shutil.move(full_path, os.path.join(target_dir, filename))\n            \n            # Increment the counter\n            moved_files_count += 1\n    \n    # Return the count of moved files\n    return moved_files_count\n```"}
{"task_id": "BigCodeBench/845", "solution": "```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[^a-zA-Z0-9 ]+')  # Modified to keep spaces\n\ndef task_func(text1, text2):\n    # Clean the texts\n    cleaned_text1 = ALPHANUMERIC.sub('', text1.lower())\n    cleaned_text2 = ALPHANUMERIC.sub('', text2.lower())\n\n    # Tokenize and count word occurrences\n    words_text1 = Counter(cleaned_text1.split())\n    words_text2 = Counter(cleaned_text2.split())\n\n    # Create word vectors\n    all_words = list(set(words_text1.keys()) | set(words_text2.keys()))\n    vector1 = np.array([words_text1.get(word, 0) for word in all_words])\n    vector2 = np.array([words_text2.get(word, 0) for word in all_words])\n\n    # Compute cosine similarity\n    dot_product = np.dot(vector1, vector2)\n    magnitude1 = np.linalg.norm(vector1)\n    magnitude2 = np.linalg.norm(vector2)\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0.0  # Added check to avoid division by zero\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(cleaned_text1, cleaned_text2)\n\n    return (cosine_similarity, levenshtein_ratio)\n\n# Example usage\ntext1 = \"Hello, world!\"\ntext2 = \"Hello world!\"\nprint(task_func(text1, text2))\n```\nThis code first imports the necessary libraries and defines a regular expression pattern that matches all non-alphanumeric characters except spaces. It then defines a function `task_func` that takes two strings as input, cleans them by removing unwanted characters and converting them to lowercase, and then computes the cosine similarity and Levenshtein ratio between the cleaned texts. These metrics are returned as a tuple. The cosine similarity is computed from term frequencies in each text, and the Levenshtein ratio is computed using the `ratio` function from the `python-Levenshtein` library."}
{"task_id": "BigCodeBench/854", "solution": "```python\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Step 3: Check if the input is a list of integers\n    if not isinstance(numbers, list) or not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    \n    # Step 4: Check for negative numbers in the list\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"Input numbers must not be negative.\")\n    \n    # Step 5: Handle the empty list case\n    if not numbers:\n        return [], []\n    \n    # Step 6: Generate all possible permutations of the input list\n    all_permutations = list(permutations(numbers))\n    \n    # Step 7: Calculate the sum of the factorials of each number in each permutation\n    factorial_sums = [sum(math.factorial(num) for num in perm) for perm in all_permutations]\n    \n    # Step 8: Return the two lists\n    return factorial_sums, [list(perm) for perm in all_permutations]\n\n# Example usage\nfac, perm = task_func([0, 4])\nprint(fac)  # Output: [25, 25]\nprint(perm)  # Output: [(0, 4), (4, 0)]\n```"}
{"task_id": "BigCodeBench/857", "solution": "```python\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n\n    # Ensure the destination directory exists.\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    # Loop through each specified extension.\n    for ext in EXTENSIONS:\n        # Create a pattern to match files with the current extension in SOURCE_DIR.\n        pattern = os.path.join(SOURCE_DIR, '*' + ext)\n        # Use glob to find all matching files.\n        files_to_transfer = glob.glob(pattern)\n\n        # Loop through each matching file.\n        for file_path in files_to_transfer:\n            # Extract just the file name for use in the destination path.\n            file_name = os.path.basename(file_path)\n            destination_path = os.path.join(DEST_DIR, file_name)\n\n            try:\n                # Attempt to move the file to the destination directory.\n                shutil.move(file_path, destination_path)\n                # Add the file name to the list of successfully transferred files.\n                transferred_files.append(file_name)\n            except Exception as e:\n                # If the transfer fails for any reason, issue a warning.\n                warnings.warn(f\"Could not transfer {file_name}: {e}\")\n\n    return transferred_files\n```\n\nThis code meets the requirements by:\n\n1. Importing the necessary modules for the task.\n2. Defining a function `task_func` that takes parameters for the source directory, destination directory, and a list of file extensions to filter by.\n3. Ensuring the destination directory exists before attempting file transfers, creating it if necessary.\n4. Looping through the specified extensions to find matching files in the source directory.\n5. Attempting to move each matching file to the destination directory, catching any exceptions that occur during the process to issue warnings.\n6. Returning a list of the names of files that were successfully transferred."}
{"task_id": "BigCodeBench/865", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Create a DataFrame from the data input\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n    \n    # Normalize the 'Count' column using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n    \n    # Normalize the 'Weight' column using min-max scaling\n    scaler = MinMaxScaler()\n    df[['Normalized Weight']] = scaler.fit_transform(df[['Weight']])\n    \n    # Select the columns to return\n    final_df = df[['Item', 'Normalized Count', 'Normalized Weight']]\n    \n    return final_df\n\n# Example usage\ndata_example = [('Apple', 100, 0.5), ('Banana', 150, 0.3), ('Cherry', 90, 0.2)]\nprint(task_func(data_example))\n```"}
{"task_id": "BigCodeBench/870", "solution": "```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]):\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Initialize a list to hold lists of numbers for each position\n    numbers_by_position = []\n    \n    # Determine the maximum length of tuples in data_list for handling tuples of varying lengths\n    max_length = max(len(t) for t in data_list)\n    \n    # Initialize lists to collect numbers for each position\n    for _ in range(max_length):\n        numbers_by_position.append([])\n    \n    # Iterate over each tuple in the list\n    for data_tuple in data_list:\n        # Iterate over each position in the tuple\n        for i, value in enumerate(data_tuple):\n            # Check if the current value is a number (int or float)\n            if isinstance(value, (int, float)):\n                # Append the number to the corresponding list\n                numbers_by_position[i].append(value)\n    \n    # Calculate the mean for each position, handling positions with no numbers by setting their mean to NaN\n    means = [np.nan if not position else np.mean(position) for position in numbers_by_position]\n    \n    # Create a DataFrame with the mean values\n    df = pd.DataFrame(means, columns=['Mean Value'])\n    \n    # Set the DataFrame index to 'Position i'\n    df.index = [f'Position {i}' for i in range(len(means))]\n    \n    return df\n\n# Example usage\ndf = task_func()\nprint(df)\n```\nThis code correctly implements the task, ensuring that:\n- An empty DataFrame is returned if the input list is empty.\n- The function can handle tuples of varying lengths within the input list.\n- Non-numeric values are ignored when calculating means.\n- The DataFrame's index labels follow the specified 'Position i' format.\n- Handles cases where there are no numeric values at a certain position by assigning NaN to the mean."}
{"task_id": "BigCodeBench/879", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if 'data' DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The 'data' parameter cannot be empty.\")\n    \n    # Check if both columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        missing_col = col1 if col1 not in data.columns else col2\n        raise ValueError(f\"'{missing_col}' is not present in the DataFrame.\")\n    \n    # Check if columns contain only categorical data\n    if not all(isinstance(data[col].dtype, pd.CategoricalDtype) or data[col].dtype == object for col in [col1, col2]):\n        raise TypeError(\"One or both of the columns contain non-categorical data.\")\n    \n    # Check if both columns have at least two categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Ensure each category within the two columns has at least 5 observations\n    if any(data[col1].value_counts() < 5) or any(data[col2].value_counts() < 5):\n        raise ValueError(\"Some categories have less than 5 observations, violating chi-square test assumptions.\")\n    \n    # Construct a contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Perform the chi-square test of independence\n    _, p_value, _, _ = chi2_contingency(contingency_table)\n    \n    # Return the p-value\n    return p_value\n\n# Example usage\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})\n\nprint(task_func(data, 'a', 'b'))\n```\nThis code snippet defines the `task_func` function that performs a chi-square test of independence on two categorical variables within a given DataFrame. It includes all necessary checks for the input data and raises appropriate exceptions as specified. The function returns the p-value of the chi-square test, indicating the probability of observing the data if the null hypothesis (that the variables are independent) is true."}
{"task_id": "BigCodeBench/897", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    dice_rolls = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Count frequency of each outcome\n    frequencies = [dice_rolls.count(value) for value in NUMBERS]\n    \n    # Convert frequencies to numpy array\n    freq_array = np.array(frequencies)\n    \n    # Plotting the histogram\n    fig, ax = plt.subplots()\n    ax.hist(dice_rolls, bins=np.arange(0.5, 7.5, 1), rwidth=0.8, color='blue', alpha=0.7)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    \n    # Adjust x-ticks to match the dice values\n    plt.xticks(NUMBERS)\n    \n    # Return the frequency array and matplotlib Axes object\n    return freq_array, ax\n\n# Example usage\nfrequencies, ax = task_func(1000, seed=42)\nplt.show()  # Display the histogram if running in an environment that does not automatically show plots\n```"}
{"task_id": "BigCodeBench/906", "solution": "```python\nimport zipfile\nimport os\nimport re\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the full path for the archive to be saved\n    archive_path = os.path.join(target_dir, archive_name)\n    \n    # Compile regex pattern to match files that end with '_processed'\n    processed_file_pattern = re.compile(r'_processed\\..*$')\n    \n    # Create a ZipFile object in write mode\n    with zipfile.ZipFile(archive_path, 'w') as archive:\n        # Walk through the source directory\n        for folderName, subfolders, filenames in os.walk(source_dir):\n            for filename in filenames:\n                # Check if the filename ends with '_processed'\n                if processed_file_pattern.search(filename):\n                    # Full path to the current file\n                    file_path = os.path.join(folderName, filename)\n                    # Add the file to the archive, using filename as the arcname\n                    archive.write(file_path, arcname=filename)\n    \n    # Return the full path to the created archive\n    return archive_path\n```"}
{"task_id": "BigCodeBench/914", "solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure 'Date' is in datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n    # Convert dates to numerical value (ordinal) for regression\n    df['DateNumeric'] = df['Date'].apply(lambda x: x.toordinal())\n    \n    # Prepare X (independent variable) and y (dependent variable)\n    X = df[['DateNumeric']].values\n    y = df['Close'].values\n    \n    # Initialize and fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    last_date = df['Date'].iloc[-1]\n    future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 8)]\n    future_dates_numeric = [x.toordinal() for x in future_dates]\n    future_dates_np = np.array(future_dates_numeric).reshape(-1, 1)\n    \n    # Predict future stock closing prices\n    predictions = model.predict(future_dates_np)\n    \n    # Plotting the results\n    plt.figure(figsize=(10, 6))\n    plt.scatter(df['Date'], df['Close'], color='blue', label='Actual Prices')  # Actual data\n    plt.plot(future_dates, predictions, color='red', label='Predicted Prices')  # Predicted data\n    plt.xlabel('Date')\n    plt.ylabel('Stock Closing Price')\n    plt.title('Stock Closing Prices Prediction')\n    plt.legend()\n    ax = plt.gca()  # Get current Axes instance for the return value\n    \n    return predictions.tolist(), ax\n\n# The function is now complete with the necessary imports, data preparation, model fitting, prediction, and plotting.\n# To use this function, ensure you have a DataFrame 'df' with columns 'Date' and 'Close' where 'Date' is in a datetime-compatible format.\n```"}
{"task_id": "BigCodeBench/915", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport pandas as pd  # Ensure pandas is imported for DataFrame functionality\n\ndef task_func(df, z_threshold=2):\n    # Calculate Z-Score for the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[np.abs(df['z_score']) > z_threshold]\n    \n    # Prepare the outliers DataFrame to be returned\n    outliers_result = outliers[['closing_price']].copy()\n    \n    # Plotting the outliers\n    fig, ax = plt.subplots()\n    ax.scatter(outliers.index, outliers['closing_price'], color='red')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    \n    # Return the outliers DataFrame and the plot object\n    return outliers_result, ax\n\n# Example usage\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\n\n# Note: To display the plot in an interactive Python environment like Jupyter Notebook, you should call plt.show()\n```"}
{"task_id": "BigCodeBench/916", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    # Create a figure and a set of subplots with 1 row and 2 columns\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Box plot for closing prices on the first subplot\n    sns.boxplot(data=df, y='Close', ax=axs[0])\n    axs[0].set_title('Box Plot of Closing Prices')  # Title for the box plot\n    \n    # Histogram for closing prices on the second subplot\n    sns.histplot(data=df, x='Close', kde=True, ax=axs[1])\n    axs[1].set_title('Histogram of Closing Prices')  # Title for the histogram\n    \n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Return the tuple of matplotlib.axes._axes.Axes objects\n    return (axs[0], axs[1])\n```"}
{"task_id": "BigCodeBench/917", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    # Assuming the DataFrame has a column named 'Close' for the closing prices\n    # and the data is in a time series format that ARIMA can handle.\n    \n    # Step 1: Prepare the data\n    closing_prices = df['Close'].astype(float)  # Ensure data type is float for ARIMA\n    \n    # Step 2: Fit the ARIMA model\n    # The parameters (p,d,q) for ARIMA need to be chosen based on the dataset.\n    # As a starting point, we use (1,1,1) which can be tuned as necessary.\n    model = ARIMA(closing_prices, order=(1,1,1))\n    model_fit = model.fit()\n    \n    # Step 3: Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    \n    # Step 4: Plot the forecast alongside historical data\n    plt.figure(figsize=(10, 6))\n    plt.plot(closing_prices.index, closing_prices, label='Historical Daily Closing Price')\n    forecast_dates = pd.date_range(start=closing_prices.index[-1], periods=8, closed='right')  # Generate forecast dates\n    plt.plot(forecast_dates, forecast, label='Forecast', linestyle='--', color='red')\n    \n    plt.title('7-Day Share Price Forecast')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.legend()\n    \n    # Ensure the plot is tight and well laid out\n    plt.tight_layout()\n    \n    # Step 5: Prepare the output\n    forecast_list = forecast.tolist()\n    ax = plt.gca()  # Get the current Axes instance on the current figure\n    \n    return forecast_list, ax\n```"}
{"task_id": "BigCodeBench/928", "solution": "```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible combinations of two letters\n    combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Initialize dictionary with 0 count for all combinations\n    combination_counts = {combination: 0 for combination in combinations}\n    \n    # Count the frequency of each two-letter combination in the word\n    for i in range(len(word) - 1):\n        pair = word[i:i+2].lower()  # Convert to lowercase to match the combinations\n        if pair in combination_counts:\n            combination_counts[pair] += 1\n    \n    return combination_counts\n```"}
{"task_id": "BigCodeBench/942", "solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Initialize an empty list to store sales data\n    sales_data = []\n\n    # Generate sales data\n    for date in dates:\n        for category in categories:\n            # Simulate sales amount\n            sales_amount = np.random.randint(100, 1000)\n            # Append the record to the sales data list\n            sales_data.append({'Date': date, 'Category': category, 'Sales': sales_amount})\n\n    # Convert list to DataFrame\n    sales_data_df = pd.DataFrame(sales_data)\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        # Filter data for the category\n        category_data = sales_data_df[sales_data_df['Category'] == category]\n        # Plot\n        ax.plot(category_data['Date'], category_data['Sales'], label=category, marker='o')\n\n    # Beautify the plot\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n\n    # Show plot\n    plt.xticks(rotation=45)  # Rotate dates for better legibility\n    plt.tight_layout()  # Adjust layout to not cut off labels\n    plt.show()\n\n    return sales_data_df, ax\n\n# Example usage\nsales_data, plot_ax = task_func()\n```"}
{"task_id": "BigCodeBench/945", "solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    # If sales_data is not provided, generate a time series of sales data\n    if sales_data is None:\n        dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = np.random.randint(100, 500, size=len(dates))\n    else:\n        # Ensure sales_data is a numpy array\n        sales_data = np.array(sales_data)\n\n    # Prepare the data for linear regression\n    if isinstance(sales_data, (np.ndarray, pd.Series)) and not isinstance(sales_data, pd.DataFrame):\n        # Generate dates for the given periods if sales_data was provided without dates\n        dates = pd.date_range(start=start_date, periods=len(sales_data), freq=freq)\n    else:\n        # If sales_data is a DataFrame, assume it includes a date column and use it\n        dates = pd.to_datetime(sales_data.index if isinstance(sales_data, pd.Series) else sales_data['date'])\n\n    # Convert dates to ordinal numbers for linear regression\n    dates_ordinal = np.array([date.toordinal() for date in dates]).reshape(-1, 1)\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(dates_ordinal, sales_data)\n    \n    # Predict future sales for the same number of periods\n    future_dates = pd.date_range(start=dates[-1], periods=periods + 1, freq=freq)[1:]\n    future_dates_ordinal = np.array([date.toordinal() for date in future_dates]).reshape(-1, 1)\n    future_sales_predictions = model.predict(future_dates_ordinal)\n    \n    # Return the numpy array containing the forecasted future sales\n    return future_sales_predictions\n\n# Example of calling the function without providing sales_data\npredicted_sales = task_func()\nprint(predicted_sales)\n```"}
{"task_id": "BigCodeBench/952", "solution": "```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Validate n_tasks is non-negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be a non-negative integer\")\n    \n    # Set the seed for reproducibility, if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n    \n    # Randomly assign tasks to employees\n    assignments = []\n    for _ in range(n_tasks):\n        task = random.choice(sanitized_tasks)\n        employee = random.choice(employees)\n        due_date = datetime.now().strftime(\"%Y-%m-%d\")  # Current system date as due date\n        assignments.append([task, employee, due_date])\n    \n    # Create a DataFrame from the assignments list\n    df = pd.DataFrame(assignments, columns=['Task Name', 'Assigned To', 'Due Date'])\n    \n    return df\n```"}
{"task_id": "BigCodeBench/955", "solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    # Check if the input text is empty\n    if not text:\n        raise ValueError(\"If the input text is empty\")\n    \n    # Convert the text to lowercase for case-insensitive operations\n    text = text.lower()\n    \n    # Replace spaces in specified words with underscores\n    for word in mystrings:\n        # Escape special characters for regex, convert the word to lowercase, and replace spaces with underscores\n        pattern = r'\\b' + re.escape(word.lower()) + r'\\b'\n        replacement = word.lower().replace(' ', '_')\n        text = re.sub(pattern, replacement, text)\n    \n    # Split the text into words and count the frequency of each unique word\n    words = text.split()\n    word_counts = Counter(words)\n    \n    # Prepare data for plotting\n    labels, values = zip(*sorted(word_counts.items(), key=lambda x: words.index(x[0])))\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 8))\n    ax.bar(labels, values)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    return ax\n```"}
{"task_id": "BigCodeBench/963", "solution": "```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(\"Source directory does not exist\")\n    \n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Prepare the full path for the zip file to be created\n    zip_file_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    \n    # Initialize zipfile\n    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Loop through each valid extension and add matching files to the zip\n        for extension in valid_extensions:\n            for file in glob.glob(f\"{source_directory}/**/*{extension}\", recursive=True):\n                # Add file to zip, specifying arcname to maintain the relative path\n                zipf.write(file, arcname=os.path.relpath(file, source_directory))\n    \n    return zip_file_path\n\n# Example usage (Commented out to avoid execution errors in this environment)\n# print(task_func(\"/path/to/source_directory\", \"/path/to/target_directory\", \"my_zip_file\"))\n```\n\nThis script correctly imports necessary modules and defines the `task_func` function which zips files with specific extensions from a source directory to a specified target directory under a given zip file name. It ensures that only files with extensions `.txt`, `.docx`, `.xlsx`, and `.csv` are included in the zip, raises an `OSError` if the source directory does not exist, and returns the full path to the created zip file. The use of `zipfile.ZIP_DEFLATED` in `zipfile.ZipFile` initialization ensures efficient compression."}
{"task_id": "BigCodeBench/964", "solution": "```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nimport openpyxl  # Needed for .xlsx files\nimport csv  # Needed for .csv files\n\ndef read_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return [file.read()]\n\ndef read_docx_file(file_path):\n    doc = docx.Document(file_path)\n    fullText = []\n    for para in doc.paragraphs:\n        fullText.append(para.text)\n    return [\"\\n\".join(fullText)]\n\ndef read_xlsx_file(file_path):\n    wb = openpyxl.load_workbook(file_path)\n    sheet = wb.active\n    data = []\n    for row in sheet.iter_rows(values_only=True):\n        data.append(' '.join([str(cell) for cell in row if cell is not None]))\n    wb.close()\n    return data\n\ndef read_csv_file(file_path):\n    with open(file_path, mode='r', encoding='utf-8') as file:\n        csv_reader = csv.reader(file)\n        data = [row[0] for row in csv_reader if row]  # Assuming the first column is the relevant text column\n    return data\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"The directory {source_directory} does not exist.\")\n    \n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    \n    file_count = 0\n    for file in os.listdir(source_directory):\n        file_path = os.path.join(source_directory, file)\n        if os.path.isfile(file_path):\n            file_extension = Path(file).suffix\n            data = []\n            if file_extension == '.txt':\n                data = read_txt_file(file_path)\n            elif file_extension == '.docx':\n                data = read_docx_file(file_path)\n            elif file_extension == '.xlsx':\n                data = read_xlsx_file(file_path)\n            elif file_extension == '.csv':\n                data = read_csv_file(file_path)\n            \n            if data:  # If data list is not empty\n                df = pd.DataFrame(data, columns=['Text'])\n                target_file_path = os.path.join(target_directory, Path(file).stem + '.csv')\n                df.to_csv(target_file_path, index=False)\n                file_count += 1\n    \n    return file_count\n```"}
{"task_id": "BigCodeBench/969", "solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Check if the DataFrame contains only numeric data types\n    if not all([np.issubdtype(dtype, np.number) for dtype in df.dtypes]):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    \n    # Check for NaN values in the DataFrame\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    \n    # Calculate the cumulative sum for each numeric column\n    cumulative_sum_df = df.cumsum()\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the cumulative sum DataFrame with MinMaxScaler\n    normalized_cumulative_sum = scaler.fit_transform(cumulative_sum_df)\n    \n    # Convert the normalized cumulative sum back to a DataFrame with original column names\n    normalized_cumulative_sum_df = pd.DataFrame(normalized_cumulative_sum, columns=df.columns, index=df.index)\n    \n    return normalized_cumulative_sum_df\n```"}
{"task_id": "BigCodeBench/971", "solution": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Verify if the directory exists\n    if not Path(directory_path).is_dir():\n        raise ValueError(f\"The directory {directory_path} does not exist.\")\n    \n    # Initialize an empty list to hold file information tuples\n    file_info_list = []\n\n    # Iterate over each item in the directory using os.scandir for efficiency\n    with os.scandir(directory_path) as it:\n        for entry in it:\n            # Ensure we only process files\n            if entry.is_file():\n                # Extract the file name\n                file_name = entry.name\n                # Extract the file size in bytes\n                file_size = entry.stat().st_size\n                # Determine creation time, handling potential platform-specific differences\n                try:\n                    # Attempt to get the actual creation time (Windows)\n                    creation_time = datetime.fromtimestamp(entry.stat().st_ctime, tz=timezone.utc).isoformat()\n                except AttributeError:\n                    # Fallback to the last metadata change time (Unix-like OSes)\n                    creation_time = datetime.fromtimestamp(entry.stat().st_mtime, tz=timezone.utc).isoformat()\n                \n                # Get the last modification time\n                modification_time = datetime.fromtimestamp(entry.stat().st_mtime, tz=timezone.utc).isoformat()\n                \n                # Compile the file information into a tuple and add it to the list\n                file_info_list.append((file_name, file_size, creation_time, modification_time))\n    \n    # Return the compiled list of file information tuples\n    return file_info_list\n```"}
{"task_id": "BigCodeBench/985", "solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Attempt to parse the JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON data.\") from e\n\n    # Validate the input data\n    if not data:\n        raise ValueError(\"JSON data is empty.\")\n    if not isinstance(data, list):  # Expecting data to be a list of records\n        raise ValueError(\"Invalid JSON format. Expected a list of records.\")\n\n    # Prepare the list to hold validated country-population data\n    validated_data = []\n\n    for item in data:\n        # Validate country name\n        if not isinstance(item.get(\"Country\"), str):\n            raise ValueError(\"Country name must be a string.\")\n        \n        # Validate population\n        population = item.get(\"Population\")\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Population must be a non-negative number.\")\n        \n        # Round down the population if it's a float\n        if isinstance(population, float):\n            population = math.floor(population)\n        \n        validated_data.append({\"Country\": item[\"Country\"], \"Population\": population})\n    \n    # Create DataFrame\n    df = pd.DataFrame(validated_data, columns=[\"Country\", \"Population\"])\n    \n    # Generate the file path\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Attempt to save the DataFrame to a CSV file\n    try:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        df.to_csv(file_path, index=False)\n    except Exception as e:\n        raise IOError(\"Failed to write CSV file to specified directory.\") from e\n    \n    # Return the file path and the DataFrame\n    return file_path, df\n\n# Example JSON data for testing purposes\n# json_data = '[{\"Country\": \"ExampleLand\", \"Population\": 123456}]'\n# print(task_func(json_data))\n```"}
{"task_id": "BigCodeBench/988", "solution": "```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Deduplicate predicates to ensure each condition is checked only once\n    predicates = list(set(predicates))\n    \n    # Check for empty or invalid predicates list\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Verify the directory path is valid and points to an existing directory\n    if not Path(dir_path).is_dir():\n        raise FileNotFoundError(f\"The specified directory does not exist: {dir_path}\")\n    \n    # Initialize a dictionary to store the results\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in os.listdir(dir_path):\n        # Initialize a list to store the result of each predicate for the current item\n        item_results = []\n        \n        # Evaluate each predicate against the item\n        for predicate in predicates:\n            try:\n                # Check if the predicate is callable and apply it to the item name\n                if callable(predicate):\n                    result = predicate(item)\n                # If the predicate is a regex pattern, match it against the item name\n                elif isinstance(predicate, str):\n                    result = bool(re.match(predicate, item))\n                else:\n                    # Skip the predicate if it's neither callable nor a regex pattern\n                    continue\n            except Exception as e:\n                # Treat any errors during predicate evaluation as a failed condition\n                result = False\n            \n            # Add the predicate result to the list for the current item\n            item_results.append(result)\n        \n        # Update the results dictionary, considering an item as meeting all conditions if all predicate results are True\n        results[item] = all(item_results)\n    \n    return results\n```"}
{"task_id": "BigCodeBench/990", "solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hex to bytes\n    bytes_data = binascii.unhexlify(hex_string)\n    \n    # Convert bytes to UTF-8\n    utf8_str = bytes_data.decode('utf-8')\n    \n    # Encode to various formats\n    # Hexadecimal\n    hex_enc = binascii.hexlify(bytes_data).decode('utf-8')\n    \n    # Base64\n    base64_enc = base64.b64encode(bytes_data).decode('utf-8')\n    \n    # UTF-16 (Encode to UTF-16 and decode back to get a string representation)\n    utf16_enc = bytes_data.decode('utf-8').encode('utf-16')[2:].decode('utf-16')  # Skipping BOM\n    \n    # UTF-32 (Encode to UTF-32 and decode back to get a string representation)\n    utf32_enc = bytes_data.decode('utf-8').encode('utf-32')[4:].decode('utf-32')  # Skipping BOM\n    \n    # ASCII\n    try:\n        ascii_enc = bytes_data.decode('ascii')\n    except UnicodeDecodeError:\n        ascii_enc = 'Not representable in ASCII'\n    \n    # URL encoding\n    url_enc = urllib.parse.quote(utf8_str)\n    \n    # ROT13\n    rot13_enc = codecs.encode(utf8_str, 'rot_13')\n    \n    return {\n        'hex': hex_enc,\n        'base64': base64_enc,\n        'utf-8': utf8_str,\n        'utf-16': utf16_enc,\n        'utf-32': utf32_enc,\n        'ASCII': ascii_enc,\n        'URL': url_enc,\n        'ROT13': rot13_enc\n    }\n```"}
{"task_id": "BigCodeBench/998", "solution": "```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    # Attempt to download the file\n    try:\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n        return False\n    \n    # Verify MD5 checksum\n    md5_hash = hashlib.md5()\n    try:\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        file_md5_checksum = md5_hash.hexdigest()\n        if file_md5_checksum != EXPECTED_MD5_CHECKSUM:\n            print(\"MD5 checksum does not match.\")\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        print(f\"Error calculating MD5 checksum: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n    \n    # Extract tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar_ref:\n            tar_ref.extractall()\n    except Exception as e:\n        print(f\"Error extracting the tar.gz file: {e}\")\n        return False\n    finally:\n        # Cleanup by removing the tar file after extraction\n        os.remove(TARGET_TAR_FILE)\n\n    # If everything is successful, return True\n    return True\n```"}
{"task_id": "BigCodeBench/999", "solution": "```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    # Attempt to download the CSV file from the given URL\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        raise IOError(\"Failed to download the file: \" + str(e))\n\n    # Verify if the file was downloaded and exists\n    if not os.path.isfile(csv_file_path):\n        raise IOError(\"The file does not exist at the specified path.\")\n\n    try:\n        # Initialize a dictionary to count occurrences of each value in the specified column\n        column_data = collections.defaultdict(int)\n\n        # Open and read the CSV file\n        with open(csv_file_path, mode='r', encoding='utf-8') as file:\n            reader = csv.DictReader(file)\n\n            # Check if the specified column exists in the CSV\n            if column_name not in reader.fieldnames:\n                os.remove(csv_file_path)  # Delete the CSV file before raising the ValueError\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n\n            # Tally occurrences of each value in the specified column\n            for row in reader:\n                column_data[row[column_name]] += 1\n\n    except ValueError:\n        raise  # Re-raise the ValueError for missing column to ensure it's not caught by the general exception clause\n    except Exception as e:\n        raise IOError(\"An error occurred while processing the CSV file: \" + str(e))\n    finally:\n        # Remove the CSV file from the system if it still exists\n        if os.path.isfile(csv_file_path):\n            os.remove(csv_file_path)\n\n    # Return the dictionary of occurrences\n    return dict(column_data)\n```"}
{"task_id": "BigCodeBench/1003", "solution": "```python\nimport urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    # Attempt to fetch the XML file from the URL\n    try:\n        with urllib.request.urlopen(url) as response:\n            xml_data = response.read()\n    except Exception:\n        raise ValueError(\"The URL is invalid or the XML file cannot be fetched.\")\n    \n    # Attempt to parse the fetched XML file\n    try:\n        root = etree.fromstring(xml_data)\n    except etree.XMLSyntaxError:\n        raise ValueError(\"The XML file contains invalid syntax.\")\n    \n    # Check for the expected XML structure and prepare data for DataFrame conversion\n    data = []\n    # Ensure the first level after the root contains 'item' elements\n    if not root.findall('.//item'):\n        raise ValueError(\"XML structure does not match expected format.\")\n    \n    for item in root.findall('.//item'):\n        row_data = {}\n        for child in item:\n            row_data[child.tag] = child.text\n        data.append(row_data)\n    \n    # Convert the list of dictionaries into a Pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    return df\n```"}
{"task_id": "BigCodeBench/1004", "solution": "```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file content from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n    \n    # Use regular expression to find all words in the text\n    # Assuming words are sequences of alphanumeric characters and case-sensitive\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count frequency of each word\n    word_counts = Counter(words)\n    \n    # Select the ten most common words\n    ten_most_common = word_counts.most_common(10)\n    \n    # Plot a bar chart for the ten most common words\n    words, frequencies = zip(*ten_most_common)  # Unzipping the words and their frequencies\n    plt.figure(figsize=(10, 8))  # Adjusting figure size for better readability\n    plt.bar(words, frequencies, color='skyblue')  # Creating the bar chart\n    plt.xlabel('Words')  # Label for the x-axis\n    plt.ylabel('Frequency')  # Label for the y-axis\n    plt.title('Top 10 Most Common Words')  # Title of the chart\n    ax = plt.gca()  # Getting the current Axes instance\n    \n    # Return the Counter object and the Axes object\n    return (word_counts, ax)\n```"}
{"task_id": "BigCodeBench/1006", "solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    # Ensure the download directory exists\n    if not os.path.exists(download_path):\n        os.makedirs(download_path)\n    \n    try:\n        # Attempt to download the ZIP file\n        response = requests.get(url, timeout=10)  # Added timeout for network issues\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n\n        # Check if the content type is ZIP\n        if 'application/zip' not in response.headers.get('Content-Type', ''):\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Write the downloaded content to a file\n        zip_path = os.path.join(download_path, \"downloaded.zip\")\n        with open(zip_path, 'wb') as file:\n            file.write(response.content)\n\n        # Attempt to extract the ZIP file\n        with ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n        os.remove(zip_path)  # Cleanup ZIP file after extraction\n\n        # Return the path to the extracted contents\n        return download_path\n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except BadZipFile:\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage\n# print(task_func(\"https://example.com/path/to/zipfile.zip\"))\n```\nThis code snippet meets the criteria by handling various exceptions and returning descriptive error messages for different failure scenarios, including network issues, incorrect file type, and corrupt ZIP files. It also ensures to create the download directory if it doesn't exist and cleans up the downloaded ZIP file after extraction."}
{"task_id": "BigCodeBench/1008", "solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url, table_id):\n    # Attempt to fetch the content of the webpage\n    try:\n        response = requests.get(url)\n        # Raise an HTTPError for bad responses\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.HTTPError(f\"Failed to fetch the webpage: {e}\")\n    \n    # Parse the webpage content with BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Search for the table with the specified ID\n    table = soup.find('table', id=table_id)\n    \n    # If the table is not found, raise a ValueError\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n    \n    # Check for the presence of data rows in the table\n    rows = table.find_all('tr')\n    if len(rows) == 0:\n        # Return an empty DataFrame if no rows are found\n        return pd.DataFrame()\n    \n    # Extract table headers\n    headers = []\n    if len(rows) > 0:\n        for header in rows[0].find_all(['th', 'td']):  # Include 'td' to handle tables without explicit headers\n            headers.append(header.text.strip())\n    \n    # Initialize a list to hold data rows\n    data = []\n    for row in rows[1:]:  # Skip the header row\n        cols = row.find_all('td')\n        if cols:  # Ensure that the row has columns to avoid adding header rows as data\n            data_row = [col.text.strip() for col in cols]\n            data.append(data_row)\n    \n    # Handle tables without headers by creating default headers\n    if len(headers) == 0 and len(data) > 0:\n        headers = range(len(data[0]))  # Generate numeric headers based on the number of columns in the data\n    \n    # Convert extracted data into a DataFrame\n    df = pd.DataFrame(data, columns=headers)\n    \n    return df\n```\nThis code snippet provides a resilient and robust solution to extract and convert data from a specified HTML table into a `pandas.DataFrame`. It handles various edge cases, including HTTP errors, missing tables, tables without headers, and tables without data rows."}
{"task_id": "BigCodeBench/1012", "solution": "```python\nimport requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure download and unzip directories exist\n    DOWNLOAD_DIR.mkdir(exist_ok=True)\n    ZIP_DIR.mkdir(exist_ok=True)\n    \n    # Define the path for the downloaded file\n    download_path = DOWNLOAD_DIR / filename\n    \n    try:\n        # Download the file\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for failed downloads\n        \n        # Write the downloaded file to disk\n        with open(download_path, 'wb') as file:\n            file.write(response.content)\n        \n        # Attempt to unzip the file\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n            unzipped_files = [f.name for f in ZIP_DIR.iterdir()]\n            \n        # Return success status and list of unzipped files\n        return (\"Download and extraction successful.\", unzipped_files)\n        \n    except requests.exceptions.RequestException as e:\n        # Handle network-related exceptions\n        return (f\"Error: Network-related exception ({e}).\", [])\n    except zipfile.BadZipFile as e:\n        # Handle exceptions related to bad zip files\n        return (f\"Error: File-related exception ({e}).\", [])\n    except Exception as e:\n        # Handle other exceptions\n        return (f\"Error: General exception ({e}).\", [])\n\n# Example URL and filename\n# Note: Replace the example URL and filename with a real one to test\nurl = \"https://example.com/somefile.zip\"\nfilename = \"somefile.zip\"\n\n# Example function call\nprint(task_func(url, filename))\n```\n\nThis code correctly follows the problem statement by downloading a zip file from a provided URL and extracting its contents to a specified directory. It handles errors gracefully by catching exceptions related to network issues, zip file corruption, and other potential errors, returning a tuple with a status message and a list of unzipped file names."}
{"task_id": "BigCodeBench/1013", "solution": "```python\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the specified URL\n    response = requests.get(url)\n    \n    # Check for successful response\n    if response.status_code != 200:\n        raise Exception(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n    \n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find all hyperlinks in the parsed HTML\n    links = soup.find_all('a')\n    \n    # Initialize a set to store unique absolute URLs\n    unique_links = set()\n    \n    # Iterate over all found links\n    for link in links:\n        # Extract the href attribute from each link\n        href = link.get('href')\n        if href:\n            # Convert relative URLs to absolute URLs\n            absolute_url = urljoin(base_url, href)\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n    \n    # Open a CSV file to write the unique absolute URLs\n    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        # Write each unique absolute URL as a row in the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n    \n    # Return the count of unique absolute links\n    return len(unique_links)\n```"}
{"task_id": "BigCodeBench/1015", "solution": "```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Attempt to fetch the webpage content\n        if webpage_url.startswith('http'):\n            # For web URLs, use requests to fetch the content\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Raises a RequestException for HTTP errors\n            content = response.content\n        else:\n            # For local files, read the content directly\n            with open(webpage_url, 'r', encoding='utf-8') as file:\n                content = file.read()\n\n        # Parse the HTML content\n        doc = html.fromstring(content)\n        # Extract table element\n        table = doc.xpath('//table')\n        if not table:\n            # No table found\n            return 0\n        \n        # Convert the first HTML table found to a pandas DataFrame\n        df = pd.read_html(html.tostring(table[0]))[0]\n        if df.empty:\n            # Table is empty\n            return 0\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n        # Replace the existing table with new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n        # Get the number of rows inserted\n        rows_inserted = len(df)\n        \n        # Close the database connection\n        conn.close()\n        \n        return rows_inserted\n\n    except requests.RequestException as e:\n        # Handle network issues\n        print(f\"Network error occurred: {e}\")\n        raise\n    except sqlite3.DatabaseError as e:\n        # Handle database issues\n        print(f\"Database error occurred: {e}\")\n        raise\n    except Exception as e:\n        # Handle other possible exceptions\n        print(f\"An error occurred: {e}\")\n        raise\n```"}
{"task_id": "BigCodeBench/1019", "solution": "```python\nfrom PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    converted_text = ''\n    try:\n        with Image.open(filename) as img:\n            text = pytesseract.image_to_string(img)\n            if text:\n                # Convert the extracted text.\n                converted_text = codecs.encode(codecs.decode(text, from_encoding), to_encoding).decode(to_encoding)\n    except Exception as e:\n        # Handle exceptions from OCR or image opening by proceeding to comments extraction.\n        pass\n\n    # If OCR extraction fails or returns an empty string, attempt to extract comments.\n    if not converted_text:\n        try:\n            with Image.open(filename) as img:\n                if \"comment\" in img.info:\n                    # Extract and try to convert the comment.\n                    comment = img.info[\"comment\"]\n                    if isinstance(comment, bytes):\n                        comment = comment.decode(from_encoding)  # Decode from the initial encoding\n                    converted_text = codecs.encode(comment, to_encoding).decode(to_encoding)  # Re-encode to the target encoding\n        except UnicodeDecodeError:\n            raise ValueError(\"UnicodeDecodeError occurred during conversion.\")\n        except LookupError:\n            raise ValueError(\"LookupError occurred due to incorrect encoding.\")\n        except Exception as e:\n            # If any other exception occurs, return an empty string.\n            converted_text = ''\n\n    if not converted_text:\n        # If both OCR and comment extraction fail, return an empty string.\n        return ''\n\n    return converted_text\n\n# Example usage:\nprint(task_func())\n```"}
{"task_id": "BigCodeBench/1020", "solution": "```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    # Initiate an HTTP GET request with a timeout of 5 seconds\n    response = requests.get(url, timeout=5)\n    \n    # Retrieve the response content in raw bytes\n    raw_data = response.content\n    \n    # Check if the response content is empty\n    if not raw_data:\n        return {}\n    \n    # Detect encoding if not specified\n    if not from_encoding:\n        detected_encoding = chardet.detect(raw_data)['encoding']\n        if detected_encoding is None:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n        from_encoding = detected_encoding\n\n    # Decode and re-encode the data\n    decoded_data = raw_data.decode(from_encoding).encode(to_encoding)\n    \n    # Parse the re-encoded data as JSON and return it as a dictionary\n    json_data = json.loads(decoded_data.decode(to_encoding))  # Ensure decoded_data is a string for json.loads\n    return json_data\n```"}
{"task_id": "BigCodeBench/1022", "solution": "```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Verify if the specified CSV file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    # Attempt to read the CSV file into a DataFrame\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # Return an empty DataFrame if the CSV file is empty\n        return pd.DataFrame()\n    \n    # Check for the presence of the specified date column in the DataFrame\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' is not present in the CSV file.\")\n    \n    # Convert the date column values to datetime objects according to the specified format\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    \n    # Filter the DataFrame to only include rows where the date is before or equal to the current date\n    df = df[df[column_name] <= datetime.now()]\n    \n    # Sort the filtered DataFrame by the date column in ascending order\n    df = df.sort_values(by=column_name)\n    \n    return df\n```"}
{"task_id": "BigCodeBench/1039", "solution": "```python\nimport ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_socket = context.wrap_socket(client_socket, server_side=True)\n        \n        # Receive the file path requested by the client\n        file_path_bytes = secure_socket.recv(buffer_size)\n        file_path = file_path_bytes.decode('utf-8')\n        \n        # Check if the file exists\n        if os.path.exists(file_path):\n            # Calculate the SHA256 hash of the file\n            sha256_hash = hashlib.sha256()\n            with open(file_path, 'rb') as file:\n                for chunk in iter(lambda: file.read(buffer_size), b''):\n                    sha256_hash.update(chunk)\n            # Convert hash digest to hexadecimal string\n            hash_hex = sha256_hash.hexdigest()\n            return hash_hex\n        else:\n            return 'File not found'\n    except Exception as e:\n        # Return the error message\n        return str(e)\n```\n\nThis function securely wraps a client socket using SSL/TLS with the provided certificate and key files. It listens for a file request from the client, checks if the file exists, and if so, calculates and returns its SHA256 hash. In case the file does not exist, it returns a 'File not found' message. Any exceptions that occur during processing are caught, and their messages are returned."}
{"task_id": "BigCodeBench/1040", "solution": "```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5):\n    # Initialize server socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(0)  # Make socket non-blocking\n    server.bind((server_address, server_port))\n    server.listen(5)  # Listen for up to 5 connections\n\n    # Setup to track sockets\n    inputs = [server]\n    outputs = []\n    message_queues = {}\n\n    # Track time for run duration\n    start_time = datetime.now()\n\n    while inputs and datetime.now() - start_time < timedelta(seconds=run_duration):\n        # Wait for at least one of the sockets to be ready for processing\n        readable, writable, exceptional = select.select(inputs, outputs, inputs)\n\n        for s in readable:\n            if s is server:\n                # Accept new connection\n                connection, client_address = s.accept()\n                connection.setblocking(0)\n                inputs.append(connection)\n\n                # Give the connection a queue for data we want to send\n                message_queues[connection] = queue.Queue()\n            else:\n                data = s.recv(buffer_size)\n                if data:\n                    # Append server's current time to received data\n                    message = f\"{data.decode()} {datetime.now()}\"\n                    message_queues[s].put(message)\n                    if s not in outputs:\n                        outputs.append(s)\n                else:\n                    # No data, close the connection\n                    if s in outputs:\n                        outputs.remove(s)\n                    inputs.remove(s)\n                    s.close()\n                    del message_queues[s]\n\n        for s in writable:\n            try:\n                next_msg = message_queues[s].get_nowait()\n            except queue.Empty:\n                # No messages waiting so stop checking for writability.\n                outputs.remove(s)\n            else:\n                s.send(next_msg.encode())\n\n        for s in exceptional:\n            # Handle \"exceptional conditions\"\n            inputs.remove(s)\n            if s in outputs:\n                outputs.remove(s)\n            s.close()\n            del message_queues[s]\n\n    # Close the server socket after running for the duration\n    server.close()\n    \n    # Calculate the total running time\n    total_run_time = datetime.now() - start_time\n    return f\"Server operation completed. Total running time was: {total_run_time}\"\n\n# Example of running the function\nprint(task_func())\n```"}
{"task_id": "BigCodeBench/1042", "solution": "```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Establish a connection to the SMTP server\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n        smtp.starttls()  # Upgrade the connection to secure\n        \n        # Prompt the user for the sender's and recipient's email addresses and sender's password\n        sender_email = input(\"Enter the sender's email address: \")\n        recipient_email = input(\"Enter the recipient's email address: \")\n        password = getpass.getpass(\"Enter the sender's password: \")\n        \n        # Use the provided credentials to log in to the sender's SMTP server\n        smtp.login(sender_email, password)\n        \n        # Receive a message from the client socket\n        message_from_client = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        \n        # Prepare the email message to be sent\n        email_message = EmailMessage()\n        email_message['From'] = sender_email\n        email_message['To'] = recipient_email\n        email_message['Subject'] = \"Message from client socket\"\n        email_message.set_content(message_from_client)\n        \n        # Send the email\n        smtp.send_message(email_message)\n        \n        print(\"Email sent successfully.\")\n\n# This is a placeholder for example usage. In real scenarios, the client_socket should be created and connected as needed.\nif __name__ == \"__main__\":\n    # Creating a dummy server and client for demonstration purposes\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    server_socket.listen()\n\n    print(\"Server listening for connections...\")\n\n    # Simulate a client connection for demonstration\n    client_socket, addr = server_socket.accept()\n    with client_socket:\n        print(f\"Connection from {addr}\")\n        # Send a dummy message to simulate client's message\n        client_socket.sendall(b\"This is a test message from the client.\")\n\n        # Close the connection to simulate end of communication\n        client_socket.close()\n\n    # Assuming the client_socket is ready and contains a message, we simulate its usage\n    # Note: In real usage, replace the below line with an actual connected socket object\n    task_func(client_socket)\n\n    # Close the server socket\n    server_socket.close()\n```\nThis code snippet demonstrates how to receive a message from a client socket and send it as an email via an SMTP server. Note that the `if __name__ == \"__main__\":` block is included to provide an example context for how `task_func` might be used, including setting up a dummy server and client for demonstration purposes. In a real-world scenario, you would replace this block with actual client socket connections."}
{"task_id": "BigCodeBench/1053", "solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nimport os\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n    \n    try:\n        # Load the CSV file\n        # Try to use 'Text' as header; if it fails, assume no header and use the first column\n        try:\n            data = pd.read_csv(file_path, usecols=['Text'])\n        except ValueError:\n            data = pd.read_csv(file_path, usecols=[0], header=None)\n            data.columns = ['Text']  # Rename column to 'Text'\n        \n        # Extract the text data\n        text_data = data['Text'].tolist()\n        \n        # Text preprocessing and vectorization\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_count = vectorizer.fit_transform(text_data)\n        \n        # Summarize word count\n        sum_words = word_count.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]\n        \n        # Prepare data for plotting\n        words = [word[0] for word in words_freq]\n        counts = [freq[1] for freq in words_freq]\n        \n        # Plotting\n        plt.figure(figsize=(10, 6))\n        plt.bar(words, counts, color='skyblue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Frequent Words')\n        \n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            return plt.gca()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```"}
{"task_id": "BigCodeBench/1057", "solution": "```python\nimport pandas as pd\nimport itertools\nimport numpy as np\nimport random\n\ndef task_func(animals=None, foods=None):\n    # Predefined lists if either animals or foods are not provided or are empty\n    default_animals = ['dog', 'cat', 'mouse']\n    default_foods = ['cheese', 'fish', 'carrot']\n    \n    # Conditional use of predefined lists based on the provided parameters\n    if animals is None or not animals:\n        animals = default_animals\n    if foods is None or not foods:\n        foods = default_foods\n    \n    # Generate all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations to ensure variety\n    random.shuffle(combinations)\n    \n    # Construct a DataFrame from the combinations, formatted as 'animal:food'\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    df['Combination'] = df['Animal'] + ':' + df['Food']\n    \n    # Drop the separate Animal and Food columns to leave only the combined column\n    df.drop(['Animal', 'Food'], axis=1, inplace=True)\n    \n    # The DataFrame structure requested does not exactly match the typical row/column format,\n    # as it implies a single column with unique animal:food combinations.\n    # Thus, we present it as a single-column DataFrame for clarity.\n    return df\n\n# Example usage\ndf = task_func(['lion', 'tiger'], ['meat', 'water'])\nprint(df)\n```"}
{"task_id": "BigCodeBench/1077", "solution": "```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Check if there are less than two timestamps\n    if len(time_strings) < 2:\n        return 0.0\n\n    # Initialize an empty list to store converted timestamps\n    converted_timestamps = []\n\n    # Convert each timestamp to the specified timezone\n    target_timezone = pytz.timezone(timezone)\n    for time_string in time_strings:\n        # Parse the timestamp string to a datetime object\n        naive_datetime = datetime.strptime(time_string, '%Y-%m-%dT%H:%M:%S%z')\n        # Convert the naive datetime to the target timezone\n        target_datetime = naive_datetime.astimezone(target_timezone)\n        # Append the converted datetime to the list\n        converted_timestamps.append(target_datetime)\n\n    # Calculate the absolute time differences in seconds between each consecutive pair\n    time_differences = []\n    for i in range(len(converted_timestamps) - 1):\n        # Calculate the absolute difference in seconds and append it to the list\n        diff = abs((converted_timestamps[i+1] - converted_timestamps[i]).total_seconds())\n        time_differences.append(diff)\n\n    # Calculate the mean of the time differences\n    if time_differences:\n        average_difference = np.mean(time_differences)\n    else:\n        # If there are no time differences, return 0.0\n        average_difference = 0.0\n\n    return average_difference\n```"}
{"task_id": "BigCodeBench/1085", "solution": "```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Split text into words\n    words = text.split()\n    # Calculate frequency of each word\n    word_counts = Counter(words)\n    # Identify and store the 10 most frequent words\n    most_common_words = word_counts.most_common(10)\n    # Prepare data for plotting\n    words, frequencies = zip(*most_common_words)\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    plt.xticks(rotation=45)\n    plt.show()\n    # Return the list of tuples and the Axes object\n    return most_common_words, ax\n\n# Example usage\ntext = \"This is a sample text. It has some words, and it is supposed to help us understand how often each word appears. It is a simple test text.\"\nresult, _ = task_func(text)\nprint(result)\n```"}
{"task_id": "BigCodeBench/1124", "solution": "```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs within a string\n    url_regex = r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\"\n    \n    # Search for URLs in the input string\n    url_search = re.search(url_regex, myString)\n    \n    # If a URL is found, validate and process it\n    if url_search:\n        url = url_search.group(0)\n        parsed_url = urlparse(url)\n        \n        # Check if the URL's scheme and netloc are present (basic validation)\n        if not all([parsed_url.scheme, parsed_url.netloc]):\n            return \"No valid URL found in the provided string.\"\n        \n        # Attempt to fetch the content of the URL\n        try:\n            response = requests.get(url)\n            # Check if the response status code indicates success\n            if response.status_code == 200:\n                # Parse the HTML content\n                soup = BeautifulSoup(response.text, 'html.parser')\n                # Find the <title> tag\n                title_tag = soup.find('title')\n                \n                # If a title tag is found, return its text\n                if title_tag:\n                    return title_tag.text\n                else:\n                    return \"No title tag found in the webpage.\"\n            else:\n                return f\"Unable to fetch the content of the URL: {url}\"\n        except requests.RequestException:\n            return f\"Unable to fetch the content of the URL: {url}\"\n    else:\n        return \"No valid URL found in the provided string.\"\n```"}
{"task_id": "BigCodeBench/1129", "solution": "```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Extract URL associated with the unknown_key\n    if unknown_key in data:\n        url = data[unknown_key]\n    else:\n        raise KeyError(f\"{unknown_key} not found in provided JSON data.\")\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        file_content = response.content\n    else:\n        raise Exception(f\"Failed to download file from {url}. Status code: {response.status_code}\")\n    \n    # Generate the timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine save directory\n    if save_dir is not None:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        filepath = os.path.join(save_dir, filename)\n    else:\n        # If save_dir is not provided, use the current working directory.\n        filepath = os.path.join(os.getcwd(), filename)\n    \n    # Save the file\n    with open(filepath, 'wb') as file:\n        file.write(file_content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(filepath)\n```"}
{"task_id": "BigCodeBench/1137", "solution": "```python\n# Import necessary libraries\nimport bs4\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    # Determine if the given URL is a website or a local file path\n    if url.startswith('http'):\n        # Fetch the content from the URL\n        response = requests.get(url)\n        content = response.text\n    else:\n        # Open and read the content of the local file\n        with open(url, 'r', encoding='utf-8') as file:\n            content = file.read()\n    \n    # Parse the content using BeautifulSoup\n    soup = BeautifulSoup(content, 'html.parser')\n    # Convert the entire content to text\n    text = soup.get_text()\n    \n    # Use regular expression to find phone numbers in the text\n    # The pattern here is quite generic to match different phone number formats\n    phone_pattern = r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n    phone_numbers = re.findall(phone_pattern, text)\n    \n    # Remove duplicates by converting list to a set then back to a list\n    unique_phone_numbers = list(set(phone_numbers))\n    \n    # Save the found phone numbers into a JSON file\n    with open(output_path, 'w', encoding='utf-8') as json_file:\n        json.dump(unique_phone_numbers, json_file, indent=4)\n    \n    # Return the list of unique phone numbers\n    return unique_phone_numbers\n```"}
